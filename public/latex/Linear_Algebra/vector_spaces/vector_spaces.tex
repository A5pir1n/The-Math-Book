\documentclass[main.tex]{subfiles}
\begin{document}
\section{Vector Spaces}

Loosely speaking, linear algebra is a branch of mathematics which studies common properties of a algebraic system. This system consists of a set and some notion of an operation, that is the ``linear combination" of the elements in the set. 

So called vector space turned out to one of the most useful abstraction of this type of algebraic system. 
\begin{definition}
    A vector space (or linear space) consists of the following:
    \begin{enumerate}
        \item a field $F$ of scalars;
        \item a set $V$ of objects, called vectors;
        \item a rule (or operation), called vector addition, which associates with each pair of vectors $\alpha, \beta$ in $V$ a vector $\alpha+\beta$ in $V$, called the sum of $\alpha$ and $\beta$, in such a way that
        \begin{enumerate}[a.]
            \item addition is commutative, $\alpha+\beta=\beta+\alpha$;
            \item addition is associative, $\alpha+(\beta+\gamma)=(\alpha+\beta)+\gamma$;
            \item there is a unique vector 0 in $V$, called the zero vector, such that $\alpha+0=\alpha$ for all $\alpha$ in $V$;
            \item for each vector $\alpha$ in $V$ there is a unique vector $-\alpha$ in $V$ such that $\alpha+(-\alpha)=0$;
        \end{enumerate}
        \item a rule (or operation), called scalar multiplication, which associates with each scalar $c$ in $\mathrm{F}$ and vector $\alpha$ in $V$ a vector $c \alpha$ in $V$, called the product of $c$ and $\alpha$, in such a way that
        \begin{enumerate}[a.]
            \item $1 \alpha=\alpha$ for every $\alpha$ in $V$;
            \item $\left(c_1 c_2\right) \alpha=c_1\left(c_2 \alpha\right)$;
            \item $c(\alpha+\beta)=c \alpha+c \beta$;
            \item $\left(c_1+c_2\right) \alpha=c_1 \alpha+c_2 \alpha$.
        \end{enumerate}
    \end{enumerate}
\end{definition}
When we associate a vector space $V$ with a field $F$, we say $V$ is a \textbf{vector space over the field} $F$. It is important note that the vectors in a vector space is much more general than the vectors we encounter in elementary algebra, which is just a special case in our more general vector space. We call it the \textbf{$n$-tuple space} and we show more examples of other vector spaces below. 

\begin{example}\label{n-tuple_space}
\textbf{The $n$-tuple space, $F^n$}. Let $F$ be any field, and let $V$ be the set of all $n$-tuples $\alpha=\left(x_1, x_2, \ldots, x_n\right)$ of scalars $x_i$ in $F$. If $\beta=$ $\left(y_1, y_2, \ldots, y_n\right)$ with $y_i$ in $F$, the sum of $\alpha$ and $\beta$ is defined by
$$\quad \alpha+\beta=\left(x_1+y_1, x_2+y_2, \ldots, x_n+y_n\right).$$
The product of a scalar $c$ and vector $\alpha$ is defined by
$$\quad c \alpha=\left(c x_1, c x_2, \ldots, c x_n\right).$$
The fact that this vector addition and scalar multiplication satisfy conditions (3) and (4) is easy to verify, using the similar properties of addition and multiplication of elements of $F$.
\end{example}
\begin{verification}
    We only need to verify 3 and 4 of the definition of the vector space when we already have a field $F$ and a set $F^n$ of vector objects. Firstly, let $\gamma = (z_1, z_2, \cdots, z_n)$. For addition, we have 
    \begin{enumerate}[(a)]
        \item Since addition in $F$ is commutative: \begin{align*}\alpha+\beta &=\left(x_1+y_1, x_2+y_2, \ldots, x_n+y_n\right)\\
        &=\left(y_1+x_1,y_2+x_2, \ldots, y_n+x_n\right)\\
        &=\beta + \alpha.\end{align*}
        \item Since addition in $F$ is associative:  \begin{align*}\alpha+(\beta+\gamma) &=\left(x_1+(y_1+z_1), x_2+(y_2+z_2), \ldots, x_n+(y_n+z_n)\right)\\
        &=\left((x_1+y_1)+z_1, (x_2+y_2)+z_2, \ldots, (x_n+y_n)+z_n\right)\\
        &=(\alpha+\beta)+\gamma.\end{align*}
        
        \item We must show there is a unique vector $\mathbf{0}$ in $V$ such that $\alpha+0=\alpha$ for all $\alpha$ in $V$. Consider $\left(0, \ldots, 0\right)$ the vector of all 0 's of length $n$. Then we have
        $$
        \left(0, \ldots, 0\right)+\left(x_1, \ldots, x_n\right)=\left(0_F+x_1, \ldots, 0_F+x_n\right)=\left(x_1, \ldots, x_n\right)
        $$
        since $0+x=x$ for all $x \in F$. Thus we can have $\mathbf{0} =  \left(0, \ldots, 0\right)+\left(x_1, \ldots, x_n\right)$. To show this vector is unique with respect to this property, suppose $\mathbf{0}'=\left(x_1, \ldots, x_n\right)$ also satisfies the property that $\mathbf{0}'+\alpha=\alpha$ for all $\alpha$ in $V$. Notice
        $$
        \left(x_1, \ldots, x_n\right)=\left(x_1+0, \ldots, x_n+0\right)=\left(x_1, \ldots, x_n\right)+\left(0, \ldots, 0\right)
        $$
        but by definition of $\mathbf{0}'$, if we let $\alpha=\left(0, \ldots, 0\right)$, this equals $\left(0, \ldots, 0\right)$. Thus $\left(x_1, \ldots, x_n\right)=\left(0, \ldots, 0\right) = \mathbf{0}$. Thus $\mathbf{0}=\mathbf{0}'$ and the zero element is unique.
    \end{enumerate}
\end{verification}
\begin{example}
\textbf{The space of $m \times n$ matrices}, $F^{m \times n}$. Let $F$ be any field and let $m$ and $n$ be positive integers. Let $F^{m \times n}$ be the set of all $m \times n$ matrices over the field $F$. The sum of two vectors $A$ and $B$ in $F^{m \times n}$ is defined by
$$
(A+B)_{i j}=A_{i j}+B_{i j} .
$$
The product of a scalar $c$ and the matrix $A$ is defined by $(2-4)$
$$
(c A)_{i j}=c A_{i j} .
$$
\end{example}

\begin{example}\label{space_of_functions_from_set_to_field}
\textbf{The space of functions from a set to a field}. Let $F$ be any field and let $S$ be any non-empty set. Let $V$ be the set of all functions from the set $S$ into $F$. The sum of two vectors $f$ and $g$ in $V$ is the vector $f+g$, i.e., the function from $S$ into $F$, defined by
$$\quad(f+g)(s)=f(s)+g(s).$$
The product of the scalar $c$ and the function $f$ is the function $c f$ defined by
$$
(c f)(s)=c f(s) .
$$
The preceding examples are special cases of this one. For an $n$-tuple of elements of $F$ may be regarded as a function from the set $S$ of integers $1, \ldots, n$ into $F$. Similarly, an $m \times n$ matrix over the field $F$ is a function from the set $S$ of pairs of integers, $(i, j), 1 \leq i \leq m, 1 \leq j \leq n$, into the field $F$. Verification:

Note in this case for condition 1 and 2, our field $F$ would be the same field $F$ where we are taking our set $S$ to and the vector $V$ will be the functions. For condition 3, vector addition:
\begin{enumerate}
    \item Since addition in $F$ is commutative,
    $$
    f(s)+g(s)=g(s)+f(s)
    $$
    for each $s$ in $S$, so the functions $f+g$ and $g+f$ are identical.
    \item Since addition in $F$ is associative,
    $$
    f(s)+[g(s)+h(s)]=[f(s)+g(s)]+h(s)
    $$
    for each $s$, so $f+(g+h)$ is the same function as $(f+g)+h$.
    \item The unique zero vector is the zero function which assigns to each element of $S$ the scalar 0 in $F$.
    \item For each $f$ in $V,(-f)$ is the function which is given by
    $$
    (-f)(s)=-f(s)
    $$
\end{enumerate}
Now for condition 4, vector multiplication:
\begin{enumerate}
    \item Since 1 exists for field $F$, we have $$
    (1f)(s) = 1f(s) = f(s)$$
    \item Since multiplication in $F$ is associative, $$(c_1c_2)f(s) = c_1(c_2f(s))$$ for each $s$. 
    \item Since multiplication distributive over addition in $F$, $$c(f + g)(s) = c(f(s) + g(s)) = cf(s) + cg(s)$$ for each $s$. So $c(f + g)$ is equivalent to $cf + cg$. 
    \item Similarly, $$(c_1 + c_2) f(s) = c_1f(s) + c_2f(s)$$ for every $s$, so $(c_1 + c_2)f$ is equivalent to $c_1f + c_2f$. 
\end{enumerate}
\end{example}

\begin{example}\label{example_4}
\textbf{The space of polynomial functions over a field $F$}. Let $F$ be a field and let $V$ be the set of all functions $f$ from $F$ into $F$ which have a rule of the form
$$
f(x)=c_0+c_1 x+\cdots+c_n x^n
$$
where $c_0, c_1, \ldots, c_n$ are fixed scalars in $F$ (independent of $x$ ). A function of this type is called a polynomial function on $F$. Let addition and scalar multiplication be defined as in Example \ref{space_of_functions_from_set_to_field}. The sum of two vectors $f$ and $g$ in $V$ is defined by
$$\quad(f+g)(s)=f(s)+g(s).$$
The product of the scalar $c$ and the function $f$ is the function $c f$ defined by
$$
(c f)(s)=c f(s) .
$$
One must observe here that if $f$ and $g$ are polynomial functions and $c$ is in $F$, then $f+g$ and $c f$ are again polynomial functions.
\end{example}

\begin{example}
The field $\mathbb{C}$ of complex numbers may be regarded as a vector space over the field $R$ of real numbers. More generally, let $F$ be the field of real numbers and let $V$ be the set of $n$-tuples $\alpha=\left(x_1, \ldots, x_n\right)$ where $x_1, \ldots, x_n$ are complex numbers. Define addition of vectors and scalar multiplication as in Example \ref{n-tuple_space}. If $\beta=$ $\left(y_1, y_2, \ldots, y_n\right)$ with $y_i$ in $F$, the sum of $\alpha$ and $\beta$ is defined by
$$\quad \alpha+\beta=\left(x_1+y_1, x_2+y_2, \ldots, x_n+y_n\right).$$
The product of a scalar $c$ and vector $\alpha$ is defined by
$$\quad c \alpha=\left(c x_1, c x_2, \ldots, c x_n\right).$$ In this way we obtain a vector space over the field $R$ which is quite different from the space $\mathbb{C}^n$ and the space $\mathbb{R}^n$.
\end{example}

\subsection*{Properties of vector spaces}
There are a few simple facts which follow almost immediately from the definition of a vector space, and we proceed to derive these. 
\begin{enumerate}
    \item If $c$ is a scalar and 0 is the zero vector, then by $3(\mathrm{c})$ and $4(\mathrm{c})$
$$
c 0=c(0+0)=c 0+c 0 .
$$
Adding $-(c 0)$ and using $3(\mathrm{~d})$, we obtain
\begin{equation}\label{property:zero_vector}
c 0=0 .
\end{equation}
\item Similarly, for the scalar 0 and any vector $\alpha$ we find that
$$\quad 0 \alpha=0 .
$$
\item If $c$ is a non-zero scalar and $\alpha$ is a vector such that $c \alpha=0$, then by \ref{property:zero_vector}, $c^{-1}(c \alpha)=0$. But
$$
c^{-1}(c \alpha)=\left(c^{-1} c\right) \alpha=1 \alpha=\alpha
$$
hence, $\alpha=0$. Thus we see that if $c$ is a scalar and $\alpha$ a vector such that $c \alpha=0$, then either $c$ is the zero scalar or $\alpha$ is the zero vector.

\item If $\alpha$ is any vector in $V$, then
$$
0=0 \alpha=(1-1) \alpha=1 \alpha+(-1) \alpha=\alpha+(-1) \alpha
$$
from which it follows that
$$
\quad(-1) \alpha=-\alpha \text {. }
$$

\item Finally, the associative and commutative properties of vector addition imply that a sum involving a number of vectors is independent of the way in which these vectors are combined and associated. For example, if $\alpha_1, \alpha_2, \alpha_3, \alpha_4$ are vectors in $V$, then
$$
\left(\alpha_1+\alpha_2\right)+\left(\alpha_3+\alpha_4\right)=\left[\alpha_2+\left(\alpha_1+\alpha_3\right)\right]+\alpha_4
$$
and such a sum may be written without confusion as
$$
\alpha_1+\alpha_2+\alpha_3+\alpha_4 .
$$
\end{enumerate}

\section{Linear Combination}
\begin{definition}
 A vector $\beta$ in $\mathrm{V}$ is said to be a \textbf{linear combination} of the vectors $\alpha_1, \ldots, \alpha_n$ in $\mathrm{V}$ provided there exist scalars $\mathrm{c}_1, \ldots, \mathrm{c}_{\mathrm{n}}$ in $\mathrm{F}$ such that
$$
\begin{aligned}
\beta & =\mathrm{c}_1 \alpha_1+\cdots+\mathrm{c}_{\mathrm{n}} \alpha_{\mathrm{n}} \\
& =\sum_{\mathrm{i}=1}^{\mathrm{n}} \mathrm{c}_{\mathrm{i}} \alpha_{\mathrm{i}} .
\end{aligned}
$$
\end{definition}

Other extensions of the associative property of vector addition and the distributive properties 4(c) and 4(d) of scalar multiplication apply to linear combinations:
$$
\begin{aligned}
\sum_{i=1}^n c_i \alpha_i+\sum_{i=1}^n d_i \alpha_i & =\sum_{i=1}^n\left(c_i+d_i\right) \alpha_i \\
c \sum_{i=1}^n c_i \alpha_i & =\sum_{i=1}^n\left(c c_i\right) \alpha_{i \cdot}
\end{aligned}
$$
\section{Basis}
\subfile{../n-tuple_space_basis}

\section{Coordinates}
One of the useful features of a basis $\mathcal{B}$ in an $n$-dimensional space $V$ is that it essentially enables one to introduce coordinates in $V$ analogous to the ``natural coordinates'' $x_i$ of a vector $\alpha=\left(x_1, \ldots, x_n\right)$ in the space $F^n$. In this scheme, the coordinates of a vector $\alpha$ in $V$ relative to the basis $Q$ will be the scalars which serve to express $\alpha$ as a linear combination of the vectors in the basis. Thus, we should like to regard the natural coordinates of a vector $\alpha$ in $F^n$ as being defined by $\alpha$ and the standard basis for $F^n$; however, in adopting this point of view we must exercise a certain amount of care. If
$$
\alpha=\left(x_1, \ldots, x_n\right)=\Sigma x_i \epsilon_i
$$
and $Q$ is the standard basis for $F^n$, just how are the coordinates of $\alpha$ determined by $B$ and $\alpha$ ? One way to phrase the answer is this. A given vector $\alpha$ has a unique expression as a linear combination of the standard basis vectors, and the $i$ th coordinate $x_i$ of $\alpha$ is the coefficient of $\epsilon_i$ in this expression. From this point of view we are able to say which is the $i$ th coordinate

\section{Norms and normed vector spaces}

\begin{definition}
For a vector space $V$ over field $F$ with element $x \in V$, a \textbf{norm} of $V$ is a function $\norm{x}: V \rightarrow [0,\infty)$ satisfying: 
\begin{enumerate}
    \item $\norm{x} = 0$ if and only if $x = 0$.
    \item (Triangle inequality.) $\norm{x + y} \leq \norm{x} + \norm{y}$ for every $x, y \in V$. 
    \item $\norm{cx} = \abs{c} \cdot \norm{x}$ for every $c \in F$ and $x \in V$. 
\end{enumerate}
\end{definition}


\begin{definition}
A pair $(V, \norm{\,\cdot \,})$ where $V$ is a vector space and $\norm{\, \cdot \,}$ is a norm on $V$ is a \textbf{normed vector space}.
\end{definition}


\subsection{Operator Norms}
The operator norms allows us to measure the 
``size'' of a norm in terms of how much the operator is capable of ``stretching" an argument.
\begin{definition}
For a normed vectors spaces $X = (U,\norm{\, \cdot \,}_U)$ and $Y = (V,\norm{\, \cdot \,}_V)$, we set
\begin{equation}
    \mathcal{L}(X,Y) = \{ T: X \rightarrow Y | T  \text{ is linear and continuous} \},
\end{equation}
and put $\mathcal{L}(X,X) = \mathcal{L}(X)$. The dual space of $\mathcal{L}(X,\mathbb{F})$ of $X$ is denoted by $X^\star$. An element $x^\star \in X^\star$ is called a \textbf{linear functional} on $X$, and one often write $\langle x,x^\star \rangle$ instead of $x^\star (x)$.

For $T \in \mathcal{L}(X,Y)$ the \textbf{operator norm} is defined as 
\begin{align}
    \norm{T}_{\mathcal{L}(X,Y)} = \norm{T}_{op}
    &= \inf\{(c \leq 0 |\, \forall x \in X \text{ we have } \norm{Tx}_Y \leq c \norm{x}_X \} < \infty \\
    &= \sup\left\{ \frac{\norm{Tx}_Y}{\norm{x}_X } \, | \, x \in X \right\}.
\end{align}
\end{definition}

\begin{prop}
TO be filled
\end{prop}

\subsubsection{Applications}
\paragraph{Approximation Theory}
This is a very useful notion for approximation theory in the situation where our approximation to the element $f$ can be described as applying an operator $T$ to $f$ to obtain the approximation $T(f)$.

\subsection{Continuity and differentiablility}
\begin{definition}
Let $X = (U,\norm{\, \cdot \,}_U)$ and $Y = (V,\norm{\, \cdot \,}_V)$ be vector spaces. Given a $\Omega \subseteq X$ and a function $T \, : \, \Omega \rightarrow Y$, we say that $T$ is \textbf{continuous at a point $x_0 \in \Omega$} if for every $\varepsilon > 0$ there exists $\delta > 0$ such that a $x \in \Omega$ with $\norm{x - x_0}_U \leq \delta$ implies that $\norm{T(x) - T(x_0)}_V \leq \varepsilon$. We say that $T$ is \textbf{continuous} if it is continuous at every point of $\Omega$. 
We say that a function $T$ is \textbf{differentiable} at $x \in \Omega$ if there exists a linear map $D(T(x)) : U \rightarrow V$ such that 
\begin{equation}
    \lim_{x \rightarrow x_0} \frac{\norm{T(x) - T(x_0) - D(T(x))(x - x_0)}_V}{\norm{x - x_0}_U} = 0, 
\end{equation}
and say that $T$ is differentiable if it is differentiable at every point of $\Omega$ in which case $DT$ defines a function $DT:\Omega \rightarrow \mathbb{L}(U, V)$. 
\end{definition}
\end{document}