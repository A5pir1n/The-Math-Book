<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">

<head>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <meta charset="utf-8" />
    <meta name="generator" content="pandoc" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
    <title>bases and dimention</title>
    <link rel="stylesheet" href="../../../css/base_and_d.css">
    <link rel="icon" href="img/icon.ico" type="image/x-ico">
    <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>

<body>
    <h1 id="basis-and-dimension">Basis and Dimension</h1>
    <p>Dimensions comes naturally when speaking of “vectors" as arrows that
        live in our geometric space. The zero dimension vector will just be
        thought of as a point with no direction, one dimension vector an arrow
        on a line, two dimension vector on a plane, three dimension an arrow in
        the space that we live in.</p>
    <p>To use the notion of dimension for our generalized vector space, we
        want to observe how vectors behave in the space. If you just have two
        vectors sitting on the one-dimensional line, no matter how you add them
        or scale them, they stay on that line. This is the notion that they are
        linearly dependent. But once the second vector deviates away from the
        line, in other words, once it has some component perpendicular to the
        line, it “opens up" another dimension and span into the 2 dimensional
        space. Then you get two linearly independent vectors. Now we generalize
        this concept in vector space:</p>
    <div class="definition">
        <p>Let <span class="math inline"><em>V</em></span> be a vector space
            over <span class="math inline">F</span>. A subset <span class="math inline"><em>S</em></span> of <span
                class="math inline"><em>V</em></span> consisting of distinct vectors
            <span
                class="math inline"><em>α</em><sub>1</sub>, <em>α</em><sub>2</sub>, …, <em>α</em><sub><em>n</em></sub></span>
            is said to be <strong>linearly dependent</strong> (or simply, dependent)
            if there exist scalars <span
                class="math inline"><em>c</em><sub>1</sub>, <em>c</em><sub>2</sub>, …, <em>c</em><sub><em>n</em></sub></span>
            in <span class="math inline"><em>F</em></span>, not all of which are 0 ,
            such that
        </p>
        <p><span
                class="math display"><em>c</em><sub>1</sub><em>α</em><sub>1</sub> + <em>c</em><sub>2</sub><em>α</em><sub>2</sub> + ⋯ + <em>c</em><sub><em>n</em></sub><em>α</em><sub><em>n</em></sub> = <strong>0</strong>.</span>
        </p>
        <p>A set which is not linearly dependent is called <strong>linearly
                independent</strong>. If the set <span class="math inline"><em>S</em></span> contains only finitely many
            vectors <span
                class="math inline"><em>α</em><sub>1</sub>, <em>α</em><sub>2</sub>, …, <em>α</em><sub><em>n</em></sub></span>,
            we sometimes say that <span
                class="math inline"><em>α</em><sub>1</sub>, <em>α</em><sub>2</sub>, …, <em>α</em><sub><em>n</em></sub></span>
            are linearly dependent (or independent).</p>
    </div>
    <p>The following are easy consequences of the definition.</p>
    <ol type="1">
        <li>
            <p><span id="consequence:subset_of_linearly_independent_set"
                    label="consequence:subset_of_linearly_independent_set"></span> Any
                subset of a linearly independent set is linearly independent.</p>
        </li>
        <li>
            <p>Any set which contains the 0 vector is linearly dependent; for
                <span class="math inline">1 ⋅ 0 = 0</span>.
            </p>
        </li>
        <li>
            <p>A set <span class="math inline"><em>S</em></span> of vectors is
                linearly independent if and only if each finite subset of <span class="math inline"><em>S</em></span> is
                linearly independent, i.e., if
                and only if for any distinct vectors <span
                    class="math inline"><em>α</em><sub>1</sub>, …, <em>α</em><sub><em>n</em></sub></span>
                of <span
                    class="math inline"><em>S</em>, <em>c</em><sub>1</sub><em>α</em><sub>1</sub> + ⋯ + <em>c</em><sub><em>n</em></sub><em>α</em><sub><em>n</em></sub> = 0</span>
                implies each <span class="math inline"><em>c</em><sub><em>i</em></sub> = <strong>0</strong></span>.
                Another way to think about it is that each vector is linearly
                independent with each other.</p>
        </li>
    </ol>
    <p>We can see how this definition fits our vector as arrows. If we have
        one vector in one dimensional line, it will have to be scaled by scalar
        0 to reach become the zero vector. So, the vector is by itself, linearly
        independent. Once we add another vector, we can always scale them to
        equal lengths and opposite direction so they add up to the zero vector.
        They are therefore linearly dependent.</p>
    <div class="definition">
        <p>Let <span class="math inline"><em>V</em></span> be a vector space. A
            basis for <span class="math inline"><em>V</em></span> is a set of
            linearly independent vectors in <span class="math inline"><em>V</em></span> that spans <span
                class="math inline"><em>V</em></span>. i.e. all vectors in <span class="math inline"><em>V</em></span>
            can be written as a linear
            combination of the elements of the basis. The space <span class="math inline"><em>V</em></span> is
            <strong>finite
                dimensional</strong> if it has a finite basis.
        </p>
    </div>
    <div class="example">
        <p>Let <span class="math inline"><em>F</em></span> be a subfield of the
            complex numbers. In <span class="math inline"><em>F</em><sup>3</sup></span> the vectors</p>
        <p><span class="math display">$$\begin{aligned}
                \alpha_{1} &amp; =(1,0,2) \\
                \alpha_{2} &amp; =(1,1,-1) \\
                \alpha_{3} &amp; =(2,-2,0) \\
                \alpha_{4} &amp; =(1,3,3)
                \end{aligned}$$</span></p>
        <p>are linearly dependent, since</p>
        <p><span
                class="math display">2<em>α</em><sub>1</sub> + <em>α</em><sub>2</sub> − <em>α</em><sub>3</sub> − <em>α</em><sub>4</sub> = 0.</span>
        </p>
        <p>Think back to arrows in three dimension, if we choose a coordinate
            system. We will need three coordinates to specify the direction of the
            arrow. This nicely corresponds to three 3-tuples in <span class="math inline"><em>F</em><sup>3</sup></span>.
            Now when we try to
            add another 3-tuple, it will have to fall into the span of the three
            3-tuples we already have. It is therefore linearly dependent with the
            original three 3-tuples. To make it linearly independent, we will have
            to make all the n-tuples 4-tuples, and the added 4-tuple needs to have a
            component perpendicular to the three dimensional space we have. We now
            have an intuition from geometry that if we have more element in the
            basis then the dimension of space, we necessarily have a linearly
            dependent basis.</p>
        <p>Now obviously: <span class="math display">$$\begin{aligned}
                &amp; \epsilon_{1}=(1,0,0) \\
                &amp; \epsilon_{2}=(0,1,0) \\
                &amp; \epsilon_{3}=(0,0,1)
                \end{aligned}$$</span></p>
        <p>are linearly independent: to linearly combine them into <span class="math inline"><strong>0</strong></span>,
            they each have to be
            scaled by 0. In fact, {<span class="math inline"><em>ϵ</em><sub>1</sub></span>, <span
                class="math inline"><em>ϵ</em><sub>2</sub></span>} is linearly
            independent by consequence <a href="#consequence:subset_of_linearly_independent_set"
                data-reference-type="ref"
                data-reference="consequence:subset_of_linearly_independent_set">[consequence:subset_of_linearly_independent_set]</a>
            above.</p>
    </div>
    <div class="example">
        <p>Let <span class="math inline"><em>P</em></span> be an invertible
            <span class="math inline"><em>n</em> × <em>n</em></span> matrix with
            entries in the field <span class="math inline"><em>F</em></span>. Let
            <span class="math inline"><em>P</em><sub>1</sub>, …, <em>P</em><sub><em>n</em></sub></span>
            be the columns of <span class="math inline"><em>P</em></span>, <span class="math display">$$\begin{bmatrix}
                \begin{bmatrix}\\\\P_1\\\\\\ \end{bmatrix}
                \begin{bmatrix}\\\\P_2\\\\\\ \end{bmatrix} \cdots
                \begin{bmatrix}\\\\P_n\\\\\\ \end{bmatrix} \end{bmatrix}$$</span> They
            form a basis for the space of column matrices, <span
                class="math inline"><em>F</em><sup><em>n</em> × 1</sup></span>. <span
                class="math display">$$\begin{bmatrix}
                x_{1} \\
                x_{2} \\
                \vdots \\
                x_{n} \\
                \end{bmatrix}$$</span>To see that they indeed form a basis, we need them
            to be linearly independent and span the space <span
                class="math inline"><em>F</em><sup><em>n</em> × 1</sup></span>. We use
            the properties of an inverse matrix here, first we write linear
            combination of column vectors <span class="math inline"><em>P</em><sub><em>k</em></sub></span> as the matrix
            <span class="math inline"><em>P</em></span> multiply by a column matrix
            <span class="math inline"><em>X</em></span> is a column matrix,
        </p>
        <p><span
                class="math display"><em>x</em><sub>1</sub><em>P</em><sub>1</sub> + ⋯ + <em>x</em><sub><em>n</em></sub><em>P</em><sub><em>n</em></sub> = <em>P</em><em>X</em>.</span>
        </p>
        <p>But <span class="math inline"><em>P</em></span> is an invertible
            matrix, so <span class="math inline"><em>P</em><em>X</em> = 0</span> has
            only the trivial solution <span class="math inline"><em>X</em> = 0</span>, and <span
                class="math inline">{<em>P</em><sub>1</sub>,…,<em>P</em><sub><em>n</em></sub>}</span>
            is a linearly independent set. To see that it spans <span
                class="math inline"><em>F</em><sub><em>n</em> × 1</sub></span>, if let
            <span class="math inline"><em>Y</em></span> be any column matrix, we
            will want to write it as a linear combination of the set <span
                class="math inline">{<em>P</em><sub>1</sub>,…,<em>P</em><sub><em>n</em></sub>}</span>.
            But <span class="math inline"><em>P</em></span> has a inverse, so we can
            always get another column matrix <span class="math inline"><em>X</em></span> such that <span
                class="math inline"><em>X</em> = <em>P</em><sup>−1</sup><em>Y</em></span>.
            Then <span class="math inline"><em>Y</em> = <em>P</em><em>X</em></span>,
            that is,
        </p>
        <p><span
                class="math display"><em>Y</em> = <em>x</em><sub>1</sub><em>P</em><sub>1</sub> + ⋯ + <em>x</em><sub><em>n</em></sub><em>P</em><sub><em>n</em></sub>.</span>
        </p>
        <p>So <span class="math inline">{<em>P</em><sub>1</sub>,…,<em>P</em><sub><em>n</em></sub>}</span>
            is a basis for <span class="math inline"><em>F</em><sup><em>n</em> × 1</sup></span>. This
            gives us the <a href="#cols_rows_independent" data-reference-type="ref"
                data-reference="cols_rows_independent">[cols_rows_independent]</a> and
            <a href="#cols_rows_span" data-reference-type="ref" data-reference="cols_rows_span">[cols_rows_span]</a> of
            our equivalence
            theorem.
        </p>
    </div>
    <div class="example">
        <p><span id="bases_and_dimension:basis_for_solution_space_of_a_system_of_linear_equations"
                label="bases_and_dimension:basis_for_solution_space_of_a_system_of_linear_equations"></span>
            <strong>Basis for solution space of a system of linear
                equations</strong> Back to solving <span class="math inline"><em>m</em></span> linear equations with
            <span class="math inline"><em>n</em></span> unknowns, we hope to find a basis
            for the solution space so we can express the solution as a linear
            combination of the bases elements. Let <span class="math inline"><em>A</em></span> be an <span
                class="math inline"><em>m</em> × <em>n</em></span> matrix and let <span
                class="math inline"><em>S</em></span> be the solution space for the
            homogeneous system <span class="math inline"><em>A</em><em>X</em> = 0</span> (Example 7). Let
            <span class="math inline"><em>R</em></span> be a rowreduced echelon
            matrix which is row-equivalent to <span class="math inline"><em>A</em></span>. Then <span
                class="math inline"><em>S</em></span> is also the solution space for the
            system <span class="math inline"><em>R</em><em>X</em> = 0</span>. If
            <span class="math inline"><em>R</em></span> has <span class="math inline"><em>r</em></span> non-zero rows,
            then the system of
            equations <span class="math inline"><em>R</em><em>X</em> = 0</span>
            simply expresses <span class="math inline"><em>r</em></span> of the
            unknowns <span class="math inline"><em>x</em><sub>1</sub>, …, <em>x</em><sub><em>n</em></sub></span>
            in terms of the remaining <span class="math inline">(<em>n</em>−<em>r</em>)</span> unknowns <span
                class="math inline"><em>x</em><sub><em>j</em></sub></span>. Suppose that
            the leading non-zero entries of the non-zero rows occur in columns <span
                class="math inline"><em>k</em><sub>1</sub>, …, <em>k</em><sub><em>r</em></sub></span>.
            Let <span class="math inline"><em>J</em></span> be the set consisting of
            the <span class="math inline"><em>n</em> − <em>r</em></span> indices
            different from <span class="math inline"><em>k</em><sub>1</sub>, …, <em>k</em><sub><em>r</em></sub></span>
            :
        </p>
        <p><span
                class="math display"><em>J</em> = {1, …, <em>n</em>} − {<em>k</em><sub>1</sub>,…,<em>k</em><sub><em>r</em></sub>}.</span>
        </p>
        <p>The system <span class="math inline"><em>R</em><em>X</em> = 0</span>
            has the form</p>
        <p><span class="math display">$$\begin{aligned}
                &amp; x_{k 1}+\sum_{J} c_{1 j} x_{j}=0 \\
                &amp; \vdots \begin{array}{cc}\vdots &amp; \vdots \\x_{k_{r}}+\sum_{J}
                c_{r j} x_{j}=0\end{array}
                \end{aligned}$$</span></p>
        <p>where the <span class="math inline"><em>c</em><sub><em>i</em><em>j</em></sub></span> are
            certain scalars. All solutions are obtained by assigning (arbitrary)
            values to those <span class="math inline"><em>x</em><sub><em>j</em></sub></span> ’s with <span
                class="math inline"><em>j</em></span> in <span class="math inline"><em>J</em></span> and computing the
            corresponding
            values of <span
                class="math inline"><em>x</em><sub><em>k</em><sub>1</sub></sub>, …, <em>x</em><sub><em>k</em><sub><em>r</em></sub></sub></span>.
            For each <span class="math inline"><em>j</em></span> in <span class="math inline"><em>J</em></span>, let
            <span class="math inline"><em>E</em><sub><em>j</em></sub></span> be the
            solution obtained by setting <span class="math inline"><em>x</em><sub><em>j</em></sub> = 1</span> and <span
                class="math inline"><em>x</em><sub><em>i</em></sub> = 0</span> for all
            other <span class="math inline"><em>i</em></span> in <span class="math inline"><em>J</em></span>. We assert
            that the <span class="math inline">(<em>n</em>−<em>r</em>)</span> vectors <span
                class="math inline"><em>E</em><sub><em>j</em></sub>, <em>j</em></span>
            in <span class="math inline"><em>J</em></span>, form a basis for the
            solution space.
        </p>
        <p>Since the column matrix <span class="math inline"><em>E</em><sub><em>j</em></sub></span> has a 1 in
            row <span class="math inline"><em>j</em></span> and zeros in the rows
            indexed by other elements of <span class="math inline"><em>J</em></span>, the reasoning of Example 13 shows
            us that the set of these vectors is linearly independent. That set spans
            the solution space, for this reason. If the column matrix <span class="math inline"><em>T</em></span>, with
            entries <span class="math inline"><em>t</em><sub>1</sub>, …, <em>t</em><sub><em>n</em></sub></span>,
            is in the solution space, the matrix</p>
            <div>
                \[ N=\sum_{J} t_{j} E_{j} \tag{1} \]
            </div>
        <p>is also in the solution space and is a solution such that <span
                class="math inline"><em>x</em><sub><em>j</em></sub> = <em>t</em><sub><em>j</em></sub></span>
            for each <span class="math inline"><em>j</em></span> in <span class="math inline"><em>J</em> </span>. The
            solution with that property
            is unique; hence, <span class="math inline"><em>N</em> = <em>T</em></span> and <span
                class="math inline"><em>T</em></span> is in the span of the vectors
            <span class="math inline"><em>E</em><sub><em>j</em></sub></span>.
        </p>
    </div>
</body>

</html>