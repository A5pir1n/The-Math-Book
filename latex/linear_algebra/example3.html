<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">

<head>
  <script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-MML-AM_CHTML">
  </script>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Hongyu" />
  <meta name="dcterms.date" content="2022-12-22" />
  <title>Linear Algebra</title>
  <style>
    html {
      line-height: 1.5;
      font-family: Georgia, serif;
      font-size: 20px;
      color: #1a1a1a;
      background-color: #fdfdfd;
    }

    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }

    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 1em;
      }

      h1 {
        font-size: 1.8em;
      }
    }

    @media print {
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }

      p,
      h2,
      h3 {
        orphans: 3;
        widows: 3;
      }

      h2,
      h3,
      h4 {
        page-break-after: avoid;
      }
    }

    p {
      margin: 1em 0;
    }

    a {
      color: #1a1a1a;
    }

    a:visited {
      color: #1a1a1a;
    }

    img {
      max-width: 100%;
    }

    h1,
    h2,
    h3,
    h4,
    h5,
    h6 {
      margin-top: 1.4em;
    }

    h5,
    h6 {
      font-size: 1em;
      font-style: italic;
    }

    h6 {
      font-weight: normal;
    }

    ol,
    ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }

    li>ol,
    li>ul {
      margin-top: 0;
    }

    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }

    code {
      font-family: Menlo, Monaco, 'Lucida Console', Consolas, monospace;
      font-size: 85%;
      margin: 0;
    }

    pre {
      margin: 1em 0;
      overflow: auto;
    }

    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }

    .sourceCode {
      background-color: transparent;
      overflow: visible;
    }

    hr {
      background-color: #1a1a1a;
      border: none;
      height: 1px;
      margin: 1em 0;
    }

    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }

    table caption {
      margin-bottom: 0.75em;
    }

    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }

    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }

    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }

    header {
      margin-bottom: 4em;
      text-align: center;
    }

    #TOC li {
      list-style: none;
    }

    #TOC ul {
      padding-left: 1.3em;
    }

    #TOC>ul {
      padding-left: 0;
    }

    #TOC a:not(:hover) {
      text-decoration: none;
    }

    code {
      white-space: pre-wrap;
    }

    span.smallcaps {
      font-variant: small-caps;
    }

    div.columns {
      display: flex;
      gap: min(4vw, 1.5em);
    }

    div.column {
      flex: auto;
      overflow-x: auto;
    }

    div.hanging-indent {
      margin-left: 1.5em;
      text-indent: -1.5em;
    }

    ul.task-list {
      list-style: none;
    }

    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }

    .display.math {
      display: block;
      text-align: center;
      margin: 0.5rem auto;
    }
  </style>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>

<body>
  <header id="title-block-header">
    <h1 class="title">Linear Algebra</h1>
    <p class="author">Hongyu</p>
    <p class="date">2022-12-22</p>
  </header>
  <h1 id="equivalence-theorems">Equivalence Theorems</h1>
  <h2 id="basis">Basis</h2>
  <p>The following statements are equivalent:</p>
  <ul>
    <li>
      <p>A set <span class="math inline">ℬ</span> is a basis for a vector
        space <span class="math inline"><em>V</em></span></p>
    </li>
    <li>
      <p>The spanning set of the vector space <span class="math inline"><em>V</em></span> does not have strict subset
        that
        is also a spanning set</p>
    </li>
    <li>
      <p>The set <span class="math inline"><em>B</em></span> does not have
        any strict superset that is also linearly independent.</p>
    </li>
  </ul>
  <h3 id="finite-dimensional-vector-space">Finite dimensional vector
    space</h3>
  <ol>
    <li>
      <p>A set <span
          class="math inline">ℬ = {<em>α</em><sub>1</sub>, <em>α</em><sub>2</sub>⋯<em>α</em><sub><em>n</em></sub>}</span>
        of <span class="math inline"><em>n</em></span> elements is a basis for a
        <span class="math inline"><em>n</em></span> dimensional vector space
        <span class="math inline"><em>V</em></span>.
      </p>
    </li>
    <li>
      <p><span id="span" label="span"></span> The set <span
          class="math inline">ℬ = {<em>α</em><sub>1</sub>, <em>α</em><sub>2</sub>⋯<em>α</em><sub><em>n</em></sub>}</span>
        of <span class="math inline"><em>n</em></span> elements spans the <span class="math inline"><em>n</em></span>
        dimensional vector space <span class="math inline"><em>V</em></span>.</p>
    </li>
  </ol>
  <h2 id="square-matrices">Square matrices</h2>
  <p>For all <span class="math inline"><em>A</em> ∈ <em>M</em><sub><em>n</em> × <em>n</em></sub></span>,
    the following statements are equivalent:</p>
  <ol>
    <li>
      <p><span id="nonsing" label="nonsing"></span> <span class="math inline"><em>A</em></span> is nonsingular;</p>
    </li>
    <li>
      <p><span id="identity" label="identity"></span> <span class="math inline">$\RREF(A) = I_n$</span></p>
    </li>
    <li>
      <p><span id="unique solution" label="unique solution"></span> for
        all <span class="math inline"><strong>b</strong> ∈ <em>M</em><sub><em>n</em> × 1</sub></span>,
        the linear system <span class="math inline"><em>A</em><strong>x</strong> = <strong>b</strong></span>
        has a unique solution;</p>
    </li>
    <li>
      <p><span id="homogeneous" label="homogeneous"></span> the
        homogeneous linear system <span class="math inline"><em>A</em><strong>x</strong> = <strong>0</strong></span>
        has only the trivial solution <span class="math inline"><strong>x</strong> = <strong>0</strong></span>;</p>
    </li>
    <li>
      <p><span id="elementary" label="elementary"></span> <span class="math inline"><em>A</em></span> is a product of
        elementary
        matrices;</p>
    </li>
    <li>
      <p><span id="no zero row" label="no zero row"></span> <span class="math inline"><em>A</em></span> is not row
        equivalent to a matrix
        with a row (or column) of zeros;</p>
    </li>
    <li>
      <p><span id="det nonzero" label="det nonzero"></span> <span class="math inline">det (<em>A</em>) ≠ 0</span>;</p>
    </li>
    <li>
      <p><span id="null_space" label="null_space"></span> <span
          class="math inline">nullspace(<em>A</em>) = {<strong>0</strong>}</span>;</p>
    </li>
    <li>
      <p><span id="cols A" label="cols A"></span> the columns of <span class="math inline"><em>A</em></span> are
        linearly independent;</p>
    </li>
    <li>
      <p><span id="nullity A" label="nullity A"></span> <span class="math inline">$\nullity(A) = 0$</span>;</p>
    </li>
    <li>
      <p><span id="rank A" label="rank A"></span> <span class="math inline">$\rank(A) = n$</span>;</p>
    </li>
    <li>
      <p><span id="row rank" label="row rank"></span> <span class="math inline">$\rrank(A) = n$</span>;</p>
    </li>
    <li>
      <p><span id="col rank" label="col rank"></span> <span class="math inline">$\crank(A) = n$</span>;</p>
    </li>
    <li>
      <p><span id="rows A" label="rows A"></span> the rows of <span class="math inline"><em>A</em></span> are linearly
        independent;</p>
    </li>
    <li>
      <p><span id="isomorphism" label="isomorphism"></span> <span class="math inline"><em>A</em></span> represents an
        isomorphism <span class="math inline">$L:\RR^n \to \RR^n$</span>;</p>
    </li>
    <li>
      <p><span id="one-to-one" label="one-to-one"></span> <span class="math inline"><em>A</em></span> represents a
        linear transformation
        <span class="math inline">$L:\RR^n \to \RR^n$</span> that is
        one-to-one;
      </p>
    </li>
    <li>
      <p><span id="onto" label="onto"></span> <span class="math inline"><em>A</em></span> represents a linear
        transformation
        <span class="math inline">$L:\RR^n \to \RR^n$</span> that is
        onto;
      </p>
    </li>
    <li>
      <p><span id="operator-invertible" label="operator-invertible"></span> <span class="math inline"><em>A</em></span>
        represents an invertible linear
        transformation <span class="math inline"><em>L</em> : <em>V</em> → <em>W</em></span> where
        <span class="math inline">dim (<em>V</em>) = dim (<em>W</em>) = <em>n</em></span>;
      </p>
    </li>
    <li>
      <p><span id="transf-iso" label="transf-iso"></span> <span class="math inline"><em>A</em></span> represents an
        isomorphism <span class="math inline"><em>L</em> : <em>V</em> → <em>W</em></span> where
        <span class="math inline">dim (<em>V</em>) = dim (<em>W</em>) = <em>n</em></span>;
      </p>
    </li>
    <li>
      <p><span id="transf-onto" label="transf-onto"></span> <span class="math inline"><em>A</em></span> represents an
        onto linear
        transformation <span class="math inline"><em>L</em> : <em>V</em> → <em>W</em></span> where
        <span class="math inline">dim (<em>V</em>) = dim (<em>W</em>) = <em>n</em></span>;
      </p>
    </li>
    <li>
      <p><span id="transf-one-to-one" label="transf-one-to-one"></span>
        <span class="math inline"><em>A</em></span> represents a one-to-one
        linear transformation <span class="math inline"><em>L</em> : <em>V</em> → <em>W</em></span> where
        <span class="math inline">dim (<em>V</em>) = dim (<em>W</em>) = <em>n</em></span>;
      </p>
    </li>
    <li>
      <p><span id="eigen" label="eigen"></span> <span class="math inline"><em>λ</em> = 0</span> is not an eigenvalue of
        <span class="math inline"><em>A</em></span>.</p>
    </li>
    <li>
      <p>The matrix <span class="math inline"><em>A</em></span> can be
        brought to diagonal form by a change of basis, which is same as saying
        that the matrix is diagonalizable.</p>
    </li>
    <li>
      <p>The eigenvectors of the matrix span the space.</p>
    </li>
  </ol>
  <h1 id="vector-spaces">Vector Spaces</h1>
  <h1 id="linear-transformationsmaps">Linear Transformations(maps)</h1>
  <h2 id="linear-transformations">Linear Transformations</h2>
  <div class="definition">
    <p><strong>Definition 3.1</strong>. Let <span class="math inline"><em>V</em></span> and <span
        class="math inline"><em>W</em></span> to be vector spaces over the field
      <span class="math inline"><em>F</em></span>. A <strong>linear
        transformation (map) from <span class="math inline"><em>V</em></span>
        into <span class="math inline"><em>W</em></span></strong> is a function
      <span class="math inline"><em>T</em></span> from <span class="math inline"><em>V</em></span> into <span
        class="math inline"><em>W</em></span> such that <span
        class="math display"><em>T</em>(<em>c</em><em>α</em>+<em>β</em>) = <em>c</em>(<em>T</em><em>α</em>) + <em>T</em><em>β</em></span>
      for all <span class="math inline"><em>α</em></span> and <span class="math inline"><em>β</em></span> in <span
        class="math inline"><em>V</em></span> and all scalars <span class="math inline"><em>c</em></span> in <span
        class="math inline"><em>F</em></span>.
    </p>
  </div>
  <div class="example">
    <p><em>Example 3.1</em>. If <span class="math inline"><em>V</em></span>
      is any vector space, the <strong>identity transformation</strong> <span class="math inline"><em>I</em></span>,
      defined by <span class="math inline"><em>I</em><em>α</em> = <em>α</em></span>, is a
      linear transformation from <span class="math inline"><em>V</em></span>
      into <span class="math inline"><em>V</em></span>. The <strong>zero
        transformation</strong> 0 , defined by <span class="math inline">0<em>α</em> = 0</span>, is a linear
      transformation
      from <span class="math inline"><em>V</em></span> into <span class="math inline"><em>V</em></span>.</p>
  </div>
  <div class="example">
    <p><em>Example 3.2</em>. Let <span class="math inline"><em>F</em></span>
      be a field and let <span class="math inline"><em>V</em></span> be the
      space of polynomial functions <span class="math inline"><em>f</em></span> from <span
        class="math inline"><em>F</em></span> into <span class="math inline"><em>F</em></span>, given by Let <span
        class="math display"><em>f</em>(<em>x</em>) = <em>c</em><sub>0</sub> + <em>c</em><sub>1</sub><em>x</em> + ⋯ + <em>c</em><sub><em>k</em></sub><em>x</em><sup><em>k</em></sup></span>
      <span
        class="math display">(<em>D</em><em>f</em>)(<em>x</em>) = <em>c</em><sub>1</sub> + 2<em>c</em><sub>2</sub><em>x</em> + ⋯ + <em>k</em><em>c</em><sub><em>k</em></sub><em>x</em><sup><em>k</em> − 1</sup>.</span>
      Then <span class="math inline"><em>D</em></span> is a linear
      transformation from <span class="math inline"><em>V</em></span> into
      <span class="math inline"><em>V</em></span>-the <strong>differentiation
        transformation.</strong>
    </p>
  </div>
  <h1 id="polynomials">Polynomials</h1>
  <h2 id="polynomial-ideals">Polynomial Ideals</h2>
  <div class="thm">
    <p><strong>Theorem 4.1</strong>. <em><strong>(Taylor’s
          formula)</strong></em></p>
  </div>
  <div class="proof">
    <p><em>Proof.</em> p130 proof:</p>
    <p><span class="math display">$$\begin{aligned}
        &amp;= \sum_m {\sum_k{\frac{D^k x^m}{k!}(c)(x-c)^k}} \\
        &amp;= \sum_m {\sum_k{\frac{m! c^{m-k}}{k!}(x-c)^k}}
        \end{aligned}$$</span> <a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a> ◻</p>
  </div>
  <h1 id="determinants">Determinants</h1>
  <h2 id="determinant-functions">Determinant functions</h2>
  <h2 id="permutations-and-uniqueness-of-determinants">Permutations and
    Uniqueness of Determinants</h2>
  <p>A permutation of degree <span class="math inline"><em>n</em></span>,
    which pass the sequence <span class="math inline">(1,2,⋯,<em>n</em>)</span> to the sequence <span
      class="math inline">(<em>σ</em>1,<em>σ</em>2,⋯,<em>σ</em><em>n</em>)</span>,
    is done by succession of interchanges of pairs. The number of
    interchanges is either always even or always odd. Then the permutation
    is then called <strong>even</strong> or <strong>odd</strong>,
    respectively. We assign a <strong>sign</strong> to a permutation by
    <span class="math display">$$\text{sgn\,}\sigma = \begin{cases}
      \hphantom{-}1, &amp; \text{if } \sigma \text{ is even} \\
      -1, &amp; \text{if } \sigma \text{ is odd}
      \end{cases}$$</span>
  </p>
  <h2 id="determinant-functions-1">Determinant functions</h2>
  <div class="definition">
    <p><strong>Definition 5.1</strong>. Let <span class="math inline"><em>K</em></span> be a commutative ring with
      identity, <span class="math inline"><em>n</em></span> a positive
      integer, and let <span class="math inline"><em>D</em></span> be a
      function which assigns to each <span class="math inline"><em>n</em> × <em>n</em></span> matrix <span
        class="math inline"><em>A</em></span> over <span class="math inline"><em>K</em></span> a scalar <span
        class="math inline"><em>D</em>(<em>A</em>)</span> in <span class="math inline"><em>K</em></span>. We say that
      <span class="math inline"><em>D</em></span> is a <strong>n-linear</strong> if
      for each <span class="math inline"><em>i</em></span>, <span
        class="math inline">1 ≤ <em>i</em> ≤ <em>n</em></span>, <span class="math inline"><em>D</em></span> is a linear
      function of the <span class="math inline"><em>i</em></span>th row when the other <span
        class="math inline">(<em>n</em>−1)</span> rows are held fixed.</p>
  </div>
  <p>To clarity this definition: if <span class="math inline"><em>D</em></span> is a function from <span
      class="math inline"><em>K</em><sup><em>n</em> × <em>n</em></sup></span>
    to <span class="math inline"><em>K</em></span>, and let <span
      class="math inline"><em>α</em><sub>1</sub>, ⋯, <em>α</em><sub><em>n</em></sub></span>
    be the rows of the matrix <span class="math inline"><em>A</em></span>.
    We can write <span
      class="math display"><em>D</em>(<em>A</em>) = <em>D</em>(<em>α</em><sub>1</sub>,⋯,<em>α</em><sub><em>n</em></sub>),</span>
    that is, let us also think of D as the function of the rows of A. The
    statement that D is n-linear means that <span
      class="math display"><em>D</em>(<em>α</em><sub>1</sub>,⋯,<em>c</em><em>α</em><sub><em>i</em></sub>+<em>α</em><sub><em>i</em></sub>′,⋯,<em>α</em><sub><em>n</em></sub>) = <em>c</em><em>D</em>(<em>α</em><sub>1</sub>,⋯,<em>c</em><em>α</em><sub><em>i</em></sub>,⋯,<em>α</em><sub><em>n</em></sub>) + <em>D</em>(<em>α</em><sub>1</sub>,⋯,<em>α</em><sub><em>i</em></sub>′,⋯,<em>α</em><sub><em>n</em></sub>).</span>
  </p>
  <p>If we fix all rows except row <span class="math inline"><em>i</em></span> and regard <span
      class="math inline"><em>D</em></span> as a function of the <span class="math inline"><em>i</em></span>th row, it
    is often convenient to
    write <span class="math inline"><em>D</em>(<em>a</em><sub><em>i</em></sub>)</span>
    for <span class="math inline"><em>D</em>(<em>A</em>)</span> . Thus, we
    may abbreviate (<a href="#n-linear_function" data-reference-type="ref"
      data-reference="n-linear_function">[n-linear_function]</a>) to <span
      class="math display"><em>D</em>(<em>c</em><em>α</em><sub><em>i</em></sub>+<em>α</em><sub><em>i</em></sub>′) = <em>c</em><em>D</em>(<em>α</em><sub><em>i</em></sub>) + <em>D</em>(<em>α</em><sub><em>i</em></sub>′)</span>
  </p>
  <div class="thm">
    <p><strong>Theorem 5.1</strong>. <em>Let <span class="math inline">K</span> be a commutative ring with identity, and
        let <span class="math inline">A</span> and <span class="math inline">B</span> be <span
          class="math inline">n × n</span>
        matrices over <span class="math inline">K</span>. Then <span
          class="math display">det (AB) = (detA)(detB).</span></em></p>
  </div>
  <p>Proof. Let <span class="math inline"><em>B</em></span> be a fixed
    <span class="math inline"><em>n</em> × <em>n</em></span> matrix over
    <span class="math inline"><em>K</em></span>, and for each <span class="math inline"><em>n</em> × <em>n</em></span>
    matrix <span class="math inline"><em>A</em></span> define <span
      class="math inline"><em>D</em>(<em>A</em>) = det (<em>A</em><em>B</em>)</span>.
    If we denote the rows of <span class="math inline"><em>A</em></span> by
    <span class="math inline"><em>α</em><sub>1</sub>, …</span>, <span
      class="math inline"><em>α</em><sub><em>n</em></sub></span>, then <span
      class="math display"><em>D</em>(<em>α</em><sub>1</sub>,…,<em>α</em><sub><em>n</em></sub>) = det (<em>α</em><sub>1</sub><em>B</em>,…,<em>α</em><sub><em>n</em></sub><em>B</em>).</span>
    Here <span class="math inline"><em>α</em><sub><em>j</em></sub><em>B</em></span>
    denotes the <span class="math inline">1 × <em>n</em></span> matrix which
    is the product of the <span class="math inline">1 × <em>n</em></span>
    matrix <span class="math inline"><em>α</em><sub><em>j</em></sub></span>
    and the <span class="math inline"><em>n</em> × <em>n</em></span> matrix
    <span class="math inline"><em>B</em></span>. Since <span
      class="math display">(<em>c</em><em>α</em><sub><em>i</em></sub>+<em>α</em><sub><em>i</em></sub><sup>′</sup>)<em>B</em> = <em>c</em><em>α</em><sub><em>i</em></sub><em>B</em> + <em>α</em><sub>1</sub><sup>′</sup><em>B</em></span>
    and det is <span class="math inline"><em>n</em></span>-linear, it is
    easy to see that <span class="math inline"><em>D</em></span> is <span class="math inline"><em>n</em></span>-linear.
    If <span class="math inline"><em>α</em><sub><em>i</em></sub> = <em>α</em><sub><em>j</em></sub></span>,
    then <span
      class="math inline"><em>α</em><sub><em>i</em></sub><em>B</em> = <em>α</em><sub><em>j</em></sub><em>B</em></span>,
    and since det is alternating, <span
      class="math display"><em>D</em>(<em>α</em><sub>1</sub>,…,<em>α</em><sub><em>n</em></sub>) = 0.</span>
    Hence, <span class="math inline"><em>D</em></span> is alternating. Now
    <span class="math inline"><em>D</em></span> is an alternating <span class="math inline"><em>n</em></span>-linear
    function, and by Theorem 2
    <span class="math display">$$D(A)=(\operatorname{det} A) D(I) \\
      $$</span> But <span
      class="math inline"><em>D</em>(<em>I</em>) = det (<em>I</em><em>B</em>) = det <em>B</em></span>,
    so <span class="math display">$$\operatorname{det}(A
      B)=D(A)=(\operatorname{det} A)(\operatorname{det} B)\\
      $$</span>
  </p>
  <h2 id="properties-of-determinants">Properties of Determinants</h2>
  <div class="note">
    <p><em>Note 5.1</em>. A non-square matrix does not have determinant
      (find out why)</p>
  </div>
  <p>In this section, we list out some useful properties of the
    determinant function on <span class="math inline"><em>n</em> × <em>n</em></span> matrices.</p>
  <p>When defining our determinant of a matrix <span class="math inline"><em>A</em></span>, the rows of <span
      class="math inline"><em>A</em></span> has played a privileged role, but
    we can just as well define the determinant using the columns of <span class="math inline"><em>A</em></span> by
    symmetry. So, we might expect
    that <span class="math inline">det <em>A</em></span> is an alternating
    (hyperlink) <span class="math inline"><em>n</em></span>-linear function
    (hyperlink) of columns of <span class="math inline"><em>A</em></span>.
    Indeed, we see:</p>
  <div class="prop">
    <p><strong>Proposition 5.1</strong>. <em><span
          class="math display">det (<em>A</em><sup><em>T</em></sup>) = det (<em>A</em>)</span>
        where <span class="math inline"><em>A</em><sup><em>T</em></sup></span>
        is the transpose of <span class="math inline"><em>A</em></span>.</em></p>
  </div>
  <div class="proof">
    <p><em>Proof.</em> Let <span class="math inline"><em>σ</em></span> be a
      permutation of degree <span class="math inline"><em>n</em></span>, <span
        class="math display"><em>A</em><sup><em>T</em></sup>(<em>i</em>,<em>σ</em><em>i</em>) = <em>A</em>(<em>σ</em><em>i</em>,<em>i</em>).</span>
      We know from <span class="citation" data-cites=""></span> that <span class="math display">$$\det (A^T) =
        \sum_\sigma \, (\text{sgn\,}\sigma)
        A(\sigma 1, 1) \cdots A(\sigma n, n).$$</span> Since <span
        class="math inline"><em>σ</em><em>σ</em><sup>−1</sup></span> is the
      identity permutation, it means they are either both even or both odd:
      <span class="math display">$$\begin{cases}
        \sigma \sigma^{-1} = 1 \Rightarrow
        &amp;(\text{sgn\,}\sigma)(\text{sgn\,}\sigma^{-1}) = 1\\
        \text{or} &amp; \text{sgn\,}(\sigma ^{-1}) = \text{sgn\,}(\sigma).
        \end{cases}$$</span> Also when <span
        class="math inline"><em>σ</em><em>i</em> = <em>j</em> ⇒ <em>i</em> = <em>σ</em><sup>−1</sup><em>j</em></span>,
      <span
        class="math inline"><em>A</em>(<em>σ</em><em>i</em>,<em>i</em>) = <em>A</em>(<em>j</em>,<em>σ</em><sup>−1</sup><em>j</em>)</span>.
      Thus, <span class="math display">$$A(\sigma 1, 1) \cdots A(\sigma n, n)
        = A(1, \sigma^{-1}1) \cdots A(\n, \sigma^{-1} n).$$</span> Finally,
      since <span class="math inline"><em>σ</em></span> varies over all
      permutations of degree <span class="math inline"><em>n</em></span>, so
      does <span class="math inline"><em>σ</em><sup>−1</sup></span>.
      Therefore, <span class="math display">$$\begin{aligned}
        \det (A^T) &amp;= \sum_\sigma \, (\text{sgn\,}{\sigma}^{-1}) A(1,
        {\sigma}^{-1}1) \cdots A(n, {\sigma}^{-1}n)\\
        &amp;= \det A
        \end{aligned}$$</span> proving equation <a href="#determinant_of_transpose" data-reference-type="ref"
        data-reference="determinant_of_transpose">[determinant_of_transpose]</a>. ◻
    </p>
  </div>
  <div class="example">
    <p><em>Example 5.1</em>. Let <span class="math inline"><em>K</em></span>
      be the ring of integers and <span class="math display">$$A =
        \begin{bmatrix}
        1 &amp; 2\\
        3 &amp; 4
        \end{bmatrix}$$</span></p>
  </div>
  <h2 id="modules">Modules</h2>
  <div class="definition">
    <p><strong>Definition 5.2</strong>. If <span class="math inline"><em>K</em></span> is a commutative ring with
      identity, a module over <span class="math inline"><em>K</em></span> is
      an algebraic system which behaves like a vector space, with <span class="math inline"><em>K</em></span> playing
      the role of the scalar
      field. To be precise, we say that <span class="math inline"><em>V</em></span> is a <strong>module over</strong>
      <span class="math inline"><em>K</em></span> (or a <span
        class="math inline"><em>K</em></span>-<strong>module</strong>) if
    </p>
    <ol>
      <li>
        <p>there is an addition <span class="math inline">(<em>α</em>,<em>β</em>) → <em>α</em> + <em>β</em></span>
          on <span class="math inline"><em>V</em></span>, under which <span class="math inline"><em>V</em></span> is a
          commutative group;</p>
      </li>
      <li>
        <p>there is multiplication <span class="math inline">(<em>c</em>,<em>α</em>) → <em>c</em><em>α</em></span>
          of elements <span class="math inline"><em>α</em></span> in <span class="math inline"><em>V</em></span> and
          <span class="math inline"><em>c</em></span> in <span class="math inline"><em>K</em></span> such that <span
            class="math display">$$\begin{aligned}
            (c_1 + c_2) \alpha &amp;= c_1 \alpha + c_2 \alpha\\
            c(\alpha_1 + \alpha_2) &amp;= c\alpha_1 + c\alpha_2\\
            (c_1 c_2)\alpha &amp;= c_1(c_2 \alpha)\\
            1 \alpha &amp;= \alpha

            \end{aligned}$$</span></p>
      </li>
    </ol>
  </div>
  <h1 id="elementary-canonical-forms">Elementary Canonical Forms</h1>
  <h2 id="eigenvalues">Eigenvalues</h2>
  <div class="definition">
    <p><strong>Definition 6.1</strong>. Let <span class="math inline"><em>V</em></span> be a vector space over the field
      <span class="math inline"><em>F</em></span> and let <span class="math inline"><em>T</em></span> be a linear
      operator on V. An
      <strong>eigenvalue</strong> of <span class="math inline"><em>T</em></span> is a scalar <span
        class="math inline"><em>c</em></span> in <span class="math inline"><em>F</em></span> such that there is a
      non-zero
      vector <span class="math inline"><em>α</em></span> in <span class="math inline"><em>V</em></span> with <span
        class="math inline"><em>T</em><em>α</em> = <em>c</em><em>α</em></span>.
      If <span class="math inline"><em>c</em></span> is a eigenvalue of <span class="math inline"><em>T</em></span>,
      then
    </p>
    <ol type="a">
      <li>
        <p>any <span class="math inline"><em>α</em></span> such that <span
            class="math inline"><em>T</em><em>α</em> = <em>c</em><em>α</em></span>
          is called a eigenvector of <span class="math inline"><em>T</em></span>
          associated with the eigenvalue <span class="math inline"><em>c</em></span>;</p>
      </li>
      <li>
        <p>the collection of all <span class="math inline"><em>α</em></span>
          such that <span class="math inline"><em>T</em><em>α</em> = <em>c</em><em>α</em></span>
          is called the eigenspace associated with <span class="math inline"><em>c</em></span>.</p>
      </li>
    </ol>
  </div>
  <p>Eigenvalues are often called characteristic roots, latent roots,
    characteristic values, proper values, or spectral values.</p>
  <p>To see the definition in another way, if <span class="math inline"><em>T</em></span> is any linear operator and
    <span class="math inline"><em>c</em></span> is any scalar, the set of vectors
    <span class="math inline"><em>α</em></span> such that <span
      class="math inline"><em>T</em><em>α</em> = <em>c</em><em>α</em></span>
    forms a subspace of <span class="math inline"><em>V</em></span>.
  </p>
  <div class="proof">
    <p><em>Proof.</em> We already have the field <span class="math inline"><em>F</em></span> of scalars and set <span
        class="math inline"><em>V</em></span> of vectors. Now we need to check
      if the rules of addition and multiplication result a new vector in the
      subspace. Suppose two vectors <span class="math inline"><em>α</em></span> and <span
        class="math inline"><em>β</em></span> in the characteristic space: <span
        class="math inline"><em>T</em><em>α</em> = <em>c</em><em>α</em></span>
      and <span class="math inline"><em>T</em><em>β</em> = <em>c</em><em>β</em></span>.
      Then <span
        class="math inline"><em>T</em>(<em>α</em>+<em>β</em>) = <em>T</em><em>α</em> + <em>T</em><em>β</em> = <em>c</em><em>α</em> + <em>c</em><em>β</em> = <em>c</em>(<em>α</em>+<em>β</em>)</span>.
      Likewise: <span
        class="math inline"><em>T</em>(<em>d</em><em>α</em>) = <em>d</em><em>T</em>(<em>α</em>) = <em>d</em><em>c</em><em>α</em> = <em>c</em><em>d</em><em>α</em></span>. ◻
    </p>
  </div>
  <p>Now <span class="math inline">(<em>T</em>−<em>c</em><em>I</em>)<em>α</em> = 0</span>,
    so this subspace is the null space of the linear trans-formation <span
      class="math inline">(<em>T</em>−<em>c</em><em>I</em>)</span>. Following
    the definition, we call <span class="math inline"><em>c</em></span> a
    characteristic value of <span class="math inline"><em>T</em></span> if
    this subspace is different from the zero subspace. This brings us to the
    negation of our equivalence theorem <a href="#null_space" data-reference-type="ref"
      data-reference="null_space">[null_space]</a>.
    But when <span class="math inline"><em>c</em></span> is the eigenvalue
    of <span class="math inline"><em>T</em></span>, <span class="math inline">0</span> is the eigenvalue of <span
      class="math inline">(<em>T</em>−<em>c</em><em>I</em>)</span>, and
    therefore number <a href="#eigen" data-reference-type="ref" data-reference="eigen">[eigen]</a> of the equivalence
    theorem. We
    summarize some of the important equivalence theorem here:</p>
  <div class="thm">
    <p><strong>Theorem 6.1</strong>. <em>Let <span class="math inline">T</span> be a linear operator on a
        finite-dimensional space <span class="math inline">V</span> and let
        <span class="math inline">c</span> be a scalar. The following are
        equivalent:</em></p>
    <ol type="1">
      <li>
        <p><em><span class="math inline">c</span> is a eigenvalue of <span class="math inline">T</span>.</em></p>
      </li>
      <li>
        <p><em>The operator <span class="math inline">(T−cI)</span> is
            singular (not invertible).</em></p>
      </li>
      <li>
        <p><em><span class="math inline">det  (T−cI) = 0</span>.</em></p>
      </li>
    </ol>
  </div>
  <p>We can use the determinant criterion (iii) to calculate the
    eigenvalues of <span class="math inline"><em>T</em></span>. Since <span
      class="math inline">det (<em>T</em>−<em>c</em><em>I</em>)</span> is a
    polynomial of degree <span class="math inline"><em>n</em></span> in the
    variable <span class="math inline"><em>c</em></span>, eigenvalues are
    the roots of that polynomial, hence the name characteristic roots. To
    write it as a polynomial, we first represent <span class="math inline"><em>T</em></span> by a matrix <span
      class="math inline"><em>A</em></span> in a ordered basis in <span class="math inline"><em>V</em></span>, <span
      class="math inline">ℬ</span>: <span class="math inline"><em>A</em> = [<em>T</em>]<sub>ℬ</sub></span>. We
    then use a polynomial <span
      class="math inline"><em>f</em>(<em>x</em>) = det (<em>A</em>−<em>x</em><em>I</em>)</span>
    and solve <span class="math inline"><em>f</em>(<em>x</em>) = 0</span>
    for the characteristic roots, which happens when <span class="math inline"><em>x</em> = <em>c</em></span>. This is
    why we call
    this polynomial <span class="math inline"><em>f</em>(<em>x</em>)</span>
    the <strong>characteristic polynomial</strong>.</p>
  <p>It is important to note that <span class="math inline"><em>f</em></span> is a monic polynomial which has
    degree exactly <span class="math inline"><em>n</em></span>. This is
    easily seen from the formula for the determinant of a matrix in terms of
    its entries. Thus, by the fundamental theorem of linear algebra, it has
    <span class="math inline"><em>n</em></span> complex roots. But some of
    these can be multiple roots, so we can say that <span class="math inline"><em>T</em></span> has at most <span
      class="math inline"><em>n</em></span> distinct eigenvalues. Depending
    how we define <span class="math inline"><em>V</em></span>, we might have
    at least one root or no root. In particular, if <span class="math inline"><em>V</em></span> is a complex vector
    space , <span class="math inline"><em>T</em></span> has at least one root. However, if
    <span class="math inline"><em>V</em></span> is a real vector space,
    <span class="math inline"><em>T</em></span> may not have nay
    eigenvalues.
  </p>
  <p>After finding eigenvalues, we can simply plug them into the equation
    <span class="math inline"><em>T</em><em>α</em> = <em>λ</em><em>α</em></span>
    to solve for eigenvectors.
  </p>
  <p>Lemma. Similar matrices have the same characteristic polynomial.
    Proof. If <span class="math inline"><em>B</em> = <em>P</em><sup>−1</sup><em>A</em><em>P</em></span>,
    then <span class="math display">$$\begin{aligned}
      \operatorname{det}(x I-B) &amp; =\operatorname{det}\left(x I-P^{-1} A
      P\right) \\
      &amp; =\operatorname{det}\left(P^{-1}(x I-A) P\right) \\
      &amp; =\operatorname{det} P^{-1} \cdot \operatorname{det}(x I-A) \cdot
      \operatorname{det} P \\
      &amp; =\operatorname{det}(x I-A) .
      \end{aligned}$$</span> This lemma enables us to define sensibly the
    characteristic polynomial of the operator <span class="math inline"><em>T</em></span> as the characteristic
    polynomial
    of any <span class="math inline"><em>n</em> × <em>n</em></span> matrix
    which represents <span class="math inline"><em>T</em></span> in some
    ordered basis for <span class="math inline"><em>V</em></span>. Just as
    for matrices, the eigenvalues of <span class="math inline"><em>T</em></span> will be the roots of the
    characteristic polynomial for <span class="math inline"><em>T</em></span>.</p>
  <div class="example">
    <p><em>Example 6.1</em>. Let <span class="math inline"><em>T</em></span>
      be the linear operator on <span class="math inline">ℝ<sup>2</sup></span>
      which is represented in the standard ordered basis by the matrix <span
        class="math display">$$A=\left[\begin{array}{rr}
        0 &amp; -1 \\
        1 &amp; 0
        \end{array}\right] \text {. }$$</span> The characteristic polynomial for
      <span class="math inline"><em>T</em></span> (or for <span class="math inline"><em>A</em></span> ) is <span
        class="math display">$$f(x) = \operatorname{det}(x
        I-A)=\left|\begin{array}{rr}
        x &amp; 1 \\
        -1 &amp; x
        \end{array}\right|=x^2+1$$</span> Since this polynomial has no real
      roots, <span class="math inline"><em>T</em></span> has no characteristic
      values. If <span class="math inline"><em>U</em></span> is the linear
      operator on <span class="math inline">ℂ<sup>2</sup></span> which is
      represented by <span class="math inline"><em>A</em></span> in the
      standard ordered basis, then <span class="math inline"><em>U</em></span>
      has two eigenvalues, <span class="math inline"><em>i</em></span> and
      <span class="math inline"> − <em>i</em></span>. Here we see a subtle
      point. In discussing the characteristic values of a matrix <span class="math inline"><em>A</em></span>, we must be
      careful to stipulate
      the field involved. The matrix <span class="math inline"><em>A</em></span> above has no eigenvalues in <span
        class="math inline"><em>R</em></span>, but has the two eigenvalues <span class="math inline"><em>i</em></span>
      and <span class="math inline"> − <em>i</em></span> in <span class="math inline">ℂ</span>.
    </p>
  </div>
  <div class="example">
    <p><em>Example 6.2</em>. Let <span class="math inline"><em>A</em></span>
      be a matrix in <span class="math inline">ℝ<sup>3</sup></span>: <span
        class="math display">$$\left[\begin{array}{rrr}
        3 &amp; 1 &amp; -1 \\
        2 &amp; 2 &amp; -1 \\
        2 &amp; 2 &amp; 0
        \end{array}\right]$$</span> Then the characteristic polynomial for <span class="math inline"><em>A</em></span>
      is <span class="math display">$$\left|\begin{array}{ccc}
        x-3 &amp; -1 &amp; 1 \\
        -2 &amp; x-2 &amp; 1 \\
        -2 &amp; -2 &amp; x
        \end{array}\right|=x^3-5 x^2+8 x-4=(x-1)(x-2)^2$$</span> Thus the
      characteristic values of <span class="math inline"><em>A</em></span> are
      <span class="math inline"><em>λ</em><sub>1</sub> = 1</span> and <span
        class="math inline"><em>λ</em><sub>2</sub> = 2</span>. Suppose that
      <span class="math inline"><em>T</em></span> is the linear operator on
      <span class="math inline"><em>R</em><sup>3</sup></span> which is
      represented by <span class="math inline"><em>A</em></span> in the
      standard basis. Let us find the characteristic vectors. Call the
      components of the eigenvector associated with the eigenvalues 1, <span
        class="math inline"><em>α</em><sub>1</sub> = (<em>a</em><sub>1</sub>,<em>a</em><sub>2</sub>,<em>a</em><sub>3</sub>)</span>;
      Now
    </p>
    <p><span class="math display">$$\left[\begin{array}{rrr}
        3 &amp; 1 &amp; -1 \\
        2 &amp; 2 &amp; -1 \\
        2 &amp; 2 &amp; 0
        \end{array}\right]
        \left[\begin{array}{l}
        a_1 \\
        a_2 \\
        a_3
        \end{array}\right]=
        1
        \left[\begin{array}{l}
        a_1 \\
        a_2 \\
        a_3
        \end{array}\right]$$</span> which yields three equations: <span class="math display">$$\begin{aligned}
        2 a_1 + a_2 - a_3 &amp; =0 \\
        2 a_1 + a_2 - a_3 &amp; =0 \\
        2 a_1 + 2a_2 - a_3 &amp; =0.
        \end{aligned}$$</span> Subtracting the first and the third equation
      yields <span class="math inline"><em>a</em><sub>2</sub> = 0</span>.
      Substituting it to the first equation yields <span
        class="math inline">2<em>a</em><sub>1</sub> = <em>a</em><sub>3</sub></span>.
      Now we have the solution: <span class="math display">$$\alpha_1=a_1\left(\begin{array}{l}
        1 \\
        0 \\
        2
        \end{array}\right), \quad \text { for } \lambda_1=0.$$</span></p>
    <p>Now for the eigenvalue <span class="math inline"><em>λ</em><sub>2</sub></span>, <span
        class="math display">$$\left[\begin{array}{rrr}
        3 &amp; 1 &amp; -1 \\
        2 &amp; 2 &amp; -1 \\
        2 &amp; 2 &amp; 0
        \end{array}\right]
        \left[\begin{array}{l}
        a_1 \\
        a_2 \\
        a_3
        \end{array}\right]=
        2
        \left[\begin{array}{l}
        a_1 \\
        a_2 \\
        a_3
        \end{array}\right]$$</span> which yields three equations: <span class="math display">$$\begin{aligned}
        1 a_1 + a_2 - a_3 &amp; =0 \\
        2 a_1 + a_3 &amp; =0 \\
        2 a_1 + 2a_2 - 2a_3 &amp; =0.
        \end{aligned}$$</span> We similarly have the solution: <span class="math display">$$\alpha_1=
        a_1\left(\begin{array}{l}
        1 \\
        1 \\
        2
        \end{array}\right) =
        \left(\begin{array}{l}
        1 \\
        1 \\
        2
        \end{array}\right), \text{if we choose } a_1 = 1.$$</span></p>
  </div>
  <div class="example">
    <p><em>Example 6.3</em>. Let matrix <span class="math inline"><em>A</em></span> be a matrix in <span
        class="math inline">ℂ<sup>3</sup></span>: <span class="math display">$$A=\left(\begin{array}{ccc}
        2 &amp; 0 &amp; -2 \\
        -2 i &amp; i &amp; 2 i \\
        1 &amp; 0 &amp; -1
        \end{array}\right)$$</span> The characteristic equation is <span class="math display">$$\left|\begin{array}{ccc}
        (2-x) &amp; 0 &amp; -2 \\
        -2 i &amp; (i-x) &amp; 2 i \\
        1 &amp; 0 &amp; (-1-x)
        \end{array}\right|=-x^3+(1+i) x^2-i x=0$$</span> and its roots are 0, 1,
      and <span class="math inline"><em>i</em></span>. Call the components of
      the first eigenvector <span
        class="math inline"><em>α</em><sub>1</sub> = (<em>a</em><sub>1</sub>,<em>a</em><sub>2</sub>,<em>a</em><sub>3</sub>)</span>;
      then <span class="math display">$$\left(\begin{array}{ccc}
        2 &amp; 0 &amp; -2 \\
        -2 i &amp; i &amp; 2 i \\
        1 &amp; 0 &amp; -1
        \end{array}\right)\left(\begin{array}{l}
        a_1 \\
        a_2 \\
        a_3
        \end{array}\right)=0\left(\begin{array}{l}
        a_1 \\
        a_2 \\
        a_3
        \end{array}\right)=\left(\begin{array}{l}
        0 \\
        0 \\
        0
        \end{array}\right)$$</span> which yields three equations: <span class="math display">$$\begin{aligned}
        2 a_1-2 a_3 &amp; =0 \\
        -2 i a_1+i a_2+2 i a_3 &amp; =0 \\
        a_1-a_3 &amp; =0 .
        \end{aligned}$$</span> The first determines <span class="math inline"><em>a</em><sub>3</sub></span> (in terms of
      <span class="math inline"><em>a</em><sub>1</sub></span> ): <span
        class="math inline"><em>a</em><sub>3</sub> = <em>a</em><sub>1</sub>;</span>
      the second determines <span class="math inline"><em>a</em><sub>2</sub> : <em>a</em><sub>2</sub> = 0</span>;
      and the third is redundant. Now we have the solution: <span
        class="math display">$$\alpha_1=a_1\left(\begin{array}{l}
        1 \\
        0 \\
        1
        \end{array}\right), \quad \text { for } \lambda_1=0.$$</span> But we can
      just pick any <span class="math inline"><em>a</em><sub>1</sub></span>,
      we might as we just pick <span class="math inline"><em>a</em><sub>1</sub> = 1</span>: <span
        class="math display">$$\alpha_1=\left(\begin{array}{l}
        1 \\
        0 \\
        1
        \end{array}\right)$$</span> For the second eigenvector (recycling the
      same notation for the components) we have <span class="math display">$$\left(\begin{array}{ccc}
        2 &amp; 0 &amp; -2 \\
        -2 i &amp; i &amp; 2 i \\
        1 &amp; 0 &amp; -1
        \end{array}\right)\left(\begin{array}{l}
        a_1 \\
        a_2 \\
        a_3
        \end{array}\right)=1\left(\begin{array}{l}
        a_1 \\
        a_2 \\
        a_3
        \end{array}\right)=\left(\begin{array}{l}
        a_1 \\
        a_2 \\
        a_3
        \end{array}\right) .$$</span> which leads to the equations <span class="math display">$$\begin{aligned}
        2 a_1-2 a_3 &amp; =a_1, \\
        -2 i a_1+i a_2+2 i a_3 &amp; =a_2 . \\
        a_1-a_3 &amp; =a_3 .
        \end{aligned}$$</span> with the solution <span
        class="math inline"><em>a</em><sub>3</sub> = (1/2)<em>a</em><sub>1</sub>, <em>a</em><sub>2</sub> = [(1−<em>i</em>)/2]<em>a</em><sub>1</sub></span>;
      this time I’ll pick <span class="math inline"><em>a</em><sub>1</sub> = 2</span>, so <span
        class="math display">$$\alpha_2=\left(\begin{array}{c}
        2 \\
        1-i \\
        1
        \end{array}\right), \quad \text { for } \lambda_2=1 \text {. }$$</span>
      Finally, for the third eigenvector, <span class="math display">$$\left(\begin{array}{ccc}
        2 &amp; 0 &amp; -2 \\
        -2 i &amp; i &amp; 2 i \\
        1 &amp; 0 &amp; -1
        \end{array}\right)\left(\begin{array}{l}
        a_1 \\
        a_2 \\
        a_3
        \end{array}\right)=i\left(\begin{array}{l}
        a_1 \\
        a_2 \\
        a_3
        \end{array}\right)=\left(\begin{array}{c}
        i a_1 \\
        i a_2 \\
        i a_3
        \end{array}\right) .$$</span> which gives the equations <span class="math display">$$\begin{aligned}
        2 a_1-2 a_3 &amp; =i a_1, \\
        -2 i a_1+i a_2+2 i a_3 &amp; =i a_2 . \\
        a_1-a_3 &amp; =i a_3 .
        \end{aligned}$$</span> whose solution is <span
        class="math inline"><em>a</em><sub>3</sub> = <em>a</em><sub>1</sub> = 0</span>,
      with <span class="math inline"><em>a</em><sub>2</sub></span>
      undetermined. Choosing <span class="math inline"><em>a</em><sub>2</sub> = 1</span>, we conclude <span
        class="math display">$$\alpha_3=\left(\begin{array}{l}
        0 \\
        1 \\
        0
        \end{array}\right), \quad \text { for } \lambda_3=i$$</span></p>
  </div>
  <div class="definition">
    <p><strong>Definition 6.2</strong>. Let <span class="math inline"><em>T</em></span> be a linear operator on the
      finite-dimensional space <span class="math inline"><em>V</em></span>. We
      say that <span class="math inline"><em>T</em></span> is diagonalizable
      if there is a basis for <span class="math inline"><em>V</em></span> each
      vector of which is a eigenvector of <span class="math inline"><em>T</em></span>. Or we can equivalently say that
      <span class="math inline"><em>T</em></span> is diagonalizable if the
      eigenvectors of <span class="math inline"><em>T</em></span> span <span class="math inline"><em>V</em></span> from
      our equivalence theorem <a href="#span" data-reference-type="ref" data-reference="span">[span]</a>
      for finite dimensional vector spaces.
    </p>
  </div>
  <h1 id="inner-product-spaces">Inner Product Spaces</h1>
  <h2 id="inner-products">Inner Products</h2>
  <div class="definition">
    <p><strong>Definition 7.1</strong>. Let <span class="math inline"><em>F</em></span> be the field of real numbers or
      the field of complex numbers, and <span class="math inline"><em>V</em></span> a vector space over <span
        class="math inline"><em>F</em></span>. An <strong>inner product</strong>
      on <span class="math inline"><em>V</em></span> is a function which
      assigns to each ordered pair of vector <span class="math inline"><em>α</em>, <em>β</em></span> in <span
        class="math inline"><em>V</em></span> a scalar <span class="math inline">(<em>α</em>|<em>β</em>)</span> in <span
        class="math inline"><em>F</em></span> in such a way that for all <span
        class="math inline"><em>α</em>, <em>β</em>, <em>γ</em></span> in <span class="math inline"><em>V</em></span> and
      all scalars <span class="math inline"><em>c</em></span>:</p>
    <ol>
      <li>
        <p><span
            class="math inline">(<em>α</em>+<em>β</em>|<em>γ</em>) = (<em>α</em>|<em>γ</em>) + (<em>β</em>|<em>γ</em>)</span>;
        </p>
      </li>
      <li>
        <p><span class="math inline">(<em>c</em><em>α</em>|<em>β</em>) = <em>c</em>(<em>α</em>|<em>β</em>)</span>;</p>
      </li>
      <li>
        <p><span class="math inline">$(\beta | \alpha) = ( \Bar{\alpha} |
            \Bar{\beta})$</span>, the bar denoting complex conjugation;</p>
      </li>
      <li>
        <p><span class="math inline">(<em>α</em>|<em>α</em>) &gt; 0</span>
          if <span class="math inline"><em>α</em> ≠ 0</span>.<br />
        </p>
      </li>
      <li>
        <p>Note that conditions (1), (2) and (3) imply that</p>
      </li>
      <li>
        <p><span class="math inline">$(\alpha | c \beta + \gamma) = \Bar{c}
            (\alpha | \beta ) + (\alpha + \gamma)$</span>.</p>
      </li>
    </ol>
  </div>
  <div class="example">
    <p><em>Example 7.1</em>. On <span class="math inline"><em>F</em><sup><em>n</em></sup></span> there is an
      inner product which we call the <strong>standard inner product</strong>.
      It is defined on <span
        class="math inline"><em>α</em> = (<em>x</em><sub>1</sub>,⋯,<em>x</em><sub><em>n</em></sub>)</span>
      and <span class="math inline"><em>β</em> = (<em>y</em><sub>1</sub>,⋯,<em>y</em><sub><em>n</em></sub>)</span>
      by <span class="math display">$$(\alpha |\beta ) = \sum_j x_j
        \Bar{y}_j.$$</span></p>
    <p>If <span class="math inline"><em>F</em> = ℝ</span>, we can write this
      as <span
        class="math display">(<em>α</em>|<em>β</em>) = ∑<sub><em>j</em></sub><em>x</em><sub><em>j</em></sub><em>y</em><sub><em>j</em></sub>.</span>
      We call this standard inner product the <strong>dot product</strong> or
      <strong>scalar product</strong> and denote it by <span class="math inline"><em>α</em> ⋅ <em>β</em></span>.
    </p>
  </div>
  <div class="example">
    <p><em>Example 7.2</em>. For <span
        class="math inline"><em>α</em> = (<em>x</em><sub>1</sub>,<em>x</em><sub>2</sub>)</span>
      and <span class="math inline"><em>β</em> = (<em>y</em><sub>1</sub>,<em>y</em><sub>2</sub>)</span>
      in <span class="math inline"><em>R</em><sup>2</sup></span>, let <span
        class="math display">(<em>α</em>∣<em>β</em>) = <em>x</em><sub>1</sub><em>y</em><sub>1</sub> − <em>x</em><sub>2</sub><em>y</em><sub>1</sub> − <em>x</em><sub>1</sub><em>y</em><sub>2</sub> + 4<em>x</em><sub>2</sub><em>y</em><sub>2</sub></span>
      Since <span
        class="math inline">(<em>α</em>∣<em>α</em>) = (<em>x</em><sub>1</sub>−<em>x</em><sub>2</sub>)<sup>2</sup> + 3<em>x</em><sub>2</sub><sup>2</sup></span>,
      it follows that <span class="math inline">(<em>α</em>∣<em>α</em>) &gt; 0</span> if <span
        class="math inline"><em>α</em> ≠ 0</span>. Conditions (1), (2), and (3)
      of the definition are easily verified.</p>
  </div>
  <div class="example">
    <p><em>Example 7.3</em>. Let <span class="math inline"><em>V</em></span>
      be <span class="math inline"><em>F</em><sup><em>n</em> × <em>n</em></sup></span>,
      the space of all <span class="math inline"><em>n</em> × <em>n</em></span> matrices over <span
        class="math inline"><em>F</em></span>. Then <span class="math inline"><em>V</em></span> is isomorphic to <span
        class="math inline"><em>F</em><sup><em>n</em><sup>2</sup></sup></span>
      in a natural way. It therefore follows from Example 1 that the equation
      <span
        class="math display">(<em>A</em>∣<em>B</em>) = ∑<sub><em>j</em>, <em>k</em></sub><em>A</em><sub><em>j</em><em>k</em></sub><em>B̄</em><sub><em>j</em><em>k</em></sub></span>
    </p>
    <p>defines an inner product on <span class="math inline"><em>V</em></span>. Furthermore, if we introduce the
      con jugate transpose matrix <span class="math inline"><em>B</em><sup>*</sup></span>, where <span
        class="math inline"><em>B</em><sub><em>k</em><em>j</em></sub><sup>*</sup> = <em>B̄</em><sub><em>j</em><em>k</em></sub></span>,
      we may express this inner product on <span class="math inline"><em>F</em><sup><em>n</em> × <em>n</em></sup></span>
      in terms of the trace function: <span
        class="math display">(<em>A</em>∣<em>B</em>) = tr (<em>A</em><em>B</em><sup>*</sup>) = tr (<em>B</em><sup>*</sup><em>A</em>).</span>
      For <span class="math display">$$\begin{aligned}
        \operatorname{tr}\left(A B^*\right) &amp; =\sum_j\left(A B^*\right)_{j
        j} \\
        &amp; =\sum_j \sum_k A_{j k} B_{k j}^* \\
        &amp; =\sum_j \sum_k A_{j k} \bar{B}_{j k}
        \end{aligned}$$</span></p>
  </div>
  <div class="example">
    <p><em>Example 7.4</em>. Let <span class="math inline"><em>F</em><sup><em>n</em> × 1</sup></span> be the
      space of <span class="math inline"><em>n</em> × 1</span> (column)
      matrices over <span class="math inline"><em>F</em></span>, and let <span class="math inline"><em>Q</em></span> be
      an <span class="math inline"><em>n</em> × <em>n</em></span> invertible matrix
      over <span class="math inline"><em>F</em></span>. For <span class="math inline"><em>X</em>, <em>Y</em></span> in
      <span class="math inline"><em>F</em><sup><em>n</em> × 1</sup></span> set <span
        class="math display">(<em>X</em>∣<em>Y</em>) = <em>Y</em><sup>*</sup><em>Q</em><sup>*</sup><em>Q</em><em>X</em></span>
      We are identifying the <span class="math inline">1 × 1</span> matrix on
      the right with its single entry. When <span class="math inline"><em>Q</em></span> is the identity matrix, this
      inner
      product is essentially the same as that in Example 1 ; we call it the
      standard inner product on <span class="math inline"><em>F</em><sub><em>n</em> × 1</sub></span>. The
      reader should note that the terminology ’standard inner product’ is used
      in two special contexts. For a general finite-dimensional vector space
      over <span class="math inline"><em>F</em></span>, there is no obvious
      inner product that one may call standard.</p>
  </div>
  <p>Let <span class="math inline"><em>V</em></span> be the vector space
    of all continuous complex valued functions on the unit interval, <span
      class="math inline">0 ≤ <em>t</em> ≤ 1</span>. Let <span class="math display">$$(f \mid g)=\int_a^b
      \overline{f(t)} g(t) d t
      .$$</span></p>
  <p>Condition 1 and 2: <span class="math display">$$( f
      \mid(b|g)+c|h))=\int \overline{f(x)}(b g(x)+c h(x)) d x=b \int
      \overline{f} g d x+c \int \overline{f} h d x=b( f \mid g)+c( f \mid h)
      .$$</span> Condition 3: <span class="math display">$$( g \mid
      f)=\int_a^b \overline{g(x)} f(x) d x=\overline{\left(\int_a^b
      \overline{f(x)} g(x) d x\right)}=\overline{( f \mid g)} .$$</span>
    Condition 4: <span class="math display">$$(f \mid f) = \int_a^b
      \abs{f(x)}^2 dx$$</span>, which is real and non-negative; it is zero
    only when <span class="math inline"><em>f</em>(<em>x</em>) = 0</span>.</p>
  <p>The reader is probably more familiar with the space of real-valued
    continuous functions on the unit interval, and for this space the
    complex conjugate on <span class="math inline"><em>g</em></span> may be
    omitted.</p>
  <p>Example 6 . This is really a whole class of examples. One may
    construct new inner products from a given one by the following method.
    Let <span class="math inline"><em>V</em></span> and <span class="math inline"><em>W</em></span> be vector spaces
    over <span class="math inline"><em>F</em></span> and suppose <span class="math inline">(∣)</span> is an inner
    product on <span class="math inline"><em>W</em></span>. If <span class="math inline"><em>T</em></span> is a
    non-singular linear
    transformation from <span class="math inline"><em>V</em></span> into
    <span class="math inline"><em>W</em></span>, then the equation <span
      class="math display"><em>p</em><sub><em>T</em></sub>(<em>α</em>,<em>β</em>) = (<em>T</em><em>α</em>∣<em>T</em><em>β</em>)</span>
    defines an inner product <span class="math inline"><em>p</em><sub><em>T</em></sub></span> on <span
      class="math inline"><em>V</em></span>. The inner product in Example 4 is
    a special case of this situation. The following are also special cases.
    (a) Let <span class="math inline"><em>V</em></span> be a
    finite-dimensional vector space, and let <span
      class="math display"><em>B</em> = {<em>α</em><sub>1</sub>,…,<em>α</em><sub><em>n</em></sub>}</span>
  </p>
  <div class="definition">
    <p><strong>Definition 7.2</strong>. We denote the positive square root
      of <span class="math inline">(<em>α</em>|<em>α</em>)</span> by <span class="math inline">$\norm{\alpha}$</span>;
      <span class="math display">$$(\alpha|\alpha) = \norm{\alpha}^2$$</span> <span
        class="math inline">$\norm{\alpha}$</span> is called the
      <strong>norm</strong> of <span class="math inline"><em>α</em></span>
      with respect to the inner product. We can think of it as a “length" or
      “magnitude" of <span class="math inline"><em>α</em></span>.
    </p>
  </div>
  <div class="definition">
    <p><strong>Definition 7.3</strong>. We can get the <strong>quadratic
        form</strong> of the inner product by the following: We know from the
      property of the inner product that <span class="math display">$$\norm{\alpha \pm \beta}^2 = \norm{\alpha}^2 \pm 2
        \Re (\alpha | \beta) + \norm{\beta}^2,$$</span> and thus if <span
        class="math inline">(<em>α</em>|<em>β</em>)</span> is real, <span class="math display">$$(\alpha | \beta) =
        \frac{1}{4} \norm{\alpha +
        \beta}^2 - \frac{1}{4} \norm{\alpha - \beta}^2,$$</span> and if <span
        class="math inline">(<em>α</em>|<em>β</em>)</span> is complex: <span class="math display">$$(\alpha | \beta) =
        \frac{1}{4} \norm{\alpha +
        \beta}^2 - \frac{1}{4} \norm{\alpha - \beta}^2 + \frac{i}{4}
        \norm{\alpha + i\beta}^2 - \frac{i}{4} \norm{\alpha -
        i\beta}^2.$$</span></p>
  </div>
  <div class="note">
    <p><em>Note 7.1</em>. Note that <span class="math display">$$\begin{aligned}
        \norm{\alpha \pm \beta}^2 &amp;= (\alpha + \beta|\alpha + \beta)\\
        &amp;=(\alpha|\alpha) + 2(\alpha|\beta) + (\beta|\beta)\\
        &amp; = \norm{\alpha}^2 + 2(\Re (\alpha|\beta) + \Im (\alpha | \beta)) +
        \norm{\beta}^2,
        \end{aligned}$$</span> but <span class="math inline">$\norm{\alpha \pm
        \beta}^2$</span> has to be real so only the real part of the <span
        class="math inline">(<em>α</em>|<em>β</em>)</span> is left.</p>
  </div>
  <h2 id="inner-product-spaces-1">Inner Product Spaces</h2>
  <div class="definition">
    <p><strong>Definition 7.4</strong>. An <strong>inner product
        space</strong> is a real or complex vector space, together with a
      specified inner product on that space.</p>
    <p>A finite-dimensional real inner product space is often called a
      <strong>Euclidean space</strong>. A complex inner product space is often
      referred to as a <strong>unitary space</strong>.
    </p>
  </div>
  <div class="thm">
    <p><strong>Theorem 7.1</strong>. <em>If <span class="math inline"><em>V</em></span> is an inner product space, then
        for any vectors <span class="math inline"><em>α</em>, <em>β</em></span>
        in <span class="math inline"><em>V</em></span> and any scalar <span class="math inline"><em>c</em></span>:</em>
    </p>
    <ol>
      <li>
        <p><em><span class="math inline">$\norm{c \alpha} = |c|
              \norm{\alpha}$</span>;</em></p>
      </li>
      <li>
        <p><em><span class="math inline">$\norm{\alpha} &gt; 0$</span> for
            <span class="math inline"><em>α</em> ≠ 0</span>;</em></p>
      </li>
      <li>
        <p><em><span class="math inline">$|(\alpha|\beta)| \leq
              \norm{\alpha} \norm{\beta}$</span>;</em></p>
      </li>
      <li>
        <p><em><span class="math inline">$\norm{\alpha + \beta} \leq
              \norm{\alpha} + \norm{\beta}$</span>.</em></p>
      </li>
    </ol>
  </div>
  <div class="proof">
    <p><em>Proof.</em> Statements (i) and (ii) follow almost immediately
      from the various definitions involved.</p>
    <p>The inequality in (iii) is clearly valid when <span class="math inline"><em>α</em> = 0</span>. If <span
        class="math inline"><em>α</em> ≠ 0</span>, put <span class="math display">$$\gamma=\beta-\frac{(\beta \mid
        \alpha)}{\|\alpha\|^2} \alpha .$$</span> Then <span class="math inline">$(\gamma \mid \alpha)=(\gamma \mid
        \beta) -
        \frac{(\beta \mid \alpha)}{\|\alpha\|^2} (\gamma \mid \alpha) =
        0$</span> and <span class="math display">$$\begin{aligned}
        0 \leq\|\gamma\|^2 &amp; =\left(\beta-\frac{(\beta \mid
        \alpha)}{\|\alpha\|^2} \alpha \mid \beta-\frac{(\beta \mid
        \alpha)}{\|\alpha\|^2} \alpha\right) \\
        &amp; =(\beta \mid \beta)-\frac{(\beta \mid \alpha)(\alpha \mid
        \beta)}{\|\alpha\|^2} \\
        &amp; =\|\beta\|^2-\frac{\|\left.(\alpha \mid
        \beta)\right|^2}{\|\alpha\|^2}
        \end{aligned}$$</span> Hence <span
        class="math inline">|(<em>α</em>∣<em>β</em>)|<sup>2</sup> ≤ ∥<em>α</em>∥|<sup>2</sup>∥<em>β</em>∥<sup>2</sup></span>.
      Now using (c) we find that <span class="math display">$$\begin{aligned}
        \|\alpha+\beta\|^2 &amp; =\|\alpha\|^2+(\alpha \mid \beta)+(\beta \mid
        \alpha)+\|\beta\|^2 \\
        &amp; =\|\alpha\|^2+2 \operatorname{Re}(\alpha \mid \beta)+\|\beta\|^2
        \\
        &amp; \leq\|\alpha\|^2+2\|\alpha\|\|\beta\|+\|\beta\|^2 \\
        &amp; =(\|\alpha\|+\|\beta\|)^2 .
        \end{aligned}$$</span> Thus, <span
        class="math inline">∥<em>α</em> + <em>β</em>∥ ≤ ∥<em>α</em>∥ + ∥<em>β</em>∥</span>. ◻</p>
  </div>
  <p>The inequality in (iii) is called the <strong>Cauchy-Schwarz
      inequality</strong>. It has a wide variety of applications. The proof
    shows that if (for example) <span class="math inline"><em>α</em></span>
    is non-zero, then <span class="math inline">|(<em>α</em>∣<em>β</em>)| &lt; ∥<em>α</em>∥∥<em>β</em>∥</span>
    unless <span class="math inline"><em>γ</em> = 0</span>, or <span class="math display">$$\beta=\frac{(\beta \mid
      \alpha)}{\|\alpha\|^2}
      \alpha .$$</span> Thus, equality occurs in (iii) if and only if <span class="math inline"><em>α</em></span> and
    <span class="math inline"><em>β</em></span> are linearly dependent.</p>
  <div class="proof">
    <p><em>Proof.</em> Now suppose that <span class="math inline"><em>W</em></span> is a finite-dimensional subspace
      of <span class="math inline"><em>V</em></span>. Then we know, as a
      corollary of Theorem 3, that <span class="math inline"><em>W</em></span>
      has an orthogonal basis. Let <span
        class="math inline">{<em>α</em><sub>1</sub>,…,<em>α</em><sub><em>n</em></sub>}</span>
      be any orthogonal basis for <span class="math inline"><em>W</em></span>
      and define <span class="math inline"><em>α</em></span> by (8-11). Then,
      by the computation in the proof of Theorem <span class="math inline">3, <em>β</em> − <em>α</em></span> is
      orthogonal to
      each of the vectors <span class="math inline"><em>α</em><sub><em>k</em></sub>(<em>β</em> − <em>α</em></span>
      is the vector obtained at the last stage when the orthogonalization
      process is applied to <span
        class="math inline"><em>α</em><sub>1</sub>, …, <em>α</em><sub><em>n</em></sub>, <em>β</em></span>
      ). Thus <span class="math inline"><em>β</em> − <em>α</em></span> is
      orthogonal to every linear combination of <span
        class="math inline"><em>α</em><sub>1</sub>, …, <em>α</em><sub><em>n</em></sub></span>,
      i.e., to every vector in <span class="math inline"><em>W</em></span>. If
      <span class="math inline"><em>γ</em></span> is in <span class="math inline"><em>W</em></span> and <span
        class="math inline"><em>γ</em> ≠ <em>α</em></span>, it follows that
      <span class="math inline">∥<em>β</em> − <em>γ</em>∥ &gt; ∥<em>β</em> − <em>α</em>∥</span>.
      Therefore, <span class="math inline"><em>α</em></span> is the best
      approximation to <span class="math inline"><em>β</em></span> that lies
      in <span class="math inline"><em>W</em></span>. ◻
    </p>
  </div>
  <div class="definition">
    <p><strong>Definition 7.5</strong>. Let <span class="math inline"><em>V</em></span> be an inner product space and
      <span class="math inline">S</span> any set of vectors in <span class="math inline"><em>V</em></span>. The
      <strong>orthogonal
        complement</strong> of <span class="math inline">S</span> is the set
      <span class="math inline">S<sup>⊥</sup></span> of all vectors in <span class="math inline"><em>V</em></span> which
      are orthogonal to every
      vector in <span class="math inline">S</span>.
    </p>
  </div>
  <p>The orthogonal complement of <span class="math inline"><em>V</em></span> is the zero subspace, and
    conversely <span class="math inline">{0}<sup>⊥</sup> = <em>V</em></span>.</p>
  <p>If <span class="math inline"><em>S</em></span> is any subset of <span class="math inline"><em>V</em></span>, its
    orthogonal complement <span class="math inline"><em>S</em><sup>⊥</sup></span> (S perp) is always a
    subspace of <span class="math inline"><em>V</em></span>. For <span class="math inline"><em>S</em></span> is
    non-empty, since it contains 0
    ; and whenever <span class="math inline"><em>α</em></span> and <span class="math inline"><em>β</em></span> are in
    <span class="math inline"><em>S</em><sup>⊥</sup></span> and <span class="math inline"><em>c</em></span> is any
    scalar, <span class="math display">$$\begin{aligned}
      (c \alpha+\beta \mid \gamma) &amp; =c(\alpha \mid \gamma)+(\beta \mid
      \gamma) \\
      &amp; =c 0+0 \\
      &amp; =0
      \end{aligned}$$</span> for every <span class="math inline"><em>γ</em></span> in <span
      class="math inline"><em>S</em></span>, thus <span class="math inline"><em>c</em><em>α</em> + <em>β</em></span>
    also lies
    in <span class="math inline"><em>S</em><sup>⊥</sup></span>.</p>
  <p>In Theorem 4 the characteristic property of the vector <span class="math inline"><em>α</em></span> is that it is
    the only vector in
    <span class="math inline"><em>W</em></span> such that <span class="math inline"><em>β</em> − <em>α</em></span>
    belongs to <span class="math inline"><em>W</em><sup>⊥</sup></span>.
  </p>
  <div class="definition">
    <p><strong>Definition 7.6</strong>. Whenever the vector <span class="math inline"><em>α</em></span> in Theorem 4
      exists it is called
      the <strong>orthogonal projection of <span class="math inline"><em>β</em></span> on <span
          class="math inline">W</span></strong>. If every vector in <span class="math inline"><em>V</em></span> has an
      orthogonal projection on
      <span class="math inline">W</span>, the mapping that assigns to each
      vector in <span class="math inline"><em>V</em></span> its orthogonal
      projection on <span class="math inline">W</span> is called the
      <strong>orthogonal projection of <span class="math inline"><em>V</em></span> on <span
          class="math inline">W</span></strong>.
    </p>
  </div>
  <p>By Theorem 4 , the orthogonal projection of an inner product space on
    a finite-dimensional subspace always exists. But Theorem 4 also implies
    the following result.</p>
  <div class="corollary">
    <p><strong>Corollary 7.1</strong>. <em>Let <span class="math inline"><em>V</em></span> be an inner product space, W
        a
        finite-dimensional subspace, and <span class="math inline">E</span> the
        orthogonal projection of <span class="math inline"><em>V</em></span> on
        <span class="math inline">W</span>. Then the mapping <span
          class="math display"><em>β</em> → <em>β</em> − <em>E</em><em>β</em></span>
        is the orthogonal projection of <span class="math inline"><em>V</em></span> on <span
          class="math inline">W<sup>⊥</sup></span>.</em></p>
  </div>
  <div class="proof">
    <p><em>Proof.</em> Let <span class="math inline"><em>β</em></span> be an
      arbitrary vector in <span class="math inline"><em>V</em></span>. Then
      <span class="math inline"><em>β</em> − <em>E</em><em>β</em></span> is in
      <span class="math inline"><em>W</em><sup>⊥</sup></span>, and for any
      <span class="math inline"><em>γ</em></span> in <span
        class="math inline"><em>W</em><sup>⊥</sup>, <em>β</em> − <em>γ</em> = <em>E</em><em>β</em> + (<em>β</em>−<em>E</em><em>β</em>−<em>γ</em>)</span>.
      Since <span class="math inline"><em>E</em><em>β</em></span> is in <span class="math inline"><em>W</em></span> and
      <span class="math inline"><em>β</em> − <em>E</em><em>β</em> − <em>γ</em></span>
      is in <span class="math inline"><em>W</em><sup>⊥</sup></span>, it
      follows that ◻
    </p>
  </div>
  <section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes">
    <hr />
    <ol>
      <li id="fn1">
        <p>Notice that (c) means this function evaluated at c, it
          is not multiplying by c<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p>
      </li>
    </ol>
  </section>
</body>

</html>