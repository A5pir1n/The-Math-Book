\documentclass{book}
\usepackage[utf8]{inputenc}
\usepackage{amsmath, amsthm, amsfonts}
\usepackage{physics} %This gives the option to use \norm{}
\usepackage{multirow} %allows merging rows

%This gives and option to combine \textbf{\textsc{...}}
\usepackage[T1]{fontenc}

\usepackage[shortlabels]{enumitem}
% This command get rid of the first figure of the section number coming from the chapter. For example, changes chapter2 section 1 from 2.1 to 1. 
\renewcommand{\thesection}{\arabic{section}}

% \renewcommand{\thesubsection}{\arabic{subsection}}

\usepackage{minted}
\usemintedstyle{autumn}


\usepackage{subfiles}

\newtheorem{thm}{Theorem}[section]
\newtheorem{prop}{Proposition}[section]
\newtheorem{corollary}{Corollary}[theorem]
\newtheorem{lemma}{Lemma}[theorem]



\theoremstyle{remark}
\newtheorem*{note}{Note}
\newtheorem{remark}{Remark}[section]
\newtheorem{example}{Example}[section]

\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]

\newcommand{\sgn}{\text{sgn\,}}
\newcommand{\inv}[1]{{#1}^{-1}}

\usepackage{subfiles}
\title{Linear Algebra}
\author{Hongyu}
\date{\today}


\begin{document}

\chapter{Equivalence Theorems}
\section{Basis}
The following statements are equivalent:
\begin{itemize}
    \item A set $\mathcal{B}$ is a basis for a vector space $V$
    \item The spanning set of the vector space $V$     does not have strict subset that is also a spanning set
    \item The set $B$ does not have any strict superset that is also linearly independent. 
\end{itemize}
\subsection{Finite dimensional vector space}
\begin{enumerate}
    \item A set $\mathcal{B} = \{\alpha_1, \alpha_2 \cdots \alpha_n \}$ of $n$ elements is a basis for a $n$ dimensional vector space $V$. 
    \item \label{span} The set $\mathcal{B} = \{\alpha_1, \alpha_2 \cdots \alpha_n \}$ of $n$ elements spans the $n$ dimensional vector space $V$. 
\end{enumerate}


\section{Square matrices}

\noindent For all $A \in M_{n\times n}$, the following statements are equivalent:
\begin{enumerate}
\item\label{nonsing} $A$ is nonsingular;
\item\label{identity} $\RREF(A) = I_n$
\item\label{unique solution} for all $\mathbf{b} \in M_{n\times 1}$, the linear system $A \mathbf{x} = \mathbf{b}$ has a unique solution;
\item\label{homogeneous} the homogeneous linear system $A \mathbf{x} = \mathbf{0}$ has only the trivial solution $\mathbf{x} = \mathbf{0}$;
\item\label{elementary} $A$ is a product of elementary matrices;
\item\label{no zero row} $A$ is not row equivalent to a matrix with a row (or column) of zeros;
\item\label{det nonzero} $\det(A) \not = 0$;
\item\label{null_space} $\mathrm{nullspace}(A) = \{\mathbf{0}\}$;
\item\label{cols A} the columns of $A$ are linearly independent;
\item\label{nullity A} $\nullity(A) = 0$;
\item\label{rank A} $\rank(A) = n$;
\item\label{row rank} $\rrank(A) = n$;
\item\label{col rank} $\crank(A) = n$;
\item\label{rows A} the rows of $A$ are linearly independent;
\item\label{isomorphism} $A$ represents an isomorphism $L:\RR^n \to \RR^n$;
\item\label{one-to-one} $A$ represents a linear transformation $L:\RR^n \to \RR^n$ that is one-to-one;
\item\label{onto} $A$ represents a linear transformation $L:\RR^n \to \RR^n$ that is onto;
\item \label{operator-invertible} $A$ represents an invertible linear transformation $L: V \to W$ where $\dim(V) = \dim(W) = n$;
\item \label{transf-iso} $A$ represents an isomorphism $L:V \to W$ where $\dim(V) = \dim(W) = n$;
\item \label{transf-onto} $A$ represents an onto linear transformation $L: V \to W$ where $\dim(V) = \dim(W) = n$;
\item \label{transf-one-to-one} $A$ represents a one-to-one linear transformation $L: V \to W$ where $\dim(V) = \dim(W) = n$;
\item \label{eigen} $\lambda = 0$ is not an eigenvalue of $A$.
\item The matrix $A$ can be brought to diagonal form by a change of basis, which is same as saying that the matrix is diagonalizable. 
\item The eigenvectors of the matrix span the space.  
\end{enumerate}


\chapter{Vector Spaces}
\subfile{Math/Algebra/linear_algebra/vector_spaces.tex}




\chapter{Linear Transformations(maps)}
\section{Linear Transformations}
\begin{definition}
Let $V$ and $W$ to be vector spaces over the field $F$. A \textbf{linear transformation (map) from $V$ into $W$} is a function $T$ from $V$ into $W$ such that 
\begin{equation}
    T(c\alpha + \beta) = c(T\alpha) + T\beta
\end{equation}
for all $\alpha$ and $\beta$ in $V$ and all scalars $c$ in $F$. 
\end{definition}

\begin{example}If $V$ is any vector space, the \textbf{identity transformation} $I$, defined by $I \alpha=\alpha$, is a linear transformation from $V$ into $V$. The \textbf{zero transformation} 0 , defined by $0 \alpha=0$, is a linear transformation from $V$ into $V$.
\end{example}
\begin{example}Let $F$ be a field and let $V$ be the space of polynomial functions $f$ from $F$ into $F$, given by
Let
$$
f(x)=c_0+c_1 x+\cdots+c_k x^k
$$
$$
(D f)(x)=c_1+2 c_2 x+\cdots+k c_k x^{k-1} .
$$
Then $D$ is a linear transformation from $V$ into $V$-the \textbf{differentiation transformation.}
\end{example}

\chapter{Polynomials}
\section{Polynomial Ideals}
\begin{thm}
    \textbf{(Taylor's formula)}
\end{thm}
\begin{proof}
    p130 proof: 

\begin{align}
    &= \sum_m {\sum_k{\frac{D^k x^m}{k!}(c)(x-c)^k}} \\
    &= \sum_m {\sum_k{\frac{m! c^{m-k}}{k!}(x-c)^k}}
\end{align}
\footnote{Notice that (c) means this function evaluated at c, it is not multiplying by c}
\end{proof}
\chapter{Determinants}
\section{Determinant functions}

\section{Permutations and Uniqueness of Determinants}
A permutation of degree $n$, which pass the sequence $(1, 2, \cdots, n)$ to the sequence $(\sigma 1, \sigma 2, \cdots, \sigma n )$, is done by succession of interchanges of pairs. The number of interchanges is either always even or always odd. Then the permutation is then called \textbf{even} or \textbf{odd}, respectively. 
We assign a \textbf{sign} to a permutation by
\begin{equation}
    \sgn \sigma = \begin{cases}
     \hphantom{-}1, & \text{if } \sigma \text{ is even} \\
    -1, & \text{if } \sigma \text{ is odd} 
    \end{cases}
\end{equation}

\section{Determinant functions}
\begin{definition}
Let $K$ be a commutative ring with identity, $n$ a positive integer, and let $D$ be a function which assigns to each $n \times n$ matrix $A$ over $K$ a scalar $D(A)$ in $K$. We say that $D$ is a \textbf{n-linear} if for each $i$, $1 \leq i \leq n$, $D$ is a linear function of the $i$th row when the other $(n - 1)$ rows are held fixed. 
\end{definition}

To clarity this definition: if $D$ is a function from $K^{n \times n}$ to $K$, and let $\alpha_1, \cdots, \alpha_n$ be the rows of the matrix $A$. We can write 
\begin{equation*}
    D(A) = D(\alpha_1, \cdots, \alpha_n),
\end{equation*}
that is, let us also think of D as the function of the rows of A. The statement that D is n-linear means that
\begin{equation}\label{n-linear_function}
    D(\alpha_1,\cdots, c\alpha_i + \alpha_i', \cdots, \alpha_n) = cD(\alpha_1,\cdots, c\alpha_i, \cdots, \alpha_n) + D(\alpha_1,\cdots, \alpha_i', \cdots, \alpha_n). 
\end{equation}

If we fix all rows except row $i$ and regard $D$ as a function of the $i$th row, it is often convenient to write $D(a_i)$ for $D(A)$ . Thus, we may abbreviate (\ref{n-linear_function}) to
\begin{equation*}
D(c\alpha_i + \alpha_i') = cD(\alpha_i) + D(\alpha_i')
\end{equation*}

\begin{thm}Let $\mathrm{K}$ be a commutative ring with identity, and let $\mathrm{A}$ and $\mathrm{B}$ be $\mathrm{n} \times \mathrm{n}$ matrices over $\mathrm{K}$. Then
$$
\operatorname{det}(\mathrm{AB})=(\operatorname{det} \mathrm{A})(\operatorname{det} \mathrm{B}).
$$
\end{thm}
Proof. Let $B$ be a fixed $n \times n$ matrix over $K$, and for each $n \times n$ matrix $A$ define $D(A)=\operatorname{det}(A B)$. If we denote the rows of $A$ by $\alpha_1, \ldots$, $\alpha_n$, then
$$
D\left(\alpha_1, \ldots, \alpha_n\right)=\operatorname{det}\left(\alpha_1 B, \ldots, \alpha_n B\right) .
$$
Here $\alpha_j B$ denotes the $1 \times n$ matrix which is the product of the $1 \times n$ matrix $\alpha_j$ and the $n \times n$ matrix $B$. Since
$$
\left(c \alpha_i+\alpha_i^{\prime}\right) B=c \alpha_i B+\alpha_1^{\prime} B
$$
and det is $n$-linear, it is easy to see that $D$ is $n$-linear. If $\alpha_i=\alpha_j$, then $\alpha_i B=\alpha_j B$, and since det is alternating,
$$
D\left(\alpha_1, \ldots, \alpha_n\right)=0 .
$$
Hence, $D$ is alternating. Now $D$ is an alternating $n$-linear function, and by Theorem 2
$$
D(A)=(\operatorname{det} A) D(I) \\
$$
But $D(I)=\operatorname{det}(I B)=\operatorname{det} B$, so
$$
\operatorname{det}(A B)=D(A)=(\operatorname{det} A)(\operatorname{det} B)\\
$$


\section{Properties of Determinants}
\begin{note}
A non-square matrix does not have determinant (find out why)
\end{note}


In this section, we list out some useful properties of the determinant function on $n \times n $ matrices. 

When defining our determinant of a matrix $A$, the rows of $A$ has played a privileged role, but we can just as well define the determinant using the columns of $A$ by symmetry. So, we might expect that $\det A$ is an alternating (hyperlink) $n$-linear function (hyperlink) of columns of $A$. Indeed, we see: 
\begin{prop}
\begin{equation}\label{determinant_of_transpose}
    \det (A^T) = \det(A)
\end{equation} 
where $A^T$ is the transpose of $A$.
\end{prop}
\begin{proof}
Let $\sigma$ be a permutation of degree $n$,
\begin{equation}
    A^T (i, \sigma i) = A (\sigma i, i).
\end{equation}
We know from \cite{} that 
\begin{equation}
    \det (A^T) = \sum_\sigma \, (\sgn \sigma) A(\sigma 1, 1) \cdots A(\sigma n, n).
\end{equation}
Since $\sigma \sigma ^{-1}$ is the identity permutation, it means they are either both even or both odd: 
\begin{equation}
    \begin{cases}
     \sigma \sigma^{-1} = 1 \Rightarrow &(\sgn \sigma)(\sgn \sigma^{-1}) = 1\\
    \text{or} & \sgn (\sigma ^{-1}) = \sgn (\sigma).
    \end{cases}
\end{equation}
Also when $\sigma i = j \Rightarrow i = \sigma^{-1}j$, $A(\sigma i, i) = A(j, \sigma^{-1}j)$. Thus,
\begin{equation}
    A(\sigma 1, 1) \cdots A(\sigma n, n) = A(1, \sigma^{-1}1) \cdots A(\n, \sigma^{-1} n). 
\end{equation}
Finally, since $\sigma$ varies over all permutations of degree $n$, so does $\sigma^{-1}$. Therefore, 
\begin{align}
    \det (A^T) &= \sum_\sigma \, (\sgn \inv{\sigma}) A(1, \inv{\sigma}1) \cdots A(n, \inv{\sigma}n)\\
    &= \det A
\end{align}
proving equation \ref{determinant_of_transpose}.
\end{proof}


\begin{example}
Let $K$ be the ring of integers and
\begin{equation*}
    A = 
    \begin{bmatrix}
        1 & 2\\
        3 & 4
    \end{bmatrix}
\end{equation*}

\end{example}

\section{Modules}
\begin{definition}
If $K$ is a commutative ring with identity, a module over $K$ is an algebraic system which behaves like a vector space, with $K$ playing the role of the scalar field. To be precise, we say that $V$ is a \textbf{module over} $K$ (or a $K$-\textbf{module}) if 
\begin{enumerate}
    \item there is an addition $(\alpha, \beta) \rightarrow \alpha + \beta$ on $V$, under which $V$ is a commutative group;
    \item there is multiplication $(c, \alpha) \rightarrow c\alpha$ of elements $\alpha$ in $V$ and $c$ in $K$ such that
    \begin{align*}
        (c_1 + c_2) \alpha &= c_1 \alpha + c_2 \alpha\\
        c(\alpha_1 + \alpha_2) &= c\alpha_1 + c\alpha_2\\
        (c_1 c_2)\alpha &= c_1(c_2 \alpha)\\
        1 \alpha &= \alpha
    \end{align*}
\end{enumerate}
\end{definition}
\chapter{Elementary Canonical Forms}
\section{Eigenvalues}
\begin{definition}
    Let $V$ be a vector space over the field $F$ and let $T$ be a linear operator on V. An \textbf{eigenvalue} of $T$ is a scalar $c$ in $F$ such that there is a non-zero vector $\alpha$ in $V$ with $T \alpha=c \alpha$. If $c$ is a  eigenvalue of $T$, then
    \begin{enumerate}[(a)]
        \item any $\alpha$ such that $T \alpha=c \alpha$ is called a eigenvector of $T$ associated with the eigenvalue $c$;
        \item the collection of all $\alpha$ such that $T \alpha=c \alpha$ is called the eigenspace associated with $c$.
    \end{enumerate}


\end{definition}


Eigenvalues are often called characteristic roots, latent roots, characteristic values, proper values, or spectral values. 

To see the definition in another way, if $T$ is any linear operator and $c$ is any scalar, the set of vectors $\alpha$ such that $T \alpha=c \alpha$ forms a subspace of $V$. 

\begin{proof}
    We already have the field $F$ of scalars and set $V$ of vectors. Now we need to check if the rules of addition and multiplication result a new vector in the subspace. Suppose two vectors $\alpha$ and $\beta$ in the characteristic space: $T\alpha = c\alpha$ and $T\beta = c\beta$. Then $T(\alpha + \beta) = T\alpha + T\beta = c\alpha + c\beta = c(\alpha + \beta)$. Likewise: $T(d\alpha) = d T(\alpha) = dc\alpha = cd\alpha$. 
\end{proof}
Now $(T - cI)\alpha = 0$, so this subspace is the null space of the linear trans-formation $(T-c I)$. Following the definition, we call $c$ a characteristic value of $T$ if this subspace is different from the zero subspace. This brings us to the negation of our equivalence theorem \ref{null_space}. But when $c$ is the eigenvalue of $T$, $0$ is the eigenvalue of $(T - cI)$, and therefore number \ref{eigen} of the equivalence theorem. We summarize some of the important equivalence theorem here: 
\begin{thm}
    Let $\mathrm{T}$ be a linear operator on a finite-dimensional space $\mathrm{V}$ and let $\mathrm{c}$ be a scalar. The following are equivalent:
    \begin{enumerate}[1.]
        \item $\mathrm{c}$ is a eigenvalue of $\mathrm{T}$.
        \item The operator $\mathrm{(T - cI)}$ is singular (not invertible).
        \item $\operatorname{det} \,(\mathrm{T}-\mathrm{cI})=0$.
    \end{enumerate}
\end{thm}

We can use the determinant criterion (iii) to calculate the eigenvalues of $T$. Since $\det (T-c I)$ is a polynomial of degree $n$ in the variable $c$, eigenvalues are the roots of that polynomial, hence the name characteristic roots. To write it as a polynomial, we first represent $T$ by a matrix $A$ in a ordered basis in $V$, $\mathcal{B}$: $A = [T]_\mathcal{B}$. We then use a polynomial $f(x) = \det (A - xI)$ and solve $f(x) = 0$ for the characteristic roots, which happens when $x = c$. This is why we call this polynomial $f(x)$ the \textbf{characteristic polynomial}.

It is important to note that $f$ is a monic polynomial which has degree exactly $n$. This is easily seen from the formula for the determinant of a matrix in terms of its entries. Thus, by the fundamental theorem of linear algebra, it has $n$ complex roots. But some of these can be multiple roots, so we can say that $T$ has at most $n$ distinct eigenvalues. Depending how we define $V$, we might have at least one root or no root. In particular, if $V$ is a complex vector space , $T$ has at least one root. However, if $V$ is a real vector space, $T$ may not have nay eigenvalues. 

After finding eigenvalues, we can simply plug them into the equation $T\alpha = \lambda \alpha$ to solve for eigenvectors. 

Lemma. Similar matrices have the same characteristic polynomial. Proof. If $B=P^{-1} A P$, then
$$
\begin{aligned}
\operatorname{det}(x I-B) & =\operatorname{det}\left(x I-P^{-1} A P\right) \\
& =\operatorname{det}\left(P^{-1}(x I-A) P\right) \\
& =\operatorname{det} P^{-1} \cdot \operatorname{det}(x I-A) \cdot \operatorname{det} P \\
& =\operatorname{det}(x I-A) .
\end{aligned}
$$
This lemma enables us to define sensibly the characteristic polynomial of the operator $T$ as the characteristic polynomial of any $n \times n$ matrix which represents $T$ in some ordered basis for $V$. Just as for matrices, the eigenvalues of $T$ will be the roots of the characteristic polynomial for $T$. 

\begin{example}
    
Let $T$ be the linear operator on $\mathbb{R}^2$ which is represented in the standard ordered basis by the matrix
$$
A=\left[\begin{array}{rr}
0 & -1 \\
1 & 0
\end{array}\right] \text {. }
$$
The characteristic polynomial for $T$ (or for $A$ ) is
$$
f(x) = \operatorname{det}(x I-A)=\left|\begin{array}{rr}
x & 1 \\
-1 & x
\end{array}\right|=x^2+1
$$
Since this polynomial has no real roots, $T$ has no characteristic values. If $U$ is the linear operator on $\mathbb{C}^2$ which is represented by $A$ in the standard ordered basis, then $U$ has two eigenvalues, $i$ and $-i$. Here we see a subtle point. In discussing the characteristic values of a matrix $A$, we must be careful to stipulate the field involved. The matrix $A$ above has no eigenvalues in $R$, but has the two eigenvalues $i$ and $-i$ in $\mathbb{C}$.
\end{example} 

\begin{example}
    Let $A$ be a matrix in $\mathbb{R}^3$:
$$
\left[\begin{array}{rrr}
3 & 1 & -1 \\
2 & 2 & -1 \\
2 & 2 & 0
\end{array}\right]
$$
Then the characteristic polynomial for $A$ is
$$
\left|\begin{array}{ccc}
x-3 & -1 & 1 \\
-2 & x-2 & 1 \\
-2 & -2 & x
\end{array}\right|=x^3-5 x^2+8 x-4=(x-1)(x-2)^2
$$
Thus the characteristic values of $A$ are $\lambda_1 = 1$ and $\lambda_2 = 2$.
Suppose that $T$ is the linear operator on $R^3$ which is represented by $A$ in the standard basis. Let us find the characteristic vectors. Call the components of the eigenvector associated with the eigenvalues 1, $\alpha_1 = \left(a_1, a_2, a_3\right)$; Now

$$
\left[\begin{array}{rrr}
3 & 1 & -1 \\
2 & 2 & -1 \\
2 & 2 & 0
\end{array}\right]
\left[\begin{array}{l}
a_1 \\
a_2 \\
a_3
\end{array}\right]=
1
\left[\begin{array}{l}
a_1 \\
a_2 \\
a_3
\end{array}\right]
$$
which yields three equations:
$$
\begin{aligned}
2 a_1 + a_2 -  a_3 & =0 \\
2 a_1 + a_2 -  a_3 & =0 \\
2 a_1 + 2a_2 -  a_3 & =0.
\end{aligned}
$$
Subtracting the first and the third equation yields $a_2 = 0$. Substituting it to the first equation yields $2a_1 = a_3$. 
Now we have the solution: 
$$
\alpha_1=a_1\left(\begin{array}{l}
1 \\
0 \\
2
\end{array}\right), \quad \text { for } \lambda_1=0.
$$

Now for the eigenvalue $\lambda_2$, 
$$
\left[\begin{array}{rrr}
3 & 1 & -1 \\
2 & 2 & -1 \\
2 & 2 & 0
\end{array}\right]
\left[\begin{array}{l}
a_1 \\
a_2 \\
a_3
\end{array}\right]=
2
\left[\begin{array}{l}
a_1 \\
a_2 \\
a_3
\end{array}\right]
$$
which yields three equations:
$$
\begin{aligned}
1 a_1 + a_2 -  a_3 & =0 \\
2 a_1 + a_3 & =0 \\
2 a_1 + 2a_2 -  2a_3 & =0.
\end{aligned}
$$
We similarly have the solution:
$$
\alpha_1=
a_1\left(\begin{array}{l}
1 \\
1 \\
2
\end{array}\right) = 
\left(\begin{array}{l}
1 \\
1 \\
2
\end{array}\right), \text{if we choose } a_1 = 1.
$$

\end{example}


\begin{example}
    Let matrix $A$ be a matrix in $\mathbb{C}^3$:
$$
A=\left(\begin{array}{ccc}
2 & 0 & -2 \\
-2 i & i & 2 i \\
1 & 0 & -1
\end{array}\right)
$$
The characteristic equation is
$$
\left|\begin{array}{ccc}
(2-x) & 0 & -2 \\
-2 i & (i-x) & 2 i \\
1 & 0 & (-1-x)
\end{array}\right|=-x^3+(1+i) x^2-i x=0
$$
and its roots are 0, 1, and $i$. Call the components of the first eigenvector $\alpha_1 = \left(a_1, a_2, a_3\right)$; then
$$
\left(\begin{array}{ccc}
2 & 0 & -2 \\
-2 i & i & 2 i \\
1 & 0 & -1
\end{array}\right)\left(\begin{array}{l}
a_1 \\
a_2 \\
a_3
\end{array}\right)=0\left(\begin{array}{l}
a_1 \\
a_2 \\
a_3
\end{array}\right)=\left(\begin{array}{l}
0 \\
0 \\
0
\end{array}\right)
$$
which yields three equations:
$$
\begin{aligned}
2 a_1-2 a_3 & =0 \\
-2 i a_1+i a_2+2 i a_3 & =0 \\
a_1-a_3 & =0 .
\end{aligned}
$$
The first determines $a_3$ (in terms of $a_1$ ): $a_3=a_1 ;$ the second determines $a_2: a_2=0$; and the third is redundant. Now we have the solution: 
$$
\alpha_1=a_1\left(\begin{array}{l}
1 \\
0 \\
1
\end{array}\right), \quad \text { for } \lambda_1=0.
$$
But we can just pick any $a_1$, we might as we just pick $a_1 = 1$: $$
\alpha_1=\left(\begin{array}{l}
1 \\
0 \\
1
\end{array}\right)$$
For the second eigenvector (recycling the same notation for the components) we have
$$
\left(\begin{array}{ccc}
2 & 0 & -2 \\
-2 i & i & 2 i \\
1 & 0 & -1
\end{array}\right)\left(\begin{array}{l}
a_1 \\
a_2 \\
a_3
\end{array}\right)=1\left(\begin{array}{l}
a_1 \\
a_2 \\
a_3
\end{array}\right)=\left(\begin{array}{l}
a_1 \\
a_2 \\
a_3
\end{array}\right) .
$$
which leads to the equations
$$
\begin{aligned}
2 a_1-2 a_3 & =a_1, \\
-2 i a_1+i a_2+2 i a_3 & =a_2 . \\
a_1-a_3 & =a_3 .
\end{aligned}
$$
with the solution $a_3=(1 / 2) a_1, a_2=[(1-i) / 2] a_1$; this time I'll pick $a_1=2$, so
$$
\alpha_2=\left(\begin{array}{c}
2 \\
1-i \\
1
\end{array}\right), \quad \text { for } \lambda_2=1 \text {. }
$$
Finally, for the third eigenvector,
$$
\left(\begin{array}{ccc}
2 & 0 & -2 \\
-2 i & i & 2 i \\
1 & 0 & -1
\end{array}\right)\left(\begin{array}{l}
a_1 \\
a_2 \\
a_3
\end{array}\right)=i\left(\begin{array}{l}
a_1 \\
a_2 \\
a_3
\end{array}\right)=\left(\begin{array}{c}
i a_1 \\
i a_2 \\
i a_3
\end{array}\right) .
$$
which gives the equations
$$
\begin{aligned}
2 a_1-2 a_3 & =i a_1, \\
-2 i a_1+i a_2+2 i a_3 & =i a_2 . \\
a_1-a_3 & =i a_3 .
\end{aligned}
$$
whose solution is $a_3=a_1=0$, with $a_2$ undetermined. Choosing $a_2=1$, we conclude
$$
\alpha_3=\left(\begin{array}{l}
0 \\
1 \\
0
\end{array}\right), \quad \text { for } \lambda_3=i
$$
\end{example}

\begin{definition}
    Let $T$ be a linear operator on the finite-dimensional space $V$. We say that $T$ is diagonalizable if there is a basis for $V$ each vector of which is a eigenvector of $T$. Or we can equivalently say that $T$ is diagonalizable if the eigenvectors of $T$ span $V$ from our equivalence theorem \ref{span} for finite dimensional vector spaces. 
\end{definition}
\chapter{Inner Product Spaces}
\section{Inner Products}

\begin{definition}
Let $F$ be the field of real numbers or the field of complex numbers, and $V$ a vector space over $F$. An \textbf{inner product} on $V$ is a function which assigns to each ordered pair of vector $\alpha, \beta$ in $V$ a scalar $(\alpha | \beta)$ in $F$ in such a way that for all $\alpha, \beta, \gamma$ in $V$ and all scalars $c$: 
\begin{enumerate}
    \item $(\alpha + \beta | \gamma) = (\alpha | \gamma) + (\beta | \gamma)$;
    \item $(c \alpha | \beta) = c(\alpha | \beta)$;
    \item $(\beta | \alpha) = ( \Bar{\alpha} | \Bar{\beta})$, the bar denoting complex conjugation;
    \item $( \alpha | \alpha ) > 0$ if $\alpha \neq 0$. \\
    \item[] Note that conditions (1), (2) and (3) imply that
    \item $(\alpha | c \beta + \gamma) = \Bar{c} (\alpha | \beta ) + (\alpha + \gamma)$. 
\end{enumerate}
\end{definition}

\begin{example}
On $F^n$ there is an inner product which we call the \textbf{standard inner product}. It is defined on $\alpha = (x_1, \cdots, x_n)$ and $\beta = (y_1, \cdots, y_n)$ by 
\begin{equation}
    (\alpha |\beta ) = \sum_j x_j \Bar{y}_j.
\end{equation}

If $F = \mathbb{R}$, we can write this as 
\begin{equation}
    (\alpha |\beta ) = \sum_j x_j y_j. 
\end{equation}
We call this standard inner product the \textbf{dot product} or \textbf{scalar product} and denote it by $\alpha \cdot \beta$. 
\end{example}

\begin{example}
    For $\alpha=\left(x_1, x_2\right)$ and $\beta=\left(y_1, y_2\right)$ in $R^2$, let
    $$
    (\alpha \mid \beta)=x_1 y_1-x_2 y_1-x_1 y_2+4 x_2 y_2
    $$
    Since $(\alpha \mid \alpha)=\left(x_1-x_2\right)^2+3 x_2^2$, it follows that $(\alpha \mid \alpha)>0$ if $\alpha \neq 0$. Conditions (1), (2), and (3) of the definition are easily verified.
\end{example}

\begin{example}
    Let $V$ be $F^{n \times n}$, the space of all $n \times n$ matrices over $F$. Then $V$ is isomorphic to $F^{n^2}$ in a natural way. It therefore follows from Example 1 that the equation
$$
(A \mid B)=\sum_{j, k} A_{j k} \bar{B}_{j k}
$$



defines an inner product on $V$. Furthermore, if we introduce the con jugate transpose matrix $B^*$, where $B_{k j}^*=\bar{B}_{j k}$, we may express this inner product on $F^{n \times n}$ in terms of the trace function:
$$
(A \mid B)=\operatorname{tr}\left(A B^*\right)=\operatorname{tr}\left(B^* A\right) .
$$
For
$$
\begin{aligned}
\operatorname{tr}\left(A B^*\right) & =\sum_j\left(A B^*\right)_{j j} \\
& =\sum_j \sum_k A_{j k} B_{k j}^* \\
& =\sum_j \sum_k A_{j k} \bar{B}_{j k}
\end{aligned}
$$

\end{example}
\begin{example}
Let $F^{n \times 1}$ be the space of $n \times 1$ (column) matrices over $F$, and let $Q$ be an $n \times n$ invertible matrix over $F$. For $X, Y$ in $F^{n \times 1}$ set
$$
(X \mid Y)=Y^* Q^* Q X
$$
We are identifying the $1 \times 1$ matrix on the right with its single entry. When $Q$ is the identity matrix, this inner product is essentially the same as that in Example 1 ; we call it the standard inner product on $F_{n \times 1}$. The reader should note that the terminology 'standard inner product' is used in two special contexts. For a general finite-dimensional vector space over $F$, there is no obvious inner product that one may call standard.
\end{example}
\example Let $V$ be the vector space of all continuous complex valued functions on the unit interval, $0 \leq t \leq 1$. Let
$$
(f \mid g)=\int_a^b \overline{f(t)} g(t) d t .
$$


Condition 1 and 2:
$$
( f \mid(b|g)+c|h))=\int \overline{f(x)}(b g(x)+c h(x)) d x=b \int \overline{f} g d x+c \int \overline{f} h d x=b( f \mid g)+c( f \mid h) .
$$
Condition 3:
$$
( g \mid f)=\int_a^b \overline{g(x)} f(x) d x=\overline{\left(\int_a^b \overline{f(x)} g(x) d x\right)}=\overline{( f \mid g)} .
$$
Condition 4:
$$(f \mid f) = \int_a^b \abs{f(x)}^2 dx $$,
which is real and non-negative; it is zero only when $f(x) = 0$. 

The reader is probably more familiar with the space of real-valued continuous functions on the unit interval, and for this space the complex conjugate on $g$ may be omitted.

Example 6 . This is really a whole class of examples. One may construct new inner products from a given one by the following method. Let $V$ and $W$ be vector spaces over $F$ and suppose $(\mid)$ is an inner product on $W$. If $T$ is a non-singular linear transformation from $V$ into $W$, then the equation
$$
p_T(\alpha, \beta)=(T \alpha \mid T \beta)
$$
defines an inner product $p_T$ on $V$. The inner product in Example 4 is a special case of this situation. The following are also special cases.
(a) Let $V$ be a finite-dimensional vector space, and let
$$
B=\left\{\alpha_1, \ldots, \alpha_n\right\}
$$


\begin{definition}
We denote the positive square root of $(\alpha|\alpha)$ by $\norm{\alpha}$;
\begin{equation}
    (\alpha|\alpha) = \norm{\alpha}^2
\end{equation}
$\norm{\alpha}$ is called the \textbf{norm} of $\alpha$ with respect to the inner product. We can think of it as a ``length" or ``magnitude" of $\alpha$. 
\end{definition}


\begin{definition}
We can get the \textbf{quadratic form} of the inner product by the following:
We know from the property of the inner product that
\begin{equation}
    \norm{\alpha \pm \beta}^2 = \norm{\alpha}^2 \pm 2 \Re (\alpha | \beta) + \norm{\beta}^2,
\end{equation}
and thus if $(\alpha | \beta)$ is real, 
\begin{equation}
    (\alpha | \beta) = \frac{1}{4} \norm{\alpha + \beta}^2 - \frac{1}{4} \norm{\alpha - \beta}^2,
\end{equation}
and if $(\alpha | \beta)$ is complex: 
\begin{equation}
    (\alpha | \beta) = \frac{1}{4} \norm{\alpha + \beta}^2 - \frac{1}{4} \norm{\alpha - \beta}^2 + \frac{i}{4} \norm{\alpha + i\beta}^2 - \frac{i}{4} \norm{\alpha - i\beta}^2.
\end{equation}
\end{definition}

\begin{note}
Note that
\begin{align*}
\norm{\alpha \pm \beta}^2 &= (\alpha + \beta|\alpha + \beta)\\
&=(\alpha|\alpha) + 2(\alpha|\beta) + (\beta|\beta)\\
& = \norm{\alpha}^2 + 2(\Re (\alpha|\beta) + \Im (\alpha | \beta)) + \norm{\beta}^2,
\end{align*}
but $\norm{\alpha \pm \beta}^2$ has to be real so only the real part of the $(\alpha |\beta)$ is left. 
\end{note}

\section{Inner Product Spaces}
\begin{definition}
    An \textbf{inner product space} is a real or complex vector space, together with a specified inner product on that space. 
    
    A finite-dimensional real inner product space is often called a \textbf{Euclidean space}. A complex inner product space is often referred to as a \textbf{unitary space}.
\end{definition}

\begin{thm}
    If $V$ is an inner product space, then for any vectors $\alpha, \beta$ in $V$ and any scalar $c$:
    \begin{enumerate}
        \item $\norm{c \alpha} = |c| \norm{\alpha}$;
        \item $\norm{\alpha} > 0$ for $\alpha \neq 0$;
        \item $|(\alpha|\beta)| \leq \norm{\alpha} \norm{\beta}$;
        \item $\norm{\alpha + \beta} \leq \norm{\alpha} + \norm{\beta}$.
    \end{enumerate}
\end{thm}
\begin{proof}
Statements (i) and (ii) follow almost immediately from the various definitions involved. 
    
The inequality in (iii) is clearly valid when $\alpha=0$. If $\alpha \neq 0$, put
$$
\gamma=\beta-\frac{(\beta \mid \alpha)}{\|\alpha\|^2} \alpha .
$$
Then $(\gamma \mid \alpha)=(\gamma \mid \beta) - \frac{(\beta \mid \alpha)}{\|\alpha\|^2} (\gamma \mid \alpha) = 0$ and
$$
\begin{aligned}
0 \leq\|\gamma\|^2 & =\left(\beta-\frac{(\beta \mid \alpha)}{\|\alpha\|^2} \alpha \mid \beta-\frac{(\beta \mid \alpha)}{\|\alpha\|^2} \alpha\right) \\
& =(\beta \mid \beta)-\frac{(\beta \mid \alpha)(\alpha \mid \beta)}{\|\alpha\|^2} \\
& =\|\beta\|^2-\frac{\|\left.(\alpha \mid \beta)\right|^2}{\|\alpha\|^2}
\end{aligned}
$$
Hence $|(\alpha \mid \beta)|^2 \leq\left.\|\alpha\|\right|^2\|\beta\|^2$. 
Now using (c) we find that
$$
\begin{aligned}
\|\alpha+\beta\|^2 & =\|\alpha\|^2+(\alpha \mid \beta)+(\beta \mid \alpha)+\|\beta\|^2 \\
& =\|\alpha\|^2+2 \operatorname{Re}(\alpha \mid \beta)+\|\beta\|^2 \\
& \leq\|\alpha\|^2+2\|\alpha\|\|\beta\|+\|\beta\|^2 \\
& =(\|\alpha\|+\|\beta\|)^2 .
\end{aligned}
$$
Thus, $\|\alpha+\beta\| \leq\|\alpha\|+\|\beta\|$.
\end{proof}
The inequality in (iii) is called the \textbf{Cauchy-Schwarz inequality}. It has a wide variety of applications. The proof shows that if (for example) $\alpha$ is non-zero, then $|(\alpha \mid \beta)|<\|\alpha\|\|\beta\|$ unless $\gamma = 0$, or
$$
\beta=\frac{(\beta \mid \alpha)}{\|\alpha\|^2} \alpha .
$$
Thus, equality occurs in (iii) if and only if $\alpha$ and $\beta$ are linearly dependent.


\begin{proof}
    Now suppose that $W$ is a finite-dimensional subspace of $V$. Then we know, as a corollary of Theorem 3, that $W$ has an orthogonal basis. Let $\left\{\alpha_1, \ldots, \alpha_n\right\}$ be any orthogonal basis for $W$ and define $\alpha$ by (8-11). Then, by the computation in the proof of Theorem $3, \beta-\alpha$ is orthogonal to each of the vectors $\alpha_k(\beta-\alpha$ is the vector obtained at the last stage when the orthogonalization process is applied to $\alpha_1, \ldots, \alpha_n, \beta$ ). Thus $\beta-\alpha$ is orthogonal to every linear combination of $\alpha_1, \ldots, \alpha_n$, i.e., to every vector in $W$. If $\gamma$ is in $W$ and $\gamma \neq \alpha$, it follows that $\|\beta-\gamma\|>\|\beta-\alpha\|$. Therefore, $\alpha$ is the best approximation to $\beta$ that lies in $W$.
\end{proof}

\begin{definition}
    Let $V$ be an inner product space and $\mathrm{S}$ any set of vectors in $V$. The \textbf{orthogonal complement} of $\mathrm{S}$ is the set $\mathrm{S}^{\perp}$ of all vectors in $V$ which are orthogonal to every vector in $\mathrm{S}$.
\end{definition}

The orthogonal complement of $V$ is the zero subspace, and conversely $\{0\}^{\perp}=V$. 

If $S$ is any subset of $V$, its orthogonal complement $S^{\perp}$ (S perp) is always a subspace of $V$. For $S$ is non-empty, since it contains 0 ; and whenever $\alpha$ and $\beta$ are in $S^{\perp}$ and $c$ is any scalar,
$$
\begin{aligned}
(c \alpha+\beta \mid \gamma) & =c(\alpha \mid \gamma)+(\beta \mid \gamma) \\
& =c 0+0 \\
& =0
\end{aligned}
$$
for every $\gamma$ in $S$, thus $c \alpha+\beta$ also lies in $S^{\perp}$. 

In Theorem 4 the characteristic property of the vector $\alpha$ is that it is the only vector in $W$ such that $\beta-\alpha$ belongs to $W^{\perp}$.

\begin{definition}Whenever the vector $\alpha$ in Theorem 4 exists it is called the \textbf{orthogonal projection of $\beta$ on $\mathrm{W}$}. If every vector in $V$ has an orthogonal projection on $\mathrm{W}$, the mapping that assigns to each vector in $V$ its orthogonal projection on $\mathrm{W}$ is called the \textbf{orthogonal projection of $V$ on $\mathrm{W}$}.\end{definition}

By Theorem 4 , the orthogonal projection of an inner product space on a finite-dimensional subspace always exists. But Theorem 4 also implies the following result.

\begin{corollary}
    Let $V$ be an inner product space, W a finite-dimensional subspace, and $\mathrm{E}$ the orthogonal projection of $V$ on $\mathrm{W}$. Then the mapping
    $$
    \beta \rightarrow \beta-E \beta
    $$
    is the orthogonal projection of $V$ on $\mathrm{W}^{\perp}$.
\end{corollary}
\begin{proof}
    Let $\beta$ be an arbitrary vector in $V$. Then $\beta-E \beta$ is in $W^{\perp}$, and for any $\gamma$ in $W^{\perp}, \beta-\gamma=E \beta+(\beta-E \beta-\gamma)$. Since $E \beta$ is in $W$ and $\beta-E \beta-\gamma$ is in $W^{\perp}$, it follows that
\end{proof}

\end{document}