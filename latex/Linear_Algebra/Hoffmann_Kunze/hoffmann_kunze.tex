\documentclass[main.tex]{subfiles}
\begin{document}

\title{
LINEAR ALGEBRA
}

\section{Second Edition}

\section{KENNETH HOFFMAN}

\author{
Professor of Mathematics
}

Massachusetts Institute of Technology

\section{RAY KUNZE}

\author{
Professor of Mathematics \\ University of California, Irvine
}

Prentice-Hall, Inc., Englewood Cliffs, New Jersey (C) 1971,1961 by

Prentice-Hall, Inc.

Englewood Cliffs, New Jersey

All rights reserved. No part of this book may be reproduced in any form or by any means without permission in writing from the publisher.

PRENTICE-Hall internatronal, INC., London PRENTICE-HALL OF AUSTRALIA, PTY. LTD., Sydney PRENTICE-Hall of CANADA, LTD., Toronto

PRentice-hall of india private limited, New Delhi PRENTICE-Hall of Japan, Inc., Tokyo

Current printing (last digit):

$\begin{array}{lllll}10 & 9 & 8 & 7 & 6\end{array}$

Library of Congress Catalog Card No. 75-142120

Printed in the United States of America 

\section{Preface}

Our original purpose in writing this book was to provide a text for the undergraduate linear algebra course at the Massachusetts Institute of Technology. This course was designed for mathematics majors at the junior level, although threefourths of the students were drawn from other scientific and technological disciplines and ranged from freshmen through graduate students. This description of the M.I.T. audience for the text remains generally accurate today. The ten years since the first edition have seen the proliferation of linear algebra courses throughout the country and have afforded one of the authors the opportunity to teach the basic material to a variety of groups at Brandeis University, Washington University (St. Louis), and the University of California (Irvine).

Our principal aim in revising Linear Algebra has been to increase the variety of courses which can easily be taught from it. On one hand, we have structured the chapters, especially the more difficult ones, so that there are several natural stopping points along the way, allowing the instructor in a one-quarter or one-semester course to exercise a considerable amount of choice in the subject matter. On the other hand, we have increased the amount of material in the text, so that it can be used for a rather comprehensive one-year course in linear algebra and even as a reference book for mathematicians.

The major changes have been in our treatments of canonical forms and inner product spaces. In Chapter 6 we no longer begin with the general spatial theory which underlies the theory of canonical forms. We first handle characteristic values in relation to triangulation and diagonalization theorems and then build our way up to the general theory. We have split Chapter 8 so that the basic material on inner product spaces and unitary diagonalization is followed by a Chapter 9 which treats sesqui-linear forms and the more sophisticated properties of normal operators, including normal operators on real inner product spaces.

We have also made a number of small changes and improvements from the first edition. But the basic philosophy behind the text is unchanged.

We have made no particular concession to the fact that the majority of the students may not be primarily interested in mathematics. For we believe a mathematics course should not give science, engineering, or social science students a hodgepodge of techniques, but should provide them with an understanding of basic mathematical concepts. On the other hand, we have been keenly aware of the wide range of backgrounds which the students may possess and, in particular, of the fact that the students have had very little experience with abstract mathematical reasoning. For this reason, we have avoided the introduction of too many abstract ideas at the very beginning of the book. In addition, we have included an Appendix which presents such basic ideas as set, function, and equivalence relation. We have found it most profitable not to dwell on these ideas independently, but to advise the students to read the Appendix when these ideas arise.

Throughout the book we have included a great variety of examples of the important concepts which occur. The study of such examples is of fundamental importance and tends to minimize the number of students who can repeat definition, theorem, proof in logical order without grasping the meaning of the abstract concepts. The book also contains a wide variety of graded exercises (about six hundred), ranging from routine applications to ones which will extend the very best students. These exercises are intended to be an important part of the text.

Chapter 1 deals with systems of linear equations and their solution by means of elementary row operations on matrices. It has been our practice to spend about six lectures on this material. It provides the student with some picture of the origins of linear algebra and with the computational technique necessary to understand examples of the more abstract ideas occurring in the later chapters. Chapter 2 deals with vector spaces, subspaces, bases, and dimension. Chapter 3 treats linear transformations, their algebra, their representation by matrices, as well as isomorphism, linear functionals, and dual spaces. Chapter 4 defines the algebra of polynomials over a field, the ideals in that algebra, and the prime factorization of a polynomial. It also deals with roots, Taylor's formula, and the Lagrange interpolation formula. Chapter 5 develops determinants of square matrices, the determinant being viewed as an alternating $n$-linear function of the rows of a matrix, and then proceeds to multilinear functions on modules as well as the Grassman ring. The material on modules places the concept of determinant in a wider and more comprehensive setting than is usually found in elementary textbooks. Chapters 6 and 7 contain a discussion of the concepts which are basic to the analysis of a single linear transformation on a finite-dimensional vector space; the analysis of characteristic (eigen) values, triangulable and diagonalizable transformations; the concepts of the diagonalizable and nilpotent parts of a more general transformation, and the rational and Jordan canonical forms. The primary and cyclic decomposition theorems play a central role, the latter being arrived at through the study of admissible subspaces. Chapter 7 includes a discussion of matrices over a polynomial domain, the computation of invariant factors and elementary divisors of a matrix, and the development of the Smith canonical form. The chapter ends with a discussion of semi-simple operators, to round out the analysis of a single operator. Chapter 8 treats finite-dimensional inner product spaces in some detail. It covers the basic geometry, relating orthogonalization to the idea of 'best approximation to a vector' and leading to the concepts of the orthogonal projection of a vector onto a subspace and the orthogonal complement of a subspace. The chapter treats unitary operators and culminates in the diagonalization of self-adjoint and normal operators. Chapter 9 introduces sesqui-linear forms, relates them to positive and self-adjoint operators on an inner product space, moves on to the spectral theory of normal operators and then to more sophisticated results concerning normal operators on real or complex inner product spaces. Chapter 10 discusses bilinear forms, emphasizing canonical forms for symmetric and skew-symmetric forms, as well as groups preserving non-degenerate forms, especially the orthogonal, unitary, pseudo-orthogonal and Lorentz groups.

We feel that any course which uses this text should cover Chapters 1, 2, and 3 thoroughly, possibly excluding Sections $3.6$ and $3.7$ which dcal with the double dual and the transpose of a linear transformation. Chapters 4 and 5 , on polynomials and determinants, may be treated with varying degrees of thoroughness. In fact, polynomial ideals and basic properties of determinants may be covered quite sketchily without serious damage to the flow of the logic in the text; however, our inclination is to deal with these chapters carefully (except the results on modules), because the material illustrates so well the basic ideas of linear algebra. An elementary course may now be concluded nicely with the first four sections of Chapter 6, together with (the new) Chapter 8. If the rational and Jordan forms are to be included, a more extensive coverage of Chapter 6 is necessary.

Our indebtedness remains to those who contributed to the first edition, especially to Professors Harry Furstenberg, Louis Howard, Daniel Kan, Edward Thorp, to Mrs. Judith Bowers, Mrs. Betty Ann (Sargent) Rose and Miss Phyllis Ruby. In addition, we would like to thank the many students and colleagues whose perceptive comments led to this revision, and the staff of Prentice-Hall for their patience in dealing with two authors caught in the throes of academic administration. Lastly, special thanks are due to Mrs. Sophia Koulouras for both her skill and her tireless efforts in typing the revised manuscript.

\author{
K. M. H. / R. A. K.
}



\section{Contents}

Chapter 1. Linear Equations 1

1.1. Fields 1

1.2. Systems of Linear Equations $\quad 3$

1.3. Matrices and Elementary Row Operations $\quad 6$

1.4. Row-Reduced Echelon Matrices 11

1.5. Matrix Multiplication 16

1.6. Invertible Matrices $\quad 21$

Chapter 2. Vector Spaces $\quad 28$

2.1. Vector Spaces 28

2.2. Subspaces $\quad 34$

2.3. Bases and Dimension $\quad 40$

2.4. Coordinates 49

2.5. Summary of Row-Equivalence $\quad 55$

2.6. Computations Concerning Subspaces $\quad 58$

Chapter 3. Linear Transformations $\quad 67$

3.1. Linear Transformations $\quad 67$

3.2. The Algebra of Linear Transformations $\quad 74$

3.3. Isomorphism 84

3.4. Representation of Transformations by Matrices 86

3.5. Linear Functionals $\quad 97$

3.6. The Double Dual 107

3.7. The Transpose of a Linear Transformation 111 Contents vii

Chapter 4. Polynomials 117

4.1. Algebras $\quad 117$

4.2. The Algebra of Polynomials $\quad 119$

4.3. Lagrange Interpolation $\quad 124$

4.4. Polynomial Ideals 127

4.5. The Prime Factorization of a Polynomial 134

Chapter 5. Determinants 140

5.1. Commutative Rings 140

5.2. Determinant Functions $\quad 141$

5.3. Permutations and the Uniqueness of Determinants 150

5.4. Additional Properties of Determinants 156

5.5. Modules 164

5.6. Multilinear Functions $\quad 166$

5.7. The Grassman Ring 173

\section{Chapter 6. Elementary Canonical Forms 181}

6.1. Introduction 181

6.2. Characteristic Values $\quad 182$

6.3. Annihilating Polynomials $\quad 190$

6.4. Invariant Subspaces 198

6.5. Simultaneous Triangulation; Simultaneous Diagonalization

6.6. Direct-Sum Decompositions $\quad 209$

6.7. Invariant Direct Sums 213

6.8. The Primary Decomposition Theorem $\quad 219$

Chapter 7. The Rational and Jordan Forms 227

7.1. Cyclic Subspaces and Annihilators $\quad 227$

7.2. Cyclic Decompositions and the Rational Form 231

7.3. The Jordan Form 244

7.4. Computation of Invariant Factors $\quad 251$

7.5. Summary; Semi-Simple Operators 262

Chapter 8. Inner Product Spaces 270

8.1. Inner Products 270

8.2. Inner Product Spaces $\quad 277$

8.3. Linear Functionals and Adjoints $\quad 290$

8.4. Unitary Operators 299

8.5. Normal Operators $\quad 311$ viii Contents

Chapter 9. Operators on Inner Product Spaces 319

9.1. Introduction $\quad 319$

9.2. Forms on Inner Product Spaces $\quad 320$

9.3. Positive Forms 325

9.4. More on Forms 332

9.5. Spectral Theory 335

9.6. Further Properties of Normal Operators $\quad 349$

Chapter 10. Bilinear Forms 359

10.1. Bilinear Forms $\quad 359$

10.2. Symmetric Bilinear Forms 367

10.3. Skew-Symmetric Bilinear Forms $\quad 375$

10.4 Groups Preserving Bilinear Forms 379

Appendix $\quad 386$

A.1. Sets $\quad 387$

A.2. Functions 388

A.3. Equivalence Relations $\quad 391$

A.4. Quotient Spaces 394

A.5. Equivalence Relations in Linear Algebra $\quad 397$

A.6. The Axiom of Choice 399

$\begin{array}{ll}\text { Bibliography } & 400\end{array}$

Index 401 

\section{Linear Equations}

\subsection{Fields}

We assume that the reader is familiar with the elementary algebra of real and complex numbers. For a large portion of this book the algebraic properties of numbers which we shall use are easily deduced from the following brief list of properties of addition and multiplication. We let $F$ denote either the set of real numbers or the set of complex numbers.

1. Addition is commutative,

for all $x$ and $y$ in $F$.

$$
x+y=y+x
$$

2. Addition is associative,

$$
x+(y+z)=(x+y)+z
$$

for all $x, y$, and $z$ in $F$.

3. There is a unique element 0 (zero) in $F$ such that $x+0=x$, for every $x$ in $F$.

4. To each $x$ in $F$ there corresponds a unique element $(-x)$ in $F$ such that $x+(-x)=0$.

5. Multiplication is commutative,

for all $x$ and $y$ in $F$.

$$
x y=y x
$$

6. Multiplication is associative,

for all $x, y$, and $z$ in $F$.

$$
x(y z)=(x y) z
$$

7. There is a unique non-zero element 1 (one) in $F$ such that $x 1=x$, for every $x$ in $F$.

8. To each non-zero $x$ in $F$ there corresponds a unique element $x^{-1}$ (or $1 / x$ ) in $F$ such that $x x^{-1}=1$.

9. Multiplication distributes over addition; that is, $x(y+z)=$ $x y+x z$, for all $x, y$, and $z$ in $F$.

Suppose one has a set $F$ of objects $x, y, z, \ldots$ and two operations on the elements of $F$ as follows. The first operation, called addition, associates with each pair of elements $x, y$ in $F$ an element $(x+y)$ in $F$; the second operation, called multiplication, associates with each pair $x, y$ an element $x y$ in $F$; and these two operations satisfy conditions (1)-(9) above. The set $F$, together with these two operations, is then called a field. Roughly speaking, a field is a set together with some operations on the objects in that set which behave like ordinary addition, subtraction, multiplication, and division of numbers in the sense that they obey the nine rules of algebra listed above. With the usual operations of addition and multiplication, the set $C$ of complex numbers is a field, as is the set $R$ of real numbers.

For most of this book the 'numbers' we use may as well be the elements from any field $F$. To allow for this generality, we shall use the word 'scalar' rather than 'number.' Not much will be lost to the reader if he always assumes that the field of scalars is a subfield of the field of complex numbers. A subfield of the field $C$ is a set $F$ of complex numbers which is itself a field under the usual operations of addition and multiplication of complex numbers. This means that 0 and 1 are in the set $F$, and that if $x$ and $y$ are elements of $F$, so are $(x+y),-x, x y$, and $x^{-1}$ (if $x \neq 0$ ). An example of such a subfield is the field $R$ of real numbers; for, if we identify the real numbers with the complex numbers $(a+i b)$ for which $b=0$, the 0 and 1 of the complex field are real numbers, and if $x$ and $y$ are real, so are $(x+y),-x, x y$, and $x^{-1}$ (if $x \neq 0$ ). We shall give other examples below. The point of our discussing subfields is essentially this: If we are working with scalars from a certain subfield of $C$, then the performance of the operations of addition, subtraction, multiplication, or division on these scalars does not take us out of the given subfield.

Example 1. The set of positive integers: $1,2,3, \ldots$, is not a subfield of $C$, for a variety of reasons. For example, 0 is not a positive integer; for no positive integer $n$ is $-n$ a positive integer; for no positive integer $n$ except 1 is $1 / n$ a positive integer.

Example 2. The set of integers: . ., $-2,-1,0,1,2, \ldots$, is not a subfield of $C$, because for an integer $n, 1 / n$ is not an integer unless $n$ is 1 or $-1$. With the usual operations of addition and multiplication, the set of integers satisfies all of the conditions (1)-(9) except condition (8).

Example 3. The set of rational numbers, that is, numbers of the form $p / q$, where $p$ and $q$ are integers and $q \neq 0$, is a subfield of the field of complex numbers. The division which is not possible within the set of integers is possible within the set of rational numbers. The interested reader should verify that any subfield of $C$ must contain every rational number.

Example 4. The set of all complex numbers of the form $x+y \sqrt{2}$, where $x$ and $y$ are rational, is a subfield of $C$. We leave it to the reader to verify this.

In the examples and exercises of this book, the reader should assume that the field involved is a subfield of the complex numbers, unless it is expressly stated that the field is more general. We do not want to dwell on this point; however, we should indicate why we adopt such a convention. If $F$ is a field, it may be possible to add the unit 1 to itself a finite number of times and obtain 0 (see Exercise 5 following Section 1.2):

$$
1+1+\cdots+1=0 .
$$

That does not happen in the complex number field (or in any subfield thereof). If it does happen in $F$, then the least $n$ such that the sum of $n$ 1 's is 0 is called the characteristic of the field $F$. If it does not happen in $F$, then (for some strange reason) $F$ is called a field of characteristic zero. Often, when we assume $F$ is a subfield of $C$, what we want to guarantee is that $F$ is a field of characteristic zero; but, in a first exposure to linear algebra, it is usually better not to worry too much about characteristics of fields.

\subsection{Systems of Linear Equations}

Suppose $F$ is a field. We consider the problem of finding $n$ scalars (elements of $F$ ) $x_{1}, \ldots, x_{n}$ which satisfy the conditions

$$
\begin{aligned}
& A_{11} x_{1}+A_{12} x_{2}+\cdots+A_{1 n} x_{n}=y_{1} \\
& A_{21} x_{1}+A_{22} x_{2}+\cdots+A_{2 n} x_{n}=y_{2} \\
& \vdots \quad \vdots \quad \vdots \quad \vdots \\
& A_{m 1} x_{1}+A_{m 2} x_{2}+\cdots+A_{m n} x_{n}=y_{m}
\end{aligned}
$$

where $y_{1}, \ldots, y_{m}$ and $A_{i j}, 1 \leq i \leq m, 1 \leq j \leq n$, are given elements of $F$. We call (1-1) a system of $m$ linear equations in $n$ unknowns. Any $n$-tuple $\left(x_{1}, \ldots, x_{n}\right)$ of elements of $F$ which satisfies each of the equations in (1-1) is called a solution of the system. If $y_{1}=y_{2}=\cdots=$ $y_{m}=0$, we say that the system is homogeneous, or that each of the equations is homogeneous.

Perhaps the most fundamental technique for finding the solutions of a system of linear equations is the technique of elimination. We can illustrate this technique on the homogeneous system

$$
\begin{aligned}
2 x_{1}-x_{2}+x_{3} & =0 \\
x_{1}+3 x_{2}+4 x_{3} & =0 .
\end{aligned}
$$

If we add (-2) times the second equation to the first equation, we obtain

$$
-7 x_{2}-7 x_{3}=0
$$

or, $x_{2}=-x_{3}$. If we add 3 times the first equation to the second equation, we obtain

$$
7 x_{1}+7 x_{3}=0
$$

or, $x_{1}=-x_{3}$. So we conclude that if $\left(x_{1}, x_{2}, x_{3}\right)$ is a solution then $x_{1}=x_{2}=$ $-x_{3}$. Conversely, one can readily verify that any such triple is a solution. Thus the set of solutions consists of all triples $(-a,-a, a)$.

We found the solutions to this system of equations by 'eliminating unknowns,' that is, by multiplying equations by scalars and then adding to produce equations in which some of the $x_{j}$ were not present. We wish to formalize this process slightly so that we may understand why it works, and so that we may carry out the computations necessary to solve a system in an organized manner.

For the general system (1-1), suppose we select $m$ scalars $c_{1}, \ldots, c_{m}$, multiply the $j$ th equation by $c_{j}$ and then add. We obtain the equation $\left(c_{1} A_{11}+\cdots+c_{m} A_{m 1}\right) x_{1}+\cdots+\left(c_{1} A_{1 n}+\cdots+c_{m} A_{m n}\right) x_{n}$

$$
=c_{1} y_{1}+\cdots+c_{m} y_{m} .
$$

Such an equation we shall call a linear combination of the equations in (1-1). Evidently, any solution of the entire system of equations (1-1) will also be a solution of this new equation. This is the fundamental idea of the elimination process. If we have another system of linear equations

$$
\begin{aligned}
& B_{11} x_{1}+\cdots+B_{1 n} x_{n}=z_{1} \\
& \begin{array}{cc}\vdots \\B_{k 1} x_{1}\end{array}+\cdots+B_{k n} x_{n}=z_{k}
\end{aligned}
$$

in which each of the $k$ equations is a linear combination of the equations in (1-1), then every solution of (1-1) is a solution of this new system. Of course it may happen that some solutions of (1-2) are not solutions of (1-1). This clearly does not happen if each equation in the original system is a linear combination of the equations in the new system. Let us say that two systems of linear equations are equivalent if each equation in each system is a linear combination of the equations in the other system. We can then formally state our observations as follows. Theorem 1. Equivalent systems of linear equations have exactly the same solutions.

If the elimination process is to be effective in finding the solutions of a system like (1-1), then one must see how, by forming linear combinations of the given equations, to produce an equivalent system of equations which is easier to solve. In the next section we shall discuss one method of doing this.

\section{Exercises}

1. Verify that the set of complex numbers described in Example 4 is a subfield of $C$.

2. Let $F$ be the field of complex numbers. Are the following two systems of linear equations equivalent? If so, express each equation in each system as a linear combination of the equations in the other system.

$$
\begin{array}{rlrl}
x_{1}-x_{2} & =0 & 3 x_{1}+x_{2} & =0 \\
2 x_{1}+x_{2} & =0 & x_{1}+x_{2} & =0
\end{array}
$$

3. Test the following systems of equations as in Exercise 2 .

$$
\begin{aligned}
-x_{1}+x_{2}+4 x_{3} & =0 & x_{1} & -x_{3} & =0 \\
x_{1}+3 x_{2}+8 x_{3} & =0 & & x_{2}+3 x_{3} & =0 \\
{ }_{2}^{1} x_{1}+x_{2}+\frac{5}{2} x_{3} & =0 & & &
\end{aligned}
$$

4. Test the following systems as in Exercise 2 .

$$
\begin{array}{rlrl}
2 x_{1}+(-1+i) x_{2}+x_{4} & =0 & & \left(1+\frac{i}{2}\right) x_{1}+8 x_{2}-i x_{3}-x_{4}=0 \\
3 x_{2}-2 i x_{3}+5 x_{4} & =0 & \frac{2}{3} x_{1}-\frac{1}{2} x_{2}+x_{3}+7 x_{4}=0
\end{array}
$$

5. Let $F$ be a set which contains exactly two elements, 0 and 1 . Define an addition and multiplication by the tables:

$$
\begin{array}{c|cc}
+ & 0 & 1 \\
\hline 0 & 0 & 1 \\
1 & 1 & 0
\end{array}
$$

\begin{tabular}{c|ll}
$\cdot$ & 0 & 1 \\
\hline 0 & 0 & 0 \\
1 & 0 & 1
\end{tabular}

Verify that the set $F$, together with these two operations, is a field.

6. Prove that if two homogeneous systems of linear equations in two unknowns have the same solutions, then they are equivalent.

7. Prove that each subfield of the field of complex numbers contains every rational number.

8. Prove that each field of characteristic zero contains a copy of the rational number field. 

\subsection{Matrices and Elementary Row Operations}

One cannot fail to notice that in forming linear combinations of linear equations there is no need to continue writing the 'unknowns' $x_{1}, \ldots, x_{n}$, since one actually computes only with the coefficients $A_{i j}$ and the scalars $y_{i}$. We shall now abbreviate the system (1-1) by

where

$$
A X=Y
$$

$$
\begin{gathered}
A=\left[\begin{array}{ccc}
A_{11} & \cdots & A_{1 n} \\
\vdots & & \vdots \\
A_{m 1} & \cdots & A_{m n}
\end{array}\right] \\
X=\left[\begin{array}{c}
x_{1} \\
\vdots \\
x_{n}
\end{array}\right] \text { and } \quad Y=\left[\begin{array}{c}
y_{1} \\
\vdots \\
y_{m}
\end{array}\right] .
\end{gathered}
$$

We call $A$ the matrix of coefficients of the system. Strictly speaking, the rectangular array displayed above is not a matrix, but is a representation of a matrix. An $m \times n$ matrix over the field $F$ is a function $A$ from the set of pairs of integers $(i, j), 1 \leq i \leq m, 1 \leq j \leq n$, into the field $F$. The entries of the matrix $A$ are the scalars $A(i, j)=A_{i j}$, and quite of ten it is most convenient to describe the matrix by displaying its entries in a rectangular array having $m$ rows and $n$ columns, as above. Thus $X$ (above) is, or defines, an $n \times 1$ matrix and $Y$ is an $m \times 1$ matrix. For the time being, $A X=Y$ is nothing more than a shorthand notation for our system of linear equations. Later, when we have defined a multiplication for matrices, it will mean that $Y$ is the product of $A$ and $X$.

We wish now to consider operations on the rows of the matrix $A$ which correspond to forming linear combinations of the equations in the system $A X=Y$. We restrict our attention to three elementary row operations on an $m \times n$ matrix $A$ over the field $F$ :

1. multiplication of one row of $A$ by a non-zero scalar $c$;

2. replacement of the $r$ th row of $A$ by row $r$ plus $c$ times row $s, c$ any scalar and $r \neq s$;

3. interchange of two rows of $A$.

An elementary row operation is thus a special type of function (rule) $e$ which associated with each $m \times n$ matrix $A$ an $m \times n$ matrix $e(A)$. One can precisely describe $e$ in the three cases as follows:

1. $e(A)_{i j}=A_{i j}$ if $i \neq r, e(A)_{r j}=c A_{r j}$.

2. $e(A)_{i j}=A_{i j}$ if $i \neq r, e(A)_{r j}=A_{r j}+c A_{s j}$.

3. $e(A)_{i j}=A_{i j}$ if $i$ is different from both $r$ and $s, e(A)_{r j}=A_{s j}$, $e(A)_{8 j}=A_{r j}$. In defining $e(A)$, it is not really important how many columns $A$ has, but the number of rows of $A$ is crucial. For example, one must worry a little to decide what is meant by interchanging rows 5 and 6 of a $5 \times 5$ matrix. To avoid any such complications, we shall agree that an elementary row operation $e$ is defined on the class of all $m \times n$ matrices over $F$, for some fixed $m$ but any $n$. In other words, a particular $e$ is defined on the class of all $m$-rowed matrices over $F$.

One reason that we restrict ourselves to these three simple types of row operations is that, having performed such an operation $e$ on a matrix $A$, we can recapture $A$ by performing a similar operation on $e(A)$.

Theorem 2. To each elementary row operation e there corresponds an elementary row operation $\mathrm{e}_{1}$, of the same type as $\mathrm{e}$, such that $\mathrm{e}_{1}(\mathrm{e}(\mathrm{A}))=$ $\mathrm{e}\left(\mathrm{e}_{\mathrm{1}}(\mathrm{A})\right)=\mathrm{A}$ for each $\mathrm{A}$. In other words, the inverse operation (function) of an elementary row operation exists and is an elementary row operation of the same type.

Proof. (1) Suppose $e$ is the operation which multiplies the $r$ th row of a matrix by the non-zero scalar $c$. Let $e_{1}$ be the operation which multiplies row $r$ by $c^{-1}$. (2) Suppose $e$ is the operation which replaces row $r$ by row $r$ plus $c$ times row $s, r \neq s$. Let $e_{1}$ be the operation which replaces row $r$ by row $r$ plus $(-c)$ times row $s$. (3) If $e$ interchanges rows $r$ and $s$, let $e_{1}=e$. In each of these three cases we clearly have $e_{1}(e(A))=e\left(e_{1}(A)\right)=A$ for each $A$.

Definition. If $\mathrm{A}$ and $\mathrm{B}$ are $\mathrm{m} \times \mathrm{n}$ matrices over the field $\mathrm{F}$, we say that $\mathrm{B}$ is row-equivalent to $\mathrm{A}$ if $\mathrm{B}$ can be obtained from $\mathrm{A}$ by a finite sequence of elementary row operations.

Using Theorem 2 , the reader should find it easy to verify the following. Each matrix is row-equivalent to itself; if $B$ is row-equivalent to $A$, then $A$ is row-equivalent to $B$; if $B$ is row-equivalent to $A$ and $C$ is row-equivalent to $B$, then $C$ is row-equivalent to $A$. In other words, row-equivalence is an equivalence relation (see Appendix).

Theorem 3. If $\mathrm{A}$ and $\mathrm{B}$ are row-equivalent $\mathrm{m} \times \mathrm{n}$ matrices, the homogeneous systems of linear equations $\mathrm{AX}=0$ and $\mathrm{BX}=0$ have exactly the same solutions.

Proof. Suppose we pass from $A$ to $B$ by a finite sequence of elementary row operations:

$$
A=A_{0} \rightarrow A_{1} \rightarrow \cdots \rightarrow A_{k}=B .
$$

It is enough to prove that the systems $A_{j} X=0$ and $A_{j+1} X=0$ have the same solutions, i.e., that one elementary row operation does not disturb the set of solutions. So suppose that $B$ is obtained from $A$ by a single elementary row operation. No matter which of the three types the operation is, (1), (2), or (3), each equation in the system $B X=0$ will be a linear combination of the equations in the system $A X=0$. Since the inverse of an elementary row operation is an elementary row operation, each equation in $A X=0$ will also be a linear combination of the equations in $B X=0$. Hence these two systems are equivalent, and by Theorem 1 they have the same solutions.

Example 5. Suppose $F$ is the field of rational numbers, and

$$
A=\left[\begin{array}{rrrr}
2 & -1 & 3 & 2 \\
1 & 4 & 0 & -1 \\
2 & 6 & -1 & 5
\end{array}\right] .
$$

We shall perform a finite sequence of elementary row operations on $A$, indicating by numbers in parentheses the type of operation performed.

$$
\begin{aligned}
& \left[\begin{array}{rrrr}2 & -1 & 3 & 2 \\1 & 4 & 0 & -1 \\2 & 6 & -1 & 5\end{array}\right] \stackrel{(2)}{\longrightarrow}\left[\begin{array}{rrrr}0 & -9 & 3 & 4 \\1 & 4 & 0 & -1 \\2 & 6 & -1 & 5\end{array}\right] \stackrel{(2)}{\longrightarrow} \\
& \left[\begin{array}{rrrr}0 & -9 & 3 & 4 \\1 & 4 & 0 & -1 \\0 & -2 & -1 & 7\end{array}\right] \stackrel{(1)}{\longrightarrow}\left[\begin{array}{rrrr}0 & -9 & 3 & 4 \\1 & 4 & 0 & -1 \\0 & 1 & \frac{1}{2} & -\frac{7}{2}\end{array}\right] \stackrel{(2)}{\longrightarrow} \\
& \left[\begin{array}{rrrr}0 & -9 & 3 & 4 \\1 & 0 & -2 & 13 \\0 & 1 & \frac{1}{2} & -\frac{7}{2}\end{array}\right] \stackrel{(2)}{\longrightarrow}\left[\begin{array}{rrrr}0 & 0 & \frac{15}{2} & -\frac{55}{2} \\1 & 0 & -2 & 13 \\0 & 1 & \frac{1}{2} & -\frac{7}{2}\end{array}\right] \stackrel{(1)}{\longrightarrow} \\
& \left[\begin{array}{rrrr}0 & 0 & 1 & -\frac{11}{3} \\1 & 0 & -2 & 13 \\0 & 1 & \frac{1}{2} & -\frac{7}{2}\end{array}\right] \stackrel{(2)}{\longrightarrow}\left[\begin{array}{lllr}0 & 0 & 1 & -\frac{11}{3} \\1 & 0 & 0 & \frac{17}{3} \\0 & 1 & \frac{1}{2} & -\frac{7}{2}\end{array}\right] \stackrel{(2)}{\longrightarrow} \\
& \left[\begin{array}{rrrr}0 & 0 & 1 & -\frac{11}{3} \\1 & 0 & 0 & \frac{17}{3} \\0 & 1 & 0 & -\frac{5}{3}\end{array}\right]
\end{aligned}
$$

The row-equivalence of $A$ with the final matrix in the above sequence tells us in particular that the solutions of

$$
\begin{aligned}
2 x_{1}-x_{2}+3 x_{3}+2 x_{4} & =0 \\
x_{1}+4 x_{2}-x_{4} & =0 \\
2 x_{1}+6 x_{2}-x_{3}+5 x_{4} & =0
\end{aligned}
$$

and

$$
\begin{aligned}
x_{3}-\frac{11}{3} x_{4} & =0 \\
x_{1} \quad+\frac{17}{3} x_{4} & =0 \\
x_{2} \quad-\frac{5}{3} x_{4} & =0
\end{aligned}
$$

are exactly the same. In the second system it is apparent that if we assign any rational value $c$ to $x_{4}$ we obtain a solution $\left(-\frac{17}{3} c, \frac{5}{3}, \frac{11}{3} c, c\right)$, and also that every solution is of this form.

Example 6. Suppose $F$ is the field of complex numbers and

$$
A=\left[\begin{array}{rr}
-1 & i \\
-i & 3 \\
1 & 2
\end{array}\right]
$$

In performing row operations it is often convenient to combine several operations of type (2). With this in mind

$$
\left[\begin{array}{rr}
-1 & i \\
-i & 3 \\
1 & 2
\end{array}\right] \stackrel{(2)}{\longrightarrow}\left[\begin{array}{cc}
0 & 2+i \\
0 & 3+2 i \\
1 & 2
\end{array}\right] \stackrel{(1)}{\longrightarrow}\left[\begin{array}{cc}
0 & 1 \\
0 & 3+2 i \\
1 & 2
\end{array}\right] \stackrel{(2)}{\longrightarrow}\left[\begin{array}{ll}
0 & 1 \\
0 & 0 \\
1 & 0
\end{array}\right] .
$$

Thus the system of equations

$$
\begin{array}{r}
-x_{1}+i x_{2}=0 \\
-i x_{1}+3 x_{2}=0 \\
x_{1}+2 x_{2}=0
\end{array}
$$

has only the trivial solution $x_{1}=x_{2}=0$.

In Examples 5 and 6 we were obviously not performing row operations at random. Our choice of row operations was motivated by a desire to simplify the coefficient matrix in a manner analogous to 'eliminating unknowns' in the system of linear equations. Let us now make a formal definition of the type of matrix at which we were attempting to arrive.

Definition. An $\mathrm{m} \times \mathrm{n}$ matrix $\mathrm{R}$ is called row-reduced if:

(a) the first non-zero entry in each non-zero row of $\mathrm{R}$ is equal to 1 ;

(b) each column of $\mathrm{R}$ which contains the leading non-zero entry of some row has all its other entries 0.

Example 7. One example of a row-reduced matrix is the $n \times n$ (square) identity matrix $I$. This is the $n \times n$ matrix defined by

$$
I_{i j}=\delta_{i j}=\left\{\begin{array}{lll}
1, & \text { if } & i=j \\
0, & \text { if } & i \neq j .
\end{array}\right.
$$

This is the first of many occasions on which we shall use the Kronecker delta $(\delta)$.

In Examples 5 and 6 , the final matrices in the sequences exhibited there are row-reduced matrices. Two examples of matrices which are not row-reduced are:

$$
\left[\begin{array}{rrrr}
1 & 0 & 0 & 0 \\
0 & 1 & -1 & 0 \\
0 & 0 & 1 & 0
\end{array}\right], \quad\left[\begin{array}{rrr}
0 & 2 & 1 \\
1 & 0 & -3 \\
0 & 0 & 0
\end{array}\right]
$$

The second matrix fails to satisfy condition (a), because the leading nonzero entry of the first row is not 1 . The first matrix does satisfy condition (a), but fails to satisfy condition (b) in column 3.

We shall now prove that we can pass from any given matrix to a rowreduced matrix, by means of a finite number of elementary row opertions. In combination with Theorem 3, this will provide us with an effective tool for solving systems of linear equations.

Theorem 4. Every $\mathrm{m} \times \mathrm{n}$ matrix over the field $\mathrm{F}$ is row-equivalent to a row-reduced matrix.

Proof. Let $A$ be an $m \times n$ matrix over $F$. If every entry in the first row of $A$ is 0 , then condition (a) is satisfied in so far as row 1 is concerned. If row 1 has a non-zero entry, let $k$ be the smallest positive integer $j$ for which $A_{1 j} \neq 0$. Multiply row 1 by $A_{1 k}^{-1}$, and then condition (a) is satisfied with regard to row 1 . Now for each $i \geq 2$, add $\left(-A_{i k}\right)$ times row 1 to row $i$. Now the leading non-zero entry of row 1 occurs in column $k$, that entry is 1 , and every other entry in column $k$ is 0 .

Now consider the matrix which has resulted from above. If every entry in row 2 is 0 , we do nothing to row 2 . If some entry in row 2 is different from 0 , we multiply row 2 by a scalar so that the leading non-zero entry is 1 . In the event that row 1 had a leading non-zero entry in column $k$, this leading non-zero entry of row 2 cannot occur in column $k$; say it occurs in column $k_{r} \neq k$. By adding suitable multiples of row 2 to the various rows, we can arrange that all entries in column $k^{\prime}$ are 0 , except the 1 in row 2 . The important thing to notice is this: In carrying out these last operations, we will not change the entries of row 1 in columns $1, \ldots, k$; nor will we change any entry of column $k$. Of course, if row 1 was identically 0 , the operations with row 2 will not affect row 1 .

Working with one row at a time in the above manner, it is clear that in a finite number of steps we will arrive at a row-reduced matrix.

\section{Exercises}

1. Find all solutions to the system of equations

$$
\begin{aligned}
(1-i) x_{1}-i x_{2} & =0 \\
2 x_{1}+(1-i) x_{2} & =0 .
\end{aligned}
$$

2. If

$$
A=\left[\begin{array}{rrr}
3 & -1 & 2 \\
2 & 1 & 1 \\
1 & -3 & 0
\end{array}\right]
$$

find all solutions of $A X=0$ by row-reducing $A$. 3. If

$$
A=\left[\begin{array}{rrr}
6 & -4 & 0 \\
4 & -2 & 0 \\
-1 & 0 & 3
\end{array}\right]
$$

find all solutions of $A X=2 X$ and all solutions of $A X=3 X$. (The symbol $c X$ denotes the matrix each entry of which is $c$ times the corresponding entry of $X$.)

4. Find a row-reduced matrix which is row-equivalent to

$$
A=\left[\begin{array}{ccr}
i & -(1+i) & 0 \\
1 & -2 & 1 \\
1 & 2 i & -1
\end{array}\right]
$$

5. Prove that the following two matrices are not row-equivalent:

$$
\left[\begin{array}{rrr}
2 & 0 & 0 \\
a & -1 & 0 \\
b & c & 3
\end{array}\right], \quad\left[\begin{array}{rrr}
1 & 1 & 2 \\
-2 & 0 & -1 \\
1 & 3 & 5
\end{array}\right]
$$

6. Let

$$
A=\left[\begin{array}{ll}
a & b \\
c & d
\end{array}\right]
$$

be a $2 \times 2$ matrix with complex entries. Suppose that $A$ is row-reduced and also that $a+b+c+d=0$. Prove that there are exactly three such matrices.

7. Prove that the interchange of two rows of a matrix can be accomplished by a finite sequence of elementary row operations of the other two types.

8. Consider the system of equations $A X=0$ where

$$
A=\left[\begin{array}{ll}
a & b \\
c & d
\end{array}\right]
$$

is a $2 \times 2$ matrix over the field $F$. Prove the following.

(a) If every entry of $A$ is 0 , then every pair $\left(x_{1}, x_{2}\right)$ is a solution of $A X=0$.

(b) If $a d-b c \neq 0$, the system $A X=0$ has only the trivial solution $x_{1}=$ $x_{2}=0$.

(c) If $a d-b c=0$ and some entry of $A$ is different from 0 , then there is a solution $\left(x_{1}^{0}, x_{2}^{0}\right)$ such that $\left(x_{1}, x_{2}\right)$ is a solution if and only if there is a scalar $y$ such that $x_{1}=y x_{1}^{0}, x_{2}=y x_{2}^{0}$.

\subsection{Row-Reduced Echelon Matrices}

Until now, our work with systems of linear equations was motivated by an attempt to find the solutions of such a system. In Section $1.3$ we established a standardized technique for finding these solutions. We wish now to acquire some information which is slightly more theoretical, and for that purpose it is convenient to go a little beyond row-reduced matrices.

Definition. An $\mathrm{m} \times \mathrm{n}$ matrix $\mathrm{R}$ is called a row-reduced echelon matrix if: (a) $\mathrm{R}$ is row-reduced;

(b) every row of $\mathrm{R}$ which has all its entries 0 occurs below every row which has a non-zero entry;

(c) if rows $1, \ldots, \mathrm{r}$ are the non-zero rows of $\mathrm{R}$, and if the leading nonzero entry of row $\mathrm{i}$ occurs in column $\mathrm{k}_{\mathrm{i}}, \mathrm{i}=1, \ldots, \mathrm{r}$, then $\mathrm{k}_{1}<$ $\mathrm{k}_{2}<\cdots<\mathrm{k}_{\mathrm{r}}$.

One can also describe an $m \times n$ row-reduced echelon matrix $R$ as follows. Either every entry in $R$ is 0 , or there exists a positive integer $r$, $1 \leq r \leq m$, and $r$ positive integers $k_{1}, \ldots, k_{r}$ with $1 \leq k_{i} \leq n$ and

(a) $R_{i j}=0$ for $i>r$, and $R_{i j}=0$ if $j<k_{i}$.

(b) $R_{i k_{i}}=\delta_{i j}, 1 \leq i \leq r, 1 \leq j \leq r$.

(c) $k_{1}<\cdots<k_{r}$.

Example 8. Two examples of row-reduced echelon matrices are the $n \times n$ identity matrix, and the $m \times n$ zero matrix $0^{m, n}$, in which all entries are 0 . The reader should have no difficulty in making other examples, but we should like to give one non-trivial one:

$$
\left[\begin{array}{rrrrr}
0 & 1 & -3 & 0 & \frac{1}{2} \\
0 & 0 & 0 & 1 & 2 \\
0 & 0 & 0 & 0 & 0
\end{array}\right] \text {. }
$$

Theorem 5. Every $\mathrm{m} \times \mathrm{n}$ matrix $\mathrm{A}$ is row-equivalent to a row-reduced echelon matrix.

Proof. We know that $A$ is row-equivalent to a row-reduced matrix. All that we need observe is that by performing a finite number of row interchanges on a row-reduced matrix we can bring it to row-reduced echelon form.

In Examples 5 and 6, we saw the significance of row-reduced matrices in solving homogeneous systems of linear equations. Let us now discuss briefly the system $R X=0$, when $R$ is a row-reduced echelon matrix. Let rows $1, \ldots, r$ be the non-zero rows of $R$, and suppose that the leading non-zero entry of row $i$ occurs in column $k_{i}$. The system $R X=0$ then consists of $r$ non-trivial equations. Also the unknown $x_{k_{i}}$ will occur (with non-zero coefficient) only in the $i$ th equation. If we let $u_{1}, \ldots, u_{n-r}$ denote the $(n-r)$ unknowns which are different from $x_{k_{1}}, \ldots, x_{k_{r} \text {, }}$ then the $r$ non-trivial equations in $R X=0$ are of the form

$$
\begin{array}{r}
x_{k_{1}}+\sum_{j=1}^{n-r} C_{1 j} u_{j}=0 \\
\vdots \\
x_{k_{r}}+\sum_{j=1}^{n-r} C_{r j} u_{j}=0 .
\end{array}
$$

All the solutions to the system of equations $R X=0$ are obtained by assigning any values whatsoever to $u_{1}, \ldots, u_{n-r}$ and then computing the corresponding values of $x_{k_{1}}, \ldots, x_{k_{r}}$ from (1-3). For example, if $R$ is the matrix displayed in Example 8, then $r=2, k_{1}=2, k_{2}=4$, and the two non-trivial equations in the system $R X=0$ are

$$
\begin{array}{rlrl}
x_{2}-3 x_{3}+\frac{1}{2} x_{5} & =0 & \text { or } & x_{2}=3 x_{3}-\frac{1}{2} x_{5} \\
x_{4}+2 x_{5}=0 & \text { or } & x_{4}=-2 x_{5} .
\end{array}
$$

So we may assign any values to $x_{1}, x_{3}$, and $x_{5}$, say $x_{1}=a, x_{3}=b, x_{5}=c$, and obtain the solution $\left(a, 3 b-\frac{1}{2} c, b,-2 c, c\right)$.

Let us observe one thing more in connection with the system of equations $R X=0$. If the number $r$ of non-zero rows in $R$ is less than $n$, then the system $R X=0$ has a non-trivial solution, that is, a solution $\left(x_{1}, \ldots, x_{n}\right)$ in which not every $x_{j}$ is 0 . For, since $r<n$, we can choose some $x_{j}$ which is not among the $r$ unknowns $x_{k_{1}}, \ldots, x_{k_{r}}$, and we can then construct a solution as above in which this $x_{j}$ is 1 . This observation leads us to one of the most fundamental facts concerning systems of homogeneous linear equations.

Theorem 6. If $\mathrm{A}$ is an $\mathrm{m} \times \mathrm{n}$ matrix and $\mathrm{m}<\mathrm{n}$, then the homogeneous system of linear equations $\mathrm{AX}=0$ has a non-trivial solution.

Proof. Let $R$ be a row-reduced echelon matrix which is rowequivalent to $A$. Then the systems $A X=0$ and $R X=0$ have the same solutions by Theorem 3. If $r$ is the number of non-zero rows in $R$, then certainly $r \leq m$, and since $m<n$, we have $r<n$. It follows immediately from our remarks above that $A X=0$ has a non-trivial solution.

Theorem 7. If $\mathrm{A}$ is an $\mathrm{n} \times \mathrm{n}$ (square) matrix, then $\mathrm{A}$ is row-equivalent to the $\mathrm{n} \times \mathrm{n}$ identity matrix if and only if the system of equations $\mathrm{AX}=0$ has only the trivial solution.

Proof. If $A$ is row-equivalent to $I$, then $A X=0$ and $I X=0$ have the same solutions. Conversely, suppose $A X=0$ has only the trivial solution $X=0$. Let $R$ be an $n \times n$ row-reduced echelon matrix which is row-equivalent to $A$, and let $r$ be the number of non-zero rows of $R$. Then $R X=0$ has no non-trivial solution. Thus $r \geq n$. But since $R$ has $n$ rows, certainly $r \leq n$, and we have $r=n$. Since this means that $R$ actually has a leading non-zero entry of 1 in each of its $n$ rows, and since these 1 's occur each in a different one of the $n$ columns, $R$ must be the $n \times n$ identity matrix.

Let us now ask what elementary row operations do toward solving a system of linear equations $A X=Y$ which is not homogeneous. At the outset, one must observe one basic difference between this and the homogeneous case, namely, that while the homogeneous system always has the trivial solution $x_{1}=\cdots=x_{n}=0$, an inhomogeneous system need have no solution at all.

We form the augmented matrix $A^{\prime}$ of the system $A X=Y$. This is the $m \times(n+1)$ matrix whose first $n$ columns are the columns of $A$ and whose last column is $Y$. More precisely,

$$
\begin{aligned}
& A_{i j}^{\prime}=A_{i j}, \quad \text { if } \quad j \leq n \\
& A_{i(n+1)}^{\prime}=y_{i} .
\end{aligned}
$$

Suppose we perform a sequence of elementary row operations on $A$, arriving at a row-reduced echelon matrix $R$. If we perform this same sequence of row operations on the augmented matrix $A^{\prime}$, we will arrive at a matrix $R^{\prime}$ whose first $n$ columns are the columns of $R$ and whose last column contains certain scalars $z_{1}, \ldots, z_{m}$. The scalars $z_{i}$ are the entries of the $m \times 1$ matrix

$$
Z=\left[\begin{array}{c}
z_{1} \\
\vdots \\
z_{m}
\end{array}\right]
$$

which results from applying the sequence of row operations to the matrix $Y$. It should be clear to the reader that, just as in the proof of Theorem 3, the systems $A X=Y$ and $R X=Z$ are equivalent and hence have the same solutions. It is very easy to determine whether the system $R X=Z$ has any solutions and to determine all the solutions if any exist. For, if $R$ has $r$ non-zero rows, with the leading non-zero entry of row $i$ occurring in column $k_{i}, i=1, \ldots, r$, then the first $r$ equations of $R X=Z$ effectively express $x_{k_{1}}, \ldots, x_{k_{r}}$ in terms of the $(n-r)$ remaining $x_{j}$ and the scalars $z_{1}, \ldots, z_{r}$. The last $(m-r)$ equations are

$$
\begin{aligned}
& 0=z_{r+1} \\
& \begin{gathered}\vdots \\0\end{gathered}=z_{m}
\end{aligned}
$$

and accordingly the condition for the system to have a solution is $z_{i}=0$ for $i>r$. If this condition is satisfied, all solutions to the system are found just as in the homogeneous case, by assigning arbitrary values to $(n-r)$ of the $x_{j}$ and then computing $x_{k_{i}}$ from the $i$ th equation.

Example 9. Let $F$ be the field of rational numbers and

$$
A=\left[\begin{array}{rrr}
1 & -2 & 1 \\
2 & 1 & 1 \\
0 & 5 & -1
\end{array}\right]
$$

and suppose that we wish to solve the system $A X=Y$ for some $y_{1}, y_{2}$, and $y_{3}$. Let us perform a sequence of row operations on the augmented matrix $A^{\prime}$ which row-reduces $A$ : 

$$
\begin{aligned}
& \left[\begin{array}{rrrr}1 & -2 & 1 & y_{1} \\2 & 1 & 1 & y_{2} \\0 & 5 & -1 & y_{3}\end{array}\right] \stackrel{(2)}{\longrightarrow}\left[\begin{array}{rrcc}1 & -2 & 1 & y_{1} \\0 & 5 & -1 & \left(y_{2}-2 y_{1}\right) \\0 & 5 & -1 & y_{3}\end{array}\right] \stackrel{(2)}{\longrightarrow} \\
& \left[\begin{array}{rrrc}1 & -2 & 1 & y_{1} \\0 & 5 & -1 & \left(y_{2}-2 y_{1}\right) \\0 & 0 & 0 & \left(y_{3}-y_{2}+2 y_{1}\right)\end{array}\right] \stackrel{(1)}{\longrightarrow}\left[\begin{array}{rrcc}1 & -2 & 1 & y_{1} \\0 & 1 & -\frac{1}{5} & \frac{1}{5}\left(y_{2}-2 y_{1}\right) \\0 & 0 & 0 & \left(y_{3}-y_{2}+2 y_{1}\right)\end{array}\right] \stackrel{(2)}{\longrightarrow} . \\
& \left[\begin{array}{rrrc}1 & 0 & \frac{3}{5} & \frac{1}{5}\left(y_{1}+2 y_{2}\right) \\0 & 1 & -\frac{1}{5} & \frac{1}{5}\left(y_{2}-2 y_{1}\right) \\0 & 0 & 0 & \left(y_{3}-y_{2}+2 y_{1}\right)\end{array}\right] \text {. }
\end{aligned}
$$

The condition that the system $A X=Y$ have a solution is thus

$$
2 y_{1}-y_{2}+y_{3}=0
$$

and if the given scalars $y_{i}$ satisfy this condition, all solutions are obtained by assigning a value $c$ to $x_{3}$ and then computing

$$
\begin{aligned}
& x_{1}=-\frac{3}{5} c+\frac{1}{5}\left(y_{1}+2 y_{2}\right) \\
& x_{2}=\frac{1}{5} c+\frac{1}{5}\left(y_{2}-2 y_{1}\right) .
\end{aligned}
$$

Let us observe one final thing about the system $A X=Y$. Suppose the entries of the matrix $A$ and the scalars $y_{1}, \ldots, y_{m}$ happen to lie in a subfield $F_{1}$ of the field $F$. If the system of equations $A X=Y$ has a solution with $x_{1}, \ldots, x_{n}$ in $F$, it has a solution with $x_{1}, \ldots, x_{n}$ in $F_{1}$. For, over either field, the condition for the system to have a solution is that certain relations hold between $y_{1}, \ldots, y_{m}$ in $F_{1}$ (the relations $z_{i}=0$ for $i>r$, above). For example, if $A X=Y$ is a system of linear equations in which the scalars $y_{k}$ and $A_{i j}$ are real numbers, and if there is a solution in which $x_{1}, \ldots, x_{n}$ are complex numbers, then there is a solution with $x_{1}, \ldots, x_{n}$ real numbers.

\section{Exercises}

1. Find all solutions to the following system of equations by row-reducing the coefficient matrix:

$$
\begin{aligned}
\frac{1}{3} x_{1}+2 x_{2}-6 x_{3} & =0 \\
-4 x_{1}+5 x_{3} & =0 \\
-3 x_{1}+6 x_{2}-13 x_{3} & =0 \\
-\frac{7}{3} x_{1}+2 x_{2}-\frac{8}{3} x_{3} & =0
\end{aligned}
$$

2. Find a row-reduced echelon matrix which is row-equivalent to

$$
A=\left[\begin{array}{cc}
1 & -i \\
2 & 2 \\
i & 1+i
\end{array}\right] \text {. }
$$

What are the solutions of $A X=0$ ? 3. Describe explicitly all $2 \times 2$ row-reduced echelon matrices.

4. Consider the system of equations

$$
\begin{aligned}
x_{1}-x_{2}+2 x_{3} & =1 \\
2 x_{1}+2 x_{3} & =1 \\
x_{1}-3 x_{2}+4 x_{3} & =2 .
\end{aligned}
$$

Does this system have a solution? If so, describe explicitly all solutions.

5. Give an example of a system of two linear equations in two unknowns which has no solution.

6. Show that the system

$$
\begin{aligned}
& x_{1}-2 x_{2}+x_{3}+2 x_{4}=1 \\
& x_{1}+x_{2}-x_{3}+x_{4}=2 \\
& x_{1}+7 x_{2}-5 x_{3}-x_{4}=3
\end{aligned}
$$

has no solution.

7. Find all solutions of

$$
\begin{aligned}
2 x_{1}-3 x_{2}-7 x_{3}+5 x_{4}+2 x_{5} & =-2 \\
x_{1}-2 x_{2}-4 x_{3}+3 x_{4}+x_{5} & =-2 \\
2 x_{1}-4 x_{3}+2 x_{4}+x_{5} & =3 \\
x_{1}-5 x_{2}-7 x_{3}+6 x_{4}+2 x_{5} & =-7 .
\end{aligned}
$$

8. Let

$$
A=\left[\begin{array}{rrr}
3 & -1 & 2 \\
2 & 1 & 1 \\
1 & -3 & 0
\end{array}\right] \text {. }
$$

For which triples $\left(y_{1}, y_{2}, y_{3}\right)$ does the system $A X=Y$ have a solution?

9. Let

$$
A=\left[\begin{array}{rrrr}
3 & -6 & 2 & -1 \\
-2 & 4 & 1 & 3 \\
0 & 0 & 1 & 1 \\
1 & -2 & 1 & 0
\end{array}\right]
$$

For which $\left(y_{1}, y_{2}, y_{3}, y_{4}\right)$ does the system of equations $A X=Y$ have a solution?

10. Suppose $R$ and $R^{\prime}$ are $2 \times 3$ row-reduced echelon matrices and that the systems $R X=0$ and $R^{\prime} X=0$ have exactly the same solutions. Prove that $R=R^{\prime}$.

\subsection{Matrix Multiplication}

It is apparent (or should be, at any rate) that the process of forming linear combinations of the rows of a matrix is a fundamental one. For this reason it is advantageous to introduce a systematic scheme for indicating just what operations are to be performed. More specifically, suppose $B$ is an $n \times p$ matrix over a field $F$ with rows $\beta_{1}, \ldots, \beta_{n}$ and that from $B$ we construct a matrix $C$ with rows $\gamma_{1}, \ldots, \gamma_{m}$ by forming certain linear combinations

$$
\gamma_{i}=A_{i 1} \beta_{1}+A_{i 2} \beta_{2}+\cdots+A_{i n} \beta_{n} .
$$

The rows of $C$ are determined by the $m n$ scalars $A_{i j}$ which are themselves the entries of an $m \times n$ matrix $A$. If (1-4) is expanded to

$$
\left(C_{i 1} \cdots C_{i p}\right)=\sum_{r=1}^{n}\left(A_{i r} B_{r 1} \cdots A_{i r} B_{r p}\right)
$$

we see that the entries of $C$ are given by

$$
C_{i j}=\sum_{r=1}^{n} A_{i r} B_{r j} .
$$

Definition. Let $\mathrm{A}$ be an $\mathrm{m} \times \mathrm{n}$ matrix over the field $\mathrm{F}$ and let $\mathrm{B}$ be an $\mathrm{n} \times \mathrm{p}$ matrix over $\mathrm{F}$. The product $\mathrm{AB}$ is the $\mathrm{m} \times \mathrm{p}$ matrix $\mathrm{C}$ whose $\mathrm{i}, \mathrm{j}$ entry is

$$
\mathrm{C}_{\mathrm{ij}}=\sum_{\mathrm{r}=1}^{\mathrm{n}} \mathrm{A}_{\mathrm{ir}} \mathrm{B}_{\mathrm{r} \mathrm{j}} .
$$

Exam ple 10. Here are some products of matrices with rational entries.

$$
\left[\begin{array}{rrr}
5 & -1 & 2 \\
0 & 7 & 2
\end{array}\right]=\left[\begin{array}{rr}
1 & 0 \\
-3 & 1
\end{array}\right]\left[\begin{array}{rrr}
5 & -1 & 2 \\
15 & 4 & 8
\end{array}\right]
$$

Here

Here

(b)

$$
\begin{aligned}
& \gamma_{1}=\left(\begin{array}{lll}5 & -1 & 2\end{array}\right)=1 \cdot\left(\begin{array}{lll}5 & -1 & 2\end{array}\right)+0 \cdot\left(\begin{array}{lll}15 & 4 & 8\end{array}\right) \\
& \gamma_{2}=\left(\begin{array}{lll}0 & 7 & 2\end{array}\right)=-3\left(\begin{array}{lll}5 & -1 & 2\end{array}\right)+1 \cdot\left(\begin{array}{lll}15 & 4 & 8\end{array}\right) \\
& \left[\begin{array}{rrr}0 & 6 & 1 \\9 & 12 & -8 \\12 & 62 & -3 \\3 & 8 & -2\end{array}\right]=\left[\begin{array}{rr}1 & 0 \\-2 & 3 \\5 & 4 \\0 & 1\end{array}\right]\left[\begin{array}{llr}0 & 6 & 1 \\3 & 8 & -2\end{array}\right] \\
& \gamma_{3}=\left(\begin{array}{lll}12 & 62 & -3\end{array}\right)=5\left(\begin{array}{lll}0 & 6 & 1\end{array}\right)+4\left(\begin{array}{lll}3 & 8 & -2\end{array}\right)
\end{aligned}
$$

Here

$$
\begin{aligned}
{\left[\begin{array}{r}
8 \\
29
\end{array}\right] } & =\left[\begin{array}{ll}
2 & 1 \\
5 & 4
\end{array}\right]\left[\begin{array}{l}
1 \\
6
\end{array}\right] \\
{\left[\begin{array}{rr}
-2 & -4 \\
6 & 12
\end{array}\right] } & =\left[\begin{array}{r}
-1 \\
3
\end{array}\right]\left[\begin{array}{ll}
2 & 4
\end{array}\right]
\end{aligned}
$$

(e)

$$
\gamma_{2}=\left(\begin{array}{ll}
6 & 12
\end{array}\right)=3\left(\begin{array}{ll}
2 & 4
\end{array}\right)
$$

$$
\begin{aligned}
& \left[\begin{array}{ll}2 & 4\end{array}\right]\left[\begin{array}{r}-1 \\3\end{array}\right]=[10]
\end{aligned}
$$

$$
\begin{aligned}
& {\left[\begin{array}{rrr}
0 & 1 & 0 \\
0 & 0 & 0 \\
0 & 0 & 0
\end{array}\right]\left[\begin{array}{rrr}
1 & -5 & 2 \\
2 & 3 & 4 \\
9 & -1 & 3
\end{array}\right]=\left[\begin{array}{lll}
2 & 3 & 4 \\
0 & 0 & 0 \\
0 & 0 & 0
\end{array}\right]} \\
& {\left[\begin{array}{rrr}
1 & -5 & 2 \\
2 & 3 & 4 \\
9 & -1 & 3
\end{array}\right]\left[\begin{array}{lll}
0 & 1 & 0 \\
0 & 0 & 0 \\
0 & 0 & 0
\end{array}\right]=\left[\begin{array}{lll}
0 & 1 & 0 \\
0 & 2 & 0 \\
0 & 9 & 0
\end{array}\right]}
\end{aligned}
$$

![](https://cdn.mathpix.com/cropped/2023_01_16_0f6afdf132d84aaf6bdeg-026.jpg?height=51&width=823&top_left_y=1310&top_left_x=223)

It is important to observe that the product of two matrices need not be defined; the product is defined if and only if the number of columns in the first matrix coincides with the number of rows in the second matrix. Thus it is meaningless to interchange the order of the factors in (a), (b), and (c) above. Frequently we shall write products such as $A B$ without explicitly mentioning the sizes of the factors and in such cases it will be understood that the product is defined. From (d), (e), (f), (g) we find that even when the products $A B$ and $B A$ are both defined it need not be true that $A B=B A$; in other words, matrix multiplication is not commutative.

\section{EXAMPLE 11.}

(a) If $I$ is the $m \times m$ identity matrix and $A$ is an $m \times n$ matrix, $I A=A$.

(b) If $I$ is the $n \times n$ identity matrix and $A$ is an $m \times n$ matrix, $A I=A$.

(c) If $0^{k, m}$ is the $k \times m$ zero matrix, $0^{k, n}=0^{k, m} A$. Similarly, $A 0^{n, p}=0^{m, p}$.

Example 12. Let $A$ be an $m \times n$ matrix over $F$. Our earlier shorthand notation, $A X=Y$, for systems of linear equations is consistent with our definition of matrix products. For if

$$
X=\left[\begin{array}{c}
x_{1} \\
x_{2} \\
\vdots \\
x_{n}
\end{array}\right]
$$

with $x_{i}$ in $F$, then $A X$ is the $m \times 1$ matrix

$$
Y=\left[\begin{array}{c}
y_{1} \\
y_{2} \\
\vdots \\
y_{m}
\end{array}\right]
$$

such that $y_{i}=A_{i 1} x_{1}+A_{i 2} x_{2}+\cdots+A_{i n} x_{n}$.

The use of column matrices suggests a notation which is frequently useful. If $B$ is an $n \times p$ matrix, the columns of $B$ are the $1 \times n$ matrices $B_{1}, \ldots, B_{p}$ defined by

$$
B_{j}=\left[\begin{array}{c}
B_{1 j} \\
\vdots \\
B_{n j}
\end{array}\right], \quad 1 \leq j \leq p .
$$

The matrix $B$ is the succession of these columns:

$$
B=\left[B_{1}, \ldots, B_{p}\right] .
$$

The $i, j$ entry of the product matrix $A B$ is formed from the $i$ th row of $A$ and the $j$ th column of $B$. The reader should verify that the $j$ th column of $A B$ is $A B_{j}$ :

$$
A B=\left[A B_{1}, \ldots, A B_{p}\right] .
$$

In spite of the fact that a product of matrices depends upon the order in which the factors are written, it is independent of the way in which they are associated, as the next theorem shows.

Theorem 8. If $\mathrm{A}, \mathrm{B}, \mathrm{C}$ are matrices over the field $\mathrm{F}$ such that the products $\mathrm{BC}$ and $\mathrm{A}(\mathrm{BC})$ are defined, then so are the products $\mathrm{AB},(\mathrm{AB}) \mathrm{C}$ and

$$
\mathrm{A}(\mathrm{BC})=(\mathrm{AB}) \mathrm{C} \text {. }
$$

Proof. Suppose $B$ is an $n \times p$ matrix. Since $B C$ is defined, $C$ is a matrix with $p$ rows, and $B C$ has $n$ rows. Because $A(B C)$ is defined we may assume $A$ is an $m \times n$ matrix. Thus the product $A B$ exists and is an $m \times p$ matrix, from which it follows that the product $(A B) C$ exists. To show that $A(B C)=(A B) C$ means to show that

$$
[A(B C)]_{i j}=[(A B) C]_{i j}
$$

for each $i, j$. By definition

$$
\begin{aligned}
{[A(B C)]_{i j} } & =\sum_{r} A_{i r}(B C)_{r j} \\
& =\sum_{r} A_{i r} \sum_{s} B_{r s} C_{s j} \\
& =\sum_{r} \sum_{s} A_{i r} B_{r s} C_{8 j} \\
& =\sum_{s} \sum_{r} A_{i r} B_{r s} C_{s j} \\
& =\sum_{s}\left(\sum_{r} A_{i r} B_{r s}\right) C_{s j} \\
& =\sum_{s}(A B)_{i s} C_{s j} \\
& =[(A B) C]_{i j} .
\end{aligned}
$$

When $A$ is an $n \times n$ (square) matrix, the product $A A$ is defined. We shall denote this matrix by $A^{2}$. By Theorem $8,(A A) A=A(A A)$ or $A^{2} A=A A^{2}$, so that the product $A A A$ is unambiguously defined. This product we denote by $A^{3}$. In general, the product $A A \cdots A$ ( $k$ times) is unambiguously defined, and we shall denote this product by $A^{k}$.

Note that the relation $A(B C)=(A B) C$ implies among other things that linear combinations of linear combinations of the rows of $C$ are again linear combinations of the rows of $C$.

If $B$ is a given matrix and $C$ is obtained from $B$ by means of an elementary row operation, then each row of $C$ is a linear combination of the rows of $B$, and hence there is a matrix $A$ such that $A B=C$. In general there are many such matrices $A$, and among all such it is convenient and possible to choose one having a number of special properties. Before going into this we need to introduce a class of matrices.

Definition. An $\mathrm{m} \times \mathrm{n}$ matrix is said to be an elementary matrix if it can be obtained from the $\mathrm{m} \times \mathrm{m}$ identity matrix by means of a single elementary row operation.

Example 13. A $2 \times 2$ elementary matrix is necessarily one of the following:

$$
\begin{aligned}
& {\left[\begin{array}{ll}
0 & 1 \\
1 & 0
\end{array}\right], \quad\left[\begin{array}{ll}
1 & c \\
0 & 1
\end{array}\right], \quad\left[\begin{array}{ll}
1 & 0 \\
c & 1
\end{array}\right]} \\
& {\left[\begin{array}{cc}
c & 0 \\
0 & 1
\end{array}\right], \quad c \neq 0, \quad\left[\begin{array}{ll}
1 & 0 \\
0 & c
\end{array}\right], \quad c \neq 0 .}
\end{aligned}
$$

Theorem 9. Let e be an elementary row operation and let $\mathrm{E}$ be the $\mathrm{m} \times \mathrm{m}$ elementary matrix $\mathrm{E}=\mathrm{e}(\mathrm{I})$. Then, for every $\mathrm{m} \times \mathrm{n}$ matrix $\mathrm{A}$,

$$
\mathrm{e}(\mathrm{A})=\mathrm{EA} \text {. }
$$

Proof. The point of the proof is that the entry in the $i$ th row and $j$ th column of the product matrix $E A$ is obtained from the $i$ th row of $E$ and the $j$ th column of $A$. The three types of elementary row operations should be taken up separately. We shall give a detailed proof for an operation of type (ii). The other two cases are even easier to handle than this one and will be left as exercises. Suppose $r \neq s$ and $e$ is the operation 'replacement of row $r$ by row $r$ plus $c$ times row $s$.' Then

$$
E_{i k}=\left\{\begin{array}{l}
\delta_{i k}, i \neq r \\
\delta_{r k}+c \delta_{s k}, \quad i=r .
\end{array}\right.
$$

Therefore,

$$
(E A)_{i j}=\sum_{k=1}^{m} E_{i k} A_{k j}=\left\{\begin{array}{l}
A_{i k}, \quad i \neq r \\
A_{r j}+c A_{s j}, \quad i=r .
\end{array}\right.
$$

In other words $E A=e(A)$.

Corollary. Let $\mathrm{A}$ and $\mathrm{B}$ be $\mathrm{m} \times \mathrm{n}$ matrices over the field $\mathrm{F}$. Then $\mathrm{B}$ is row-equivalent to $\mathrm{A}$ if and only if $\mathrm{B}=\mathrm{PA}$, where $\mathrm{P}$ is a product of $\mathrm{m} \times \mathrm{m}$ elementary matrices.

Proof. Suppose $B=P A$ where $P=E_{s} \cdots E_{2} E_{1}$ and the $E_{i}$ are $m \times m$ elementary matrices. Then $E_{1} A$ is row-equivalent to $A$, and $E_{2}\left(E_{1} A\right)$ is row-equivalent to $E_{1} A$. So $E_{2} E_{1} A$ is row-equivalent to $A$; and continuing in this way we see that $\left(E_{s} \cdots E_{1}\right) A$ is row-equivalent to $A$.

Now suppose that $B$ is row-equivalent to $A$. Let $E_{1}, E_{2}, \ldots, E_{s}$ be the elementary matrices corresponding to some sequence of elementary row operations which carries $A$ into $B$. Then $B=\left(E_{s} \cdots E_{1}\right) A$. 

\section{Exercises}

1. Let

$$
A=\left[\begin{array}{rrr}
2 & -1 & 1 \\
1 & 2 & 1
\end{array}\right], \quad B=\left[\begin{array}{r}
3 \\
1 \\
-1
\end{array}\right], \quad C=\left[\begin{array}{ll}
1 & -1
\end{array}\right]
$$

Compute $A B C$ and $C A B$.

2. Let

$$
A=\left[\begin{array}{rrr}
1 & -1 & 1 \\
2 & 0 & 1 \\
3 & 0 & 1
\end{array}\right], \quad B=\left[\begin{array}{rr}
2 & -2 \\
1 & 3 \\
4 & 4
\end{array}\right] \text {. }
$$

Verify directly that $A(A B)=A^{2} B$.

3. Find two different $2 \times 2$ matrices $A$ such that $A^{2}=0$ but $A \neq 0$.

4. For the matrix $A$ of Exercise 2, find elementary matrices $E_{1}, E_{2}, \ldots, E_{k}$ such that

$$
E_{k} \cdots E_{2} E_{1} A=I \text {. }
$$

5. Let

$$
A=\left[\begin{array}{rr}
1 & -1 \\
2 & 2 \\
1 & 0
\end{array}\right], \quad B=\left[\begin{array}{rr}
3 & 1 \\
-4 & 4
\end{array}\right] \text {. }
$$

Is there a matrix $C$ such that $C A=B$ ?

6. Let $A$ be an $m \times n$ matrix and $B$ an $n \times k$ matrix. Show that the columns of $C=A B$ are linear combinations of the columns of $A$. If $\alpha_{1}, \ldots, \alpha_{n}$ are the columns of $A$ and $\gamma_{1}, \ldots, \gamma_{k}$ are the columns of $C$, then

$$
\gamma_{j}=\sum_{r=1}^{n} B_{r j} \alpha_{r} .
$$

7. Let $A$ and $B$ be $2 \times 2$ matrices such that $A B=I$. Prove that $B A=I$.

8. Let

$$
C=\left[\begin{array}{ll}
C_{11} & C_{12} \\
C_{21} & C_{22}
\end{array}\right]
$$

be a $2 \times 2$ matrix. We inquire when it is possible to find $2 \times 2$ matrices $A$ and $B$ such that $C=A B-B A$. Prove that such matrices can be found if and only if $C_{11}+C_{22}=0$

\subsection{Invertible Matrices}

Suppose $P$ is an $m \times m$ matrix which is a product of elementary matrices. For each $m \times n$ matrix $A$, the matrix $B=P A$ is row-equivalent to $A$; hence $A$ is row-equivalent to $B$ and there is a product $Q$ of elementary matrices such that $A=Q B$. In particular this is true when $A$ is the $m \times m$ identity matrix. In other words, there is an $m \times m$ matrix $Q$, which is itself a product of elementary matrices. such that $Q P=I$. As we shall soon see, the existence of a $Q$ with $Q P=I$ is equivalent to the fact that $P$ is a product of elementary matrices.

Definition. Let $\mathrm{A}$ be an $\mathrm{n} \times \mathrm{n}$ (square) matrix over the field $\mathrm{F}$. An $\mathrm{n} \times \mathrm{n}$ matrix $\mathrm{B}$ such that $\mathrm{BA}=\mathrm{I}$ is called $a$ left inverse of $\mathrm{A} ;$ an $\mathrm{n} \times \mathrm{n}$ matrix $\mathrm{B}$ such that $\mathrm{AB}=\mathrm{I}$ is called $a$ right inverse of $\mathrm{A}$. If $\mathrm{AB}=\mathrm{BA}=\mathrm{I}$, then $\mathrm{B}$ is called a two-sided inverse of $\mathrm{A}$ and $\mathrm{A}$ is said to be invertible.

Lemma. If $\mathrm{A}$ has a left inverse $\mathrm{B}$ and a right inverse $\mathrm{C}$, then $\mathrm{B}=\mathrm{C}$. Proof. Suppose $B A=I$ and $A C=I$. Then

$$
B=B I=B(A C)=(B A) C=I C=C .
$$

Thus if $A$ has a left and a right inverse, $A$ is invertible and has a unique two-sided inverse, which we shall denote by $A^{-1}$ and simply call the inverse of $A$.

Theorem 10. Let $\mathrm{A}$ and $\mathrm{B}$ be $\mathrm{n} \times \mathrm{n}$ matrices over $\mathrm{F}$.

(i) If $\mathrm{A}$ is invertible, so is $\mathrm{A}^{-1}$ and $\left(\mathrm{A}^{-1}\right)^{-1}=\mathrm{A}$.

(ii) If both $\mathrm{A}$ and $\mathrm{B}$ are invertible, so is $\mathrm{AB}$, and $(\mathrm{AB})^{-1}=\mathrm{B}^{-1} \mathrm{~A}^{-1}$.

Proof. The first statement is evident from the symmetry of the definition. The second follows upon verification of the relations

$$
(A B)\left(B^{-1} A^{-1}\right)=\left(B^{-1} A^{-1}\right)(A B)=I .
$$

Corollary. A product of invertible matrices is invertible.

Theorem 11. An elementary matrix is invertible.

Proof. Let $E$ be an elementary matrix corresponding to the elementary row operation $e$. If $e_{1}$ is the inverse operation of $e$ (Theorem 2) and $E_{1}=e_{1}(I)$, then

$$
E E_{1}=e\left(E_{1}\right)=e\left(e_{1}(I)\right)=I
$$

and

$$
E_{1} E=e_{1}(E)=e_{1}(e(I))=I
$$

so that $E$ is invertible and $E_{1}=E^{-1}$.

\section{EXAMple 14.}

(a)

$$
\left[\begin{array}{ll}
0 & 1 \\
1 & 0
\end{array}\right]^{-1}=\left[\begin{array}{ll}
0 & 1 \\
1 & 0
\end{array}\right]
$$

(b)

$$
\left[\begin{array}{ll}
1 & c \\
0 & 1
\end{array}\right]^{-1}=\left[\begin{array}{rr}
1 & -c \\
0 & 1
\end{array}\right]
$$

(c)

(d) When $c \neq 0$,

$\left[\begin{array}{ll}1 & 0 \\ c & 1\end{array}\right]^{-1}=\left[\begin{array}{rr}1 & 0 \\ -c & 1\end{array}\right]$

$$
\left[\begin{array}{ll}
c & 0 \\
0 & 1
\end{array}\right]^{-1}=\left[\begin{array}{ll}
c^{-1} & 0 \\
0 & 1
\end{array}\right] \text { and }\left[\begin{array}{ll}
1 & 0 \\
0 & c
\end{array}\right]^{-1}=\left[\begin{array}{ll}
1 & 0 \\
0 & c^{-1}
\end{array}\right] \text {. }
$$

Theorem 12. If $\mathrm{A}$ is an $\mathrm{n} \times \mathrm{n}$ matrix, the following are equivalent.

(i) A is invertible.

(ii) $\mathrm{A}$ is row-equivalent to the $\mathrm{n} \times \mathrm{n}$ identity matrix.

(iii) A is a product of elementary matrices.

Proof. Let $R$ be a row-reduced echelon matrix which is rowequivalent to $A$. By Theorem 9 (or its corollary),

$$
R=E_{k} \cdots E_{2} E_{1} A
$$

where $E_{1}, \ldots, E_{k}$ are elementary matrices. Each $E_{j}$ is invertible, and so

$$
A=E_{1}^{-1} \cdots E_{k}^{-1} R .
$$

Since products of invertible matrices are invertible, we see that $A$ is invertible if and only if $R$ is invertible. Since $R$ is a (square) row-reduced echelon matrix, $R$ is invertible if and only if each row of $R$ contains a non-zero entry, that is, if and only if $R=I$. We have now shown that $A$ is invertible if and only if $R=I$, and if $R=I$ then $A=E_{k}^{-1} \cdots E_{1}^{-1}$. It should now be apparent that (i), (ii), and (iii) are equivalent statements about $A$.

Corollary. If $\mathrm{A}$ is an invertible $\mathrm{n} \times \mathrm{n}$ matrix and if a sequence of elementary row operations reduces A to the identity, then that same sequence of operations when applied to I yields $\mathrm{A}^{-1}$.

Corollary. Let $\mathrm{A}$ and $\mathrm{B}$ be $\mathrm{m} \times \mathrm{n}$ matrices. Then $\mathrm{B}$ is row-equivalent to $\mathrm{A}$ if and only if $\mathrm{B}=\mathrm{PA}$ where $\mathrm{P}$ is an invertible $\mathrm{m} \times \mathrm{m}$ matrix.

Theorem 13. For an $\mathrm{n} \times \mathrm{n}$ matrix $\mathrm{A}$, the following are equivalent.

(i) A is invertible.

(ii) The homogeneous system $\mathrm{AX}=0$ has only the trivial solution $\mathrm{X}=0$.

(iii) The system of equations $\mathrm{AX}=\mathrm{Y}$ has a solution $\mathrm{X}$ for each $\mathrm{n} \times 1$ matrix Y.

Proof. According to Theorem 7, condition (ii) is equivalent to the fact that $A$ is row-equivalent to the identity matrix. By Theorem 12, (i) and (ii) are therefore equivalent. If $A$ is invertible, the solution of $A X=Y$ is $X=A^{-1} Y$. Conversely, suppose $A X=Y$ has a solution for each given $Y$. Let $R$ be a row-reduced echelon matrix which is row- equivalent to $A$. We wish to show that $R=I$. That amounts to showing that the last row of $R$ is not (identically) 0 . Let

$$
E=\left[\begin{array}{c}
0 \\
0 \\
\vdots \\
0 \\
1
\end{array}\right] \text {. }
$$

If the system $R X=E$ can be solved for $X$, the last row of $R$ cannot be 0 . We know that $R=P A$, where $P$ is invertible. Thus $R X=E$ if and only if $A X=P^{-1} E$. According to (iii), the latter system has a solution.

Corollary. A square matrix with either a left or right inverse is invertible.

Proof. Let $A$ be an $n \times n$ matrix. Suppose $A$ has a left inverse, i.e., a matrix $B$ such that $B A=I$. Then $A X=0$ has only the trivial solution, because $X=I X=B(A X)$. Therefore $A$ is invertible. On the other hand, suppose $A$ has a right inverse, i.e., a matrix $C$ such that $A C=I$. Then $C$ has a left inverse and is therefore invertible. It then follows that $A=C^{-1}$ and so $A$ is invertible with inverse $C$.

Corollary. Let $\mathrm{A}=\mathrm{A}_{1} \mathrm{~A}_{2} \cdots A_{\mathrm{k}}$, where $\mathrm{A}_{1} \ldots, A_{\mathrm{k}}$ are $\mathrm{n} \times \mathrm{n}$ (square) matrices. Then $\mathrm{A}$ is invertible if and only if each $\mathrm{A}_{\mathrm{j}}$ is invertible.

Proof. We have already shown that the product of two invertible matrices is invertible. From this one sees easily that if each $A_{j}$ is invertible then $A$ is invertible.

Suppose now that $A$ is invertible. We first prove that $A_{k}$ is invertible. Suppose $X$ is an $n \times 1$ matrix and $A_{k} X=0$. Then $A X=$ $\left(A_{1} \cdots A_{k-1}\right) A_{k} X=0$. Since $A$ is invertible we must have $X=0$. The system of equations $A_{k} X=0$ thus has no non-trivial solution, so $A_{k}$ is invertible. But now $A_{1} \cdots A_{k-1}=A A_{k}^{-1}$ is invertible. By the preceding argument, $A_{k-1}$ is invertible. Continuing in this way, we conclude that each $A_{j}$ is invertible.

We should like to make one final comment about the solution of linear equations. Suppose $A$ is an $m \times n$ matrix and we wish to solve the system of equations $A X=Y$. If $R$ is a row-reduced echelon matrix which is row-equivalent to $A$, then $R=P A$ where $P$ is an $m \times m$ invertible matrix. The solutions of the system $A X=Y$ are exactly the same as the solutions of the system $R X=P Y(=Z)$. In practice, it is not much more difficult to find the matrix $P$ than it is to row-reduce $A$ to $R$. For, suppose we form the augmented matrix $A^{\prime}$ of the system $A X=Y$, with arbitrary scalars $y_{1}, \ldots, y_{m}$ occurring in the last column. If we then perform on $\mathrm{A}^{\prime}$ a sequence of elementary row operations which leads from $A$ to $R$, it will become evident what the matrix $P$ is. (The reader should refer to Example 9 where we essentially carried out this process.) In particular, if $A$ is a square matrix, this process will make it clear whether or not $A$ is invertible and if $A$ is invertible what the inverse $P$ is. Since we have already given the nucleus of one example of such a computation, we shall content ourselves with a $2 \times 2$ example.

Example 15. Suppose $F$ is the field of rational numbers and

Then

$$
A=\left[\begin{array}{rr}
2 & -1 \\
1 & 3
\end{array}\right] \text {. }
$$

$\left[\begin{array}{rrr}2 & -1 & y_{1} \\ 1 & 3 & y_{2}\end{array}\right] \stackrel{(3)}{\longrightarrow}\left[\begin{array}{rrr}1 & 3 & y_{2} \\ 2 & -1 & y_{1}\end{array}\right] \stackrel{(2)}{\longrightarrow}\left[\begin{array}{rrc}1 & 3 & y_{2} \\ 0 & -7 & y_{1}-2 y_{2}\end{array}\right] \stackrel{(1)}{\longrightarrow}$

$$
\left[\begin{array}{ccc}
1 & 3 & y_{2} \\
0 & 1 & \frac{1}{7}\left(2 y_{2}-y_{1}\right)
\end{array}\right] \stackrel{(2)}{\longrightarrow}\left[\begin{array}{ccc}
1 & 0 & \frac{1}{7}\left(y_{2}+3 y_{1}\right) \\
0 & 1 & \frac{1}{7}\left(2 y_{2}-y_{1}\right)
\end{array}\right]
$$

from which it is clear that $A$ is invertible and

$$
A^{-1}=\left[\begin{array}{rr}
\frac{3}{7} & \frac{1}{7} \\
-\frac{1}{7} & \frac{2}{7}
\end{array}\right] .
$$

It may seem cumbersome to continue writing the arbitrary scalars $y_{1}, y_{2}, \ldots$ in the computation of inverses. Some people find it less awkward to carry along two sequences of matrices, one describing the reduction of $A$ to the identity and the other recording the effect of the same sequence of operations starting from the identity. The reader may judge for himself which is a neater form of bookkeeping.

Example 16. Let us find the inverse of

$$
\begin{aligned}
& A=\left[\begin{array}{lll}1 & \frac{1}{2} & \frac{1}{3} \\\frac{1}{2} & \frac{1}{3} & \frac{1}{4} \\\frac{1}{3} & \frac{1}{4} & \frac{1}{5}\end{array}\right] . \\
& \left[\begin{array}{lll}1 & \frac{1}{2} & \frac{1}{3} \\\frac{1}{2} & \frac{1}{3} & \frac{1}{4} \\\frac{1}{3} & \frac{1}{4} & \frac{1}{5}\end{array}\right], \quad\left[\begin{array}{lll}1 & 0 & 0 \\0 & 1 & 0 \\0 & 0 & 1\end{array}\right] \\
& \left[\begin{array}{rrr}1 & \frac{1}{2} & \frac{1}{3} \\0 & \frac{1}{12} & \frac{1}{12} \\0 & \frac{1}{12} & \frac{4}{45}\end{array}\right], \quad\left[\begin{array}{rrr}1 & 0 & 0 \\-\frac{1}{2} & 1 & 0 \\-\frac{1}{3} & 0 & 1\end{array}\right] \\
& \left[\begin{array}{rrr}1 & \frac{1}{2} & \frac{1}{3} \\0 & \frac{1}{12} & \frac{1}{12} \\0 & 0 & \frac{1}{180}\end{array}\right], \quad\left[\begin{array}{rrr}1 & 0 & 0 \\-\frac{1}{2} & 1 & 0 \\\frac{1}{6} & -1 & 1\end{array}\right] \\
& \left[\begin{array}{lll}1 & \frac{1}{2} & \frac{1}{3} \\0 & 1 & 1 \\0 & 0 & 1\end{array}\right], \quad\left[\begin{array}{rrr}1 & 0 & 0 \\-6 & 12 & 0 \\30 & -180 & 180\end{array}\right] 
\end{aligned}
$$



$$
\begin{aligned}
& \left[\begin{array}{lll}1 & \frac{1}{2} & 0 \\0 & 1 & 0 \\0 & 0 & 1\end{array}\right], \quad\left[\begin{array}{rrr}-9 & 60 & -60 \\-36 & 192 & -180 \\30 & -180 & 180\end{array}\right] \\
& \left[\begin{array}{lll}1 & 0 & 0 \\0 & 1 & 0 \\0 & 0 & 1\end{array}\right], \quad\left[\begin{array}{rrr}9 & -36 & 30 \\-36 & 192 & -180 \\30 & -180 & 180\end{array}\right] \text {. }
\end{aligned}
$$

It must have occurred to the reader that we have carried on a lengthy discussion of the rows of matrices and have said little about the columns. We focused our attention on the rows because this seemed more natural from the point of view of linear equations. Since there is obviously nothing sacred about rows, the discussion in the last sections could have been carried on using columns rather than rows. If one defines an elementary column operation and column-equivalence in a manner analogous to that of elementary row operation and row-equivalence, it is clear that each $m \times n$ matrix will be column-equivalent to a 'column-reduced echelon' matrix. Also each elementary column operation will be of the form $A \rightarrow A E$, where $E$ is an $n \times n$ elementary matrix-and so on.

\section{Exercises}

1. Let

$$
A=\left[\begin{array}{rrrr}
1 & 2 & 1 & 0 \\
-1 & 0 & 3 & 5 \\
1 & -2 & 1 & 1
\end{array}\right] \text {. }
$$

Find a row-reduced echelon matrix $R$ which is row-equivalent to $A$ and an invertible $3 \times 3$ matrix $P$ such that $R=P A$.

2. Do Exercise 1, but with

$$
A=\left[\begin{array}{rrr}
2 & 0 & i \\
1 & -3 & -i \\
i & 1 & 1
\end{array}\right] .
$$

3. For each of the two matrices

$$
\left[\begin{array}{rrr}
2 & 5 & -1 \\
4 & -1 & 2 \\
6 & 4 & 1
\end{array}\right], \quad\left[\begin{array}{rrr}
1 & -1 & 2 \\
3 & 2 & 4 \\
0 & 1 & -2
\end{array}\right]
$$

use elementary row operations to discover whether it is invertible, and to find the inverse in case it is.

4. Let

$$
A=\left[\begin{array}{lll}
5 & 0 & 0 \\
1 & 5 & 0 \\
0 & 1 & 5
\end{array}\right]
$$

For which $X$ does there exist a scalar $c$ such that $A X=c X$ ?

5. Discover whether

$$
A=\left[\begin{array}{llll}
1 & 2 & 3 & 4 \\
0 & 2 & 3 & 4 \\
0 & 0 & 3 & 4 \\
0 & 0 & 0 & 4
\end{array}\right]
$$

is invertible, and find $A^{-1}$ if it exists.

6. Suppose $A$ is a $2 \times 1$ matrix and that $B$ is a $1 \times 2$ matrix. Prove that $C=A B$ is not invertible.

7. Let $A$ be an $n \times n$ (square) matrix. Prove the following two statements:

(a) If $A$ is invertible and $A B=0$ for some $n \times n$ matrix $B$, then $B=0$.

(b) If $A$ is not invertible, then there exists an $n \times n$ matrix $B$ such that $A B=0$ but $B \neq 0$.

8. Let

$$
A=\left[\begin{array}{ll}
a & b \\
c & d
\end{array}\right]
$$

Prove, using elementary row operations, that $A$ is invertible if and only if $(a d-b c) \neq 0$

9. An $n \times n$ matrix $A$ is called upper-triangular if $A_{i j}=0$ for $i>j$, that is, if every entry below the main diagonal is 0 . Prove that an upper-triangular (square) matrix is invertible if and only if every entry on its main diagonal is different from 0.

10. Prove the following generalization of Exercise 6. If $A$ is an $m \times n$ matrix, $B$ is an $n \times m$ matrix and $n<m$, then $A B$ is not invertible.

11. Let $A$ be an $m \times n$ matrix. Show that by means of a finite number of elementary row and/or column operations one can pass from $A$ to a matrix $R$ which is both 'row-reduced echelon' and 'column-reduced echelon,' i.e., $R_{i j}=0$ if $i \neq j$, $R_{i i}=1,1 \leq i \leq r, R_{i i}=0$ if $i>r$. Show that $R=P A Q$, where $P$ is an invertible $m \times m$ matrix and $Q$ is an invertible $n \times n$ matrix.

12. The result of Example 16 suggests that perhaps the matrix

$$
A=\left[\begin{array}{cccc}
1 & \frac{1}{2} & \cdots & \frac{1}{n} \\
\frac{1}{2} & \frac{1}{3} & \cdots & \frac{1}{n+1} \\
\vdots & \vdots & & \vdots \\
\frac{1}{n} & \frac{1}{n+1} & \cdots & \frac{1}{2 n-1}
\end{array}\right]
$$

is invertible and $A^{-1}$ has integer entries. Can you prove that? 

\section{Vector Spaces}

\subsection{Vector Spaces}

In various parts of mathematics, one is confronted with a set, such that it is both meaningful and interesting to deal with 'linear combinations' of the objects in that set. For example, in our study of linear equations we found it quite natural to consider linear combinations of the rows of a matrix. It is likely that the reader has studied calculus and has dealt there with linear combinations of functions; certainly this is so if he has studied differential equations. Perhaps the reader has had some experience with vectors in three-dimensional Euclidean space, and in particular, with linear combinations of such vectors.

Loosely speaking, linear algebra is that branch of mathematics which treats the common properties of algebraic systems which consist of a set, together with a reasonable notion of a 'linear combination' of elements in the set. In this section we shall define the mathematical object which experience has shown to be the most useful abstraction of this type of algebraic system.

Definition. A vector space (or linear space) consists of the following:

1. a field $\mathrm{F}$ of scalars;

2. a set V of objects, called vectors;

3. a rule (or operation), called vector addition, which associates with each pair of vectors $\alpha, \beta$ in $\mathrm{V}$ a vector $\alpha+\beta$ in $\mathrm{V}$, called the sum of $\alpha$ and $\beta$, in such a way that

(a) addition is commutative, $\alpha+\beta=\beta+\alpha$;

(b) addition is associative, $\alpha+(\beta+\gamma)=(\alpha+\beta)+\gamma$; (c) there is a unique vector 0 in $\mathrm{V}$, called the zero vector, such that $\alpha+0=\alpha$ for all $\alpha$ in $\mathrm{V}$

(d) for each vector $\alpha$ in $\mathrm{V}$ there is a unique vector $-\alpha$ in $\mathrm{V}$ such that $\alpha+(-\alpha)=0$

4. a rule (or operation), called scalar multiplication, which associates with each scalar $\mathrm{c}$ in $\mathrm{F}$ and vector $\alpha$ in $\mathrm{V}$ a vector $\mathrm{c} \alpha$ in $\mathrm{V}$, called the product of $\mathrm{c}$ and $\alpha$, in such a way that

(a) $1 \alpha=\alpha$ for every $\alpha$ in $\mathrm{V}$;

(b) $\left(\mathrm{c}_{1} \mathrm{c}_{2}\right) \alpha=\mathrm{c}_{1}\left(\mathrm{c}_{2} \alpha\right)$;

(c) $\mathrm{c}(\alpha+\beta)=\mathrm{c} \alpha+\mathrm{c} \beta$;

(d) $\left(\mathrm{c}_{1}+\mathrm{c}_{2}\right) \alpha=\mathrm{c}_{1} \alpha+\mathrm{c}_{2} \alpha$.

It is important to observe, as the definition states, that a vector space is a composite object consisting of a field, a set of 'vectors,' and two operations with certain special properties. The same set of vectors may be part of a number of distinct vector spaces (see Example 5 below). When there is no chance of confusion, we may simply refer to the vector space as $V$, or when it is desirable to specify the field, we shall say $V$ is a vector space over the field $F$. The name 'vector' is applied to the elements of the set $V$ largely as a matter of convenience. The origin of the name is to be found in Example 1 below, but one should not attach too much significance to the name, since the variety of objects occurring as the vectors in $V$ may not bear much resemblance to any preassigned concept of vector which the reader has. We shall try to indicate this variety by a list of examples; our list will be enlarged considerably as we begin to study vector spaces.

EXample 1. The $n$-tuple space, $F^{n}$. Let $F$ be any field, and let $V$ be the set of all $n$-tuples $\alpha=\left(x_{1}, x_{2}, \ldots, x_{n}\right)$ of scalars $x_{i}$ in $F$. If $\beta=$ $\left(y_{1}, y_{2}, \ldots, y_{n}\right)$ with $y_{i}$ in $F$, the sum of $\alpha$ and $\beta$ is defined by

$$
\alpha+\beta=\left(x_{1}+y_{1}, x_{2}+y_{2}, \ldots, x_{n}+y_{n}\right) .
$$

The product of a scalar $c$ and vector $\alpha$ is defined by

$$
c \alpha=\left(c x_{1}, c x_{2}, \ldots, c x_{n}\right) .
$$

The fact that this vector addition and scalar multiplication satisfy conditions (3) and (4) is easy to verify, using the similar properties of addition and multiplication of elements of $F$.

Example 2 . The space of $m \times n$ matrices, $F^{m \times n}$. Let $F$ be any field and let $m$ and $n$ be positive integers. Let $F^{m \times n}$ be the set of all $m \times n$ matrices over the field $F$. The sum of two vectors $A$ and $B$ in $F^{m \times n}$ is defined by

$$
(A+B)_{i j}=A_{i j}+B_{i j} .
$$

The product of a scalar $c$ and the matrix $A$ is defined by

$$
(c A)_{i j}=c A_{i j} .
$$

Note that $F^{1 \times n}=F^{n}$.

Example 3. The space of funclions from a set to a field. Let $F$ be any field and let $S$ be any non-empty set. Let $V$ be the set of all functions from the set $S$ into $F$. The sum of two vectors $f$ and $g$ in $V$ is the vector $f+g$, i.e., the function from $S$ into $F$, defined by

$$
(f+g)(s)=f(s)+g(s) .
$$

The product of the scalar $c$ and the function $f$ is the function $c f$ defined by

$$
(c f)(s)=c f(s) .
$$

The preceding examples are special cases of this one. For an $n$-tuple of elements of $F$ may be regarded as a function from the set $S$ of integers $1, \ldots, n$ into $F$. Similarly, an $m \times n$ matrix over the field $F$ is a function from the set $S$ of pairs of integers, $(i, j), 1 \leq i \leq m, 1 \leq j \leq n$, into the field $F$. For this third example we shall indicate how one verifies that the operations we have defined satisfy conditions (3) and (4). For vector addition:

(a) Since addition in $F$ is commutative,

$$
f(s)+g(s)=g(s)+f(s)
$$

for each $s$ in $S$, so the functions $f+g$ and $g+f$ are identical.

(b) Since addition in $F$ is associative,

$$
f(s)+[g(s)+h(s)]=[f(s)+g(s)]+h(s)
$$

for each $s$, so $f+(g+h)$ is the same function as $(f+g)+h$.

(c) The unique zero vector is the zero function which assigns to each element of $S$ the scalar 0 in $F$.

(d) For each $f$ in $V,(-f)$ is the function which is given by

$$
(-f)(s)=-f(s) \text {. }
$$

The reader should find it easy to verify that scalar multiplication satisfies the conditions of (4), by arguing as we did with the vector addition.

Example 4 . The space of polynomial functions over a field $F$. Let $F$ be a field and let $V$ be the set of all functions $f$ from $F$ into $F$ which have a rule of the form

$$
f(x)=c_{0}+c_{1} x+\cdots+c_{n} x^{n}
$$

where $c_{0}, c_{1}, \ldots, c_{n}$ are fixed scalars in $F$ (independent of $x$ ). A function of this type is called a polynomial function on $F$. Let addition and scalar multiplication be defined as in Example 3. One must observe here that if $f$ and $g$ are polynomial functions and $c$ is in $F$, then $f+g$ and $c f$ are again polynomial functions. Example 5. The field $C$ of complex numbers may be regarded as a vector space over the field $R$ of real numbers. More generally, let $F$ be the field of real numbers and let $V$ be the set of $n$-tuples $\alpha=\left(x_{1}, \ldots, x_{n}\right)$ where $x_{1}, \ldots, x_{n}$ are complex numbers. Define addition of vectors and scalar multiplication by (2-1) and (2-2), as in Example 1. In this way we obtain a vector space over the field $R$ which is quite different from the space $C^{n}$ and the space $R^{n}$.

There are a few simple facts which follow almost immediately from the definition of a vector space, and we proceed to derive these. If $c$ is a scalar and 0 is the zero vector, then by 3 (c) and $4(\mathrm{c})$

$$
c 0=c(0+0)=c 0+c 0 .
$$

Adding $-(c 0)$ and using $3(\mathrm{~d})$, we obtain

$$
c 0=0 \text {. }
$$

Similarly, for the scalar 0 and any vector $\alpha$ we find that

$$
0 \alpha=0 .
$$

If $c$ is a non-zero scalar and $\alpha$ is a vector such that $c \alpha=0$, then by $(2-8)$, $c^{-1}(c \alpha)=0$. But

$$
c^{-1}(c \alpha)=\left(c^{-1} c\right) \alpha=1 \alpha=\alpha
$$

hence, $\alpha=0$. Thus we see that if $c$ is a scalar and $\alpha$ a vector such that $c \alpha=0$, then either $c$ is the zero scalar or $\alpha$ is the zero vector.

If $\alpha$ is any vector in $V$, then

$$
0=0 \alpha=(1-1) \alpha=1 \alpha+(-1) \alpha=\alpha+(-1) \alpha
$$

from which it follows that

$$
(-1) \alpha=-\alpha \text {. }
$$

Finally, the associative and commutative properties of vector addition imply that a sum involving a number of vectors is independent of the way in which these vectors are combined and associated. For example, if $\alpha_{1}, \alpha_{2}, \alpha_{3}, \alpha_{4}$ are vectors in $V$, then

$$
\left(\alpha_{1}+\alpha_{2}\right)+\left(\alpha_{3}+\alpha_{4}\right)=\left[\alpha_{2}+\left(\alpha_{1}+\alpha_{3}\right)\right]+\alpha_{4}
$$

and such a sum may be written without confusion as

$$
\alpha_{1}+\alpha_{2}+\alpha_{3}+\alpha_{4} .
$$

Definition. A vector $\beta$ in $\mathrm{V}$ is said to be a linear combination of the vectors $\alpha_{1}, \ldots, \alpha_{n}$ in $\mathrm{V}$ provided there exist scalars $\mathrm{c}_{1}, \ldots, \mathrm{c}_{\mathrm{n}}$ in $\mathrm{F}$ such that

$$
\begin{aligned}
\beta & =c_{1} \alpha_{1}+\cdots+c_{n} \alpha_{n} \\
& =\sum_{i=1}^{n} c_{i} \alpha_{i} .
\end{aligned}
$$

Other extensions of the associative property of vector addition and the distributive properties $4(\mathrm{c})$ and $4(\mathrm{~d})$ of scalar multiplication apply to linear combinations:

$$
\begin{aligned}
\sum_{i=1}^{n} c_{i} \alpha_{i}+\sum_{i=1}^{n} d_{i} \alpha_{i} & =\sum_{i=1}^{n}\left(c_{i}+d_{i}\right) \alpha_{i} \\
c \sum_{i=1}^{n} c_{i} \alpha_{i} & =\sum_{i=1}^{n}\left(c c_{i}\right) \alpha_{i .}
\end{aligned}
$$

Certain parts of linear algebra are intimately related to geometry. The very word 'space' suggests something geometrical, as does the word 'vector' to most people. As we proceed with our study of vector spaces, the reader will observe that much of the terminology has a geometrical connotation. Before concluding this introductory section on vector spaces, we shall consider the relation of vector spaces to geometry to an extent which will at least indicate the origin of the name 'vector space.' This will be a brief intuitive discussion.

Let us consider the vector space $R^{3}$. In analytic geometry, one identifies triples $\left(x_{1}, x_{2}, x_{3}\right)$ of real numbers with the points in three-dimensional Euclidean space. In that context, a vector is usually defined as a directed line segment $P Q$, from a point $P$ in the space to another point $Q$. This amounts to a careful formulation of the idea of the 'arrow' from $P$ to $Q$. As vectors are used, it is intended that they should be determined by their length and direction. Thus one must identify two directed line segments if they have the same length and the same direction.

The directed line segment $P Q$, from the point $P=\left(x_{1}, x_{2}, x_{3}\right)$ to the point $Q=\left(y_{1}, y_{2}, y_{3}\right)$, has the same length and direction as the directed line segment from the origin $O=(0,0,0)$ to the point $\left(y_{1}-x_{1}, y_{2}-x_{2}\right.$, $\left.y_{3}-x_{3}\right)$. Furthermore, this is the only segment emanating from the origin which has the same length and direction as $P Q$. Thus, if one agrees to treat only vectors which emanate from the origin, there is exactly one vector associated with each given length and direction.

The vector $O P$, from the origin to $P=\left(x_{1}, x_{2}, x_{3}\right)$, is completely determined by $P$, and it is therefore possible to identify this vector with the point $P$. In our definition of the vector space $R^{3}$, the vectors are simply defined to be the triples $\left(x_{1}, x_{2}, x_{3}\right)$.

Given points $P=\left(x_{1}, x_{2}, x_{3}\right)$ and $Q=\left(y_{1}, y_{2}, y_{3}\right)$, the definition of the sum of the vectors $O P$ and $O Q$ can be given geometrically. If the vectors are not parallel, then the segments $O P$ and $O Q$ determine a plane and these segments are two of the edges of a parallelogram in that plane (see Figure 1). One diagonal of this parallelogram extends from $O$ to a point $S$, and the sum of $O P$ and $O Q$ is defined to be the vector $O S$. The coordinates of the point $S$ are $\left(x_{1}+y_{1}, x_{2}+y_{2}, x_{3}+y_{3}\right)$ and hence this geometrical definition of vector addition is equivalent to the algebraic definition of Example 1. 

![](https://cdn.mathpix.com/cropped/2023_01_16_0f6afdf132d84aaf6bdeg-042.jpg?height=697&width=815&top_left_y=189&top_left_x=233)

Frgure 1

Scalar multiplication has a simpler geometric interpretation. If $c$ is a real number, then the product of $c$ and the vector $O P$ is the vector from the origin with length $|c|$ times the length of $O P$ and a direction which agrees with the direction of $O P$ if $c>0$, and which is opposite to the direction of $O P$ if $c<0$. This scalar multiplication just yields the vector $O T$ where $T=\left(c x_{1}, c x_{2}, c x_{3}\right)$, and is therefore consistent with the algebraic definition given for $R^{3}$.

From time to time, the reader will probably find it helpful to 'think geometrically' about vector spaces, that is, to draw pictures for his own benefit to illustrate and motivate some of the ideas. Indeed, he should do this. However, in forming such illustrations he must bear in mind that, because we are dealing with vector spaces as algebraic systems, all proofs we give will be of an algebraic nature.

\section{Exercises}

1. If $F$ is a field, verify that $F^{n}$ (as defined in Example 1) is a vector space over the field $F$.

2. If $V$ is a vector space over the field $F$, verify that

$$
\left(\alpha_{1}+\alpha_{2}\right)+\left(\alpha_{3}+\alpha_{4}\right)=\left[\alpha_{2}+\left(\alpha_{3}+\alpha_{1}\right)\right]+\alpha_{4}
$$

for all vectors $\alpha_{1}, \alpha_{2}, \alpha_{3}$, and $\alpha_{4}$ in $V$.

3. If $C$ is the field of complex numbers, which vectors in $C^{3}$ are linear combinations of $(1,0,-1),(0,1,1)$, and $(1,1,1)$ ? 4. Let $V$ be the set of all pairs $(x, y)$ of real numbers, and let $F$ be the field of real numbers. Define

$$
\begin{aligned}
(x, y)+\left(x_{1}, y_{1}\right) & =\left(x+x_{1}, y+y_{1}\right) \\
c(x, y) & =(c x, y) .
\end{aligned}
$$

Is $V$, with these operations, a vector space over the field of real numbers?

5. On $R^{n}$, define two operations

$$
\begin{aligned}
\alpha \oplus \beta & =\alpha-\beta \\
c \cdot \alpha & =-c \alpha .
\end{aligned}
$$

The operations on the right are the usual ones. Which of the axioms for a vector space are satisfied by $\left(R^{n}, \oplus, \cdot\right)$ ?

6. Let $V$ be the set of all complex-valued functions $f$ on the real line such that (for all $t$ in $R$ )

$$
f(-t)=\overline{f(t)} .
$$

The bar denotes complex conjugation. Show that $V$, with the operations

$$
\begin{aligned}
(f+g)(t) & =f(t)+g(t) \\
(c f)(t) & =c f(t)
\end{aligned}
$$

is a vector space over the field of real numbers. Give an example of a function in $V$ which is not real-valued.

7. Let $V$ be the set of pairs $(x, y)$ of real numbers and let $F$ be the field of real numbers. Define

$$
\begin{aligned}
(x, y)+\left(x_{1}, y_{1}\right) & =\left(x+x_{1}, 0\right) \\
c(x, y) & =(c x, 0) .
\end{aligned}
$$

Is $V$, with these operations, a vector space?

\subsection{Subspaces}

In this section we shall introduce some of the basic concepts in the study of vector spaces.

Definition. Let $\mathrm{V}$ be a vector space over the field F. A subspace of $\mathrm{V}$ is a subset $\mathrm{W}$ of $\mathrm{V}$ which is itself a vector space over $\mathrm{F}$ with the operations of vector addition and scalar multiplication on $\mathrm{V}$.

A direct check of the axioms for a vector space shows that the subset $W$ of $V$ is a subspace if for each $\alpha$ and $\beta$ in $W$ the vector $\alpha+\beta$ is again in $W$; the 0 vector is in $W$; for each $\alpha$ in $W$ the vector $(-\alpha)$ is in $W$; for each $\alpha$ in $W$ and each scalar $c$ the vector $c \alpha$ is in $W$. The commutativity and associativity of vector addition, and the properties (4)(a), (b), (c), and (d) of scalar multiplication do not need to be checked, since these are properties of the operations on $V$. One can simplify things still further. Theorem 1. A non-empty subset $\mathrm{W}$ of $\mathrm{V}$ is a subspace of $\mathrm{V}$ if and only if for each pair of vectors $\alpha, \beta$ in $\mathrm{W}$ and each scalar $\mathrm{c}$ in $\mathrm{F}$ the vector $c \alpha+\beta$ is again in $\mathrm{W}$.

Proof. Suppose that $W$ is a non-empty subset of $V$ such that c $\alpha+\beta$ belongs to $W$ for all vectors $\alpha, \beta$ in $W$ and all scalars $c$ in $F$. Since $W$ is non-empty, there is a vector $\rho$ in $W$, and hence $(-1) \rho+\rho=0$ is in $W$. Then if $\alpha$ is any vector in $W$ and $c$ any scalar, the vector $c \alpha=c \alpha+0$ is in $W$. In particular, $(-1) \alpha=-\alpha$ is in $W$. Finally, if $\alpha$ and $\beta$ are in $W$, then $\alpha+\beta=1 \alpha+\beta$ is in $W$. Thus $W$ is a subspace of $V$.

Conversely, if $W$ is a subspace of $V, \alpha$ and $\beta$ are in $W$, and $c$ is a scalar, certainly $c \alpha+\beta$ is in $W$.

Some people prefer to use the $c \alpha+\beta$ property in Theorem 1 as the definition of a subspace. It makes little difference. The important point is that, if $W$ is a non-empty subset of $V$ such that $c \alpha+\beta$ is in $V$ for all $\alpha$, $\beta$ in $W$ and all $c$ in $F$, then (with the operations inherited from $V$ ) $W$ is a vector space. This provides us with many new examples of vector spaces.

EXAmple 6.

(a) If $V$ is any vector space, $V$ is a subspace of $V$; the subset consisting of the zero vector alone is a subspace of $V$, called the zero subspace of $V$.

(b) In $F^{n}$, the set of $n$-tuples $\left(x_{1}, \ldots, x_{n}\right)$ with $x_{1}=0$ is a subspace; however, the set of $n$-tuples with $x_{1}=1+x_{2}$ is not a subspace $(n \geq 2)$.

(c) The space of polynomial functions over the field $F$ is a subspace of the space of all functions from $F$ into $F$.

(d) An $n \times n$ (square) matrix $A$ over the field $F$ is symmetric if $A_{i j}=A_{j i}$ for each $i$ and $j$. The symmetric matrices form a subspace of the space of all $n \times n$ matrices over $F$.

(e) $\operatorname{An} n \times n$ (square) matrix $A$ over the field $C$ of complex numbers is Hermitian (or self-adjoint) if

$$
A_{j k}=\overline{A_{k j}}
$$

for each $j, k$, the bar denoting complex conjugation. A $2 \times 2$ matrix is Hermitian if and only if it has the form

$$
\left[\begin{array}{cc}
z & x+i y \\
x-i y & w
\end{array}\right]
$$

where $x, y, z$, and $w$ are real numbers. The set of all Hermitian matrices is not a subspace of the space of all $n \times n$ matrices over $C$. For if $A$ is Hermitian, its diagonal entries $A_{11}, A_{22}, \ldots$, are all real numbers, but the diagonal entries of $i A$ are in general not real. On the other hand, it is easily verified that the set of $n \times n$ complex Hermitian matrices is a vector space over the field $R$ of real numbers (with the usual operations). EXAMPLE 7. The solution space of a system of homogeneous linear equations. Let $A$ be an $m \times n$ matrix over $F$. Then the set of all $n \times 1$ (column) matrices $X$ over $F$ such that $A X=0$ is a subspace of the space of all $n \times 1$ matrices over $F$. To prove this we must show that $A(c X+Y)=0$ when $A X=0, A Y=0$, and $c$ is an arbitrary scalar in $F$. This follows immediately from the following general fact.

Lemma. If $\mathrm{A}$ is an $\mathrm{m} \times \mathrm{n}$ matrix over $\mathrm{F}$ and $\mathrm{B}, \mathrm{C}$ are $\mathrm{n} \times \mathrm{p}$ matrices over $\mathrm{F}$ then

$$
\mathrm{A}(\mathrm{dB}+\mathrm{C})=\mathrm{d}(\mathrm{AB})+\mathrm{AC}
$$

for each scalar $\mathrm{d}$ in $\mathrm{F}$.

$$
\text { Proof. } \begin{aligned}
{[A(d B+C)]_{i j} } & =\sum_{k} A_{i k}(d B+C)_{k j} \\
& =\sum_{k}\left(d A_{i k} B_{k j}+A_{i k} C_{k j}\right) \\
& =d \sum_{k} A_{i k} B_{k j}+\sum_{k} A_{i k} C_{k j} \\
& =d(A B)_{i j}+(A C)_{i j} \\
& =[d(A B)+A C]_{i j} .
\end{aligned}
$$

Similarly one can show that $(d B+C) A=d(B A)+C A$, if the matrix sums and products are defined.

Theorem 2. Let $\mathrm{V}$ be a vector space over the field F. The intersection of any collection of subspaces of $\mathrm{V}$ is a subspace of $\mathrm{V}$.

Proof. Let $\left\{W_{a}\right\}$ be a collection of subspaces of $V$, and let $W=$ $\cap W_{a}$ be their intersection. Recall that $W$ is defined as the set of all elea

ments belonging to every $W_{a}$ (see Appendix). Since each $W_{a}$ is a subspace, each contains the zero vector. Thus the zero vector is in the intersection $W$, and $W$ is non-empty. Let $\alpha$ and $\beta$ be vectors in $W$ and let $c$ be a scalar. By definition of $W$, both $\alpha$ and $\beta$ belong to each $W_{a}$, and because each $W_{a}$ is a subspace, the vector $(c \alpha+\beta)$ is in every $W_{a}$. Thus $(c \alpha+\beta)$ is again in $W$. By Theorem $1, W$ is a subspace of $V$.

From Theorem 2 it follows that if $S$ is any collection of vectors in $V$, then there is a smallest subspace of $V$ which contains $S$, that is, a subspace which contains $S$ and which is contained in every other subspace containing $S$.

Definition. Let $\mathrm{S}$ be a set of vectors in a vector space $\mathrm{V}$. The subspace spanned by $\mathrm{S}$ is defined to be the intersection $\mathrm{W}$ of all subspaces of $\mathrm{V}$ which contain $\mathrm{S}$. When $\mathrm{S}$ is a finite set of vectors, $\mathrm{S}=\left\{\alpha_{1}, \alpha_{2}, \ldots, \alpha_{\mathrm{n}}\right\}$, we shall simply call $\mathrm{W}$ the subspace spanned by the vectors $\alpha_{1}, \alpha_{2}, \ldots, \alpha_{\mathrm{n}}$. Theorem 3. The subspace spanned by a non-empty subset $\mathrm{S}$ of a vector space $\mathrm{V}$ is the set of all linear combinations of vectors in $\mathrm{S}$.

Proof. Let $W$ be the subspace spanned by $S$. Then each linear combination

$$
\alpha=x_{1} \alpha_{1}+x_{2} \alpha_{2}+\cdots+x_{m} \alpha_{m}
$$

of vectors $\alpha_{1}, \alpha_{2}, \ldots, \alpha_{m}$ in $S$ is clearly in $W$. Thus $W$ contains the set $L$ of all linear combinations of vectors in $S$. The set $L$, on the other hand, contains $S$ and is non-empty. If $\alpha, \beta$ belong to $L$ then $\alpha$ is a linear combination,

$$
\alpha=x_{1} \alpha_{1}+x_{2} \alpha_{2}+\cdots+x_{m} \alpha_{m}
$$

of vectors $\alpha_{i}$ in $S$, and $\beta$ is a linear combination,

$$
\beta=y_{1} \beta_{1}+y_{2} \beta_{2}+\cdots+y_{n} \beta_{n}
$$

of vectors $\beta_{j}$ in $S$. For each scalar $c$,

$$
c \alpha+\beta=\sum_{i=1}^{m}\left(c x_{i}\right) \alpha_{i}+\sum_{j=1}^{n} y_{j} \beta_{j} .
$$

Hence $c \alpha+\beta$ belongs to $L$. Thus $L$ is a subspace of $V$.

Now we have shown that $L$ is a subspace of $V$ which contains $S$, and also that any subspace which contains $S$ contains $L$. It follows that $L$ is the intersection of all subspaces containing $S$, i.e., that $L$ is the subspace spanned by the $\operatorname{set} S$.

Definition. If $\mathrm{S}_{1}, \mathrm{~S}_{2}, \ldots, \mathrm{S}_{\mathrm{k}}$ are subsets of a vector space $\mathrm{V}$, the set of all sums

$$
\alpha_{1}+\alpha_{2}+\cdots+\alpha_{\mathrm{k}}
$$

of vectors $\alpha_{\mathrm{i}}$ in $\mathrm{S}_{\mathrm{i}}$ is called the sum of the subsets $\mathrm{S}_{1}, \mathrm{~S}_{2}, \ldots, \mathrm{S}_{\mathbf{k}}$ and is denoted by

$$
S_{1}+S_{2}+\cdots+S_{k}
$$

or by

$$
\sum_{i=1}^{k} S_{i} .
$$

If $W_{1}, W_{2}, \ldots, W_{k}$ are subspaces of $V$, then the sum

$$
W=W_{1}+W_{2}+\cdots+W_{k}
$$

is easily seen to be a subspace of $V$ which contains each of the subspaces $W_{i}$. From this it follows, as in the proof of Theorem 3, that $W$ is the subspace spanried by the union of $W_{1}, W_{2}, \ldots, W_{k}$.

Example 8. Let $F$ be a subfield of the field $C$ of complex numbers. Suppose 

$$
\begin{aligned}
& \alpha_{1}=(1,2,0,3,0) \\
& \alpha_{2}=(0,0,1,4,0) \\
& \alpha_{3}=(0,0,0,0,1) .
\end{aligned}
$$

By Theorem 3, a vector $\alpha$ is in the subspace $W$ of $F^{5}$ spanned by $\alpha_{1}, \alpha_{2}, \alpha_{3}$ if and only if there exist scalars $c_{1}, c_{2}, c_{3}$ in $F$ such that

$$
\alpha=c_{1} \alpha_{1}+c_{2} \alpha_{2}+c_{3} \alpha_{3} .
$$

Thus $W$ consists of all vectors of the form

$$
\alpha=\left(c_{1}, 2 c_{1}, c_{2}, 3 c_{1}+4 c_{2}, c_{3}\right)
$$

where $c_{1}, c_{2}, c_{3}$ are arbitrary scalars in $F$. Alternatively, $W$ can be described as the set of all 5 -tuples

$$
\alpha=\left(x_{1}, x_{2}, x_{3}, x_{4}, x_{5}\right)
$$

with $x_{i}$ in $F$ such that

$$
\begin{aligned}
& x_{2}=2 x_{1} \\
& x_{4}=3 x_{1}+4 x_{3} .
\end{aligned}
$$

Thus $(-3,-6,1,-5,2)$ is in $W$, whereas $(2,4,6,7,8)$ is not.

Example 9 . Let $F$ be a subfield of the field $C$ of complex numbers, and let $V$ be the vector space of all $2 \times 2$ matrices over $F$. Let $W_{1}$ be the subset of $V$ consisting of all matrices of the form

$$
\left[\begin{array}{ll}
x & y \\
z & 0
\end{array}\right]
$$

where $x, y, z$ are arbitrary scalars in $F$. Finally, let $W_{2}$ be the subset of $V$ consisting of all matrices of the form

$$
\left[\begin{array}{ll}
x & 0 \\
0 & y
\end{array}\right]
$$

where $x$ and $y$ are arbitrary scalars in $F$. Then $W_{1}$ and $W_{2}$ are subspaces of $V$. Also

$$
V=W_{1}+W_{2}
$$

because

$$
\left[\begin{array}{ll}
a & b \\
c & d
\end{array}\right]=\left[\begin{array}{ll}
a & b \\
c & 0
\end{array}\right]+\left[\begin{array}{ll}
0 & 0 \\
0 & d
\end{array}\right] .
$$

The subspace $W_{1} \cap W_{2}$ consists of all matrices of the form

$$
\left[\begin{array}{ll}
x & 0 \\
0 & 0
\end{array}\right] \text {. }
$$

Example 10. Let $A$ be an $m \times n$ matrix over a field $F$. The row vectors of $A$ are the vectors in $F^{n}$ given by $\alpha_{i}=\left(A_{i 1}, \ldots, A_{i n}\right), i=1, \ldots$, $m$. The subspace of $F^{n}$ spanned by the row vectors of $A$ is called the row space of $A$. The subspace considered in Example 8 is the row space of the matrix

$$
A=\left[\begin{array}{lllll}
1 & 2 & 0 & 3 & 0 \\
0 & 0 & 1 & 4 & 0 \\
0 & 0 & 0 & 0 & 1
\end{array}\right]
$$

It is also the row space of the matrix

$$
B=\left[\begin{array}{rrrrr}
1 & 2 & 0 & 3 & 0 \\
0 & 0 & 1 & 4 & 0 \\
0 & 0 & 0 & 0 & 1 \\
-4 & -8 & 1 & -8 & 0
\end{array}\right]
$$

Example 11. Let $V$ be the space of all polynomial functions over $F$. Let $S$ be the subset of $V$ consisting of the polynomial functions $f_{0}, f_{1}, f_{2}, \ldots$ defined by

$$
f_{n}(x)=x^{n}, \quad n=0,1,2, \ldots
$$

Then $V$ is the subspace spanned by the set $S$.

\section{Exercises}

1. Which of the following sets of vectors $\alpha=\left(a_{1}, \ldots, a_{n}\right)$ in $R^{n}$ are subspaces of $R^{n}(n \geq 3)$ ?

(a) all $\alpha$ such that $a_{1} \geq 0$

(b) all $\alpha$ such that $a_{1}+3 a_{2}=a_{3}$;

(c) all $\alpha$ such that $a_{2}=a_{1}^{2}$;

(d) all $\alpha$ such that $a_{1} a_{2}=0$;

(e) all $\alpha$ such that $a_{2}$ is rational.

2. Let $V$ be the (real) vector space of all functions $f$ from $R$ into $R$. Which of the following sets of functions are subspaces of $V$ ?

(a) all $f$ such that $f\left(x^{2}\right)=f(x)^{2}$;

(b) all $f$ such that $f(0)=f(1)$

(c) all $f$ such that $f(3)=1+f(-5)$;

(d) all $f$ such that $f(-1)=0$

(e) all $f$ which are continuous.

3. Is the vector $(3,-1,0,-1)$ in the subspace of $R^{5}$ spanned by the vectors $(2,-1,3,2),(-1,1,1,-3)$, and $(1,1,9,-5)$ ?

4. Let $W$ be the set of all $\left(x_{1}, x_{2}, x_{3}, x_{4}, x_{5}\right)$ in $R^{5}$ which satisfy

$$
\begin{aligned}
2 x_{1}-x_{2}+\frac{1}{3} x_{3}-x_{4} & =0 \\
x_{1}+\frac{2}{3} x_{3}-x_{5} & =0 \\
9 x_{1}-3 x_{2}+6 x_{3}-3 x_{4}-3 x_{5} & =0 .
\end{aligned}
$$

Find a finite set of vectors which spans $W$. 5. Let $F$ be a field and let $n$ be a positive integer $(n \geq 2)$. Let $V$ be the vector space of all $n \times n$ matrices over $F$. Which of the following sets of matrices $A$ in $V$ are subspaces of $V$ ?

(a) all invertible $A$;

(b) all non-invertible $A$;

(c) all $A$ such that $A B=B A$, where $B$ is some fixed matrix in $V$;

(d) all $A$ such that $A^{2}=A$.

6. (a) Prove that the only subspaces of $R^{1}$ are $R^{1}$ and the zero subspace.

(b) Prove that a subspace of $R^{2}$ is $R^{2}$, or the zero subspace, or consists of all scalar multiples of some fixed vector in $R^{2}$. (The last type of subspace is, intuitively, a straight line through the origin.)

(c) Can you describe the subspaces of $R^{3}$ ?

7. Let $W_{1}$ and $W_{2}$ be subspaces of a vector space $V$ such that the set-theoretic union of $W_{1}$ and $W_{2}$ is also a subspace. Prove that one of the spaces $W_{i}$ is contained in the other.

8. Let $V$ be the vector space of all functions from $R$ into $R$; let $V_{e}$ be the subset of even functions, $f(-x)=f(x)$; let $V_{o}$ be the subset of odd functions, $f(-x)=-f(x)$

(a) Prove that $V_{e}$ and $V_{o}$ are subspaces of $V$.

(b) Prove that $V_{e}+V_{o}=V$.

(c) Prove that $V_{e} \cap V_{o}=\{0\}$.

9. Let $W_{1}$ and $W_{2}$ be subspaces of a vector space $V$ such that $W_{1}+W_{2}=V$ and $W_{1} \cap W_{2}=\{0\}$. Prove that for each vector $\alpha$ in $V$ there are unique vectors $\alpha_{1}$ in $W_{1}$ and $\alpha_{2}$ in $W_{2}$ such that $\alpha=\alpha_{1}+\alpha_{2}$.

\subsection{Bases and Dimension}

We turn now to the task of assigning a dimension to certain vector spaces. Although we usually associate 'dimension' with something geometrical, we must find a suitable algebraic definition of the dimension of a vector space. This will be done through the concept of a basis for the space.

Definition. Let $\mathrm{V}$ be a vector space over $\mathrm{F}$. A subset $\mathrm{S}$ of $\mathrm{V}$ is said to be linearly dependent (or simply, dependent) if there exist distinct vectors $\alpha_{1}, \alpha_{2}, \ldots, \alpha_{\mathrm{n}}$ in $\mathrm{S}$ and scalars $\mathrm{c}_{1}, \mathrm{c}_{2}, \ldots, \mathrm{c}_{\mathrm{n}}$ in $\mathrm{F}$, not all of which are 0 , such that

$$
\mathrm{c}_{1} \alpha_{1}+\mathrm{c}_{2} \alpha_{2}+\cdots+\mathrm{c}_{\mathrm{n}} \alpha_{\mathrm{n}}=0 .
$$

A set which is not linearly dependent is called linearly independent. If the set $\mathrm{S}$ contains only finitely many vectors $\alpha_{1}, \alpha_{2}, \ldots, \alpha_{n}$, we sometimes say that $\alpha_{1}, \alpha_{2}, \ldots, \alpha_{n}$ are dependent (or independent) instead of saying $\mathrm{S}$ is dependent (or independent). The following are easy consequences of the definition.

1. Any set which contains a linearly dependent set is linearly dependent.

2. Any subset of a linearly independent set is linearly independent.

3. Any set which contains the 0 vector is linearly dependent; for $1 \cdot 0=0$.

4. A set $S$ of vectors is linearly independent if and only if each finite subset of $S$ is linearly independent, i.e., if and only if for any distinct vectors $\alpha_{1}, \ldots, \alpha_{n}$ of $S, c_{1} \alpha_{1}+\cdots+c_{n} \alpha_{n}=0$ implies each $c_{i}=0$.

Definition. Let $\mathrm{V}$ be a vector space. A basis for $\mathrm{V}$ is a linearly independent set of vectors in $\mathrm{V}$ which spans the space V. The space $\mathrm{V}$ is finitedimensional if it has a finite basis.

Example 12. Let $F$ be a subfield of the complex numbers. In $F^{3}$ the vectors

$$
\begin{aligned}
\alpha_{1} & =(3,0,-3) \\
\alpha_{2} & =(-1,1,2) \\
\alpha_{3} & =(4,2,-2) \\
\alpha_{4} & =(2,1,1)
\end{aligned}
$$

are linearly dependent, since

$$
2 \alpha_{1}+2 \alpha_{2}-\alpha_{3}+0 \cdot \alpha_{4}=0 .
$$

The vectors

$$
\begin{aligned}
& \epsilon_{1}=(1,0,0) \\
& \epsilon_{2}=(0,1,0) \\
& \epsilon_{3}=(0,0,1)
\end{aligned}
$$

are linearly independent

Example 13. Let $F$ be a field and in $F^{n}$ let $S$ be the subset consisting of the vectors $\epsilon_{1}, \epsilon_{2}, \ldots, \epsilon_{n}$ defined by

$$
\begin{aligned}
\epsilon_{1}= & (1,0,0, \ldots, 0) \\
\epsilon_{2}= & (0,1,0, \ldots, 0) \\
& \cdots . . \\
\epsilon_{n}= & (0,0,0, \ldots, 1) .
\end{aligned}
$$

Let $x_{1}, x_{2}, \ldots, x_{n}$ be scalars in $F$ and put $\alpha=x_{1} \epsilon_{1}+x_{2} \epsilon_{2}+\cdots+x_{n} \epsilon_{n}$. Then

$$
\alpha=\left(x_{1}, x_{2}, \ldots, x_{n}\right) .
$$

This shows that $\epsilon_{1}, \ldots, \epsilon_{n}$ span $F^{n}$. Since $\alpha=0$ if and only if $x_{1}=$ $x_{2}=\cdots=x_{n}=0$, the vectors $\epsilon_{1}, \ldots, \epsilon_{n}$ are linearly independent. The set $S=\left\{\epsilon_{1}, \ldots, \epsilon_{n}\right\}$ is accordingly a basis for $F^{n}$. We shall call this particular basis the standard basis of $F^{n}$. Example 14. Let $P$ be an invertible $n \times n$ matrix with entries in the field $F$. Then $P_{1}, \ldots, P_{n}$, the columns of $P$, form a basis for the space of column matrices, $F^{n \times 1}$. We see that as follows. If $X$ is a column matrix, then

$$
P X=x_{1} P_{1}+\cdots+x_{n} P_{n} .
$$

Since $P X=0$ has only the trivial solution $X=0$, it follows that $\left\{P_{1}, \ldots, P_{n}\right\}$ is a linearly independent set. Why does it $\operatorname{span} F_{n \times 1}$ ? Let $Y$ be any column matrix. If $X=P^{-1} Y$, then $Y=P X$, that is,

$$
Y=x_{1} P_{1}+\cdots+x_{n} P_{n} .
$$

So $\left\{P_{1}, \ldots, P_{n}\right\}$ is a basis for $F^{n \times 1}$.

Example 15. Let $A$ be an $m \times n$ matrix and let $S$ be the solution space for the homogeneous system $A X=0$ (Example 7). Let $R$ be a rowreduced echelon matrix which is row-equivalent to $A$. Then $S$ is also the solution space for the system $R X=0$. If $R$ has $r$ non-zero rows, then the system of equations $R X=0$ simply expresses $r$ of the unknowns $x_{1}, \ldots, x_{n}$ in terms of the remaining $(n-r)$ unknowns $x_{j}$. Suppose that the leading non-zero entries of the non-zero rows occur in columns $k_{1}, \ldots, k_{r}$. Let $J$ be the set consisting of the $n-r$ indices different from $k_{1}, \ldots, k_{r}$ :

$$
J=\{1, \ldots, n\}-\left\{k_{1}, \ldots, k_{r}\right\} .
$$

The system $R X=0$ has the form

$$
\begin{aligned}
& x_{k 1}+\sum_{J} c_{1 j} x_{j}=0 \\
& \vdots \begin{array}{cc}\vdots & \vdots \\x_{k_{r}}+\sum_{J} c_{r j} x_{j}=0\end{array}
\end{aligned}
$$

where the $c_{i j}$ are certain scalars. All solutions are obtained by assigning (arbitrary) values to those $x_{j}$ 's with $j$ in $J$ and computing the corresponding values of $x_{k_{1}}, \ldots, x_{k_{r}}$. For each $j$ in $J$, let $E_{j}$ be the solution obtained by setting $x_{j}=1$ and $x_{i}=0$ for all other $i$ in $J$. We assert that the $(n-r)$ vectors $E_{j}, j$ in $J$, form a basis for the solution space.

Since the column matrix $E_{j}$ has a 1 in row $j$ and zeros in the rows indexed by other elements of $J$, the reasoning of Example 13 shows us that the set of these vectors is linearly independent. That set spans the solution space, for this reason. If the column matrix $T$, with entries $t_{1}, \ldots, t_{n}$, is in the solution space, the matrix

$$
N=\sum_{J} t_{j} E_{j}
$$

is also in the solution space and is a solution such that $x_{j}=t_{j}$ for each $j$ in $J$. The solution with that property is unique; hence, $N=T$ and $T$ is in the span of the vectors $E_{j}$. Example 16. We shall now give an example of an infinite basis. Let $F$ be a subfield of the complex numbers and let $V$ be the space of polynomial functions over $F$. Recall that these functions are the functions from $F$ into $F$ which have a rule of the form

$$
f(x)=c_{0}+c_{1} x+\cdots+c_{n} x^{n} .
$$

Let $f_{k}(x)=x_{k}, k=0,1,2, \ldots$ The (infinite) set $\left\{f_{0}, f_{1}, f_{2}, \ldots\right\}$ is a basis for $V$. Clearly the set spans $V$, because the function $f$ (above) is

$$
f=c_{0} f_{0}+c_{1} f_{1}+\cdots+c_{n} f_{n} .
$$

The reader should see that this is virtually a repetition of the definition of polynomial function, that is, a function $f$ from $F$ into $F$ is a polynomial function if and only if there exists an integer $n$ and scalars $c_{0}, \ldots, c_{n}$ such that $f=c_{0} f_{0}+\cdots+c_{n} f_{n}$. Why are the functions independent? To show that the set $\left\{f_{0}, f_{1}, f_{2}, \ldots\right\}$ is independent means to show that each finite subset of it is independent. It will suffice to show that, for each $n$, the set $\left\{f_{0}, \ldots, f_{n}\right\}$ is independent. Suppose that

$$
c_{0} f_{0}+\cdots+c_{n} f_{n}=0 .
$$

This says that

$$
c_{0}+c_{1} x+\cdots+c_{n} x^{n}=0
$$

for every $x$ in $F$; in other words, every $x$ in $F$ is a root of the polynomial $f(x)=c_{0}+c_{1} x+\cdots+c_{n} x^{n}$. We assume that the reader knows that a polynomial of degree $n$ with complex coefficients cannot have more than $n$ distinct roots. It follows that $c_{0}=c_{1}=\cdots=c_{n}=0$.

We have exhibited an infinite basis for $V$. Does that mean that $V$ is not finite-dimensional? As a matter of fact it does; however, that is not immediate from the definition, because for all we know $V$ might also have a finite basis. That possibility is easily eliminated. (We shall eliminate it in general in the next theorem.) Suppose that we have a finite number of polynomial functions $\boldsymbol{g}_{1}, \ldots, \boldsymbol{g}_{r}$. There will be a largest power of $x$ which appears (with non-zero coefficient) in $\boldsymbol{g}_{1}(x), \ldots, \boldsymbol{g}_{r}(x)$. If that power is $k$, clearly $f_{k+1}(x)=x^{k+1}$ is not in the linear span of $\boldsymbol{g}_{1}, \ldots, \boldsymbol{g}_{r}$. So $V$ is not finite-dimensional.

A final remark about this example is in order. Infinite bases have nothing to do with 'infinite linear combinations.' The reader who feels an irresistible urge to inject power series

$$
\sum_{k=0}^{\infty} c_{k} x^{k}
$$

into this example should study the example carefully again. If that does not effect a cure, he should consider restricting his attention to finitedimensional spaces from now on. Theorem 4. Let $\mathrm{V}$ be a vector space which is spanned by a finite set of vectors $\beta_{1}, \beta_{2}, \ldots, \beta_{\mathrm{m}}$. Then any independent set of vectors in $\mathrm{V}$ is finite and contains no more than $\mathrm{m}$ elements.

Proof. To prove the theorem it suffices to show that every subset $S$ of $V$ which contains more than $m$ vectors is linearly dependent. Let $S$ be such a set. In $S$ there are distinct vectors $\alpha_{1}, \alpha_{2}, \ldots, \alpha_{n}$ where $n>m$. Since $\beta_{1}, \ldots, \beta_{m}$ span $V$, there exist scalars $A_{i j}$ in $F$ such that

$$
\alpha_{j}=\sum_{i=1}^{m} A_{i j} \beta_{i} .
$$

For any $n$ scalars $x_{1}, x_{2}, \ldots, x_{n}$ we have

$$
\begin{aligned}
x_{1} \alpha_{1}+\cdots+x_{n} \alpha_{n} & =\sum_{j=1}^{n} x_{j} \alpha_{j} \\
& =\sum_{j=1}^{n} x_{j} \sum_{i=1}^{m} A_{i j} \beta_{i} \\
& =\sum_{j=1}^{n} \sum_{i=1}^{m}\left(A_{i j} x_{j}\right) \beta_{i} \\
& =\sum_{i=1}^{m}\left(\sum_{j=1}^{n} A_{i j} x_{j}\right) \beta_{i} .
\end{aligned}
$$

Since $n>m$, Theorem 6 of Chapter 1 implies that there exist scalars $x_{1}, x_{2}, \ldots, x_{n}$ not all 0 such that

$$
\sum_{j=1}^{n} A_{i j} x_{j}=0, \quad 1 \leq i \leq m .
$$

Hence $x_{1} \alpha_{1}+x_{2} \alpha_{2}+\cdots+x_{n} \alpha_{n}=0$. This shows that $S$ is a linearly dependent set.

Corollary 1. If $\mathrm{V}$ is a finite-dimensional vector space, then any two bases of $\mathrm{V}$ have the same ( finite) number of elements.

Proof. Since $V$ is finite-dimensional, it has a finite basis

$$
\left\{\beta_{1}, \beta_{2}, \ldots, \beta_{m}\right\} .
$$

By Theorem 4 every basis of $V$ is finite and contains no more than $m$ elements. Thus if $\left\{\alpha_{1}, \alpha_{2}, \ldots, \alpha_{n}\right\}$ is a basis, $n \leq m$. By the same argument, $m \leq n$. Hence $m=n$.

This corollary allows us to define the dimension of a finite-dimensional vector space as the number of elements in a basis for $V$. We shall denote the dimension of a finite-dimensional space $V$ by $\operatorname{dim} V$. This allows us to reformulate Theorem 4 as follows.

Corollary 2. Let $\mathrm{V}$ be a finite-dimensional vector space and let $\mathrm{n}=$ $\operatorname{dim} \mathrm{V}$. Then (a) any subset of $\mathrm{V}$ which contains more than $\mathrm{n}$ vectors is linearly dependent;

(b) no subset of $\mathrm{V}$ which contains fewer than $\mathrm{n}$ vectors can span $\mathrm{V}$.

EXample 17. If $F$ is a field, the dimension of $F^{n}$ is $n$, because the standard basis for $F^{n}$ contains $n$ vectors. The matrix space $F^{m \times n}$ has dimension $m n$. That should be clear by analogy with the case of $F^{n}$, because the $m n$ matrices which have a 1 in the $i, j$ place with zeros elsewhere form a basis for $F^{m \times n}$. If $A$ is an $m \times n$ matrix, then the solution space for $A$ has dimension $n-r$, where $r$ is the number of non-zero rows in a row-reduced echelon matrix which is row-equivalent to $A$. See Example 15.

If $V$ is any vector space over $F$, the zero subspace of $V$ is spanned by the vector 0 , but $\{0\}$ is a linearly dependent set and not a basis. For this reason, we shall agree that the zero subspace has dimension 0. Alternatively, we could reach the same conclusion by arguing that the empty set is a basis for the zero subspace. The empty set spans $\{0\}$, because the intersection of all subspaces containing the empty set is $\{0\}$, and the empty set is linearly independent because it contains no vectors.

Lemma. Let $\mathrm{S}$ be a linearly independent subset of a vector space V. Suppose $\beta$ is a vector in $\mathrm{V}$ which is not in the subspace spanned by $\mathrm{S}$. Then the set obtained by adjoining $\beta$ to $\mathrm{S}$ is linearly independent.

Proof. Suppose $\alpha_{1}, \ldots, \alpha_{m}$ are distinct vectors in $S$ and that

$$
c_{1} \alpha_{1}+\cdots+c_{m} \alpha_{m}+b \beta=0 .
$$

Then $b=0$; for otherwise,

$$
\beta=\left(-\frac{c_{1}}{b}\right) \alpha_{1}+\cdots+\left(-\frac{c_{m}}{b}\right) \alpha_{m}
$$

and $\beta$ is in the subspace spanned by $S$. Thus $c_{1} \alpha_{1}+\cdots+c_{m} \alpha_{m}=0$, and since $S$ is a linearly independent set each $c_{i}=0$.

Theorem 5. If $\mathrm{W}$ is a subspace of a finite-dimensional vector space $\mathrm{V}$, every linearly independent subset of $\mathrm{W}$ is finite and is part of a (finite) basis for W.

Proof. Suppose $S_{0}$ is a linearly independent subset of $W$. If $S$ is a 'inearly independent subset of $W$ containing $S_{0}$, then $S$ is also a linearly independent subset of $V$; since $V$ is finite-dimensional, $S$ contains no more than $\operatorname{dim} V$ elements.

We extend $S_{0}$ to a basis for $W$, as follows. If $S_{0}$ spans $W$, then $S_{0}$ is a basis for $W$ and we are done. If $S_{0}$ does not span $W$, we use the preceding lemma to find a vector $\beta_{1}$ in $W$ such that the set $S_{1}=S_{0} \cup\left\{\beta_{1}\right\}$ is independent. If $S_{1}$ spans $W$, fine. If not, apply the lemma to obtain a vector $\beta_{2}$ in $W$ such that $S_{2}=S_{1} \cup\left\{\beta_{2}\right\}$ is independent. If we continue in this way, then (in not more than $\operatorname{dim} V$ steps) we reach a set

$$
S_{m}=S_{0} \cup\left\{\beta_{1}, \ldots, \beta_{m}\right\}
$$

which is a basis for $W$.

Corollary 1. If $\mathrm{W}$ is a proper subspace of a finite-dimensional vector space $\mathrm{V}$, then $\mathrm{W}$ is finite-dimensional and $\operatorname{dim} \mathrm{W}<\operatorname{dim} \mathrm{V}$.

Proof. We may suppose $W$ contains a vector $\alpha \neq 0$. By Theorem 5 and its proof, there is a basis of $W$ containing $\alpha$ which contains no more than $\operatorname{dim} V$ elements. Hence $W$ is finite-dimensional, and $\operatorname{dim} W \leq \operatorname{dim} V$. Since $W$ is a proper subspace, there is a vector $\beta$ in $V$ which is not in $W$. Adjoining $\beta$ to any basis of $W$, we obtain a linearly independent subset of $V$. Thus $\operatorname{dim} W<\operatorname{dim} V$.

Corollary 2. In a finite-dimensional vector space $\mathrm{V}$ every non-empty linearly independent set of vectors is part of a basis.

Corollary 3. Let $\mathrm{A}$ be an $\mathrm{n} \times \mathrm{n}$ matrix over a field $\mathrm{F}$, and suppose the row vectors of $\mathrm{A}$ form a linearly independent set of vectors in $\mathrm{F}^{\mathrm{n}}$. Then $\mathrm{A}$ is invertible.

Proof. Let $\alpha_{1}, \alpha_{2}, \ldots, \alpha_{n}$ be the row vectors of $A$, and suppose $W$ is the subspace of $F^{n}$ spanned by $\alpha_{1}, \alpha_{2}, \ldots, \alpha_{n}$. Since $\alpha_{1}, \alpha_{2}, \ldots, \alpha_{n}$ are linearly independent, the dimension of $W$ is $n$. Corollary 1 now shows that $W=F^{n}$. Hence there exist scalars $B_{i j}$ in $F$ such that

$$
\epsilon_{i}=\sum_{j=1}^{n} B_{i j} \alpha_{j}, \quad 1 \leq i \leq n
$$

where $\left\{\epsilon_{1}, \epsilon_{2}, \ldots, \epsilon_{n}\right\}$ is the standard basis of $F^{n}$. Thus for the matrix $B$ with entries $B_{i j}$ we have

$$
B A=I \text {. }
$$

Theorem 6. If $\mathrm{W}_{1}$ and $\mathrm{W}_{2}$ are finite-dimensional subspaces of a vector space $\mathrm{V}$, then $\mathrm{W}_{1}+\mathrm{W}_{2}$ is finite-dimensional and

$$
\operatorname{dim} \mathrm{W}_{1}+\operatorname{dim} \mathrm{W}_{2}=\operatorname{dim}\left(\mathrm{W}_{1} \cap \mathrm{W}_{2}\right)+\operatorname{dim}\left(\mathrm{W}_{1}+\mathrm{W}_{2}\right) .
$$

Proof. By Theorem 5 and its corollaries, $W_{1} \cap W_{2}$ has a finite basis $\left\{\alpha_{1}, \ldots, \boldsymbol{\alpha}_{k}\right\}$ which is part of a basis

$$
\left\{\alpha_{1}, \ldots, \alpha_{k}, \beta_{1}, \ldots, \beta_{m}\right\} \text { for } W_{1}
$$

and part of a basis

$$
\left\{\alpha_{1}, \ldots, \alpha_{k}, \quad \gamma_{1}, \ldots, \gamma_{n}\right\} \text { for } W_{2} \text {. }
$$

The subspace $W_{1}+W_{2}$ is spanned by the vectors

$$
\alpha_{1}, \ldots, \alpha_{k}, \quad \beta_{1}, \ldots, \beta_{m}, \quad \gamma_{1}, \ldots, \gamma_{n}
$$

and these vectors form an independent set. For suppose

$$
\Sigma x_{i} \alpha_{i}+\Sigma y_{j} \beta_{j}+\Sigma z_{r} \gamma_{r}=0 .
$$

Then

$$
-\Sigma z_{r} \gamma_{r}=\Sigma x_{i} \alpha_{i}+\Sigma y_{j} \beta_{j}
$$

which shows that $\Sigma z_{r} \gamma_{r}$ belongs to $W_{1}$. As $\Sigma z_{r} \gamma_{r}$ also belongs to $W_{2}$ it follows that

$$
\boldsymbol{\Sigma}_{z_{r} \gamma_{r}}=\boldsymbol{\Sigma}_{c_{i} \alpha_{i}}
$$

for certain scalars $c_{1}, \ldots, c_{k}$. Because the set

$$
\left\{\alpha_{1}, \ldots, \alpha_{k}, \quad \gamma_{1}, \ldots, \gamma_{n}\right\}
$$

is independent, each of the scalars $z_{r}=0$. Thus

and since

$$
\Sigma x_{i} \alpha_{i}+\Sigma y_{j} \beta_{j}=0
$$

$$
\left\{\alpha_{1}, \ldots, \alpha_{k}, \quad \beta_{1}, \ldots, \beta_{m}\right\}
$$

is also an independent set, each $x_{i}=0$ and each $y_{j}=0$. Thus,

$$
\left\{\alpha_{1}, \ldots, \alpha_{k}, \quad \beta_{1}, \ldots, \beta_{m}, \quad \gamma_{1}, \ldots, \gamma_{n}\right\}
$$

is a basis for $W_{1}+W_{2}$. Finally

$$
\begin{aligned}
\operatorname{dim} W_{1}+\operatorname{dim} W_{2} & =(k+m)+(k+n) \\
& =k+(m+k+n) \\
& =\operatorname{dim}\left(W_{1} \cap W_{2}\right)+\operatorname{dim}\left(W_{1}+W_{2}\right)
\end{aligned}
$$

Let us close this section with a remark about linear independence and dependence. We defined these concepts for sets of vectors. It is useful to have them defined for finite sequences (ordered $n$-tuples) of vectors: $\alpha_{1}, \ldots, \alpha_{n}$. We say that the vectors $\alpha_{1}, \ldots, \alpha_{n}$ are linearly dependent if there exist scalars $c_{1}, \ldots, c_{n}$, not all 0 , such that $c_{1} \alpha_{1}+\cdots+c_{n} \alpha_{n}=0$. This is all so natural that the reader may find that he has been using this terminology already. What is the difference between a finite sequence $\alpha_{1}, \ldots, \alpha_{n}$ and a set $\left\{\alpha_{1}, \ldots, \alpha_{n}\right\}$ ? There are two differences, identity and order.

If we discuss the set $\left\{\alpha_{1}, \ldots, \alpha_{n}\right\}$, usually it is presumed that no two of the vectors $\alpha_{1}, \ldots, \alpha_{n}$ are identical. In a sequence $\alpha_{1}, \ldots, \alpha_{n}$ all the $\alpha_{i}^{\prime}$ 's may be the same vector. If $\alpha_{i}=\alpha_{j}$ for some $i \neq j$, then the sequence $\alpha_{1}, \ldots, \alpha_{n}$ is linearly dependent:

$$
\alpha_{i}+(-1) \alpha_{j}=0 .
$$

Thus, if $\alpha_{1}, \ldots, \alpha_{n}$ are linearly independent, they are distinct and we may talk about the set $\left\{\alpha_{1}, \ldots, \alpha_{n}\right\}$ and know that it has $n$ vectors in it. So, clearly, no confusion will arise in discussing bases and dimension. The dimension of a finite-dimensional space $V$ is the largest $n$ such that some $n$-tuple of vectors in $V$ is linearly independent-and so on. The reader who feels that this paragraph is much ado about nothing might ask himself whether the vectors

$$
\begin{aligned}
& \alpha_{1}=\left(e^{\pi / 2}, 1\right) \\
& \alpha_{2}=(\sqrt[3]{110}, 1)
\end{aligned}
$$

are linearly independent in $R^{2}$.

The elements of a sequence are enumerated in a specific order. A set is a collection of objects, with no specified arrangement or order. Of course, to describe the set we may list its members, and that requires choosing an order. But, the order is not part of the set. The sets $\{1,2,3,4\}$ and $\{4,3,2,1\}$ are identical, whereas $1,2,3,4$ is quite a different sequence from $4,3,2,1$. The order aspect of sequences has no bearing on questions of independence, dependence, etc., because dependence (as defined) is not affected by the order. The sequence $\alpha_{n}, \ldots, \alpha_{1}$ is dependent if and only if the sequence $\alpha_{1}, \ldots, \alpha_{n}$ is dependent. In the next section, order will be important.

\section{Exercises}

1. Prove that if two vectors are linearly dependent, one of them is a scalar multiple of the other.

2. Are the vectors

$$
\begin{aligned}
& \alpha_{1}=(1,1,2,4), \quad \alpha_{2}=(2,-1,-5,2) \\
& \alpha_{3}=(1,-1,-4,0), \quad \alpha_{4}=(2,1,1,6)
\end{aligned}
$$

linearly independent in $R^{4}$ ?

3. Find a basis for the subspace of $R^{4}$ spanned by the four vectors of Exercise 2 .

4. Show that the vectors

$$
\alpha_{1}=(1,0,-1), \quad \alpha_{2}=(1,2,1), \quad \alpha_{3}=(0,-3,2)
$$

form a basis for $R^{3}$. Express each of the standard basis vectors as linear combinations of $\alpha_{1}, \alpha_{2}$, and $\alpha_{3}$.

5. Find three vectors in $R^{3}$ which are linearly dependent, and are such that any two of them are linearly independent.

6. Let $V$ be the vector space of all $2 \times 2$ matrices over the field $F$. Prove that $V$ has dimension 4 by exhibiting a basis for $V$ which has four elements.

7. Let $V$ be the vector space of Exercise 6 . Let $W_{1}$ be the set of matrices of the form

$$
\left[\begin{array}{rr}
x & -x \\
y & z
\end{array}\right]
$$

and let $W_{2}$ be the set of matrices of the form

$$
\left[\begin{array}{rr}
a & b \\
-a & c
\end{array}\right]
$$

(a) Prove that $W_{1}$ and $W_{2}$ are subspaces of $V$.

(b) Find the dimensions of $W_{1}, W_{2}, W_{1}+W_{2}$, and $W_{1} \cap W_{2}$.

8. Again let $V$ be the space of $2 \times 2$ matrices over $F$. Find a basis $\left\{A_{1}, A_{2}, A_{3}, A_{4}\right\}$ for $V$ such that $A_{i}^{2}=A_{j}$ for each $j$.

9. Let $V$ be a vector space over a subfield $F$ of the complex numbers. Suppose $\alpha, \beta$, and $\gamma$ are linearly independent vectors in $V$. Prove that $(\alpha+\beta),(\beta+\gamma)$, and $(\gamma+\alpha)$ are linearly independent.

10. Let $V$ be a vector space over the field $F$. Suppose there are a finite number of vectors $\alpha_{1}, \ldots, \alpha_{r}$ in $V$ which span $V$. Prove that $V$ is finite-dimensional.

11. Let $V$ be the set of all $2 \times 2$ matrices $A$ with complex entries which satisfy $A_{11}+A_{22}=0$.

(a) Show that $V$ is a vector space over the field of real numbers, with the usual operations of matrix addition and multiplication of a matrix by a scalar.

(b) Find a basis for this vector space.

(c) Let $W$ be the set of all matrices $A$ in $V$ such that $A_{21}=-\bar{A}_{12}$ (the bar denotes complex conjugation). Prove that $W$ is a subspace of $V$ and find a basis for $W$.

12. Prove that the space of all $m \times n$ matrices over the field $F$ has dimension $m n$, by exhibiting a basis for this space.

13. Discuss Exercise 9 , when $V$ is a vector space over the field with two elements described in Exercise 5, Section 1.1.

14. Let $V$ be the set of real numbers. Regard $V$ as a vector space over the field of rational numbers, with the usual operations. Prove that this vector space is not finite-dimensional.

\subsection{Coordinates}

One of the useful features of a basis $B$ in an $n$-dimensional space $V$ is that it essentially enables one to introduce coordinates in $V$ analogous to the 'natural coordinates' $x_{i}$ of a vector $\alpha=\left(x_{1}, \ldots, x_{n}\right)$ in the space $F^{n}$. In this scheme, the coordinates of a vector $\alpha$ in $V$ relative to the basis $B$ will be the scalars which serve to express $\alpha$ as a linear combination of the vectors in the basis. Thus, we should like to regard the natural coordinates of a vector $\alpha$ in $F^{n}$ as being defined by $\alpha$ and the standard basis for $F^{n}$; however, in adopting this point of view we must exercise a certain amount of care. If

$$
\alpha=\left(x_{1}, \ldots, x_{n}\right)=\Sigma x_{i} \epsilon_{i}
$$

and $B$ is the standard basis for $F^{n}$, just how are the coordinates of $\alpha$ determined by $\Theta$ and $\alpha$ ? One way to phrase the answer is this. A given vector $\alpha$ has a unique expression as a linear combination of the standard basis vectors, and the $i$ th coordinate $x_{i}$ of $\alpha$ is the coefficient of $\epsilon_{i}$ in this expression. From this point of view we are able to say which is the $i$ th coordinate because we have a 'natural' ordering of the vectors in the standard basis, that is, we have a rule for determining which is the 'first' vector in the basis, which is the 'second,' and so on. If $B$ is an arbitrary basis of the $n$-dimensional space $V$, we shall probably have no natural ordering of the vectors in $Q$, and it will therefore be necessary for us to impose some order on these vectors before we can define 'the $i$ th coordinate of $\alpha$ relative to \&.' To put it another way, coordinates will be defined relative to sequences of vectors rather than sets of vectors.

Definition. If $\mathrm{V}$ is a finite-dimensional vector space, an ordered basis for $\mathrm{V}$ is a finite sequence of vectors which is linearly independent and spans $\mathrm{V}$.

If the sequence $\alpha_{1}, \ldots, \alpha_{n}$ is an ordered basis for $V$, then the set $\left\{\alpha_{1}, \ldots, \alpha_{n}\right\}$ is a basis for $V$. The ordered basis is the set, together with the specified ordering. We shall engage in a slight abuse of notation and describe all that by saying that

$$
B=\left\{\alpha_{1}, \ldots, \alpha_{n}\right\}
$$

is an ordered basis for $V$.

Now suppose $V$ is a finite-dimensional vector space over the field $F$ and that

$$
\text { Q }=\left\{\alpha_{1}, \ldots, \alpha_{n}\right\}
$$

is an ordered basis for $V$. Given $\alpha$ in $V$, there is a unique $n$-tuple $\left(x_{1}, \ldots, x_{n}\right)$ of scalars such that

$$
\alpha=\sum_{i=1}^{n} x_{i} \alpha_{i} .
$$

The $n$-tuple is unique, because if w e also have

$$
\alpha=\sum_{i=1}^{n} z_{i} \alpha_{i}
$$

then

$$
\sum_{i=1}^{n}\left(x_{i}-z_{i}\right) \alpha_{i}=0
$$

and the linear independence of the $\alpha_{i}$ tells us that $x_{i}-z_{i}=0$ for each $i$. We shall call $x_{i}$ the $i$ th coordinate of $\alpha$ relative to the ordered basis

If

$$
B=\left\{\alpha_{1}, \ldots, \alpha_{n}\right\} \text {. }
$$

$$
\beta=\sum_{i=1}^{n} y_{i} \alpha_{i}
$$

then

$$
\alpha+\beta=\sum_{i=1}^{n}\left(x_{i}+y_{i}\right) \alpha_{i}
$$

so that the $i$ th coordinate of $(\alpha+\beta)$ in this ordered basis is $\left(x_{i}+y_{i}\right)$. Similarly, the $i$ th coordinate of $(c \alpha)$ is $c x_{i}$. One should also note that every $n$-tuple $\left(x_{1}, \ldots, x_{n}\right)$ in $F^{n}$ is the $n$-tuple of coordinates of some vector in $V$, namely the vector

$$
\sum_{i=1}^{n} x_{i} \alpha_{i} .
$$

To summarize, each ordered basis for $V$ determines a one-one correspondence

$$
\alpha \rightarrow\left(x_{1}, \ldots, x_{n}\right)
$$

between the set of all vectors in $V$ and the set of all $n$-tuples in $F^{n}$. This correspondence has the property that the correspondent of $(\alpha+\beta)$ is the sum in $F^{n}$ of the correspondents of $\alpha$ and $\beta$, and that the correspondent of $(c \alpha)$ is the product in $F^{n}$ of the scalar $c$ and the correspondent of $\alpha$.

One might wonder at this point why we do not simply select some ordered basis for $V$ and describe each vector in $V$ by its corresponding $n$-tuple of coordinates, since we would then have the convenience of operating only with $n$-tuples. This would defeat our purpose, for two reasons. First, as our axiomatic definition of vector space indicates, we are attempting to learn to reason with vector spaces as abstract àlgebraic systems. Second, even in those situations in which we use coordinates, the significant results follow from our ability to change the coordinate system, i.e., to change the ordered basis.

Frequently, it will be more convenient for us to use the coordinate matrix of $\alpha$ relative to the ordered basis $B$ :

$$
X=\left[\begin{array}{c}
x_{1} \\
\vdots \\
x_{n}
\end{array}\right]
$$

rather than the $n$-tuple $\left(x_{1}, \ldots, x_{n}\right)$ of coordinates. To indicate the dependence of this coordinate matrix on the basis, we shall use the symbol

$$
[\alpha]_{\mathscr{Q}}
$$

for the coordinate matrix of the vector $\alpha$ relative to the ordered basis $B$. This notation will be particularly useful as we now proceed to describe what happens to the coordinates of a vector $\alpha$ as we change from one ordered basis to another.

Suppose then that $V$ is $n$-dimensional and that

$$
B=\left\{\alpha_{1}, \ldots, \alpha_{n}\right\} \quad \text { and } B^{\prime}=\left\{\alpha_{1}^{\prime}, \ldots, \alpha_{n}^{\prime}\right\}
$$

are two ordered bases for $V$. There are unique scalars $P_{i j}$ such that

$$
\alpha_{j}^{\prime}=\sum_{i=1}^{n} P_{i j} \alpha_{i}, \quad 1 \leq j \leq n .
$$

Let $x_{1}^{\prime}, \ldots, x_{n}^{\prime}$ be the coordinates of a given vector $\alpha$ in the ordered basis $\mathbb{B}^{\prime}$. Then 

$$
\begin{aligned}
\alpha & =x_{1}^{\prime} \alpha_{1}^{\prime}+\cdots+x_{n}^{\prime} \alpha_{n}^{\prime} \\
& =\sum_{j=1}^{n} x_{j}^{\prime} \alpha_{j}^{\prime} \\
& =\sum_{j=1}^{n} x_{j}^{\prime} \sum_{i=1}^{n} P_{i j} \alpha_{i} \\
& =\sum_{j=1}^{n} \sum_{i=1}^{n}\left(P_{i j} x_{j}^{\prime}\right) \alpha_{i} \\
& =\sum_{i=1}^{n}\left(\sum_{j=1}^{n} P_{i j} x_{j}^{\prime}\right) \alpha_{i} .
\end{aligned}
$$

Thus we obtain the relation

$$
\alpha=\sum_{i=1}^{n}\left(\sum_{j=1}^{n} P_{i j} x_{j}^{\prime}\right) \alpha_{i} .
$$

Since the coordinates $x_{1}, x_{2}, \ldots, x_{n}$ of $\alpha$ in the ordered basis $B$ are uniquely determined, it follows from (2-14) that

$$
x_{i}=\sum_{j=1}^{n} P_{i j} x_{j}^{\prime}, \quad 1 \leq i \leq n .
$$

Let $P$ be the $n \times n$ matrix whose $i, j$ entry is the scalar $P_{i j}$, and let $X$ and $X^{\prime}$ be the coordinate matrices of the vector $\alpha$ in the ordered bases $B$ and $\mathrm{B}^{\prime}$. Then we may reformulate $(2-15)$ as

$$
X=P X^{\prime} \text {. }
$$

Since $B$ and $B^{\prime}$ are linearly independent sets, $X=0$ if and only if $X^{\prime}=0$. Thus from (2-16) and Theorem 7 of Chapter 1, it follows that $P$ is invertible. Hence

$$
X^{\prime}=P^{-1} X
$$

If we use the notation introduced above for the coordinate matrix of a vector relative to an ordered basis, then $(2-16)$ and $(2-17)$ say

$$
\begin{aligned}
{[\alpha]_{\mathscr{B}} } & =P[\alpha]_{B^{\prime}} \\
{[\alpha]_{Q^{\prime}} } & =P^{-1}[\alpha]_{Q} .
\end{aligned}
$$

Thus the preceding discussion may be summarized as follows.

Theorem 7. Let $\mathrm{V}$ be an n-dimensional vector space over the field $\mathrm{F}$, and let $\mathrm{B}$ and $\mathrm{B}^{\prime}$ be two ordered bases of $\mathrm{V}$. Then there is a unique, necessarily invertible, $\mathrm{n} \times \mathrm{n}$ matrix $\mathrm{P}$ with entries in $\mathrm{F}$ such that

$$
\begin{aligned}
{[\alpha]_{\mathscr{B}} } & =\mathrm{P}[\alpha]_{\mathbb{B}^{\prime}} \\
{[\alpha]_{\mathbb{B}^{\prime}} } & =\mathrm{P}^{-1}[\alpha]_{\mathscr{B}}
\end{aligned}
$$

for every vertor $\alpha$ in $\mathrm{V}$. The columns of $\mathrm{P}$ are given by

$$
P_{j}=\left[\alpha_{j}^{\prime}\right]_{\$,}, j=1, \ldots, n .
$$

To complete the above analysis we shall also prove the following result.

Theorem 8. Suppose $\mathrm{P}$ is an $\mathrm{n} \times \mathrm{n}$ invertible matrix over $\mathrm{F}$. Let $\mathrm{V}$ be an $\mathrm{n}$-dimensional vector space over $\mathrm{F}$, and let $B$ be an ordered basis of $\mathrm{V}$. Then there $i$ s a unique ordered basis $B^{\prime}$ of $\mathrm{V}$ such that

$$
\begin{aligned}
{[\alpha]_{\mathscr{Q}} } & =\mathrm{P}[\alpha]_{Q^{\prime}} \\
{[\alpha]_{G^{\prime}} } & =\mathrm{P}^{-1}[\alpha]_{\mathscr{Q}}
\end{aligned}
$$

for every vector $\alpha$ in $\mathrm{V}$.

Proof. Let $Q$ consist of the vectors $\alpha_{1}, \ldots, \alpha_{n}$. If $Q^{\prime}=$ $\left\{\alpha_{1}^{\prime}, \ldots, \alpha_{n}^{\prime}\right\}$ is an ordered basis of $V$ for which (i) is valid, it is clear that

$$
\alpha_{j}^{\prime}=\sum_{i=1}^{n} P_{i j} \alpha_{i} .
$$

Thus we need only show that the vectors $\alpha_{j}^{\prime}$, defined by these equations, form a basis. Let $Q=P^{-1}$. Then

$$
\begin{aligned}
\sum_{j} Q_{j k} \alpha_{j}^{\prime} & =\sum_{j} Q_{j k} \sum_{i} P_{i j} \alpha_{i} \\
& =\sum_{j} \sum_{i} P_{i j} Q_{j k} \alpha_{i} \\
& =\sum_{i}\left(\sum_{j} P_{i j} Q_{j k}\right) \alpha_{i} \\
& =\alpha_{k} .
\end{aligned}
$$

Thus the subspace spanned by the set

$$
\mathbb{B}^{\prime}=\left\{\alpha_{1}^{\prime}, \ldots, \alpha_{n}^{\prime}\right\}
$$

contains $\mathbb{B}$ and hence equals $V$. Thus $\mathbb{B}^{\prime}$ is a basis, and from its definition and Theorem 7 , it is clear that (i) is valid and hence also (ii).

Example 18. Let $F$ be a field and let

$$
\alpha=\left(x_{1}, x_{2}, \ldots, x_{n}\right)
$$

be a vector in $F^{n}$. If $Q$ is the standard ordered basis of $F^{n}$,

$$
B=\left\{\epsilon_{1}, \ldots, \epsilon_{n}\right\}
$$

the coordinate matrix of the vector $\alpha$ in the basis $B$ is given by

$$
[\alpha]_{B 1}=\left[\begin{array}{c}
x_{1} \\
x_{2} \\
\vdots \\
x_{n}
\end{array}\right] \text {. }
$$

Example 19. Let $R$ be the field of the real numbers and let $\theta$ be a fixed real number. The matrix

$$
P=\left[\begin{array}{rr}
\cos \theta & -\sin \theta \\
\sin \theta & \cos \theta
\end{array}\right]
$$

is invertible with inverse,

$$
P^{-1}=\left[\begin{array}{rr}
\cos \theta & \sin \theta \\
-\sin \theta & \cos \theta
\end{array}\right] .
$$

Thus for each $\theta$ the set $Q^{\prime}$ consisting of the vectors $(\cos \theta, \sin \theta),(-\sin \theta$, $\cos \theta)$ is a basis for $\boldsymbol{R}^{2}$; intuitively this basis may be described as the one obtained by rotating the standard basis through the angle $\theta$. If $\alpha$ is the vector $\left(x_{1}, x_{2}\right)$, then

or

$$
[\alpha]_{G^{\prime}}=\left[\begin{array}{rr}
\cos \theta & \sin \theta \\
-\sin \theta & \cos \theta
\end{array}\right]\left[\begin{array}{l}
x_{1} \\
x_{2}
\end{array}\right]
$$

$$
\begin{aligned}
& x_{1}^{\prime}=x_{1} \cos \theta+x_{2} \sin \theta \\
& x_{2}^{\prime}=-x_{1} \sin \theta+x_{2} \cos \theta .
\end{aligned}
$$

Example 20. Let $F$ be a subfield of the complex numbers. The matrix

$$
P=\left[\begin{array}{rrr}
-1 & 4 & 5 \\
0 & 2 & -3 \\
0 & 0 & 8
\end{array}\right]
$$

is invertible with inverse

Thus the vectors

$$
P^{-1}=\left[\begin{array}{rcc}
-1 & 2 & \frac{11}{8} \\
0 & \frac{1}{2} & \frac{3}{16} \\
0 & 0 & \frac{1}{8}
\end{array}\right]
$$

$$
\begin{aligned}
& \boldsymbol{\alpha}_{1}^{\prime}=(-1, \quad 0,0) \\
& \alpha_{2}^{\prime}=(4, \quad 2,0) \\
& \alpha_{3}^{\prime}=(5,-3,8)
\end{aligned}
$$

form a basis $Q^{\prime}$ of $F^{3}$. The coordinates $x_{1}^{\prime}, x_{2}^{\prime}, x_{3}^{\prime}$ of the vector $\alpha=\left(x_{1}, x_{2}, x_{3}\right)$ in the basis $B^{\prime}$ are given by

$$
\left[\begin{array}{l}
x_{1}^{\prime} \\
x_{2}^{\prime} \\
x_{3}^{\prime}
\end{array}\right]=\left[\begin{array}{c}
-x_{1}+2 x_{2}+\frac{11}{8} x_{3} \\
\frac{1}{2} x_{2}+\frac{3}{16} x_{3} \\
\frac{1}{8} x_{3}
\end{array}\right]=\left[\begin{array}{rcc}
-1 & 2 & \frac{11}{8} \\
0 & \frac{1}{2} & \frac{3}{16} \\
0 & 0 & \frac{1}{8}
\end{array}\right]\left[\begin{array}{c}
x_{1} \\
x_{2} \\
x_{3}
\end{array}\right] .
$$

In particular,

$$
(3,2,-8)=-10 \alpha_{1}^{\prime}-\frac{1}{2} \alpha_{2}^{\prime}-\alpha_{3}^{\prime} .
$$

\section{Exercises}

1. Show that the vectors

$$
\begin{array}{ll}
\alpha_{1}=(1,1,0,0), & \alpha_{2}=(0,0,1,1) \\
\alpha_{3}=(1,0,0,4), & \alpha_{4}=(0,0,0,2)
\end{array}
$$

form a basis for $R^{4}$. Find the coordinates of each of the standard basis vectors in the ordered basis $\left\{\alpha_{1}, \alpha_{2}, \alpha_{3}, \alpha_{4}\right\}$. 2. Find the coordinate matrix of the vector $(1,0,1)$ in the basis of $C^{3}$ consisting of the vectors $(2 i, 1,0),(2,-1,1),(0,1+i, 1-i)$, in that order.

3. Let $\mathcal{B}=\left\{\alpha_{1}, \alpha_{2}, \alpha_{3}\right\}$ be the ordered basis for $R^{3}$ consisting of

$$
\alpha_{1}=(1,0,-1), \quad \alpha_{2}=(1,1,1), \quad \alpha_{3}=(1,0,0) .
$$

What are the coordinates of the vector $(a, b, c)$ in the ordered basis $\varangle$ ?

4. Let $W$ be the subspace of $C^{3}$ spanned by $\alpha_{1}=(1,0, i)$ and $\alpha_{2}=(1+i, 1,-1)$.

(a) Show that $\alpha_{1}$ and $\alpha_{2}$ form a basis for $W$.

(b) Show that the vectors $\beta_{1}=(1,1,0)$ and $\beta_{2}=(1, i, 1+i)$ are in $W$ and form another basis for $W$.

(c) What are the coordinates of $\alpha_{1}$ and $\alpha_{2}$ in the ordered basis $\left\{\beta_{1}, \beta_{2}\right\}$ for $W$ ?

5. Let $\alpha=\left(x_{1}, x_{2}\right)$ and $\beta=\left(y_{1}, y_{2}\right)$ be vectors in $R^{2}$ such that

$$
x_{1} y_{1}+x_{2} y_{2}=0, \quad x_{1}^{2}+x_{2}^{2}=y_{1}^{2}+y_{2}^{2}=1 .
$$

Prove that $\beta=\{\alpha, \beta\}$ is a basis for $R^{2}$. Find the coordinates of the vector $(a, b)$ in the ordered basis $\mathfrak{B}=\{\alpha, \beta\}$. (The conditions on $\alpha$ and $\beta$ say, geometrically, that $\alpha$ and $\beta$ are perpendicular and each has length 1.)

6. Let $V$ be the vector space over the complex numbers of all functions from $R$ into $C$, i.e., the space of all complex-valued functions on the real line. Let $f_{1}(x)=1$, $f_{2}(x)=e^{i x}, f_{3}(x)=e^{-i x}$.

(a) Prove that $f_{1}, f_{2}$, and $f_{3}$ are linearly independent.

(b) Let $g_{1}(x)=1, g_{2}(x)=\cos x, g_{3}(x)=\sin x$. Find an invertible $3 \times 3$ matrix $P$ such that

$$
g_{j}=\sum_{i=1}^{3} P_{i j} f_{i}
$$

7. Let $V$ be the (real) vector space of all polynomial functions from $R$ into $R$ of degree 2 or less, i.e., the space of all functions $f$ of the form

$$
f(x)=c_{0}+c_{1} x+c_{2} x^{2} .
$$

Let $t$ be a fixed real number and define

$$
g_{1}(x)=1, \quad g_{2}(x)=x+t, \quad g_{3}(x)=(x+t)^{2} .
$$

Prove that $\mathrm{B}=\left\{g_{1}, g_{2}, g_{3}\right\}$ is a basis for $V$. If

$$
f(x)=c_{0}+c_{1} x+c_{2} x^{2}
$$

what are the coordinates of $f$ in this ordered basis $B$ ?

\subsection{Summary of Row-Equivalence}

In this section we shall utilize some elementary facts on bases and dimension in finite-dimensional vector spaces to complete our discussion of row-equivalence of matrices. We recall that if $A$ is an $m \times n$ matrix over the field $F$ the row vectors of $A$ are the vectors $\alpha_{1}, \ldots, \alpha_{m}$ in $F^{n}$ defined by

$$
\alpha_{i}=\left(A_{i 1}, \ldots, A_{i n}\right)
$$

and that the row space of $A$ is the subspace of $F^{n}$ spanned by these vectors. The row rank of $A$ is the dimension of the row space of $A$.

If $P$ is a $k \times m$ matrix over $F$, then the product $B=P A$ is a $k \times n$ matrix whose row vectors $\beta_{1}, \ldots, \beta_{k}$ are linear combinations

$$
\beta_{i}=P_{i 1} \alpha_{1}+\cdots+P_{i m} \alpha_{m}
$$

of the row vectors of $A$. Thus the row space of $B$ is a subspace of the row space of $A$. If $P$ is an $m \times m$ invertible matrix, then $B$ is row-equivalent to $A$ so that the symmetry of row-equivalence, or the equation $A=P^{-1} B$, implies that the row space of $A$ is also a subspace of the row space of $B$.

Theorem 9. Row-equivalent matrices have the same row space.

Thus we see that to study the row space of $A$ we may as well study the row space of a row-reduced echelon matrix which is row-equivalent to $A$. This we proceed to do.

Theorem 10. Let $\mathrm{R}$ be a non-zero row-reduced echelon matrix. Then the non-zero row vectors of $\mathrm{R}$ form a basis for the row space of $\mathrm{R}$.

Proof. Let $\rho_{1}, \ldots, \rho_{r}$ be the non-zero row vectors of $R$ :

$$
\rho_{i}=\left(R_{i 1}, \ldots, R_{i n}\right) \text {. }
$$

Certainly these vectors span the row space of $R$; we need only prove they are linearly independent. Since $R$ is a row-reduced echelon matrix, there are positive integers $k_{1}, \ldots, k_{r}$ such that, for $i \leq r$

(a) $R(i, j)=0$ if $j<k_{i}$

(b) $R\left(i, k_{j}\right)=\delta_{i j}$

(c) $k_{1}<\cdots<k_{r}$.

Suppose $\beta=\left(b_{1}, \ldots, b_{n}\right)$ is a vector in the row space of $R$ :

$$
\beta=c_{1} \rho_{1}+\cdots+c_{r} \rho_{r} .
$$

Then we claim that $c_{j}=b_{k^{i}}$. For, by $(2-18)$

$$
\begin{aligned}
b_{k_{j}} & =\sum_{i=1}^{r} c_{i} R\left(i, k_{j}\right) \\
& =\sum_{i=1}^{r} c_{i} \delta_{i j} \\
& =c_{j} .
\end{aligned}
$$

In particular, if $\beta=0$, i.e., if $c_{1} \rho_{1}+\cdots+c_{r} \rho_{r}=0$, then $c_{j}$ must be the $k_{j}$ th coordinate of the zero vector so that $c_{j}=0, j=1, \ldots, r$. Thus $\rho_{1}, \ldots, \rho_{r}$ are linearly independent.

Theorem 11. Let $\mathrm{m}$ and $\mathrm{n}$ be positive integers and let $\mathrm{F}$ be a field. Suppose $\mathrm{W}$ is a subspace of $\mathrm{F}^{\mathrm{n}}$ and $\operatorname{dim} \mathrm{W} \leq \mathrm{m}$. Then there is precisely one $\mathrm{m} \times \mathrm{n}$ row-reduced echelon matrix over $\mathrm{F}$ which has $\mathrm{W}$ as its row space. Proof. There is at least one $m \times n$ row-reduced echelon matrix with row space $W$. Since $\operatorname{dim} W \leq m$, we can select some $m$ vectors $\alpha_{1}, \ldots, \alpha_{m}$ in $W$ which span $W$. Let $A$ be the $m \times n$ matrix with row vectors $\alpha_{1}, \ldots, \alpha_{m}$ and let $R$ be a row-reduced echelon matrix which is row-equivalent to $A$. Then the row space of $R$ is $W$.

Now let $R$ be any row-reduced echelon matrix which has $W$ as its row space. Let $\rho_{1}, \ldots, \rho_{r}$ be the non-zero row vectors of $R$ and suppose that the leading non-zero entry of $\rho_{i}$ occurs in column $k_{i}, i=1, \ldots, r$. The vectors $\rho_{1}, \ldots, \rho_{r}$ form a basis for $W$. In the proof of Theorem 10, we observed that if $\beta=\left(b_{1}, \ldots, b_{n}\right)$ is in $W$, then

$$
\beta=c_{1} \rho_{1}+\cdots+c_{r} \rho_{r},
$$

and $c_{i}=b_{k_{i}}$; in other words, the unique expression for $\beta$ as a linear combination of $\rho_{1}, \ldots, \rho_{r}$ is

$$
\beta=\sum_{i=1}^{r} b_{k_{i} \rho_{i}} .
$$

Thus any vector $\beta$ is determined if one knows the coordinates $b_{k_{1}}, i=1, \ldots$, $r$. For example, $\rho_{s}$ is the unique vector in $W$ which has $k_{s}$ th coordinate 1 and $k_{i}$ th coordinate 0 for $i \neq s$.

Suppose $\beta$ is in $W$ and $\beta \neq 0$. We claim the first non-zero coordinate of $\beta$ occurs in one of the columns $k_{s .}$. Since

and $\beta \neq 0$, we can write

$$
\beta=\sum_{i=1}^{r} b_{k_{i} \rho_{i}}
$$

$$
\beta=\sum_{i=s}^{r} b_{k_{i}} \rho_{i}, \quad b_{k_{t}} \neq 0 .
$$

From the conditions (2-18) one has $R_{i j}=0$ if $i>s$ and $j \leq k_{8}$. Thus

$$
\beta=\left(0, \ldots, 0, \quad b_{k_{s}}, \ldots, b_{n}\right), \quad b_{k_{t}} \neq 0
$$

and the first non-zero coordinate of $\beta$ occurs in column $k_{s}$. Note also that for each $k_{s}, s=1, \ldots, r$, there exists a vector in $W$ which has a non-zero $k_{s}$ th coordinate, namely $\rho_{s}$.

It is now clear that $R$ is uniquely determined by $W$. The description of $R$ in terms of $W$ is as follows. We consider all vectors $\beta=\left(b_{1}, \ldots, b_{n}\right)$ in $W$. If $\beta \neq 0$, then the first non-zero coordinate of $\beta$ must occur in some column $t$ :

$$
\beta=\left(0, \ldots, 0, \quad b_{t}, \ldots, b_{n}\right), \quad b_{t} \neq 0 .
$$

Let $k_{1}, \ldots, k_{r}$ be those positive integers $t$ such that there is some $\beta \neq 0$ in $W$, the first non-zero coordinate of which occurs in column $t$. Arrange $k_{1}, \ldots, k_{r}$ in the order $k_{1}<k_{2}<\cdots<k_{r}$. For each of the positive integers $k_{s}$ there will be one and only one vector $\rho_{s}$ in $W$ such that the $k_{s}$ th coordinate of $\rho_{s}$ is 1 and the $k_{i}$ th coordinate of $\rho_{s}$ is 0 for $i \neq s$. Then $R$ is the $m \times n$ matrix which has row vectors $\rho_{1}, \ldots, \rho_{r}, 0, \ldots, 0$. Corollary. Each $\mathrm{m} \times \mathrm{n}$ matrix $\mathrm{A}$ is row-equivalent to one and only one row-reduced echelon matrix.

Proof. We know that $A$ is row-equivalent to at least one rowreduced echelon matrix $R$. If $A$ is row-equivalent to another such matrix $R^{\prime}$, then $R$ is row-equivalent to $R^{\prime}$; hence, $R$ and $R^{\prime}$ have the same row space and must be identical.

Corollary. Let $\mathrm{A}$ and $\mathrm{B}$ be $\mathrm{m} \times \mathrm{n}$ matrices over the field $\mathrm{F}$. Then $\mathrm{A}$ and $\mathrm{B}$ are row-equivalent if and only if they have the same row space.

Proof. We know that if $A$ and $B$ are row-equivalent, then they have the same row space. So suppose that $A$ and $B$ have the same row space. Now $A$ is row-equivalent to a row-reduced echelon matrix $R$ and $B$ is row-equivalent to a row-reduced echelon matrix $R^{\prime}$. Since $A$ and $B$ have the same row space, $R$ and $R^{\prime}$ have the same row space. Thus $R=R^{\prime}$ and $A$ is row-equivalent to $B$.

To summarize-if $A$ and $B$ are $m \times n$ matrices over the field $F$, the following statements are equivalent:

1. $A$ and $B$ are row-equivalent.

2. $A$ and $B$ have the same row space.

3. $B=P A$, where $P$ is an invertible $m \times m$ matrix.

A fourth equivalent statement is that the homogeneous systems $A X=0$ and $B X=0$ have the same solutions; however, although we know that the row-equivalence of $A$ and $B$ implies that these systems have the same solutions, it seems best to leave the proof of the converse until later.

\subsection{Computations Concerning Subspaces}

We should like now to show how elementary row operations provide a standardized method of answering certain concrete questions concerning subspaces of $F^{n}$. We have already derived the facts we shall need. They are gathered here for the convenience of the reader. The discussion applies to any $n$-dimensional vector space over the field $F$, if one selects a fixed ordered basis $\Theta$ and describes each vector $\alpha$ in $V$ by the $n$-tuple $\left(x_{1}, \ldots, x_{n}\right)$ which gives the coordinates of $\alpha$ in the ordered basis $\Theta$.

Suppose we are given $m$ vectors $\alpha_{1}, \ldots, \alpha_{m}$ in $F^{n}$. We consider the following questions.

1. How does one determine if the vectors $\alpha_{1}, \ldots, \alpha_{m}$ are linearly independent? More generally, how does one find the dimension of the subspace $W$ spanned by these vectors? 2. Given $\beta$ in $F^{n}$, how does one determine whether $\beta$ is a linear combination of $\alpha_{1}, \ldots, \alpha_{m}$, i.e., whether $\beta$ is in the subspace $W$ ?

3. How can one give an explicit description of the subspace $W$ ?

The third question is a little vague, since it does not specify what is meant by an 'explicit description'; however, we shall clear up this point by giving the sort of description we have in mind. With this description, questions (1) and (2) can be answered immediately.

Let $A$ be the $m \times n$ matrix with row vectors $\alpha_{i}$ :

$$
\alpha_{i}=\left(A_{i 1}, \ldots, A_{i n}\right) .
$$

Perform a sequence of elementary row operations, starting with $A$ and terminating with a row-reduced echelon matrix $R$. We have previously described how to do this. At this point, the dimension of $W$ (the row space of $A$ ) is apparent, since this dimension is simply the number of non-zero row vectors of $R$. If $\rho_{1}, \ldots, \rho_{r}$ are the non-zero row vectors of $R$, then $B=\left\{\rho_{1}, \ldots, \rho_{r}\right\}$ is a basis for $W$. If the first non-zero coordinate of $\rho_{i}$ is the $k_{i}$ th one, then we have for $i \leq r$

(a)

(c)

$$
\begin{aligned}
& R(i, j)=0, \text { if } j<k_{i} \\
& R\left(i, k_{j}\right)=\delta_{i j} \\
& k_{1}<\cdots<k_{r} .
\end{aligned}
$$

The subspace $W$ consists of all vectors

$$
\begin{aligned}
\beta & =c_{1} \rho_{1}+\cdots+c_{r} \rho_{r} \\
& =\sum_{i=1}^{r} c_{i}\left(R_{i 1}, \ldots, R_{i n}\right) .
\end{aligned}
$$

The coordinates $b_{1}, \ldots, b_{n}$ of such a vector $\beta$ are then

$$
b_{j}=\sum_{i=1}^{r} c_{i} \boldsymbol{R}_{i j} .
$$

In particular, $b_{k_{i}}=c_{j}$, and so if $\beta=\left(b_{1}, \ldots, b_{n}\right)$ is a linear combination of the $\rho_{i}$, it must be the particular linear combination

$$
\beta=\sum_{i=1}^{r} b_{k_{i}} \rho_{i} .
$$

The conditions on $\beta$ that (2-24) should hold are

$$
b_{j}=\sum_{i=1}^{r} b_{k i} R_{i j}, \quad j=1, \ldots, n .
$$

Now (2-25) is the explicit description of the subspace $W$ spanned by $\alpha_{1}, \ldots, \alpha_{m}$, that is, the subspace consists of all vectors $\beta$ in $F^{n}$ whose coordinates satisf y (2-25). What kind of description is (2-25)? In the first place it describes $W$ as all solutions $\beta=\left(b_{1}, \ldots, b_{n}\right)$ of the system of homogeneous linear equations (2-25). This system of equations is of a very special nature, because it expresses $(n-r)$ of the coordinates as linear combinations of the $r$ distinguished coordinates $b_{k_{1}}, \ldots, b_{k_{r}}$. One has complete freedom of choice in the coordinates $b_{k_{i}}$; that is, if $c_{1}, \ldots, c_{r}$ are any $r$ scalars, there is one and only one vector $\beta$ in $W$ which has $c_{i}$ as its $k_{i}$ th coordinate.

The significant point here is this: Given the vectors $\alpha_{i}$, row-reduction is a straightforward method of determining the integers $r, k_{1}, \ldots, k_{r}$ and the scalars $R_{i j}$ which give the description (2-25) of the subspace spanned by $\alpha_{1}, \ldots, \alpha_{m}$. One should observe as we did in Theorem 11 that every subspace $W$ of $F^{n}$ has a description of the type (2-25). We should also point out some things about question (2). We have already stated how one can find an invertible $m \times m$ matrix $P$ such that $R=P A$, in Section 1.4. The knowledge of $P$ enables one to find the scalars $x_{1}, \ldots, x_{m}$ such that

$$
\beta=x_{1} \alpha_{1}+\cdots+x_{m} \alpha_{m}
$$

when this is possible. For the row vectors of $R$ are given by

$$
\rho_{i}=\sum_{j=1}^{m} P_{i j} \alpha_{j}
$$

so that if $\beta$ is a linear combination of the $\alpha_{j}$, we have

$$
\begin{aligned}
\beta & =\sum_{i=1}^{r} b_{k_{i}, \rho_{i}} \\
& =\sum_{i=1}^{r} b_{k_{1}} \sum_{j=1}^{m} P_{i j} \alpha_{j} \\
& =\sum_{j=1}^{m} \sum_{i=1}^{r} b_{k_{i}} P_{i j} \alpha_{j}
\end{aligned}
$$

and thus

$$
x_{j}=\sum_{i=1}^{r} b_{k i} P_{i j}
$$

is one possible choice for the $x_{j}$ (there may be many).

The question of whether $\beta=\left(b_{1}, \ldots, b_{n}\right)$ is a linear combination of the $\alpha_{i}$, and if so, what the scalars $x_{i}$ are, can also be looked at by asking whether the system of equations

$$
\sum_{i=1}^{m} A_{i j} x_{i}=b_{j}, \quad j=1, \ldots, n
$$

has a solution and what the solutions are. The coefficient matrix of this system of equations is the $n \times m$ matrix $B$ with column vectors $\alpha_{1}, \ldots, \alpha_{m}$. In Chapter 1 we discussed the use of elementary row operations in solving a system of equations $B X=Y$. Let us consider one example in which we adopt both points of view in answering questions about subspaces of $F^{n}$.

Example 21. Let us pose the following problem. Let $W$ be the subspace of $R^{4}$ spanned by the vectors 

$$
\begin{aligned}
& \alpha_{1}=(1,2,2,1) \\
& \alpha_{2}=(0,2,0,1) \\
& \alpha_{3}=(-2,0,-4,3) .
\end{aligned}
$$

(a) Prove that $\alpha_{1}, \alpha_{2}, \alpha_{3}$ form a basis for $W$, i.e., that these vectors are linearly independent.

(b) Let $\beta=\left(b_{1}, b_{2}, b_{3}, b_{4}\right)$ be a vector in $W$. What are the coordinates of $\beta$ relative to the ordered basis $\left\{\alpha_{1}, \alpha_{2}, \alpha_{3}\right\}$ ?

(c) Let

$$
\begin{aligned}
& \alpha_{1}^{\prime}=(1,0,2,0) \\
& \alpha_{2}^{\prime}=(0,2,0,1) \\
& \alpha_{3}^{\prime}=(0,0,0,3) .
\end{aligned}
$$

Show that $\alpha_{1}^{\prime}, \alpha_{2}^{\prime}, \alpha_{3}^{\prime}$ form a basis for $W$.

(d) If $\beta$ is in $W$, let $X$ denote the coordinate matrix of $\beta$ relative to the $\alpha$-basis and $X^{\prime}$ the coordinate matrix of $\beta$ relative to the $\alpha^{\prime}$-basis. Find the $3 \times 3$ matrix $P$ such that $X=P X^{\prime}$ for every such $\beta$.

To answer these questions by the first method we form the matrix $A$ with row vectors $\alpha_{1}, \alpha_{2}, \alpha_{3}$, find the row-reduced echelon matrix $R$ which is row-equivalent to $A$ and simultaneously perform the same operations on the identity to obtain the invertible matrix $Q$ such that $R=Q A$ :

$$
\begin{gathered}
{\left[\begin{array}{rrrr}
1 & 2 & 2 & 1 \\
0 & 2 & 0 & 1 \\
-2 & 0 & -4 & 3
\end{array}\right] \rightarrow R=\left[\begin{array}{llll}
1 & 0 & 2 & 0 \\
0 & 1 & 0 & 0 \\
0 & 0 & 0 & 1
\end{array}\right]} \\
{\left[\begin{array}{lll}
1 & 0 & 0 \\
0 & 1 & 0 \\
0 & 0 & 1
\end{array}\right] \rightarrow Q=\frac{1}{6}\left[\begin{array}{rrr}
6 & -6 & 0 \\
-2 & 5 & -1 \\
4 & -4 & 2
\end{array}\right]}
\end{gathered}
$$

(a) Clearly $R$ has rank 3 , so $\alpha_{1}, \alpha_{2}$ and $\alpha_{3}$ are independent.

(b) Which vectors $\beta=\left(b_{1}, b_{2}, b_{3}, b_{4}\right)$ are in $W$ ? We have the basis for $W$ given by $\rho_{1}, \rho_{2}, \rho_{3}$, the row vectors of $R$. One can see at a glance that the span of $\rho_{1}, \rho_{2}, \rho_{3}$ consists of the vectors $\beta$ for which $b_{3}=2 b_{1}$. For such a $\beta$ we have

$$
\begin{aligned}
& \beta=b_{1} \rho_{1}+b_{2} \rho_{2}+b_{4} \rho_{3} \\
& =\left[b_{1}, b_{2}, b_{4}\right] R \\
& =\left[\begin{array}{lll}b_{1} & b_{2} & b_{4}\end{array}\right] Q A \\
& =x_{1} \alpha_{1}+x_{2} \alpha_{2}+x_{3} \alpha_{3}
\end{aligned}
$$

where $x_{i}=\left[\begin{array}{lll}b_{1} & b_{2} & b_{4}\end{array}\right] Q_{i}$ :

$$
\begin{array}{lr}
x_{1}= & b_{1}-\frac{1}{3} b_{2}+\frac{2}{3} b_{4} \\
x_{2}= & -b_{1}+\frac{5}{6} b_{2}-\frac{2}{3} b_{4} \\
x_{3}= & -\frac{1}{6} b_{2}+\frac{1}{3} b_{4} .
\end{array}
$$

(c) The vectors $\alpha_{1}^{\prime}, \alpha_{2}^{\prime}, \alpha_{3}^{\prime}$ are all of the form $\left(y_{1}, y_{2}, y_{3}, y_{4}\right)$ with $y_{3}=2 y_{1}$ and thus they are in $W$. One can see at a glance that they are independent.

(d) The matrix $P$ has for its columns

$$
P_{j}=\left[\alpha_{j}^{\prime}\right]_{\otimes}
$$

where $B=\left\{\alpha_{1}, \alpha_{2}, \alpha_{3}\right\}$. The equations (2-26) tell us how to find the coordinate matrices for $\alpha_{1}^{\prime}, \alpha_{2}^{\prime}, \alpha_{3}^{\prime}$. For example with $\beta=\alpha_{1}^{\prime}$ we have $b_{1}=1$, $b_{2}=0, b_{3}=2, b_{4}=0$, and

$$
\begin{aligned}
& x_{1}=1-\frac{1}{3}(0)+\frac{2}{3}(0)=1 \\
& x_{2}=-1+\frac{5}{6}(0)-\frac{2}{3}(0)=-1 \\
& x_{3}=-\frac{1}{6}(0)+\frac{1}{3}(0)=0 .
\end{aligned}
$$

Thus $\alpha_{1}^{\prime}=\alpha_{1}-\alpha_{2}$. Similarly we obtain $\alpha_{2}^{\prime}=\alpha_{2}$ and $\alpha_{3}^{\prime}=2 \alpha_{1}-2 \alpha_{2}+\alpha_{3}$. Hence

$$
P=\left[\begin{array}{rrr}
1 & 0 & 2 \\
-1 & 1 & -2 \\
0 & 0 & 1
\end{array}\right]
$$

Now let us see how we would answer the questions by the second method which we described. We form the $4 \times 3$ matrix $B$ with column vectors $\alpha_{1}, \alpha_{2}, \alpha_{3}$ :

$$
B=\left[\begin{array}{rrr}
1 & 0 & -2 \\
2 & 2 & 0 \\
2 & 0 & -4 \\
1 & 1 & 3
\end{array}\right]
$$

We inquire for which $y_{1}, y_{2}, y_{3}, y_{4}$ the system $B X=Y$ has a solution.

$$
\begin{gathered}
{\left[\begin{array}{rrrr}
1 & 0 & -2 & y_{1} \\
2 & 2 & 0 & y_{2} \\
2 & 0 & -4 & y_{3} \\
1 & 1 & 3 & y_{4}
\end{array}\right] \rightarrow\left[\begin{array}{rrrc}
1 & 0 & -2 & y_{1} \\
0 & 2 & 4 & y_{2}-2 y_{1} \\
0 & 0 & 0 & y_{3}-2 y_{1} \\
0 & 1 & 5 & y_{4}-y_{1}
\end{array}\right] \rightarrow} \\
{\left[\begin{array}{rrrrc}
1 & 0 & -2 & y_{1} \\
0 & 0 & -6 & y_{2}-2 y_{4} \\
0 & 1 & 5 & y_{4}-y_{1} \\
0 & 0 & 0 & y_{3}-2 y_{1}
\end{array}\right] \rightarrow\left[\begin{array}{rccc}
1 & 0 & 0 & y_{1}-\frac{1}{3} y_{2}+\frac{2}{3} y_{4} \\
0 & 0 & 1 & \frac{1}{6}\left(2 y_{4}-y_{2}\right) \\
0 & 1 & 0 & -y_{1}+\frac{5}{6} y_{2}-\frac{2}{3} y_{4} \\
0 & 0 & 0 & y_{3}-2 y_{1}
\end{array}\right]}
\end{gathered}
$$

Thus the condition that the system $B X=Y$ have a solution is $y_{3}=2 y_{1}$. So $\beta=\left(b_{1}, b_{2}, b_{3}, b_{4}\right)$ is in $W$ if and only if $b_{3}=2 b_{1}$. If $\beta$ is in $W$, then the coordinates $\left(x_{1}, x_{2}, x_{3}\right)$ in the ordered basis $\left\{\alpha_{1}, \alpha_{2}, \alpha_{3}\right\}$ can be read off from the last matrix above. We obtain once again the formulas (2-26) for those coordinates.

The questions (c) and (d) are now answered as before. Example 22. We consider the $5 \times 5$ matrix

$$
A=\left[\begin{array}{rrrrr}
1 & 2 & 0 & 3 & 0 \\
1 & 2 & -1 & -1 & 0 \\
0 & 0 & 1 & 4 & 0 \\
2 & 4 & 1 & 10 & 1 \\
0 & 0 & 0 & 0 & 1
\end{array}\right]
$$

and the following problems concerning $A$

(a) Find an invertible matrix $P$ such that $P A$ is a row-reduced echelon matrix $R$.

(b) Find a basis for the row space $W$ of $A$.

(c) Say which vectors $\left(b_{1}, b_{2}, b_{3}, b_{4}, b_{5}\right)$ are in $W$.

(d) Find the coordinate matrix of each vector $\left(b_{1}, b_{2}, b_{3}, b_{4}, b_{5}\right)$ in $W$ in the ordered basis chosen in (b).

(e) Write each vector $\left(b_{1}, b_{2}, b_{3}, b_{4}, b_{5}\right)$ in $W$ as a linear combination of the rows of $A$.

(f) Give an explicit description of the vector space $V$ of all $5 \times 1$ column matrices $X$ such that $A X=0$.

(g) Find a basis for $V$.

(h) For what $5 \times 1$ column matrices $Y$ does the equation $A X=Y$ have solutions $X$ ?

To solve these problems we form the augmented matrix $A^{\prime}$ of the system $A X=Y$ and apply an appropriate sequence of row operations to $A^{\prime}$.

$\left[\begin{array}{rrrrrc}1 & 2 & 0 & 3 & 0 & y_{1} \\ 1 & 2 & -1 & -1 & 0 & y_{2} \\ 0 & 0 & 1 & 4 & 0 & y_{3} \\ 2 & 4 & 1 & 10 & 1 & y_{4} \\ 0 & 0 & 0 & 0 & 1 & y_{5}\end{array}\right] \rightarrow\left[\begin{array}{rrrrcc}1 & 2 & 0 & 3 & 0 & y_{1} \\ 0 & 0 & -1 & -4 & 0 & -y_{1}+y_{2} \\ 0 & 0 & 1 & 4 & 0 & y_{3} \\ 0 & 0 & 1 & 4 & 1 & -2 y_{1}+y_{4} \\ 0 & 0 & 0 & 0 & 1 & y_{5}\end{array}\right] \rightarrow$

$\left[\begin{array}{lllllc}1 & 2 & 0 & 3 & 0 & y_{1} \\ 0 & 0 & 1 & 4 & 0 & y_{1}-y_{2} \\ 0 & 0 & 0 & 0 & 0 & -y_{1}+y_{2}+y_{3} \\ 0 & 0 & 0 & 0 & 1 & -3 y_{1}+y_{2}+y_{4} \\ 0 & 0 & 0 & 0 & 1 & y_{5}\end{array}\right] \rightarrow$

$$
\left[\begin{array}{cccccc}
1 & 2 & 0 & 3 & 0 & y_{1} \\
0 & 0 & 1 & 4 & 0 & y_{1}-y_{2} \\
0 & 0 & 0 & 0 & 1 & y_{5} \\
0 & 0 & 0 & 0 & 0 & -y_{1}+y_{2}+y_{3} \\
0 & 0 & 0 & 0 & 0 & -3 y_{1}+y_{2}+y_{4}-y_{5}
\end{array}\right]
$$

(a) If

for all $Y$, then

$$
P Y=\left[\begin{array}{c}
y_{1} \\
y_{1}-y_{2} \\
y_{6} \\
-y_{1}+y_{2}+y_{3} \\
-3 y_{1}+y_{2}+y_{4}-y_{5}
\end{array}\right]
$$

$$
P=\left[\begin{array}{rrrrr}
1 & 0 & 0 & 0 & 0 \\
1 & -1 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 & 1 \\
-1 & 1 & 1 & 0 & 0 \\
-3 & 1 & 0 & 1 & -1
\end{array}\right]
$$

hence $P A$ is the row-reduced echelon matrix

$$
R=\left[\begin{array}{lllll}
1 & 2 & 0 & 3 & 0 \\
0 & 0 & 1 & 4 & 0 \\
0 & 0 & 0 & 0 & 1 \\
0 & 0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 & 0
\end{array}\right] .
$$

It should be stressed that the matrix $P$ is not unique. There are, in fact, many invertible matrices $P$ (which arise from different choices for the operations used to reduce $A^{\prime}$ ) such that $P A=R$.

(b) As a basis for $W$ we may take the non-zero rows

$$
\begin{aligned}
& \rho_{1}=\left(\begin{array}{lllll}
1 & 2 & 0 & 3 & 0
\end{array}\right) \\
& \rho_{2}=\left(\begin{array}{lllll}
0 & 0 & 1 & 4 & 0
\end{array}\right) \\
& \rho_{3}=\left(\begin{array}{lllll}
0 & 0 & 0 & 0 & 1
\end{array}\right)
\end{aligned}
$$

of $R$.

(c) The row-space $W$ consists of all vectors of the form

$$
\begin{aligned}
\beta & =c_{1} \rho_{1}+c_{2} \rho_{2}+c_{3} \rho_{3} \\
& =\left(c_{1}, 2 c_{1}, c_{2}, 3 c_{1}+4 c_{2}, c_{3}\right)
\end{aligned}
$$

where $c_{1}, c_{2}, c_{3}$ are arbitrary scalars. Thus $\left(b_{1}, b_{2}, b_{3}, b_{4}, b_{5}\right)$ is in $W$ if and only if

$$
\left(b_{1}, b_{2}, b_{3}, b_{4}, b_{5}\right)=b_{1} \rho_{1}+b_{3} \rho_{2}+b_{5} \rho_{3}
$$

which is true if and only if

$$
\begin{aligned}
& b_{2}=2 b_{1} \\
& b_{4}=3 b_{1}+4 b_{3} .
\end{aligned}
$$

These equations are instances of the general system (2-25), and using them we may tell at a glance whether a given vector lies in $W$. Thus $(-5,-10,1,-11,20)$ is a linear combination of the rows of $A$, but $(1,2,3,4,5)$ is not.

(d) The coordinate matrix of the vector $\left(b_{1}, 2 b_{1}, b_{3}, 3 b_{1}+4 b_{3}, b_{5}\right)$ in the basis $\left\{\rho_{1}, \rho_{2}, \rho_{3}\right\}$ is evidently 

$$
\left[\begin{array}{l}
b_{1} \\
b_{3} \\
b_{5}
\end{array}\right] \text {. }
$$

(e) There are many ways to write the vectors in $W$ as linear combinations of the rows of $A$. Perhaps the easiest method is to follow the first procedure indicated before Example 21:

$$
\begin{aligned}
\beta & =\left(b_{1}, 2 b_{1}, b_{3}, 3 b_{1}+4 b_{3}, b_{5}\right) \\
& =\left[b_{1}, b_{3}, b_{5}, 0,0\right] \cdot R \\
& =\left[b_{1}, b_{3}, b_{5}, 0,0\right] \cdot P A \\
& =\left[b_{1}, b_{3}, b_{5}, 0,0\right]\left[\begin{array}{rrrrr}
1 & 0 & 0 & 0 & 0 \\
1 & -1 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 & 1 \\
-1 & 1 & 1 & 0 & 0 \\
-3 & 1 & 0 & 1 & -1
\end{array}\right] \cdot A \\
& =\left[b_{1}+b_{3},-b_{3}, 0,0, b_{5}\right] \cdot A .
\end{aligned}
$$

In particular, with $\beta=(-5,-10,1,-11,20)$ we have

$$
\beta=(-4,-1,0,0,20)\left[\begin{array}{rrrrr}
1 & 2 & 0 & 3 & 0 \\
1 & 2 & -1 & -1 & 0 \\
0 & 0 & 1 & 4 & 0 \\
2 & 4 & 1 & 10 & 1 \\
0 & 0 & 0 & 0 & 1
\end{array}\right] .
$$

(f) The equations in the system $R X=0$ are

$$
\begin{aligned}
x_{1}+2 x_{2}+3 x_{4} & =0 \\
x_{3}+4 x_{4} & =0 \\
x_{5} & =0 .
\end{aligned}
$$

Thus $V$ consists of all columns of the form

$$
X=\left[\begin{array}{c}
-2 x_{2}-3 x_{4} \\
x_{2} \\
-4 x_{4} \\
x_{4} \\
0
\end{array}\right]
$$

where $x_{2}$ and $x_{4}$ are arbitrary.

(g) The columns

$$
\left[\begin{array}{r}
-2 \\
1 \\
0 \\
0 \\
0
\end{array}\right] \quad\left[\begin{array}{r}
-3 \\
0 \\
-4 \\
1 \\
0
\end{array}\right]
$$

form a basis of $V$. This is an example of the basis described in Example 15 . (h) The equation $A X=Y$ has solutions $X$ if and only if

$$
\begin{aligned}
-y_{1}+y_{2}+y_{3} & =0 \\
-3 y_{1}+y_{2}+y_{4}-y_{5} & =0
\end{aligned}
$$

\section{Exercises}

1. Let $s<n$ and $A$ an $s \times n$ matrix with entries in the field $F$. Use Theorem 4 (not its proof) to show that there is a non-zero $X$ in $F^{n \times 1}$ such that $A X=0$.

2. Let

$$
\alpha_{1}=(1,1,-2,1), \quad \alpha_{2}=(3,0,4,-1), \quad \alpha_{3}=(-1,2,5,2) .
$$

Let

$$
\alpha=(4,-5,9,-7), \quad \beta=(3,1,-4,4), \quad \gamma=(-1,1,0,1) .
$$

(a) Which of the vectors $\alpha, \beta, \gamma$ are in the subspace of $R^{4}$ spanned by the $\alpha_{i}$ ?

(b) Which of the vectors $\alpha, \beta, \gamma$ are in the subspace of $C^{4}$ spanned by the $\alpha_{i}$ ?

(c) Does this suggest a theorem?

3. Consider the vectors in $R^{4}$ defined by

$$
\alpha_{1}=(-1,0,1,2), \quad \alpha_{2}=(3,4,-2,5), \quad \alpha_{3}=(1,4,0,9) .
$$

Find a system of homogeneous linear equations for which the space of solutions is exactly the subspace of $R^{4}$ spanned by the three given vectors.

4. In $C^{3}$, let

$$
\alpha_{1}=(1,0,-i), \quad \alpha_{2}=(1+i, 1-i, 1), \quad \alpha_{3}=(i, i, i) .
$$

Prove that these vectors form a basis for $C^{3}$. What are the coordinates of the vector $(a, b, c)$ in this basis?

5. Give an explicit description of the type (2-25) for the vectors

$$
\beta=\left(b_{1}, b_{2}, b_{3}, b_{4}, b_{6}\right)
$$

in $\mathrm{R}^{5}$ which are linear combinations of the vectors

$$
\begin{aligned}
\alpha_{1} & =(1,0,2,1,-1), & \alpha_{2} & =(-1,2,-4,2,0) \\
\alpha_{3} & =(2,-1,5,2,1), & \alpha_{4} & =(2,1,3,5,2) .
\end{aligned}
$$

6. Let $V$ be the real vector space spanned by the rows of the matrix

$$
A=\left[\begin{array}{rrrrr}
3 & 21 & 0 & 9 & 0 \\
1 & 7 & -1 & -2 & -1 \\
2 & 14 & 0 & 6 & 1 \\
6 & 42 & -1 & 13 & 0
\end{array}\right] \text {. }
$$

(a) Find a basis for $V$.

(b) Tell which vectors $\left(x_{1}, x_{2}, x_{3}, x_{4}, x_{5}\right)$ are elements of $V$.

(c) If $\left(x_{1}, x_{2}, x_{3}, x_{4}, x_{5}\right)$ is in $V$ what are its coordinates in the basis chosen in part (a)?

7. Let $A$ be an $m \times n$ matrix over the field $F$, and consider the system of equations $A X=Y$. Prove that this system of equations has a solution if and only if the row rank of $A$ is equal to the row rank of the augmented matrix of the system. 

\section{Linear Transformations}

\subsection{Linear Transformations}

We shall now introduce linear transformations, the objects which we shall study in most of the remainder of this book. The reader may find it helpful to read (or reread) the discussion of functions in the Appendix, since we shall freely use the terminology of that discussion.

Definition. Let $\mathrm{V}$ and $\mathrm{W}$ be vector spaces over the field $\mathrm{F}$. A linear transformation from $\mathrm{V}$ into $\mathrm{W}$ is a function $\mathrm{T}$ from $\mathrm{V}$ into $\mathrm{W}$ such that

$$
\mathrm{T}(\mathrm{c} \alpha+\beta)=\mathrm{c}(\mathrm{T} \alpha)+\mathrm{T} \beta
$$

for all $\alpha$ and $\beta$ in $\mathrm{V}$ and all scalars $\mathrm{c}$ in $\mathrm{F}$.

Example 1. If $V$ is any vector space, the identity transformation $I$, defined by $I \alpha=\alpha$, is a linear transformation from $V$ into $V$. The zero transformation 0 , defined by $0 \alpha=0$, is a linear transformation from $V$ into $V$.

Example 2. Let $F$ be a field and let $V$ be the space of polynomial functions $f$ from $F$ into $F$, given by

Let

$$
f(x)=c_{0}+c_{1} x+\cdots+c_{k} x^{k} .
$$

$$
(D f)(x)=c_{1}+2 c_{2} x+\cdots+k c_{k} x^{k-1} .
$$

Then $D$ is a linear transformation from $V$ into $V$-the differentiation transformation. EXAMPLE 3 . Let $A$ be a fixed $m \times n$ matrix with entries in the field $F$. The function $T$ defined by $T(X)=A X$ is a linear transformation from $F_{n \times 1}$ into $F^{m \times 1}$. The function $U$ defined by $U(\alpha)=\alpha A$ is a linear transformation from $F^{m}$ into $F^{n}$.

EXAMple 4. Let $P$ be a fixed $m \times m$ matrix with entries in the field $F$ and let $Q$ be a fixed $n \times n$ matrix over $F$. Define a function $T$ from the space $F^{m \times n}$ into itself by $T(A)=P A Q$. Then $T$ is a linear transformation from $F^{m \times n}$ into $F^{m \times n}$, because

$$
\begin{aligned}
T(c A+B) & =P(c A+B) Q \\
& =(c P A+P B) Q \\
& =c P A Q+P B Q \\
& =c T(A)+T(B) .
\end{aligned}
$$

Example 5 . Let $R$ be the field of real numbers and let $V$ be the space of all functions from $R$ into $R$ which are continuous. Define $T$ by

$$
(T f)(x)=\int_{0}^{x} f(t) d t .
$$

Then $T$ is a linear transformation from $V$ into $V$. The function $T f$ is not only continuous but has a continuous first derivative. The linearity of integration is one of its fundamental properties.

The reader should have no difficulty in verifying that the transformations defined in Examples 1, 2, 3, and 5 are linear transformations. We shall expand our list of examples considerably as we learn more about linear transformations.

It is important to note that if $T$ is a linear transformation from $V$ into $W$, then $T(0)=0$; one can see this from the definition because

$$
T(0)=T(0+0)=T(0)+T(0) .
$$

This point is of ten confusing to the person who is studying linear algebra for the first time, since he probably has been exposed to a slightly different use of the term 'linear function.' A brief comment should clear up the confusion. Suppose $V$ is the vector space $R^{1}$. A linear transformation from $V$ into $V$ is then a particular type of real-valued function on the real line $R$. In a calculus course, one would probably call such a function linear if its graph is a straight line. A linear transformation from $R^{1}$ into $R^{1}$, according to our definition, will be a function from $R$ into $R$, the graph of which is a straight line passing through the origin.

In addition to the property $T(0)=0$, let us point out another property of the general linear transformation $T$. Such a transformation 'preserves' linear combinations; that is, if $\alpha_{1}, \ldots, \alpha_{n}$ are vectors in $V$ and $c_{1}, \ldots, c_{n}$ are scalars, then

$$
T\left(c_{1} \alpha_{1}+\cdots+c_{n} \alpha_{n}\right)=c_{1}\left(T \alpha_{1}\right)+\cdots+c_{n}\left(T \alpha_{n}\right) .
$$

This follows readily from the definition. For example,

$$
\begin{aligned}
T\left(c_{1} \alpha_{1}+c_{2} \alpha_{2}\right) & =c_{1}\left(T \alpha_{1}\right)+T\left(c_{2} \alpha_{2}\right) \\
& =c_{1}\left(T \alpha_{1}\right)+c_{2}\left(T \alpha_{2}\right) .
\end{aligned}
$$

Theorem 1. Let $\mathrm{V}$ be a finite-dimensional vector space over the field $\mathrm{F}$ and let $\left\{\alpha_{1}, \ldots, \alpha_{n}\right\}$ be an ordered basis for $\mathrm{V}$. Let $\mathrm{W}$ be a vector space over the same field $\mathrm{F}$ and let $\beta_{1}, \ldots, \beta_{n}$ be any vectors in $\mathrm{W}$. Then there is precisely one linear transformation $\mathrm{T}$ from $\mathrm{V}$ into $\mathrm{W}$ such that

$$
\mathrm{T} \alpha_{\mathrm{j}}=\beta_{\mathrm{j}}, \quad \mathrm{j}=1, \ldots, \mathrm{n} .
$$

Proof. To prove there is some linear transformation $T$ with $T \alpha_{j}=$ $\beta_{j}$ we proceed as follows. Given $\alpha$ in $V$, there is a unique $n$-tuple $\left(x_{1}, \ldots, x_{n}\right)$ such that

$$
\alpha=x_{1} \alpha_{1}+\cdots+x_{n} \alpha_{n} .
$$

For this vector $\alpha$ we define

$$
T \alpha=x_{1} \beta_{1}+\cdots+x_{n} \beta_{n} .
$$

Then $T$ is a well-defined rule for associating with each vector $\alpha$ in $V$ a vector $T \alpha$ in $W$. From the definition it is clear that $T \alpha_{j}=\beta_{j}$ for each $j$. To see that $T$ is linear, let

$$
\beta=y_{1} \alpha_{1}+\cdots+y_{n} \alpha_{n}
$$

be in $V$ and let $c$ be any scalar. Now

$$
c \alpha+\beta=\left(c x_{1}+y_{1}\right) \alpha_{1}+\cdots+\left(c x_{n}+y_{n}\right) \alpha_{n}
$$

and so by definition

$$
T(c \alpha+\beta)=\left(c x_{1}+y_{1}\right) \beta_{1}+\cdots+\left(c x_{n}+y_{n}\right) \beta_{n} .
$$

On the other hand,

and thus

$$
\begin{aligned}
c(T \alpha)+T \beta & =c \sum_{i=1}^{n} x_{i} \beta_{i}+\sum_{i=1}^{n} y_{i} \beta_{i} \\
& =\sum_{i=1}^{n}\left(c x_{i}+y_{i}\right) \beta_{i}
\end{aligned}
$$

$$
T(c \alpha+\beta)=c(T \alpha)+T \beta .
$$

If $U$ is a linear transformation from $V$ into $W$ with $U \alpha_{j}=\beta_{j}, j=$ $1, \ldots, n$, then for the vector $\alpha=\sum_{i=1}^{n} x_{i} \alpha_{i}$ we have

$$
\begin{aligned}
U \alpha & =U\left(\sum_{i=1}^{n} x_{i} \alpha_{i}\right) \\
& =\sum_{i=1}^{n} x_{i}\left(U \alpha_{i}\right) \\
& =\sum_{i=1}^{n} x_{i} \beta_{i}
\end{aligned}
$$

so that $U$ is exactly the rule $T$ which we defined above. This shows that the linear transformation $T$ with $T \alpha_{j}=\beta_{j}$ is unique.

Theorem 1 is quite elementary; however, it is so basic that we have stated it formally. The concept of function is very general. If $V$ and $W$ are (non-zero) vector spaces, there is a multitude of functions from $V$ into $W$. Theorem 1 helps to underscore the fact that the functions which are linear are extremely special.

Example 6 . The vectors

$$
\begin{aligned}
& \alpha_{1}=(1,2) \\
& \alpha_{2}=(3,4)
\end{aligned}
$$

are linearly independent and therefore form a basis for $R^{2}$. According to Theorem 1 , there is a unique linear transformation from $R^{2}$ into $R^{3}$ such that

$$
\begin{aligned}
& T \alpha_{1}=(3,2,1) \\
& T \alpha_{2}=(6,5,4) .
\end{aligned}
$$

If so, we must be able to find $T\left(\epsilon_{1}\right)$. We find scalars $c_{1}$, $c_{2}$ such that $\epsilon_{1}=$ $c_{1} \alpha_{1}+c_{2} \alpha_{2}$ and then we know that $T \epsilon_{1}=c_{1} T \alpha_{1}+c_{2} T \alpha_{2}$. If $(1,0)=$ $c_{1}(1,2)+c_{2}(3,4)$ then $c_{1}=-2$ and $c_{2}=1$. Thus

$$
\begin{aligned}
T(1,0) & =-2(3,2,1)+(6,5,4) \\
& =(0,1,2) .
\end{aligned}
$$

EXAmple 7. Let $T$ be a linear transformation from the $m$-tuple space $F^{m}$ into the $n$-tuple space $F^{n}$. Theorem 1 tells us that $T$ is uniquely determined by the sequence of vectors $\beta_{1}, \ldots, \beta_{m}$ where

$$
\beta_{i}=T \epsilon_{i}, \quad i=1, \ldots, m .
$$

In short, $T$ is uniquely determined by the images of the standard basis vectors. The determination is

$$
\begin{aligned}
\alpha & =\left(x_{1}, \ldots, x_{n}\right) \\
T^{\prime} \alpha & =x_{1} \beta_{1}+\cdots+x_{m} \beta_{m} .
\end{aligned}
$$

If $B$ is the $m \times n$ matrix which has row vectors $\beta_{1}, \ldots, \beta_{m}$, this says that

$$
T \alpha=\alpha B \text {. }
$$

In other words, if $\beta_{i}=\left(B_{i 1}, \ldots, B_{i n}\right)$, then

$$
T\left(x_{1}, \ldots, x_{n}\right)=\left[x_{1} \cdots x_{m}\right]\left[\begin{array}{lll}
B_{11} & \cdots & B_{1 n} \\
\vdots & & \vdots \\
B_{m 1} & \cdots & B_{m n}
\end{array}\right] \text {. }
$$

This is a very explicit description of the linear transformation. In Section $3.4$ we shall make a serious study of the relationship between linear trans- formations and matrices. We shall not pursue the particular description $T \alpha=\alpha B$ because it has the matrix $B$ on the right of the vector $\alpha$, and that can lead to some confusion. The point of this example is to show that we can give an explicit and reasonably simple description of all linear transformations from $F^{m}$ into $F^{n}$.

If $T$ is a linear transformation from $V$ into $W$, then the range of $T$ is not only a subset of $W$; it is a subspace of $W$. Let $R_{T}$ be the range of $T$, that is, the set of all vectors $\beta$ in $W$ such that $\beta=T \alpha$ for some $\alpha$ in $V$. Let $\beta_{1}$ and $\boldsymbol{\beta}_{2}$ be in $R_{T}$ and let $c$ be a scalar. There are vectors $\alpha_{1}$ and $\alpha_{2}$ in $V$ such that $T \alpha_{1}=\beta_{1}$ and $T \alpha_{2}=\beta_{2}$. Since $T$ is linear

$$
\begin{aligned}
T\left(c \alpha_{1}+\alpha_{2}\right) & =c T \alpha_{1}+T \alpha_{2} \\
& =c \beta_{1}+\beta_{2},
\end{aligned}
$$

which shows that $c \beta_{1}+\beta_{2}$ is also in $R_{T}$.

Another interesting subspace associated with the linear transformation $T$ is the set $N$ consisting of the vectors $\alpha$ in $V$ such that $T \alpha=0$. It is a subspace of $V$ because

(a) $T(0)=0$, so that $N$ is non-empty;

(b) if $T \alpha_{1}=T \alpha_{2}=0$, then

$$
\begin{aligned}
T\left(c \alpha_{1}+\alpha_{2}\right) & =c T \alpha_{1}+T \alpha_{2} \\
& =c 0+0 \\
& =0
\end{aligned}
$$

so that $c \alpha_{1}+\alpha_{2}$ is in $N$.

Definition. Let $\mathrm{V}$ and $\mathrm{W}$ be vector spaces over the field $\mathrm{F}$ and let $\mathrm{T}$ be a linear transformation from $\mathrm{V}$ into $\mathrm{W}$. The null space of $\mathrm{T}$ is the set of all vectors $\alpha$ in $\mathrm{V}$ such that $\mathrm{T} \alpha=0$.

If $\mathrm{V}$ is finite-dimensional, the rank of $\mathrm{T}$ is the dimension of the range of $\mathrm{T}$ and the nullity of $\mathrm{T}$ is the dimension of the null space of $\mathrm{T}$.

The following is one of the most important results in linear algebra.

Theorem 2. Let $\mathrm{V}$ and $\mathrm{W}$ be vector spaces over the field $\mathrm{F}$ and let $\mathrm{T}$ be a linear transformation from $\mathrm{V}$ into $\mathrm{W}$. Suppose that $\mathrm{V}$ is finite-dimensional. Then

$$
\operatorname{rank}(\mathrm{T})+\text { nullity }(\mathrm{T})=\operatorname{dim} \mathrm{V} .
$$

Proof. Let $\left\{\alpha_{1}, \ldots, \alpha_{k}\right\}$ be a basis for $N$, the null space of $T$. There are vectors $\alpha_{k+1}, \ldots, \alpha_{n}$ in $V$ such that $\left\{\alpha_{1}, \ldots, \alpha_{n}\right\}$ is a basis for $V$. We shall now prove that $\left\{T \alpha_{k+1}, \ldots, T \alpha_{n}\right\}$ is a basis for the range of $T$. The vectors $T \alpha_{1}, \ldots, T \alpha_{n}$ certainly span the range of $T$, and since $T \alpha_{j}=0$, for $j \leq k$, we see that $T \alpha_{k+1}, \ldots, T \alpha_{n}$ span the range. To see that these vectors are independent, suppose we have scalars $c_{i}$ such that

$$
\sum_{i=k+1}^{n} c_{i}\left(T \alpha_{i}\right)=0 .
$$

This says that

$$
T\left(\sum_{i=k+1}^{n} c_{i} \alpha_{i}\right)=0
$$

and accordingly the vector $\alpha=\sum_{i=k+1}^{n} c_{i} \alpha_{i}$ is in the null space of $T$. Since $\alpha_{1}, \ldots, \alpha_{k}$ form a basis for $N$, there must be scalars $b_{1}, \ldots, b_{k}$ such that

$$
\alpha=\sum_{i=1}^{k} b_{i} \alpha_{i} \text {. }
$$

Thus

$$
\sum_{i=1}^{k} b_{i} \alpha_{i}-\sum_{j=k+1}^{n} c_{j} \alpha_{j}=0
$$

and since $\alpha_{1}, \ldots, \alpha_{n}$ are linearly independent we must have

$$
b_{1}=\cdots=b_{k}=c_{k+1}=\cdots=c_{n}=0 .
$$

If $r$ is the rank of $T$, the fact that $T \alpha_{k+1}, \ldots, T \alpha_{n}$ form a basis for the range of $T$ tells us that $r=n-k$. Since $k$ is the nullity of $T$ and $n$ is the dimension of $V$, we are done.

Theorem 3. If $\mathrm{A}$ is an $\mathrm{m} \times \mathrm{n}$ matrix with entries in the field $\mathrm{F}$, then row $\operatorname{rank}(\mathrm{A})=$ column $\operatorname{rank}(\mathrm{A})$.

Proof. Let $T$ be the linear transformation from $F^{n \times 1}$ into $F^{m \times 1}$ defined by $T(X)=A X$. The null space of $T$ is the solution space for the system $A X=0$, i.e., the set of all column matrices $X$ such that $A X=0$. The range of $T$ is the set of all $m \times 1$ column matrices $Y$ such that $A X=$ $Y$ has a solution for $X$. If $A_{1}, \ldots, A_{n}$ are the columns of $A$, then

$$
A X=x_{1} A_{1}+\cdots+x_{n} A_{n}
$$

so that the range of $T$ is the subspace spanned by the columns of $A$. In other words, the range of $T$ is the column space of $A$. Therefore,

$$
\operatorname{rank}(T)=\text { column } \operatorname{rank}(A) .
$$

Theorem 2 tells us that if $S$ is the solution space for the system $A X=0$, then

$$
\operatorname{dim} S+\text { column } \operatorname{rank}(A)=n .
$$

We now refer to Example 15 of Chapter 2. Our deliberations there showed that, if $r$ is the dimension of the row space of $A$, then the solution space $S$ has a basis consisting of $n-r$ vectors:

$$
\operatorname{dim} S=n-\operatorname{row} \operatorname{rank}(A) .
$$

It is now apparent that

$$
\text { row } \operatorname{rank}(A)=\text { column } \operatorname{rank}(A) \text {. }
$$

The proof of Theorem 3 which we have just given depends upon explicit calculations concerning systems of linear equations. There is a more conceptual proof which does not rely on such calculations. We shall give such a proof in Section 3.7.

\section{Exercises}

1. Which of the following functions $T$ from $R^{2}$ into $R^{2}$ are linear transformations?
(a) $T\left(x_{1}, x_{2}\right)=\left(1+x_{1}, x_{2}\right)$;
(b) $T\left(x_{1}, x_{2}\right)=\left(x_{2}, x_{1}\right)$;
(c) $T\left(x_{1}, x_{2}\right)=\left(x_{1}^{2}, x_{2}\right)$;
(d) $T\left(x_{1}, x_{2}\right)=\left(\sin x_{1}, x_{2}\right)$;
(e) $T\left(x_{1}, x_{2}\right)=\left(x_{1}-x_{2}, 0\right)$.

2. Find the range, rank, null space, and nullity for the zero transformation and the identity transformation on a finite-dimensional space $V$.

3. Describe the range and the null space for the differentiation transformation of Example 2. Do the same for the integration transformation of Example 5.

4. Is there a linear transformation $T$ from $R^{3}$ into $R^{2}$ such that $T(1,-1,1)=$ $(1,0)$ and $T(1,1,1)=(0,1)$ ?

5. If

$$
\begin{array}{ll}
\alpha_{1}=(1,-1), & \beta_{1}=(1,0) \\
\alpha_{2}=(2,-1), & \beta_{2}=(0,1) \\
\alpha_{3}=(-3,2), & \beta_{3}=(1,1)
\end{array}
$$

is there a linear transformation $T$ from $R^{2}$ into $R^{2}$ such that $T \alpha_{i}=\beta_{i}$ for $i=1,2$ and 3 ?

6. Describe explicitly (as in Exercises 1 and 2) the linear transformation $T$ from $F^{2}$ into $F^{2}$ such that $T \epsilon_{1}=(a, b), T \epsilon_{2}=(c, d)$.

7. Let $F$ be a subfield of the complex numbers and let $T$ be the function from $F^{3}$ into $F^{3}$ defined by

$$
T\left(x_{1}, x_{2}, x_{\S}\right)=\left(x_{1}-x_{2}+2 x_{3}, 2 x_{1}+x_{2},-x_{1}-2 x_{2}+2 x_{\S}\right) .
$$

(a) Verify that $T$ is a linear transformation.

(b) If $(a, b, c)$ is a vector in $F^{3}$, what are the conditions on $a, b$, and $c$ that the vector be in the range of $T$ ? What is the rank of $T$ ?

(c) What are the conditions on $a, b$, and $c$ that $(a, b, c)$ be in the null space of $T$ ? What is the nullity of $T$ ?

8. Describe explicitly a linear transformation from $R^{3}$ into $R^{3}$ which has as its range the subspace spanned by $(1,0,-1)$ and $(1,2,2)$.

9. Let $V$ be the vector space of all $n \times n$ matrices over the field $F$, and let $B$ be a fixed $n \times n$ matrix. If

$$
T(A)=A B-B A
$$

verify that $T$ is a linear transformation from $V$ into $V$.

10. Let $V$ be the set of all complex numbers regarded as a vector space over the field of real numbers (usual operations). Find a function from $V$ into $V$ which is a linear transformation on the above vector space, but which is not a linear transformation on $C^{1}$, i.e., which is not complex linear.

11. Let $V$ be the space of $n \times 1$ matrices over $F$ and let $W$ be the space of $m \times 1$ matrices over $F$. Let $A$ be a fixed $m \times n$ matrix over $F$ and let $T$ be the linear transformation from $V$ into $W$ defined by $T(X)=A X$. Prove that $T$ is the zero transformation if and only if $A$ is the zero matrix.

12. Let $V$ be an $n$-dimensional vector space over the field $F$ and let $T$ be a linear transformation from $V$ into $V$ such that the range and null space of $T$ are identical. Prove that $n$ is even. (Can you give an example of such a linear transformation T??)

13. Let $V$ be a vector space and $T$ a linear transformation from $V$ into $V$. Prove that the following two statements about $T$ are equivalent.

(a) The intersection of the range of $T$ and the null space of $T$ is the zero subspace of $V$.

(b) If $T(T \alpha)=0$, then $T \alpha=0$

\subsection{The Algebra of Linear Transformations}

In the study of linear transformations from $V$ into $W$, it is of fundamental importance that the set of these transformations inherits a natural vector space structure. The set of linear transformations from a space $V$ into itself has even more algebraic structure, because ordinary composition of functions provides a 'multiplication' of such transformations. We shall explore these ideas in this section.

Theorem 4. Let $\mathrm{V}$ and $\mathrm{W}$ be vector spaces over the field $\mathrm{F}$. Let $\mathrm{T}$ and $\mathrm{U}$ be linear transformations from $\mathrm{V}$ into $\mathrm{W}$. The function $(\mathrm{T}+\mathrm{U})$ defined by

$$
(\mathrm{T}+\mathrm{U})(\alpha)=\mathrm{T} \alpha+\mathrm{U} \alpha
$$

is a linear transformation from $\mathrm{V}$ into $\mathrm{W}$. If $\mathrm{c}$ is any element of $\mathrm{F}$, the function (cT) defined by

$$
(\mathrm{cT})(\alpha)=\mathrm{c}(\mathrm{T} \alpha)
$$

is a linear transformation from $\mathrm{V}$ into $\mathrm{W}$. The set of all linear transformations from $\mathrm{V}$ into $\mathrm{W}$, together with the addition and scalar multiplication defined above, is a vector space over the field $\mathrm{F}$.

Proof. Suppose $T$ and $U$ are linear transformations from $V$ into $W$ and that we define $(T+U)$ as above. Then

$$
\begin{aligned}
(T+U)(c \alpha+\beta) & =T(c \alpha+\beta)+U(c \alpha+\beta) \\
& =c(T \alpha)+T \beta+c(U \alpha)+U \beta \\
& =c(T \alpha+U \alpha)+(T \beta+U \beta) \\
& =c(T+U)(\alpha)+(T+U)(\beta)
\end{aligned}
$$

which shows that $(T+U)$ is a linear transformation. Similarly, 

$$
\begin{aligned}
(c T)(d \alpha+\beta) & =c[T(d \alpha+\beta)] \\
& =c[d(T \alpha)+T \beta] \\
& =c d(T \alpha)+c(T \beta) \\
& =d[c(T \alpha)]+c(T \beta) \\
& =d[(c T) \alpha]+(c T) \beta
\end{aligned}
$$

which shows that $(c T)$ is a linear transformation.

To verify that the set of linear transformations of $V$ into $W$ (together with these operations) is a vector space, one must directly check each of the conditions on the vector addition and scalar multiplication. We leave the bulk of this to the reader, and content ourselves with this comment: The zero vector in this space will be the zero transformation, which sends every vector of $V$ into the zero vector in $W$; each of the properties of the two operations follows from the corresponding property of the operations in the space $W$.

We should perhaps mention another way of looking at this theorem. If one defines sum and scalar multiple as we did above, then the set of all functions from $V$ into $W$ becomes a vector space over the field $F$. This has nothing to do with the fact that $V$ is a vector space, only that $V$ is a non-empty set. When $V$ is a vector space we can define a linear transformation from $V$ into $W$, and Theorem 4 says that the linear transformations are a subspace of the space of all functions from $V$ into $W$.

We shall denote the space of linear transformations from $V$ into $W$ by $L(V, W)$. We remind the reader that $L(V, W)$ is defined only when $V$ and $W$ are vector spaces over the same field.

Theorem 5. Let $\mathrm{V}$ be an $\mathrm{n}$-dimensional vector space over the field $\mathrm{F}$, and let $\mathrm{W}$ be an $\mathrm{m}$-dimensional vector space over $\mathrm{F}$. Then the space $\mathrm{L}(\mathrm{V}, \mathrm{W})$ is finite-dimensional and has dimension $\mathrm{mn}$.

Proof. Let

$$
B=\left\{\alpha_{1}, \ldots, \alpha_{n}\right\} \quad \text { and } \quad \mathbb{B}^{\prime}=\left\{\beta_{1}, \ldots, \beta_{m}\right\}
$$

be ordered bases for $V$ and $W$, respectively. For each pair of integers $(p, q)$ with $1 \leq p \leq m$ and $1 \leq q \leq n$, we define a linear transformation $E^{p, q}$ from $V$ into $W$ by

$$
\begin{aligned}
E^{p, q}\left(\alpha_{i}\right) & =\left\{\begin{array}{lll}
0, & \text { if } & i \neq q \\
\beta_{p}, & \text { if } & i=q
\end{array}\right. \\
& =\delta_{i q} \beta_{p} .
\end{aligned}
$$

According to Theorem 1, there is a unique linear transformation from $V$ into $W$ satisfying these conditions. The claim is that the $m n$ transformations $E^{p, q}$ form a basis for $L(V, W)$.

Let $T$ be a linear transformation from $V$ into $W$. For each $j, 1 \leq j \leq n$, let $A_{i j}, \ldots, A_{m j}$ be the coordinates of the vector $T \alpha_{j}$ in the ordered basis $\mathcal{B}^{\prime}$, i.e.,

We wish to show that

$$
T \alpha_{j}=\sum_{p=1}^{m} A_{p j} \beta_{p}
$$

$$
T=\sum_{p=1}^{m} \sum_{q=1}^{n} A_{p q} E^{p, q} .
$$

Let $U$ be the linear transformation in the right-hand member of (3-2). Then for each $j$

$$
\begin{aligned}
U \alpha_{j} & =\sum_{p} \sum_{q} A_{p q} E^{p, q}\left(\alpha_{j}\right) \\
& =\sum_{p} \sum_{q} A_{p q} \delta_{j q} \beta_{p} \\
& =\sum_{p=1}^{m} A_{p j} \beta_{p} \\
& =T \alpha_{j}
\end{aligned}
$$

and consequently $U=T$. Now (3-2) shows that the $E^{p, q} \operatorname{span} L(V, W)$; we must prove that they are independent. But this is clear from what we did above; for, if the transformation

$$
U=\sum_{p} \sum_{q} A_{p q} E^{p, q}
$$

is the zero transformation, then $U \alpha_{j}=0$ for each $j$, so

$$
\sum_{p=1}^{m} A_{p j} \beta_{p}=0
$$

and the independence of the $\beta_{p}$ implies that $A_{p j}=0$ for every $p$ and $j$.

Theorem 6. Let $\mathrm{V}, \mathrm{W}$, and $\mathrm{Z}$ be vector spaces over the field $\mathrm{F}$. Let $\mathrm{T}$ be a linear transformation from $\mathrm{V}$ into $\mathrm{W}$ and $\mathrm{U}$ a linear transformation from W into Z. Then the composed function UT defined by (UT) $(\alpha)=$ $\mathrm{U}(\mathrm{T}(\alpha))$ is a linear transformation from $\mathrm{V}$ into $\mathrm{Z}$.

\section{Proof.}

$$
\begin{aligned}
(U T)(c \alpha+\beta) & =U[T(c \alpha+\beta)] \\
& =U(c T \alpha+T \beta) \\
& =c[U(T \alpha)]+U(T \beta) \\
& =c(U T)(\alpha)+(U T)(\beta) .
\end{aligned}
$$

In what follows, we shall be primarily concerned with linear transformation of a vector space into itself. Since we would so often have to write ' $T$ is a linear transformation from $V$ into $V$,' we shall replace this with ' $T$ is a linear operator on $V$ '

Definition. If $\mathrm{V}$ is a vector space over the field $\mathrm{F}$, a linear operator on $\mathbf{V}$ is a linear transformation from $\mathbf{V}$ into $\mathbf{V}$. In the case of Theorem 6 when $V=W=Z$, so that $U$ and $T$ are linear operators on the space $V$, we see that the composition $U T$ is again a linear operator on $V$. Thus the space $L(V, V)$ has a 'multiplication' defined on it by composition. In this case the operator $T U$ is also defined, and one should note that in general $U T \neq T U$, i.e., $U T-T U \neq 0$. We should take special note of the fact that if $T$ is a linear operator on $V$ then we can compose $T$ with $T$. We shall use the notation $T^{2}=T T$, and in general $T^{n}=T \cdots T$ ( $n$ times) for $n=1,2,3, \ldots$. We define $T^{0}=I$ if $T \neq 0$.

Lemma. Let $\mathrm{V}$ be a vector space over the field $\mathrm{F}$; let $\mathrm{U}, \mathrm{T}_{1}$ and $\mathrm{T}_{2}$ be linear operators on $\mathrm{V}$; let $\mathrm{c}$ be an element of $\mathrm{F}$.

(a) $\mathrm{IU}=\mathrm{UI}=\mathrm{U}$;

(b) $\mathrm{U}\left(\mathrm{T}_{1}+\mathrm{T}_{2}\right)=\mathrm{UT}_{1}+\mathrm{UT}_{2} ;\left(\mathrm{T}_{1}+\mathrm{T}_{2}\right) \mathrm{U}=\mathrm{T}_{1} \mathrm{U}+\mathrm{T}_{2} \mathrm{U}$;

(c) $\mathrm{c}\left(\mathrm{UT}_{1}\right)=(\mathrm{cU}) \mathrm{T}_{1}=\mathrm{U}\left(\mathrm{cT} \mathrm{T}_{1}\right)$.

Proof. (a) This property of the identity function is obvious. We have stated it here merely for emphasis.

(b)

$$
\begin{aligned}
{\left[U\left(T_{1}+T_{2}\right)\right](\alpha) } & =U\left[\left(T_{1}+T_{2}\right)(\alpha)\right] \\
& =U\left(T_{1} \alpha+T_{2} \alpha\right) \\
& =U\left(T_{1} \alpha\right)+U\left(T_{2} \alpha\right) \\
& =\left(U T_{1}\right)(\alpha)+\left(U T_{2}\right)(\alpha)
\end{aligned}
$$

so that $U\left(T_{1}+T_{2}\right)=U T_{1}+U T_{2}$. Also

$$
\begin{aligned}
{\left[\left(T_{1}+T_{2}\right) U\right](\alpha) } & =\left(T_{1}+T_{2}\right)(U \alpha) \\
& =T_{1}(U \alpha)+T_{2}(U \alpha) \\
& =\left(T_{1} U\right)(\alpha)+\left(T_{2} U\right)(\alpha)
\end{aligned}
$$

so that $\left(T_{1}+T_{2}\right) U=T_{1} U+T_{2} U$. (The reader may note that the proofs of these two distributive laws do not use the fact that $T_{1}$ and $T_{2}$ are linear, and the proof of the second one does not use the fact that $U$ is linear either.)

(c) We leave the proof of part (c) to the reader.

The contents of this lemma and a portion of Theorem 5 tell us that the vector space $L(V, V)$, together with the composition operation, is what is known as a linear algebra with identity. We shall discuss this in Chapter 4.

Example 8. If $A$ is an $m \times n$ matrix with entries in $F$, we have the linear transformation $T$ defined by $T(X)=A X$, from $F_{n \times 1}$ into $F^{m \times 1}$. If $B$ is a $p \times m$ matrix, we have the linear transformation $U$ from $F^{m \times 1}$ into $F^{\text {p }}$ 1 defined by $U(Y)=B Y$. The composition $U T$ is easily described:

$$
\begin{aligned}
(U T)(X) & =U(T(X)) \\
& =U(A X) \\
& =B(A X) \\
& =(B A) X .
\end{aligned}
$$

Thus $U T$ is 'left multiplication by the product matrix $B A$.' Example 9 . Let $F$ be a field and $V$ the vector space of all polynomial functions from $F$ into $F$. Let $D$ be the differentiation operator defined in Example 2 , and let $T$ be the linear operator 'multiplication by $x$ ':

$$
(T f)(x)=x f(x) .
$$

Then $D T \neq T D$. In fact, the reader should find it easy to verify that $D T-T D=I$, the identity operator.

Even though the 'multiplication' we have on $L(V, V)$ is not commutative, it is nicely related to the vector space operations of $L(V, V)$.

Example 10. Let $B=\left\{\alpha_{1}, \ldots, \alpha_{n}\right\}$ be an ordered basis for a vector space $V$. Consider the linear operators $E^{p, q}$ which arose in the proof of Theorem 5 :

$$
E^{p, q}\left(\alpha_{i}\right)=\delta_{i q} \alpha_{p} .
$$

These $n^{2}$ linear operators form a basis for the space of linear operators on $V$. What is $E^{p, q} E^{r, s}$ ? We have

$$
\begin{aligned}
\left(E^{p, q} E^{r, s}\right)\left(\alpha_{i}\right) & =E^{p, q}\left(\delta_{i s} \alpha_{r}\right) \\
& =\delta_{i s} E^{p, q}\left(\alpha_{r}\right) \\
& =\delta_{i s} \delta_{r q} \alpha_{p} .
\end{aligned}
$$

Therefore,

$$
E^{p, q} E^{r, s}=\left\{\begin{array}{l}
0, \quad \text { if } \quad r \neq q \\
E^{p, s}, \quad \text { if } q=r .
\end{array}\right.
$$

Let $T$ be a linear operator on $V$. We showed in the proof of Theorem 5 that if

$$
\begin{aligned}
A_{j} & =\left[T \alpha_{j}\right]_{ß} \\
A & =\left[A_{1}, \ldots, A_{n}\right]
\end{aligned}
$$

then

If

$$
T=\sum_{p} \sum_{q} A_{p q} E^{p, q} .
$$

$$
U=\sum_{r} \sum_{s} B_{r s} E^{r, s}
$$

is another linear operator on $V$, then the last lemma tells us that

$$
\begin{aligned}
& T U=\left(\sum_{p} \sum_{q} A_{p q} E^{p, q}\right)\left(\underset{r}{\Sigma} \sum_{s} B_{r s} E^{r, s}\right) \\
& =\sum_{p} \sum_{q} \sum_{r} \sum_{s} A_{p q} B_{r s} E^{p, q} E^{r, s} .
\end{aligned}
$$

As we have noted, the only terms which survive in this huge sum are the terms where $q=r$, and since $E^{p, r} E^{r, s}=E^{p, s}$, we have

$$
\begin{aligned}
T U & =\sum_{p} \sum_{s}\left(\underset{r}{\sum} A_{p r} B_{r s}\right) E^{p, s} \\
& =\sum_{p} \sum_{8}(A B)_{p 8} E^{p, s} .
\end{aligned}
$$

Thus, the effect of composing $T$ and $U$ is to multiply the matrices $A$ and $B$. In our discussion of algebraic operations with linear transformations we have not yet said anything about invertibility. One specific question of interest is this. For which linear operators $T$ on the space $V$ does there exist a linear operator $T^{-1}$ such that $T T^{-1}=T^{-1} T=I$ ?

The function $T$ from $V$ into $W$ is called invertible if there exists a function $U$ from $W$ into $V$ such that $U T$ is the identity function on $V$ and $T U$ is the identity function on $W$. If $T$ is invertible, the function $U$ is unique and is denoted by $T^{-1}$. (See Appendix.) Furthermore, $T$ is invertible if and only if

1. $T$ is $1: 1$, that is, $T \alpha=T \beta$ implies $\alpha=\beta$;

2. $T$ is onto, that is, the range of $T$ is (all of) $W$.

Theorem 7. Let $\mathrm{V}$ and $\mathrm{W}$ be vector spaces over the field $\mathrm{F}$ and let $\mathrm{T}$ be a linear transformation from $\mathrm{V}$ into $\mathrm{W}$. If $\mathrm{T}$ is invertible, then the inverse function $\mathrm{T}^{-1}$ is a linear transformation from $\mathrm{W}$ onto $\mathrm{V}$.

Proof. We repeat ourselves in order to underscore a point. When $T$ is one-one and onto, there is a uniquely determined inverse function $T^{-1}$ which maps $W$ onto $V$ such that $T^{-1} T$ is the identity function on $V$, and $T T^{-1}$ is the identity function on $W$. What we are proving here is that if a linear function $T$ is invertible, then the inverse $T^{-1}$ is also linear.

Let $\beta_{1}$ and $\beta_{2}$ be vectors in $W$ and let $c$ be a scalar. We wish to show that

$$
T^{-1}\left(c \beta_{1}+\beta_{2}\right)=c T^{-1} \beta_{1}+T^{-1} \beta_{2} .
$$

Let $\alpha_{i}=T^{-1} \beta_{i}, i=1,2$, that is, let $\alpha_{i}$ be the unique vector in $V$ such that $T \alpha_{i}=\beta_{i}$. Since $T$ is linear,

$$
\begin{aligned}
T\left(c \alpha_{1}+\alpha_{2}\right) & =c T \alpha_{1}+T \alpha_{2} \\
& =c \beta_{1}+\beta_{2} .
\end{aligned}
$$

Thus $c \alpha_{1}+\alpha_{2}$ is the unique vector in $V$ which is sent by $T$ into $c \beta_{1}+\beta_{2}$, and so

$$
\begin{aligned}
T^{-1}\left(c \beta_{1}+\beta_{2}\right) & =c \alpha_{1}+\alpha_{2} \\
& =c\left(T^{-1} \beta_{1}\right)+T^{-1} \beta_{2}
\end{aligned}
$$

and $T^{-1}$ is linear.

Suppose that we have an invertible linear transformation $T$ from $V$ onto $W$ and an invertible linear transformation $U$ from $W$ onto $Z$. Then $U T$ is invertible and $(U T)^{-1}=T^{-1} U^{-1}$. That conclusion does not require the linearity nor does it involve checking separately that $U T$ is $1: 1$ and onto. All it involves is verifying that $T^{-1} U^{-1}$ is both a left and a right inverse for $U T$.

If $T$ is linear, then $T(\alpha-\beta)=T \alpha-T \beta$; hence, $T \alpha=T \beta$ if and only if $T(\alpha-\beta)=0$. This simplifies enormously the verification that $T$ is $1: 1$. Let us call a linear transformation $T$ non-singular if $T \gamma=0$ implies $\gamma=0$, i.e., if the null space of $T$ is $\{0\}$. Evidently, $T$ is $1: 1$ if and only if $T$ is non-singular. The extension of this remark is that non-singular linear transformations are those which preserve linear independence.

Theorem 8. Let $\mathrm{T}$ be a linear transformation from $\mathrm{V}$ into $\mathrm{W}$. Then $\mathrm{T}$ is non-singular if and only if $\mathrm{T}$ carries each linearly independent subset of $\mathrm{V}$ onto a linearly independent subset of $\mathrm{W}$.

Proof. First suppose that $T$ is non-singular. Let $S$ be a linearly independent subset of $V$. If $\alpha_{1}, \ldots, \alpha_{k}$ are vectors in $S$, then the vectors $T \alpha_{1}, \ldots, T \alpha_{k}$ are linearly independent; for if

$$
c_{1}\left(T \alpha_{1}\right)+\cdots+c_{k}\left(T \alpha_{k}\right)=0
$$

then

$$
T\left(c_{1} \alpha_{1}+\cdots+c_{k} \alpha_{k}\right)=0
$$

and since $T$ is non-singular

$$
c_{1} \alpha_{1}+\cdots+c_{k} \alpha_{k}=0
$$

from which it follows that each $c_{i}=0$ because $S$ is an independent set. This argument shows that the image of $S$ under $T$ is independent.

Suppose that $T$ carries independent subsets onto independent subsets. Let $\alpha$ be a non-zero vector in $V$. Then the set $S$ consisting of the one vector $\alpha$ is independent. The image of $S$ is the set consisting of the one vector $T \alpha$, and this set is independent. Therefore $T \alpha \neq 0$, because the set consisting of the zero vector alone is dependent. This shows that the null space of $T$ is the zero subspace, i.e., $T$ is non-singular.

Example 11. Let $F$ be a subfield of the complex numbers (or a field of characteristic zero) and let $V$ be the space of polynomial functions over $F$. Consider the differentiation operator $D$ and the 'multiplication by $x$ ' operator $T$, from Example 9. Since $D$ sends all constants into $0, D$ is singular; however, $V$ is not finite dimensional, the range of $D$ is all of $V$, and it is possible to define a right inverse for $D$. For example, if $E$ is the indefinite integral operator:

$$
E\left(c_{0}+c_{1} x+\cdots+c_{n} x^{n}\right)=c_{0} x+\frac{1}{2} c_{1} x^{2}+\cdots+\frac{1}{n+1} c_{n} x^{n+1}
$$

then $E$ is a linear operator on $V$ and $D E=I$. On the other hand, $E D \neq I$ because $E D$ sends the constants into 0 . The operator $T$ is in what we might call the reverse situation. If $x f(x)=0$ for all $x$, then $f=0$. Thus $T$ is nonsingular and it is possible to find a left inverse for $T$. For example if $U$ is the operation 'remove the constant term and divide by $x$ ':

$$
U\left(c_{0}+c_{1} x+\cdots+c_{n} x^{n}\right)=c_{1}+c_{2} x+\cdots+c_{n} x^{n-1}
$$

then $U$ is a linear operator on $V$ and $U T=I$. But $T U \neq I$ since every function in the range of $T U$ is in the range of $T$, which is the space of polynomial functions $f$ such that $f(0)=0$.

Example 12. Let $F$ be a field and let $T$ be the linear operator on $F^{2}$ defined by

$$
T\left(x_{1}, x_{2}\right)=\left(x_{1}+x_{2}, x_{1}\right) .
$$

Then $T$ is non-singular, because if $T\left(x_{1}, x_{2}\right)=0$ we have

$$
\begin{aligned}
x_{1}+x_{2} & =0 \\
x_{1} & =0
\end{aligned}
$$

so that $x_{1}=x_{2}=0$. We also see that $T$ is onto; for, let $\left(z_{1}, z_{2}\right)$ be any vector in $F^{2}$. To show that $\left(z_{1}, z_{2}\right)$ is in the range of $T$ we must find scalars $x_{1}$ and $x_{2}$ such that

$$
\begin{aligned}
x_{1}+x_{2} & =z_{1} \\
x_{1} & =z_{2}
\end{aligned}
$$

and the obvious solution is $x_{1}=z_{2}, x_{2}=z_{1}-z_{2}$. This last computation gives us an explicit formula for $T^{-1}$, namely,

$$
T^{-1}\left(z_{1}, z_{2}\right)=\left(z_{2}, z_{1}-z_{2}\right) .
$$

We have seen in Example 11 that a linear transformation may be non-singular without being onto and may be onto without being nonsingular. The present example illustrates an important case in which that cannot happen.

Theorem 9. Let $\mathrm{V}$ and $\mathrm{W}$ be finite-dimensional vector spaces over the field $\mathrm{F}$ such that $\operatorname{dim} \mathrm{V}=\operatorname{dim} \mathrm{W}$. If $\mathrm{T}$ is a linear transformation from $\mathrm{V}$ into $\mathrm{W}$, the following are equivalent:

(i) $\mathrm{T}$ is invertible.

(ii) $\mathrm{T}$ is non-singular.

(iii) $\mathrm{T}$ is onto, that $i$ s, the range of $\mathrm{T}$ is $\mathrm{W}$.

Proof. Let $n=\operatorname{dim} V=\operatorname{dim} W$. From Theorem 2 we know that

$$
\operatorname{rank}(T)+\operatorname{nullity}(T)=n .
$$

Now $T$ is non-singular if and only if nullity $(T)=0$, and (since $n=\operatorname{dim}$ $W$ ) the range of $T$ is $W$ if and only if rank $(T)=n$. Since the rank plus the nullity is $n$, the nullity is 0 precisely when the rank is $n$. Therefore $T$ is non-singular if and only if $T(V)=W$. So, if either condition (ii) or (iii) holds, the other is satisfied as well and $T$ is invertible.

We caution the reader not to apply Theorem 9 except in the presence of finite-dimensionality and with $\operatorname{dim} V=\operatorname{dim} W$. Under the hypotheses of Theorem 9, the conditions (i), (ii), and (iii) are also equivalent to these.

(iv) If $\left\{\alpha_{1}, \ldots, \alpha_{\mathrm{n}}\right\}$ is basis for $\mathrm{V}$, then $\left\{\mathrm{T} \alpha_{1}, \ldots, \mathrm{T} \alpha_{\mathrm{n}}\right\}$ is a basis for W. (v) There is some basis $\left\{\alpha_{1}, \ldots, \alpha_{\mathrm{n}}\right\}$ for $\mathrm{V}$ such that $\left\{\mathrm{T} \alpha_{1}, \ldots, \mathrm{T} \alpha_{\mathrm{n}}\right\}$ is a basis for W.

We shall give a proof of the equivalence of the five conditions which contains a different proof that (i), (ii), and (iii) are equivalent.

(i) $\rightarrow$ (ii). If $T$ is invertible, $T$ is non-singular. (ii) $\rightarrow$ (iii). Suppose $T$ is non-singular. Let $\left\{\alpha_{1}, \ldots, \alpha_{n}\right\}$ be a basis for $V$. By Theorem 8 , $\left\{T \alpha_{1}, \ldots, T \alpha_{n}\right\}$ is a linearly independent set of vectors in $W$, and since the dimension of $W$ is also $n$, this set of vectors is a basis for $W$. Now let $\beta$ be any vector in $W$. There are scalars $c_{1}, \ldots, c_{n}$ such that

$$
\begin{aligned}
\beta & =c_{1}\left(T \alpha_{1}\right)+\cdots+c_{n}\left(T \alpha_{n}\right) \\
& =T\left(c_{1} \alpha_{1}+\cdots+c_{n} \alpha_{n}\right)
\end{aligned}
$$

which shows that $\beta$ is in the range of $T$. (iii) $\rightarrow$ (iv). We now assume that $T$ is onto. If $\left\{\alpha_{1}, \ldots, \alpha_{n}\right\}$ is any basis for $V$, the vectors $T \alpha_{1}, \ldots, T \alpha_{n}$ span the range of $T$, which is all of $W$ by assumption. Since the dimension of $W$ is $n$, these $n$ vectors must be linearly independent, that is, must comprise a basis for $W$. (iv) $\rightarrow$ (v). This requires no comment. (v) $\rightarrow$ (i). Suppose there is some basis $\left\{\alpha_{1}, \ldots, \alpha_{n}\right\}$ for $V$ such that $\left\{T \alpha_{1}, \ldots, T \alpha_{n}\right\}$ is a basis for $W$. Since the $T \alpha_{i}$ span $W$, it is clear that the range of $T$ is all of $W$. If $\alpha=c_{1} \alpha_{1}+\cdots+c_{n} \alpha_{n}$ is in the null space of $T$, then

$$
T\left(c_{1} \alpha_{1}+\cdots+c_{n} \alpha_{n}\right)=0
$$

or

$$
c_{1}\left(T \alpha_{1}\right)+\cdots+c_{n}\left(T \alpha_{n}\right)=0
$$

and since the $T \alpha_{i}$ are independent each $c_{i}=0$, and thus $\alpha=0$. We have shown that the range of $T$ is $W$, and that $T$ is non-singular, hence $T$ is invertible.

The set of invertible linear operators on a space $V$, with the operation of composition, provides a nice example of what is known in algebra as a 'group.' Although we shall not have time to discuss groups in any detail, we shall at least give the definition.

Definition. A group consists of the following.

1. A set $\mathrm{G}$;

2. A rule (or operation) which associates with each pair of elements $\mathrm{x}$, $\mathrm{y}$ in $\mathrm{G}$ an element $\mathrm{xy}$ in $\mathrm{G}$ in such a way that

(a) $\mathrm{x}(\mathrm{yz})=(\mathrm{xy}) \mathrm{z}$, for all $\mathrm{x}, \mathrm{y}$, and $\mathrm{z}$ in $\mathrm{G}$ (associativity);

(b) there is an element $\mathrm{e}$ in $\mathrm{G}$ such that $\mathrm{ex}=\mathrm{xe}=\mathrm{x}$, for every $\mathrm{x}$ in $\mathrm{G}$;

(c) to each element $\mathrm{x}$ in $\mathrm{G}$ there corresponds an element $\mathrm{x}^{-1}$ in $\mathrm{G}$ such that $\mathrm{xx}^{-1}=\mathrm{x}^{-1} \mathrm{x}=\mathrm{e}$.

We have seen that composition $(U, T) \rightarrow U T$ associates with each pair of invertible linear operators on a space $V$ another invertible operator on $V$. Composition is an associative operation. The identity operator $I$ satisfies $I T=T I$ for each $T$, and for an invertible $T$ there is (by Theorem 7) an invertible linear operator $T^{-1}$ such that $T^{\prime} T^{-1}=T^{-1} T=I$. Thus the set of invertible linear operators on $V$, together with this operation, is a group. The set of invertible $n \times n$ matrices with matrix multiplication as the operation is another example of a group. A group is called commutative if it satisfies the condition $x y=y x$ for each $x$ and $y$. The two examples we gave above are not commutative groups, in general. O ne of ten writes the operation in a commutative group as $(x, y) \rightarrow x+y$, rather than $(x, y) \rightarrow x y$, and then uses the symbol 0 for the 'identity' element $e$. The set of vectors in a vector space, together with the operation of vector addition, is a commutative group. A field can be described as a set with two operations, called addition and multiplication, which is a commutative group under addition, and in which the non-zero elements form a commutative group under multiplication, with the distributive law $x(y+z)=x y+x z$ holding.

\section{Exercises}

1. Let $T$ and $U$ be the linear operators on $R^{2}$ defined by

$$
T\left(x_{1}, x_{2}\right)=\left(x_{2}, x_{1}\right) \text { and } U\left(x_{1}, x_{2}\right)=\left(x_{1}, 0\right) \text {. }
$$

(a) How would you describe $T$ and $U$ geometrically?

(b) Give rules like the ones defining $T$ and $U$ for each of the transformations $(U+T), U T, T U, T^{2}, U^{2}$.

2. Let $T$ be the (unique) linear operator on $C^{3}$ for which

$$
T \epsilon_{1}=(1,0, i), \quad T \epsilon_{2}=(0,1,1), \quad T \epsilon_{3}=(i, 1,0) .
$$

Is $T$ invertible?

3. Let $T$ be the linear operator on $R^{3}$ defined by

$$
T\left(x_{1}, x_{2}, x_{3}\right)=\left(3 x_{1}, x_{1}-x_{2}, 2 x_{1}+x_{2}+x_{3}\right) .
$$

Is $T$ invertible? If so, find a rule for $T^{-1}$ like the one which defines $T$.

4. For the linear operator $T$ of Exercise 3, prove that

$$
\left(T^{2}-I\right)(T-3 I)=0 .
$$

5. Let $C^{2 \times 2}$ be the complex vector space of $2 \times 2$ matrices with complex entries. Let

$$
B=\left[\begin{array}{rr}
1 & -1 \\
-4 & 4
\end{array}\right]
$$

and let $T$ be the linear operator on $C^{2 \times 2}$ defined by $T(A)=B A$. What is the rank of $T$ ? Can you describe $T^{2}$ ?

6. Let $T$ be a linear transformation from $R^{3}$ into $R^{2}$, and let $U$ be a linear transformation from $R^{2}$ into $R^{3}$. Prove that the transformation $U T$ is not invertible. Generalize the theorem. 7. Find two linear operators $T$ and $U$ on $R^{2}$ such that $T U=0$ but $U T \neq 0$.

8. Let $V$ be a vector space over the field $F$ and $T$ a linear operator on $V$. If $T^{2}=0$, what can you say about the relation of the range of $T$ to the null space of $T$ ? Give an example of a linear operator $T$ on $R^{2}$ such that $T^{2}=0$ but $T \neq 0$.

9. Let $T$ be a linear operator on the finite-dimensional space $V$. Suppose there is a linear operator $U$ on $V$ such that $T U=I$. Prove that $T$ is invertible and $U=T^{-1}$. Give an example which shows that this is false when $V$ is not finitedimensional. (Hint: Let $T=D$, the differentiation operator on the space of polynomial functions.)

10. Let $A$ be an $m \times n$ matrix with entries in $F$ and let $T$ be the linear transformation from $F^{n \times 1}$ into $F^{m \times 1}$ defined by $T(X)=A X$. Show that if $m<n$ it may happen that $T$ is onto without being non-singular. Similarly, show that if $m>n$ we may have $T$ non-singular but not onto.

11. Let $V$ be a finite-dimensional vector space and let $T$ be a linear operator on $V$. Suppose that rank $\left(T^{2}\right)=\operatorname{rank}(T)$. Prove that the range and null space of $T$ are disjoint, i.e., have only the zero vector in common.

12. Let $p, m$, and $n$ be positive integers and $F$ a field. Let $V$ be the space of $m \times n$ matrices over $F$ and $W$ the space of $p \times n$ matrices over $F$. Let $B$ be a fixed $p \times m$ matrix and let $T$ be the linear transformation from $V$ into $W$ defined by $T(A)=B A$. Prove that $T$ is invertible if and only if $p=m$ and $B$ is an invertible $m \times m$ matrix.

\subsection{Isomorphism}

If $V$ and $W$ are vector spaces over the field $F$, any one-one linear transformation $T$ of $V$ onto $W$ is called an isomorphism of $V$ onto $W$. If there exists an isomorphism of $V$ onto $W$, we say that $V$ is isomorphic to $W$.

Note that $V$ is trivially isomorphic to $V$, the identity operator being an isomorphism of $V$ onto $V$. Also, if $V$ is isomorphic to $W$ via an isomorphism $T$, then $W$ is isomorphic to $V$, because $T^{-1}$ is an isomorphism of $W$ onto $V$. The reader should find it easy to verify that if $V$ is isomorphic to $W$ and $W$ is isomorphic to $Z$, then $V$ is isomorphic to $Z$. Briefly, isomorphism is an equivalence relation on the class of vector spaces. If there exists an isomorphism of $V$ onto $W$, we may sometimes say that $V$ and $W$ are isomorphic, rather than $V$ is isomorphic to $W$. This will cause no confusion because $V$ is isomorphic to $W$ if and only if $W$ is isomorphic to $V$.

Theorem 10. Every $\mathrm{n}$-dimensional vector space over the field $\mathrm{F}$ is isomorphic to the space $\mathrm{F}^{\mathrm{n}}$.

Proof. Let $V$ be an $n$-dimensional space over the field $F$ and let $B=\left\{\alpha_{1}, \ldots, \alpha_{n}\right\}$ be an ordered basis for $V$. We define a function $T$ from $V$ into $F^{n}$, as follows: If $\alpha$ is in $V$, let $T \alpha$ be the $n$-tuple $\left(x_{1}, \ldots, x_{n}\right)$ of coordinates of $\alpha$ relative to the ordered basis $\boldsymbol{\theta}$, i.e., the $n$-tuple such that

$$
\alpha=x_{1} \alpha_{1}+\cdots+x_{n} \alpha_{n} .
$$

In our discussion of coordinates in Chapter 2, we verified that this $T$ is linear, one-one, and maps $V$ onto $F^{n}$.

For many purposes one of ten regards isomorphic vector spaces as being 'the same,' although the vectors and operations in the spaces may be quite different, that is, one often identifies isomorphic spaces. We shall not attempt a lengthy discussion of this idea at present but shall let the understanding of isomorphism and the sense in which isomorphic spaces are 'the same' grow as we continue our study of vector spaces.

We shall make a few brief comments. Suppose $T$ is an isomorphism of $V$ onto $W$. If $S$ is a subset of $V$, then Theorem 8 tells us that $S$ is linearly independent if and only if the set $T(S)$ in $W$ is independent. Thus in deciding whether $S$ is independent it doesn't matter whether we look at $S$ or $T(S)$. From this one sees that an isomorphism is 'dimension preserving,' that is, any finite-dimensional subspace of $V$ has the same dimension as its image under $T$. Here is a very simple illustration of this idea. Suppose $A$ is an $m \times n$ matrix over the field $F$. We have really given two definitions of the solution space of the matrix $A$. The first is the set of all $n$-tuples $\left(x_{1}, \ldots, x_{n}\right)$ in $F^{n}$ which satisf y each of the equations in the system $A X=$ 0 . The second is the set of all $n \times 1$ column matrices $X$ such that $A X=0$. The first solution space is thus a subspace of $F^{n}$ and the second is a subspace of the space of all $n \times 1$ matrices over $F$. Now there is a completely obvious isomorphism between $F^{n}$ and $F^{n \times 1}$, namely,

$$
\left(x_{1}, \ldots, x_{n}\right) \rightarrow\left[\begin{array}{c}
x_{1} \\
\vdots \\
x_{n}
\end{array}\right] .
$$

Under this isomorphism, the first solution space of $A$ is carried onto the second solution space. These spaces have the same dimension, and so if we want to prove a theorem about the dimension of the solution space, it is immaterial which space we choose to discuss. In fact, the reader would probably not balk if we chose to identify $F^{n}$ and the space of $n \times 1$ matrices. We may do this when it is convenient, and when it is not convenient we shall not.

\section{Exercises}

1. Let $V$ be the set of complex numbers and let $F$ be the field of real numbers. With the usual operations, $V$ is a vector space over $F$. Describe explicitly an isomorphism of this space onto $R^{2}$. 2. Let $V$ be a vector space over the field of complex numbers, and suppose there is an isomorphism $T$ of $V$ onto $C^{3}$. Let $\alpha_{1}, \alpha_{2}, \alpha_{3}, \alpha_{4}$ be vectors in $V$ such that

$$
\begin{aligned}
& T \alpha_{1}=(1,0, i), \quad T \alpha_{2}=(-2,1+i, 0), \\
& T \alpha_{3}=(-1,1,1), \quad T \alpha_{4}=(\sqrt{2}, i, 3) \text {. }
\end{aligned}
$$

(a) Is $\alpha_{1}$ in the subspace spanned by $\alpha_{2}$ and $\alpha_{3}$ ?

(b) Let $W_{1}$ be the subspace spanned by $\alpha_{1}$ and $\alpha_{2}$, and let $W_{2}$ be the subspace spanned by $\alpha_{3}$ and $\alpha_{4}$. What is the intersection of $W_{1}$ and $W_{2}$ ?

(c) Find a basis for the subspace of $V$ spanned by the four vectors $\alpha_{j}$.

3. Let $W$ be the set of all $2 \times 2$ complex Hermitian matrices, that is, the set of $2 \times 2$ complex matrices $A$ such that $A_{i j}=\overline{A_{j i}}$ (the bar denoting complex conjugation). As we pointed out in Example 6 of Chapter 2, $W$ is a vector space over the field of real numbers, under the usual operations. Verify that

$$
(x, y, z, t) \rightarrow\left[\begin{array}{ll}
t+x & y+i z \\
y-i z & t-x
\end{array}\right]
$$

is an isomorphism of $R^{4}$ onto $W$.

4. Show that $F^{m \times n}$ is isomorphic to $F^{m n}$.

5. Let $V$ be the set of complex numbers regarded as a vector space over the field of real numbers (Exercise 1). We define a function $T$ from $V$ into the space of $2 \times 2$ real matrices, as follows. If $z=x+i y$ with $x$ and $y$ real numbers, then

$$
T(z)=\left[\begin{array}{cc}
x+7 y & 5 y \\
-10 y & x-7 y
\end{array}\right] .
$$

(a) Verify that $T$ is a one-one (real) linear transformation of $V$ into the space of $2 \times 2$ real matrices.

(b) Verify that $T\left(z_{1} z_{2}\right)=T\left(z_{1}\right) T\left(z_{2}\right)$.

(c) How would you describe the range of $T$ ?

6. Let $V$ and $W$ be finite-dimensional vector spaces over the field $F$. Prove that $V$ and $W$ are isomorphic if and only if $\operatorname{dim} V=\operatorname{dim} W$.

7. Let $V$ and $W$ be vector spaces over the field $F$ and let $U$ be an isomorphism of $V$ onto $W$. Prove that $T \rightarrow U T U^{-1}$ is an isomorphism of $L(V, V)$ onto $L(W, W)$.

\subsection{Representation of Transformations by Matrices}

Let $V$ be an $n$-dimensional vector space over the field $F$ and let $W$ be an $m$-dimensional vector space over $F$. Let $B=\left\{\alpha_{1}, \ldots, \alpha_{n}\right\}$ be an ordered basis for $V$ and $B^{\prime}=\left\{\beta_{1}, \ldots, \beta_{m}\right\}$ an ordered basis for $W$. If $T$ is any linear transformation from $V$ into $W$, then $T$ is determined by its action on the vectors $\alpha_{j}$. Each of the $n$ vectors $T \alpha_{j}$ is uniquely expressible as a linear combination

$$
T \alpha_{j}=\sum_{i=1}^{m} A_{i j} \beta_{i}
$$

of the $\beta_{i}$, the scalars $A_{1 j}, \ldots, A_{m j}$ being the coordinates of $T \alpha_{j}$ in the ordered basis $\mathbb{B}^{\prime}$. Accordingly, the transformation $T$ is determined by the $m n$ scalars $A_{i j}$ via the formulas (3-3). The $m \times n$ matrix $A$ defined by $A(i, j)=A_{i j}$ is called the matrix of $T$ relative to the pair of ordered bases $B$ and $B^{\prime}$. Our immediate task is to understand explicitly how the matrix $A$ determines the linear transformation $T$.

$$
\text { If } \begin{aligned}
\alpha=x_{1} \alpha_{1}+\cdots+x_{n} \alpha_{n} & \text { is a vector in } V \text {, then } \\
T^{\prime} \alpha & =T\left(\sum_{j=1}^{n} x_{j} \alpha_{j}\right) \\
& =\sum_{j=1}^{n} x_{j}\left(T \alpha_{j}\right) \\
& =\sum_{j=1}^{n} x_{j} \sum_{i=1}^{m} A_{i j} \beta_{i} \\
& =\sum_{i=1}^{m}\left(\sum_{j=1}^{n} A_{i j} x_{j}\right) \beta_{i} .
\end{aligned}
$$

If $X$ is the coordinate matrix of $\alpha$ in the ordered basis $B$, then the computation above shows that $A X$ is the coordinate matrix of the vector $T \alpha$ in the ordered basis $\mathbf{B}^{\prime}$, because the scalar

$$
\sum_{j=1}^{n} A_{i j} x_{j}
$$

is the entry in the $i$ th row of the column matrix $A X$. Let us also observe that if $A$ is any $m \times n$ matrix over the field $F$, then

$$
T\left(\sum_{j=1}^{n} x_{j} \alpha_{j}\right)=\sum_{i=1}^{n}\left(\sum_{j=1}^{n} A_{i j} x_{j}\right) \beta_{i}
$$

defines a linear transformation $T$ from $V$ into $W$, the matrix of which is $A$, relative to $B, \beta^{\prime}$. We summarize formally:

Theorem 11. Let $\mathrm{V}$ be an $\mathrm{n}$-dimensional vector space over the field $\mathrm{F}$ and $\mathrm{W}$ an $\mathrm{m}$-dimensional vector space over $\mathrm{F}$. Let $\mathbb{B}$ be an ordered basis for $\mathrm{V}$ and $\mathbb{B}^{\prime}$ an ordered basis for $\mathrm{W}$. For each linear transformation $\mathrm{T}$ from $\mathrm{V}$ into $\mathrm{W}$, there is an $\mathrm{m} \times \mathrm{n}$ matrix $\mathrm{A}$ with entries in $\mathrm{F}$ such that

$$
[\mathrm{T} \alpha]_{\mathbb{B}^{\prime}}=\mathrm{A}[\alpha]_{\mathscr{B}}
$$

for every vector $\alpha$ in $\mathrm{V}$. Furthermore, $\mathrm{T} \rightarrow \mathrm{A}$ is a one-one correspondence between the set of all linear transformations from $\mathrm{V}$ into $\mathrm{W}$ and the set of all $\mathrm{m} \times \mathrm{n}$ matrices over the field $\mathrm{F}$.

The matrix $A$ which is associated with $T$ in Theorem 11 is called the matrix of $T$ relative to the ordered bases $\left(\mathbb{R}, \mathbb{B}^{\prime}\right.$. Note that Equation (3-3) says that $A$ is the matrix whose columns $A_{1}, \ldots, A_{n}$ are given by

$$
A_{j}=\left[T \alpha_{j}\right]_{Q^{\prime}}, \quad j=1, \ldots, n .
$$

If $U$ is another linear transformation from $V$ into $W$ and $B=\left[B_{1}, \ldots, B_{n}\right]$ is the matrix of $U$ relative to the ordered bases $B, B^{\prime}$ then $c A+B$ is the matrix of $c T+U$ relative to $B, B^{\prime}$. That is clear because

$$
\begin{aligned}
c A_{j}+B_{j} & =c\left[T \alpha_{j}\right]_{Q^{\prime}}+\left[U \alpha_{j}\right]_{Q^{\prime}} \\
& =\left[c T \alpha_{j}+U \alpha_{j}\right]_{Q^{\prime}} \\
& =\left[(c T+U) \alpha_{j}\right]_{Q^{\prime}} .
\end{aligned}
$$

Theorem 12. Let $\mathrm{V}$ be an $\mathrm{n}$-dimensional vector space over the field $\mathrm{F}$ and let $\mathrm{W}$ be an $\mathrm{m}$-dimensional vector space over F. For each pair of ordered bases $\mathbb{B}, \mathbb{B}^{\prime}$ for $\mathrm{V}$ and $\mathrm{W}$ respectively, the function which assigns to a linear transformation $\mathrm{T}$ its matrix relative to $\mathbb{B}^{\prime}, \mathrm{C}^{\prime}$ is an isomorphism between the space $\mathrm{L}(\mathrm{V}, \mathrm{W})$ and the space of all $\mathrm{m} \times \mathrm{n}$ matrices over the field $\mathrm{F}$.

Proof. We observed above that the function in question is linear, and as stated in Theorem 11, this function is one-one and maps $L(V, W)$ onto the set of $m \times n$ matrices.

We shall be particularly interested in the representation by matrices of linear transformations of a space into itself, i.e., linear operators on a space $V$. In this cease it is most convenient to use the same ordered basis in each case, that is, to take $B=B^{\prime}$. We shall then call the representing matrix simply the matrix of $T$ relative to the ordered basis $B B$. Since this concept will be so important to us, we shall review its definition. If $T$ is a linear operator on the finite-dimensional vector space $V$ and $B=$ $\left\{\alpha_{1}, \ldots, \alpha_{n}\right\}$ is an ordered basis for $V$, the matrix of $T$ relative to $B$ (or, the matrix of $T$ in the ordered basis $B$ ) is the $n \times n$ matrix $A$ whose entries $A_{i j}$ are defined by the equations

$$
T \alpha_{j}=\sum_{i=1}^{n} A_{i j} \alpha_{i}, \quad j=1, \ldots, n .
$$

One must always remember that this matrix representing $T$ depends upon the ordered basis $B$, and that there is a representing matrix for $T$ in each ordered basis for $V$. (For transformations of one space into another the matrix depends upon two ordered bases, one for $V$ and one for $W$.) In order that we shall not forget this dependence, we shall use the notation

$$
[T]_{\mathscr{B}}
$$

for the matrix of the linear operator $T$ in the ordered basis $\Theta$. The manner in which this matrix and the ordered basis describe $T$ is that for each $\alpha$ in $V$

$$
[T \alpha]_{\mathscr{Q}}=[T]_{\mathscr{Q}}[\alpha]_{\mathscr{B}} .
$$

Example 13. Let $V$ be the space of $n \times 1$ column matrices over the field $F$; let $W$ be the space of $m \times 1$ matrices over $F$; and let $A$ be a fixed $m \times n$ matrix over $F$. Let $T$ be the linear transformation of $V$ into $W$ defined by $T(X)=A X$. Let $\$$ be the ordered basis for $V$ analogous to the standard basis in $F^{n}$, i.e., the $i$ th vector in $\mathbb{B}$ in the $n \times 1$ matrix $X_{i}$ with a 1 in row $i$ and all other entries 0 . Let $\mathbb{B}^{\prime}$ be the corresponding ordered basis for $W$, i.e., the $j$ th vector in $Q^{\prime}$ is the $m \times 1$ matrix $Y_{j}$ with a 1 in row $j$ and all other entries 0 . Then the matrix of $T$ relative to the pair $Q, B^{\prime}$ is the matrix $A$ itself. This is clear because the matrix $A X_{j}$ is the $j$ th column of $A$.

Example 14. Let $F$ be a field and let $T$ be the operator on $F^{2}$ defined by

$$
T\left(x_{1}, x_{2}\right)=\left(x_{1}, 0\right) .
$$

It is easy to see that $T$ is a linear operator on $F^{2}$. Let $B$ be the standard ordered basis for $F^{2}, Q=\left\{\epsilon_{1}, \epsilon_{2}\right\}$. Now

$$
\begin{aligned}
& T \epsilon_{1}=T(1,0)=(1,0)=1 \epsilon_{1}+0 \epsilon_{2} \\
& T \epsilon_{2}=T(0,1)=(0,0)=0 \epsilon_{1}+0 \epsilon_{2}
\end{aligned}
$$

so the matrix of $T$ in the ordered basis $Q$ is

![](https://cdn.mathpix.com/cropped/2023_01_16_0f6afdf132d84aaf6bdeg-098.jpg?height=103&width=264&top_left_y=856&top_left_x=479)

EXAmple 15. Let $V$ be the space of all polynomial functions from $R$ into $R$ of the form

$$
f(x)=c_{0}+c_{1} x+c_{2} x^{2}+c_{3} x^{3}
$$

that is, the space of polynomial functions of degree three or less. The differentiation operator $D$ of Example 2 maps $V$ into $V$, since $D$ is 'degree decreasing.' Let $Q$ be the ordered basis for $V$ consisting of the four functions $f_{1}, f_{2}, f_{3}, f_{4}$ defined by $f_{j}(x)=x^{i-1}$. Then

$$
\begin{array}{lll}
\left(D f_{1}\right)(x)=0, & D f_{1}=0 f_{1}+0 f_{2}+0 f_{3}+0 f_{4} \\
\left(D f_{2}\right)(x)=1, & D f_{2}=1 f_{1}+0 f_{2}+0 f_{3}+0 f_{4} \\
\left(D f_{3}\right)(x)=2 x, & D f_{3}=0 f_{1}+2 f_{2}+0 f_{3}+0 f_{4} \\
\left(D f_{4}\right)(x)=3 x^{2}, & D f_{4}=0 f_{1}+0 f_{2}+3 f_{3}+0 f_{4}
\end{array}
$$

so that the matrix of $D$ in the ordered basis $B$ is

$$
[D]_{\varangle}=\left[\begin{array}{llll}
0 & 1 & 0 & 0 \\
0 & 0 & 2 & 0 \\
0 & 0 & 0 & 3 \\
0 & 0 & 0 & 0
\end{array}\right] .
$$

We have seen what happens to representing matrices when transformations are added, namely, that the matrices add. We should now like to ask what happens when we compose transformations. More specifically, let $V, W$, and $Z$ be vector spaces over the field $F$ of respective dimensions $n, m$, and $p$. Let $T$ be a linear transformation from $V$ into $W$ and $U$ a linear transformation from $W$ into $Z$. Suppose we have ordered bases

$$
B=\left\{\alpha_{1}, \ldots, \alpha_{n}\right\}, \quad \mathbb{B}^{\prime}=\left\{\beta_{1}, \ldots, \beta_{m}\right\}, \quad \mathbb{B}^{\prime \prime}=\left\{\gamma_{1}, \ldots, \gamma_{p}\right\}
$$

for the respective spaces $V, W$, and $Z$. Let $A$ be the matrix of $T$ relative to the pair $B, B^{\prime}$ and let $B$ be the matrix of $U$ relative to the pair $B^{\prime}, Q^{\prime \prime}$. It is then easy to see that the matrix $C$ of the transformation $U T$ relative to the pair $B, Q^{\prime \prime}$ is the product of $B$ and $A$; for, if $\alpha$ is any vector in $V$

$$
\begin{aligned}
{[T \alpha]_{\Omega^{\prime}} } & =A[\alpha]_{(\dot{s}} \\
{[U(T \alpha)]_{\mathfrak{G}^{\prime \prime}} } & =B[T \alpha]_{\mathbb{B}^{\prime}}
\end{aligned}
$$

and so

$$
[(U T)(\alpha)]_{\Theta^{\prime \prime}}=B A[\alpha]_{\Theta}
$$

and hence, by the definition and uniqueness of the representing matrix, we must have $C=B A$. One can also see this by carrying out the computation

$$
\begin{aligned}
(U T)\left(\alpha_{j}\right) & =U\left(T \alpha_{j}\right) \\
& =U\left(\sum_{k=1}^{m} A_{k j} \beta_{k}\right) \\
& =\sum_{k=1}^{m} A_{k j}\left(U \beta_{k}\right) \\
& =\sum_{k=1}^{m} A_{k j} \sum_{i=1}^{p} B_{i k} \gamma_{i} \\
& =\sum_{i=1}^{p}\left(\sum_{k=1}^{m} B_{i k} A_{k j}\right) \gamma_{i}
\end{aligned}
$$

so that we must have

$$
C_{i j}=\sum_{k=1}^{m} B_{i k} A_{k j} .
$$

We motivated the definition (3-6) of matrix multiplication via operations on the rows of a matrix. One sees here that a very strong motivation for the definition is to be found in composing linear transformations. Let us summarize formally.

Theorem 13. Let $\mathrm{V}, \mathrm{W}$, and $\mathrm{Z}$ be finite-dimensional vector spaces over the field $\mathrm{F}$; let $\mathrm{T}$ be a linear transformation from $\mathrm{V}$ into $\mathrm{W}$ and $\mathrm{U}$ a linear transformation from $\mathrm{W}$ into $\mathrm{Z}$. If $\mathrm{Q}, \mathrm{Q}^{\prime}$, and $\mathbb{B}^{\prime \prime}$ are ordered bases for the spaces $\mathrm{V}, \mathrm{W}$, and $\mathrm{Z}$, respectively, if $\mathrm{A}$ is the matrix of $\mathrm{T}$ relative to the pair $B, Q^{\prime}$, and $\mathrm{B}$ is the matrix of $\mathrm{U}$ relative to the pair $\Omega^{\prime},{ }^{\prime}{ }^{\prime \prime}$, then the matrix of the composition UT relative to the pair $B, B^{\prime \prime}$ is the product matrix $\mathrm{C}=\mathrm{BA}$.

We remark that Theorem 13 gives a proof that matrix multiplication is associative--a proof which requires no calculations and is independent of the proof we gave in Chapter 1 . We should also point out that we proved a special case of Theorem 13 in Example 12.

It is important to note that if $T$ and $U$ are linear operators on a space $V$ and we are representing by a single ordered basis $\Theta$, then Theorem 13 assumes the simple form $[U T]_{\mathscr{Q}}=[U]_{\mathscr{B}}[T]_{\mathscr{B}}$. Thus in this case, the correspondence which $B$ determines between operators and matrices is not only a vector space isomorphism but also preserves products. A simple consequence of this is that the linear operator $T$ is invertible if and only if $[T]_{\mathscr{B}}$ is an invertible matrix. For, the identity operator $I$ is represented by the identity matrix in any ordered basis, and thus

$$
U T=T U=I
$$

is equivalent to

$$
[U]_{Q}[T]_{Q}=[T]_{\Theta}[U]_{Q}=I .
$$

Of course, when $T$ is invertible

$$
\left[T^{-1}\right]_{\mathscr{B}}=[T]_{\mathbb{B}^{-1}} \text {. }
$$

Now we should like to inquire what happens to representing matrices when the ordered basis is changed. For the sake of simplicity, we shall consider this question only for linear operators on a space $V$, so that we can use a single ordered basis. The specific question is this. Let $T$ be a linear operator on the finite-dimensional space $V$, and let

$$
B=\left\{\alpha_{1}, \ldots, \alpha_{n}\right\} \text { and } \mathscr{B}^{\prime}=\left\{\alpha_{1}^{\prime}, \ldots, \alpha_{n}^{\prime}\right\}
$$

be two ordered bases for $V$. How are the matrices $[T]_{\mathscr{Q}}$ and $[T]_{Q^{\prime}}$ related? As we observed in Chapter 2 , there is a unique (invertible) $n \times n$ matrix $P$ such that

$$
[\alpha]_{\mathscr{Q}}=P[\alpha]_{\mathscr{Q}^{\prime}}
$$

for every vector $\alpha$ in $V$. It is the matrix $P=\left[P_{1}, \ldots, P_{n}\right]$ where $P_{j}=$ $\left[\alpha_{j}^{\prime}\right]_{\bigotimes}$. By definition

$$
[T \alpha]_{\mathscr{\Omega}}=[T]_{\circledast}[\alpha]_{\circledast} .
$$

Applying (3-7) to the vector $T \alpha$, we have

$$
[T \alpha]_{\mathscr{B}}=P[T \alpha]_{\Omega^{\prime}} .
$$

Combining (3-7), (3-8), and (3-9), we obtain

$$
[T]_{\Omega} P[\alpha]_{\bigotimes^{\prime}}=P[T \alpha]_{\Theta^{\prime}}
$$

or

$$
P^{-1}[T]_{\mathscr{Q}} P[\alpha]_{\Theta^{\prime}}=[T \alpha]_{\Theta^{\prime}}
$$

and so it must be that

$$
[T]_{\mathscr{B}^{\prime}}=P^{-1}[T]_{\mathscr{\Omega}} P .
$$

This answers our question.

Before stating this result formally, let us observe the following. There is a unique linear operator $U$ which carries $\mathbb{B}$ onto $\mathbb{B}^{\prime}$, defined by

$$
U \alpha_{j}=\alpha_{j}^{\prime}, \quad j=1, \ldots, n .
$$

This operator $U$ is invertible since it carries a basis for $V$ onto a basis for $V$. The matrix $P$ (above) is precisely the matrix of the operator $U$ in the ordered basis $B$. For, $P$ is defined by

$$
\alpha_{j}^{\prime}=\sum_{i=1}^{n} P_{i j} \alpha_{i}
$$

and since $U \alpha_{j}=\alpha_{j}^{\prime}$, this equation can be written

$$
U \alpha_{j}=\sum_{i=1}^{n} P_{i j} \alpha_{i} .
$$

So $P=[U]_{Q}$, by definition.

Theorem 14. Let $\mathrm{V}$ be a finite-dimensional vector space over the field $\mathrm{F}$, and let

$$
B=\left\{\alpha_{1}, \ldots, \alpha_{n}\right\} \text { and } \mathbb{B}^{\prime}=\left\{\alpha_{1}^{\prime}, \ldots, \alpha_{n}^{\prime}\right\}
$$

be ordered bases for $\mathrm{V}$. Suppose $\mathrm{T}$ is a linear operator on $\mathrm{V}$. If $\mathrm{P}=\left[\mathrm{P}_{1}, \ldots\right.$, $\mathrm{P}_{\mathrm{n}}$ ] is the $\mathrm{n} \times \mathrm{n}$ matrix with columns $\mathrm{P}_{\mathrm{j}}=\left[\alpha_{\mathrm{j}}^{\prime}\right]_{\mathscr{Q}}$, then

$$
[\mathrm{T}]_{Q^{\prime}}=\mathrm{P}^{-1}[\mathrm{~T}]_{\mathscr{Q}} \mathrm{P} \text {. }
$$

Alternatively, if $\mathrm{U}$ is the invertible operator on $\mathrm{V}$ defined by $\mathrm{U} \alpha_{\mathrm{j}}=\alpha_{\mathrm{j}}^{\prime}, \mathrm{j}=$ $1, \ldots, n$, then

$$
[\mathrm{T}]_{Q^{\prime}}=[\mathrm{U}]_{\bar{a}^{-}}{ }^{1}[\mathrm{~T}]_{\mathscr{Q}}[\mathrm{U}]_{\mathscr{Q}} .
$$

EXAmple 16 . Let $T$ be the linear operator on $R^{2}$ defined by $T\left(x_{1}, x_{2}\right)=$ $\left(x_{1}, 0\right)$. In Example 14 we showed that the matrix of $T$ in the standard ordered basis $Q=\left\{\epsilon_{1}, \epsilon_{2}\right\}$ is

$$
[T]_{\omega}=\left[\begin{array}{ll}
1 & 0 \\
0 & 0
\end{array}\right] .
$$

Suppose $Q^{\prime}$ is the ordered basis for $R^{2}$ consisting of the vectors $\epsilon_{1}^{\prime}=(1,1)$, $\epsilon_{2}^{\prime}=(2,1)$. Then

so that $P$ is the matrix

$$
\begin{aligned}
\epsilon_{1}^{\prime} & =\epsilon_{1}+\epsilon_{2} \\
\epsilon_{2}^{\prime} & =2 \epsilon_{1}+\epsilon_{2}
\end{aligned}
$$

By a short computation

$$
P=\left[\begin{array}{ll}
1 & 2 \\
1 & 1
\end{array}\right] \text {. }
$$

Thus

$$
P^{-1}=\left[\begin{array}{rr}
-1 & 2 \\
1 & -1
\end{array}\right] \text {. }
$$

$$
\begin{aligned}
{[T]_{\Theta^{\prime}} } & =P^{-1}\left[T_{\Theta}\right]_{\mathscr{Q}} P \\
& =\left[\begin{array}{rr}
-1 & 2 \\
1 & -1
\end{array}\right]\left[\begin{array}{ll}
1 & 0 \\
0 & 0
\end{array}\right]\left[\begin{array}{ll}
1 & 2 \\
1 & 1
\end{array}\right] \\
& =\left[\begin{array}{rr}
-1 & 2 \\
1 & -1
\end{array}\right]\left[\begin{array}{ll}
1 & 2 \\
0 & 0
\end{array}\right] \\
& =\left[\begin{array}{rr}
-1 & -2 \\
1 & 2
\end{array}\right] .
\end{aligned}
$$

We can easily check that this is correct because

$$
\begin{aligned}
& T \epsilon_{1}^{\prime}=(1,0)=-\epsilon_{1}^{\prime}+\epsilon_{2}^{\prime} \\
& T \epsilon_{2}^{\prime}=(2,0)=-2 \epsilon_{1}^{\prime}+2 \epsilon_{2}^{\prime} .
\end{aligned}
$$

Example 17. Let $V$ be the space of polynomial functions from $R$ into $R$ which have 'degree' less than or equal to 3 . As in Example 15 , let $D$ be the differentiation operator on $V$, and let

$$
B=\left\{f_{1}, f_{2}, f_{3}, f_{4}\right\}
$$

be the ordered basis for $V$ defined by $f_{i}(x)=x^{i-1}$. Let $t$ be a real number and define $g_{i}(x)=(x+t)^{i-1}$, that is

Since the matrix

$$
\begin{aligned}
& g_{1}=f_{1} \\
& g_{2}=t f_{1}+f_{2} \\
& g_{3}=t^{2} f_{1}+2 t f_{2}+f_{3} \\
& g_{4}=t^{3} f_{1}+3 t^{2} f_{2}+3 t f_{3}+f_{4} .
\end{aligned}
$$

$$
P=\left[\begin{array}{rrrr}
1 & t & t^{2} & t^{3} \\
0 & 1 & 2 t & 3 t^{2} \\
0 & 0 & 1 & 3 t \\
0 & 0 & 0 & 1
\end{array}\right]
$$

is easily seen to be invertible with

$$
P^{-1}=\left[\begin{array}{rrrr}
1 & -t & t^{2} & -t^{3} \\
0 & 1 & -2 t & 3 t^{2} \\
0 & 0 & 1 & -3 t \\
0 & 0 & 0 & 1
\end{array}\right]
$$

it follows that $Q^{\prime}=\left\{g_{1}, g_{2}, g_{3}, g_{4}\right\}$ is an ordered basis for $V$. In Example 15, we found that the matrix of $D$ in the ordered basis $B$ is

$$
[D]_{B}=\left[\begin{array}{llll}
0 & 1 & 0 & 0 \\
0 & 0 & 2 & 0 \\
0 & 0 & 0 & 3 \\
0 & 0 & 0 & 0
\end{array}\right] \text {. }
$$

The matrix of $D$ in the ordered basis $Q^{\prime}$ is thus

$$
\begin{aligned}
P^{-1}[D]_{\mathrm{B}} P & =\left[\begin{array}{rrrr}
1 & -t & t^{2} & t^{3} \\
0 & 1 & -2 t & 3 t^{2} \\
0 & 0 & 1 & -3 t \\
0 & 0 & 0 & 1
\end{array}\right]\left[\begin{array}{llll}
0 & 1 & 0 & 0 \\
0 & 0 & 2 & 0 \\
0 & 0 & 0 & 3 \\
0 & 0 & 0 & 0
\end{array}\right]\left[\begin{array}{cccc}
1 & t & t^{2} & t^{3} \\
0 & 1 & 2 t & 3 t^{2} \\
0 & 0 & 1 & 3 t \\
0 & 0 & 0 & 1
\end{array}\right] \\
& =\left[\begin{array}{rrrr}
1 & -t & t^{2} & t^{3} \\
0 & 1 & -2 t & 3 t^{2} \\
0 & 0 & 1 & -3 t \\
0 & 0 & 0 & 1
\end{array}\right]\left[\begin{array}{cccc}
0 & 1 & 2 t & 3 t^{2} \\
0 & 0 & 2 & 6 t \\
0 & 0 & 0 & 3 \\
0 & 0 & 0 & 0
\end{array}\right] \\
& =\left[\begin{array}{llrr}
0 & 1 & 0 & 0 \\
0 & 0 & 2 & 0 \\
0 & 0 & 0 & 3 \\
0 & 0 & 0 & 0
\end{array}\right] .
\end{aligned}
$$

Thus $D$ is represented by the same matrix in the ordered bases $\mathbb{B}$ and $\mathbb{B}^{\prime}$. Of course, one can see this somewhat more directly since

$$
\begin{aligned}
& D g_{1}=0 \\
& D g_{2}=g_{1} \\
& D g_{3}=2 g_{2} \\
& D g_{4}=3 g_{3}
\end{aligned}
$$

This example illustrates a good point. If one knows the matrix of a linear operator in some ordered basis $B$ and wishes to find the matrix in another ordered basis $\mathbf{a}^{\prime}$, it is of ten most convenient to perform the coordinate change using the invertible matrix $P$; however, it may be a much simpler task to find the representing matrix by a direct appeal to its definition.

Definition. Let $\mathrm{A}$ and $\mathrm{B}$ be $\mathrm{n} \times \mathrm{n}$ (square) matrices over the field $\mathrm{F}$. $W$ e say that $\mathrm{B}$ is similar to $\mathrm{A}$ over $\mathrm{F}$ if there is an invertible $\mathrm{n} \times \mathrm{n}$ matrix $\mathrm{P}$ over $\mathrm{F}$ such that $\mathrm{B}=\mathrm{P}^{-1} \mathrm{AP}$.

According to Theorem 14, we have the following: If $V$ is an $n$-dimensional vector space over $F$ and $B$ and $B^{\prime}$ are two ordered bases for $V$, then for each linear operator $T$ on $V$ the matrix $B=[T]_{Q^{\prime}}$ is similar to the matrix $A=[T]_{\mathscr{S}}$. The argument also goes in the other direction. Suppose $A$ and $B$ are $n \times n$ matrices and that $B$ is similar to $A$. Let $V$ be any $n$-dimensional space over $F$ and let $Q$ be an ordered basis for $V$. Let $T$ be the linear operator on $V$ which is represented in the basis $B$ by $A$. If $B=P^{-1} A P$, let $B^{\prime}$ be the ordered basis for $V$ obtained from $B$ by $P$, i.e.,

$$
\alpha_{j}^{\prime}=\sum_{i=1}^{n} P_{i j} \alpha_{i} .
$$

Then the matrix of $T$ in the ordered basis $\mathbb{B}^{\prime}$ will be $B$.

Thus the statement that $B$ is similar to $A$ means that on each $n$ dimensional space over $F$ the matrices $A$ and $B$ represent the same linear transformation in two (possibly) different ordered bases.

Note that each $n \times n$ matrix $A$ is similar to itself, using $P=I$; if $B$ is similar to $A$, then $A$ is similar to $B$, for $B=P^{-1} A P$ implies that $A=\left(P^{-1}\right)^{-1} B P^{-1}$; if $B$ is similar to $A$ and $C$ is similar to $B$, then $C$ is similar to $A$, for $B=P^{-1} A P$ and $C=Q^{-1} B Q$ imply that $C=(P Q)^{-1} A(P Q)$. Thus, similarity is an equivalence relation on the set of $n \times n$ matrices over the field $F$. Also note that the only matrix similar to the identity matrix $I$ is $I$ itself, and that the only matrix similar to the zero matrix is the zero matrix itself. 

\section{Exercises}

1. Let $T$ be the linear operator on $C^{2}$ defined by $T\left(x_{1}, x_{2}\right)=\left(x_{1}, 0\right)$. Let $B$ be the standard ordered basis for $C^{2}$ and let $\Omega^{\prime}=\left\{\alpha_{1}, \alpha_{2}\right\}$ be the ordered basis defined by $\alpha_{1}=(1, i), \alpha_{2}=(-i, 2)$.

(a) What is the matrix of $T$ relative to the pair $B, B^{\prime}$ ?

(b) What is the matrix of $T$ relative to the pair $B^{\prime}, B$ ?

(c) What is the matrix of $T$ in the ordered basis $B^{\prime}$ ?

(d) What is the matrix of $T$ in the ordered basis $\left\{\alpha_{2}, \alpha_{1}\right\}$ ?

2. Let $T$ be the linear transformation from $R^{3}$ into $R^{2}$ defined by

$$
T\left(x_{1}, x_{2}, x_{3}\right)=\left(x_{1}+x_{2}, 2 x_{3}-x_{1}\right) .
$$

(a) If $B$ is the standard ordered basis for $R^{3}$ and $B^{\prime}$ is the standard ordered basis for $R^{2}$, what is the matrix of $T$ relative to the pair $B, B^{\prime}$ ?

(b) If $B=\left\{\alpha_{1}, \alpha_{2}, \alpha_{3}\right\}$ and $B^{\prime}=\left\{\beta_{1}, \beta_{2}\right\}$, where

$\alpha_{1}=(1,0,-1), \quad \alpha_{2}=(1,1,1), \quad \alpha_{3}=(1,0,0), \quad \beta_{1}=(0,1), \quad \beta_{2}=(1,0)$

what is the matrix of $T$ relative to the pair $\left(B^{\prime}, \boldsymbol{B}^{\prime}\right.$ ?

3. Let $T$ be a linear operator on $F^{n}$, let $A$ be the matrix of $T$ in the standard ordered basis for $F^{n}$, and let $W$ be the subspace of $F^{n}$ spanned by the column vectors of $A$. What does $W$ have to do with $T$ ?

4. Let $V$ be a two-dimensional vector space over the field $F$, and let $B$ be an ordered basis for $V$. If $T$ is a linear operator on $V$ and

$$
[T]_{\circlearrowleft}=\left[\begin{array}{ll}
a & b \\
c & d
\end{array}\right]
$$

prove that $T^{2}-(a+d) T+(a d-b c) I=0$.

5. Let $T$ be the linear operator on $R^{3}$, the matrix of which in the standard ordered basis is

$$
A=\left[\begin{array}{rrr}
1 & 2 & 1 \\
0 & 1 & 1 \\
-1 & 3 & 4
\end{array}\right]
$$

Find a basis for the range of $T$ and a basis for the null space of $T$.

6. Let $T$ be the linear operator on $R^{2}$ defined by

$$
T\left(x_{1}, x_{2}\right)=\left(-x_{2}, x_{1}\right) .
$$

(a) What is the matrix of $T$ in the standard ordered basis for $R^{2}$ ?

(b) What is the matrix of $T$ in the ordered basis $B=\left\{\alpha_{1}, \alpha_{2}\right\}$, where $\alpha_{1}=(1,2)$ and $\alpha_{2}=(1,-1) ?$

(c) Prove that for every real number $c$ the operator $(T-c I)$ is invertible.

(d) Prove that if $B$ is any ordered basis for $R^{2}$ and $[T]_{\odot}=A$, then $A_{12} A_{21} \neq 0$.

7. Let $T$ be the linear operator on $R^{3}$ defined by

$$
T\left(x_{1}, x_{2}, x_{3}\right)=\left(3 x_{1}+x_{3},-2 x_{1}+x_{2},-x_{1}+2 x_{2}+4 x_{3}\right) .
$$

(a) What is the matrix of $T$ in the standard ordered basis for $R^{3}$ ? (b) What is the matrix of $T$ in the ordered basis

$$
\left\{\alpha_{1}, \alpha_{2}, \alpha_{3}\right\}
$$

where $\alpha_{1}=(1,0,1), \alpha_{2}=(-1,2,1)$, and $\alpha_{3}=(2,1,1) ?$

(c) Prove that $T$ is invertible and give a rule for $T^{-1}$ like the one which defines $T$

8. Let $\theta$ be a real number. Prove that the following two matrices are similar over the field of complex numbers:

$$
\left[\begin{array}{rr}
\cos \theta & -\sin \theta \\
\sin \theta & \cos \theta
\end{array}\right], \quad\left[\begin{array}{ll}
e^{i \theta} & 0 \\
0 & e^{-i \theta}
\end{array}\right]
$$

(Hint: Let $T$ be the linear operator on $C^{2}$ which is represented by the first matrix in the standard ordered basis. Then find vectors $\alpha_{1}$ and $\alpha_{2}$ such that $T \alpha_{1}=e^{i \theta} \alpha_{1}$, $T \alpha_{2}=e^{-i \theta} \alpha_{2}$, and $\left\{\alpha_{1}, \alpha_{2}\right\}$ is a basis.)

9. Let $V$ be a finite-dimensional vector space over the field $F$ and let $S$ and $T$ be linear operators on $V$. We ask: When do there exist ordered bases $B$ and $B^{\prime}$ for $V$ such that $[S]_{\mathscr{Q}}=[T]_{\mathbb{B}^{\prime}}$ ? Prove that such bases exist if and only if there is an invertible linear operator $U$ on $V$ such that $T=U S U^{-1}$. (Outline of proof: If $[S]_{\mathscr{B}}=[T]_{Q^{\prime}}$, let $U$ be the operator which carries $B$ onto $B^{\prime}$ and show that $S=U T U^{-1}$. Conversely, if $T=U S U^{-1}$ for some invertible $U$, let $B$ be any ordered basis for $V$ and let $B^{\prime}$ be its image under $U$. Then show that $\left.[S]_{B}=[T]_{B^{\prime}}.\right)$

10. We have seen that the linear operator $T$ on $R^{2}$ defined by $T\left(x_{1}, x_{2}\right)=\left(x_{1}, 0\right)$ is represented in the standard ordered basis by the matrix

$$
A=\left[\begin{array}{ll}
1 & 0 \\
0 & 0
\end{array}\right] \text {. }
$$

This operator satisfies $T^{2}=T$. Prove that if $S$ is a linear operator on $R^{2}$ such that $S^{2}=S$, then $S=0$, or $S=I$, or there is an ordered basis $B$ for $R^{2}$ such that $[S]_{\mathbb{B}}=A$ (above)

11. Let $W$ be the space of all $n \times 1$ column matrices over a field $F$. If $A$ is an $n \times n$ matrix over $F$, then $A$ defines a linear operator $L_{A}$ on $W$ through left multiplication: $L_{A}(X)=A X$. Prove that every linear operator on $W$ is left multiplication by some $n \times n$ matrix, i.e., is $L_{A}$ for some $A$.

Now suppose $V$ is an $n$-dimensional vector space over the field $F$, and let $B$ be an ordered basis for $V$. For each $\alpha$ in $V$, define $U \alpha=[\alpha]_{Q}$. Prove that $U$ is an isomorphism of $V$ onto $W$. If $T$ is a linear operator on $V$, then $U T U^{-1}$ is a linear operator on $W$. Accordingly, $U T U^{-1}$ is left multiplication by some $n \times n$ matrix $A$. What is $A$ ?

12. Let $V$ be an $n$-dimensional vector space over the field $F$, and let $B=$ $\left\{\alpha_{1}, \ldots, \alpha_{n}\right\}$ be an ordered basis for $V$.

(a) According to Theorem 1, there is a unique linear operator $T$ on $V$ such that

$$
T \alpha_{j}=\alpha_{j+1}, \quad j=1, \ldots, n-1, \quad T \alpha_{n}=0
$$

What is the matrix $A$ of $T$ in the ordered basis $B$ ?

(b) Prove that $T^{n}=0$ but $T^{n-1} \neq 0$.

(c) Let $S$ be any linear operator on $V$ such that $S^{n}=0$ but $S^{n-1} \neq 0$. Prove that there is an ordered basis $Q^{\prime}$ for $V$ such that the matrix of $S$ in the ordered basis $Q^{\prime}$ is the matrix $A$ of part (a). (d) Prove that if $M$ and $N$ are $n \times n$ matrices over $F$ such that $M^{n}=N^{n}=0$ but $M^{n-1} \neq 0 \neq N^{n-1}$, then $M$ and $N$ are similar.

13. Let $V$ and $W$ be finite-dimensional vector spaces over the field $F$ and let $T$ be a linear transformation from $V$ into $W$. If

$$
B=\left\{\alpha_{1}, \ldots, \alpha_{n}\right\} \text { and } B^{\prime}=\left\{\beta_{1}, \ldots, \beta_{m}\right\}
$$

are ordered bases for $V$ and $W$, respectively, define the linear transformations $E^{p, q}$ as in the proof of Theorem 5: $E^{p, q}\left(\alpha_{i}\right)=\delta_{i q} \beta_{p}$. Then the $E^{p, q}, 1 \leq p \leq m$, $1 \leq q \leq n$, form a basis for $L(V, W)$, and so

$$
T=\sum_{p=1}^{m} \sum_{q=1}^{n} A_{p q} E^{p, q}
$$

for certain scalars $A_{p q}$ (the coordinates of $T$ in this basis for $L(V, W)$ ). Show that the matrix $A$ with entries $A(p, q)=A_{p q}$ is precisely the matrix of $T$ relative to the pair $C B, B^{\prime}$.

\subsection{Linear Functionals}

If $V$ is a vector space over the field $F$, a linear transformation $f$ from $V$ into the scalar field $F$ is also called a linear functional on $V$. If we start from scratch, this means that $f$ is a function from $V$ into $F$ such that

$$
f(c \alpha+\beta)=c f(\alpha)+f(\beta)
$$

for all vectors $\alpha$ and $\beta$ in $V$ and all scalars $c$ in $F$. The concept of linear functional is important in the study of finite-dimensional spaces because it helps to organize and clarify the discussion of subspaces, linear equations, and coordinates.

Example 18. Let $F$ be a field and let $a_{1}, \ldots, a_{n}$ be scalars in $F$. Define a function $f$ on $F^{n}$ by

$$
f\left(x_{1}, \ldots, x_{n}\right)=a_{1} x_{1}+\cdots+a_{n} x_{n} .
$$

Then $f$ is a linear functional on $F^{n}$. It is the linear functional which is represented by the matrix $\left[a_{1} \cdots a_{n}\right]$ relative to the standard ordered basis for $F^{n}$ and the basis $\{1\}$ for $F$ :

$$
a_{j}=f\left(\epsilon_{j}\right), \quad j=1, \ldots, n .
$$

Every linear functional on $F^{n}$ is of this form, for some scalars $a_{1}, \ldots, a_{n}$. That is immediate from the definition of linear functional because we define $a_{j}=f\left(\epsilon_{j}\right)$ and use the linearity

$$
\begin{aligned}
f\left(x_{1}, \ldots, x_{n}\right) & =f\left(\sum_{j} x_{j} \epsilon_{j}\right) \\
& =\sum_{j} x_{j} f\left(\epsilon_{j}\right) \\
& =\sum_{j} a_{j} x_{j} .
\end{aligned}
$$

Example 19. Here is an important example of a linear functional. Let $n$ be a positive integer and $F$ a field. If $A$ is an $n \times n$ matrix with entries in $F$, the trace of $A$ is the scalar

$$
\operatorname{tr} A=A_{11}+A_{22}+\cdots+A_{n n} .
$$

The trace function is a linear functional on the matrix space $F^{n \times n}$ because

$$
\begin{aligned}
\operatorname{tr}(c A+B) & =\sum_{i=1}^{n}\left(c A_{i i}+B_{i i}\right) \\
& =c \sum_{i=1}^{n} A_{i i}+\sum_{i=1}^{n} B_{i i} \\
& =c \operatorname{tr} A+\operatorname{tr} B .
\end{aligned}
$$

Example 20. Let $V$ be the space of all polynomial functions from the field $F$ into itself. Let $t$ be an element of $F$. If we define

$$
L_{t}(p)=p(t)
$$

then $L_{t}$ is a linear functional on $V$. One usually describes this by saying that, for each $t$, 'evaluation at $t$ ' is a linear functional on the space of polynomial functions. Perhaps we should remark that the fact that the functions are polynomials plays no role in this example. Evaluation at $t$ is a linear functional on the space of all functions from $F$ into $F$.

Example 21. This may be the most important linear functional in mathematics. Let $[a, b]$ be a closed interval on the real line and let $C([a, b])$ be the space of continuous real-valued functions on $[a, b]$. Then

$$
L(g)=\int_{a}^{b} g(t) d t
$$

defines a linear functional $L$ on $C([a, b])$.

If $V$ is a vector space, the collection of all linear functionals on $V$ forms a vector space in a natural way. It is the space $L(V, F)$. We denote this space by $V^{*}$ and call it the dual space of $V$ :

$$
V^{*}=L(V, F) .
$$

If $V$ is finite-dimensional, we can obtain a rather explicit description of the dual space $V^{*}$. From Theorem 5 we know something about the space $V^{*}$, namely that

$$
\operatorname{dim} V^{*}=\operatorname{dim} V .
$$

Let $B=\left\{\alpha_{1}, \ldots, \alpha_{n}\right\}$ be a basis for $V$. According to Theorem 1, there is (for each $i$ ) a unique linear functional $f_{i}$ on $V$ such that

$$
f_{i}\left(\alpha_{j}\right)=\delta_{i j} .
$$

In this way we obtain from $B$ a set of $n$ distinct linear functionals $f_{1}, \ldots, f_{n}$ on $V$. These functionals are also linearly independent. For, suppose 

$$
f=\sum_{i=1}^{n} c_{i} f_{i} .
$$

Then

$$
\begin{aligned}
f\left(\alpha_{j}\right) & =\sum_{i=1}^{n} c_{i} f_{i}\left(\alpha_{j}\right) \\
& =\sum_{i=1}^{n} c_{i} \delta_{i j} \\
& =c_{j} .
\end{aligned}
$$

In particular, if $f$ is the zero functional, $f\left(\alpha_{j}\right)=0$ for each $j$ and hence the scalars $c_{j}$ are all 0 . Now $f_{1}, \ldots, f_{n}$ are $n$ linearly independent functionals, and since we know that $V^{*}$ has dimension $n$, it must be that $Q^{*}=\left\{f_{1}, \ldots, f_{n}\right\}$ is a basis for $V^{*}$. This basis is called the dual basis of $B$.

Theorem 15. Let $\mathrm{V}$ be a finite-dimensional vector space over the field $\mathrm{F}$, and let $B=\left\{\alpha_{1}, \ldots, \alpha_{n}\right\}$ be a basis for $\mathrm{V}$. Then there is a unique dual basis $\mathbb{B}^{*}=\left\{\mathrm{f}_{1}, \ldots, \mathrm{f}_{\mathrm{n}}\right\}$ for $\mathrm{V}^{*}$ such that $\mathrm{f}_{\mathrm{i}}\left(\alpha_{\mathrm{j}}\right)=\delta_{\mathrm{i} \mathrm{j}}$. For each linear functional f on $\mathrm{V}$ we have

$$
f=\sum_{i=1}^{n} f\left(\alpha_{i}\right) f_{i}
$$

and for each vector $\alpha$ in $\mathrm{V}$ we have

$$
\alpha=\sum_{i=1}^{n} f_{i}(\alpha) \alpha_{i} .
$$

Proof. We have shown above that there is a unique basis which is 'dual' to $B$. If $f$ is a linear functional on $V$, then $f$ is some linear combination (3-12) of the $f_{i}$, and as we observed after (3-12) the scalars $c_{j}$ must be given by $c_{j}=f\left(\alpha_{j}\right)$. Similarly, if

is a vector in $V$, then

$$
\alpha=\sum_{i=1}^{n} x_{i} \alpha_{i}
$$

$$
\begin{aligned}
f_{j}(\alpha) & =\sum_{i=1}^{n} x_{i} f_{j}\left(\alpha_{i}\right) \\
& =\sum_{i=1}^{n} x_{i} \delta_{i j} \\
& =x_{j}
\end{aligned}
$$

so that the unique expression for $\alpha$ as a linear combination of the $\alpha_{i}$ is

$$
\alpha=\sum_{i=1}^{n} f_{i}(\alpha) \alpha_{i} .
$$

Equation (3-14) provides us with a nice way of describing what the dual basis is. It says, if $B=\left\{\alpha_{1}, \ldots, \alpha_{n}\right\}$ is an ordered basis for $V$ and ه $^{*}=\left\{f_{1}, \ldots, f_{n}\right\}$ is the dual basis, then $f_{i}$ is precisely the function which assigns to each vector $\alpha$ in $V$ the $i$ th coordinate of $\alpha$ relative to the ordered basis $Q$. Thus we may also call the $f_{i}$ the coordinate functions for @. The formula (3-13), when combined with (3-14) tells us the following: If $f$ is in $V^{*}$, and we let $f\left(\alpha_{i}\right)=\alpha_{i}$, then when

we have

$$
\alpha=x_{1} \alpha_{1}+\cdots+x_{n} \alpha_{n}
$$

$$
f(\alpha)=a_{1} x_{1}+\cdots+a_{n} x_{n} .
$$

In other words, if we choose an ordered basis $B$ for $V$ and describe each vector in $V$ by its $n$-tuple of coordinates $\left(x_{1}, \ldots, x_{n}\right)$ relative to $B$, then every linear functional on $V$ has the form (3-15). This is the natural generalization of Example 18, which is the special case $V=F^{n}$ and $B=$ $\left\{\epsilon_{1}, \ldots, \epsilon_{n}\right\}$.

EXAMple 22. Let $V$ be the vector space of all polynomial functions from $R$ into $R$ which have degree less than or equal to 2 . Let $t_{1}, t_{2}$, and $t_{3}$ be any three distinct real numbers, and let

$$
L_{i}(p)=p\left(t_{i}\right) .
$$

Then $L_{1}, L_{2}$, and $L_{3}$ are linear functionals on $V$. These functionals are linearly independent; for, suppose

$$
L=c_{1} L_{1}+c_{2} L_{2}+c_{3} L_{3} .
$$

If $L=0$, i.e., if $L(p)=0$ for each $p$ in $V$, then applying $L$ to the particular polynomial 'functions' $1, x, x^{2}$, we obtain

$$
\begin{aligned}
c_{1}+c_{2}+c_{3} & =0 \\
t_{1} c_{1}+t_{2} c_{2}+t_{3} c_{3} & =0 \\
t_{1}^{2} c_{1}+t_{2}^{2} c_{2}+t_{3}^{2} c_{3} & =0
\end{aligned}
$$

From this it follows that $c_{1}=c_{2}=c_{3}=0$, because (as a short computation shows) the matrix

$$
\left[\begin{array}{ccc}
1 & 1 & 1 \\
t_{1} & t_{2} & t_{3} \\
t_{1}^{2} & t_{2}^{2} & t_{3}^{2}
\end{array}\right]
$$

is invertible when $t_{1}, t_{2}$, and $t_{3}$ are distinct. Now the $L_{i}$ are independent, and since $V$ has dimension 3 , these functionals form a basis for $V^{*}$. What is the basis for $V$, of which this is the dual? Such a basis $\left\{p_{1}, p_{2}, p_{3}\right\}$ for $V$ must satisfy

or

$$
L_{i}\left(p_{j}\right)=\delta_{i j}
$$

$$
p_{j}\left(t_{i}\right)=\delta_{i j} .
$$

These polynomial functions are rather easily seen to be 

$$
\begin{aligned}
& p_{1}(x)=\frac{\left(x-t_{2}\right)\left(x-t_{3}\right)}{\left(t_{1}-t_{2}\right)\left(t_{1}-t_{3}\right)} \\
& p_{2}(x)=\frac{\left(x-t_{1}\right)\left(x-t_{3}\right)}{\left(t_{2}-t_{1}\right)\left(t_{2}-t_{3}\right)} \\
& p_{3}(x)=\frac{\left(x-t_{1}\right)\left(x-t_{2}\right)}{\left(t_{3}-t_{1}\right)\left(t_{3}-t_{2}\right)} .
\end{aligned}
$$

The basis $\left\{p_{1}, p_{2}, p_{3}\right\}$ for $V$ is interesting, because according to (3-14) we have for each $p$ in $V$

$$
p=p\left(t_{1}\right) p_{1}+p\left(t_{2}\right) p_{2}+p\left(t_{3}\right) p_{3} .
$$

Thus, if $c_{1}, c_{2}$, and $c_{3}$ are any real numbers, there is exactly one polynomial function $p$ over $R$ which has degree at most 2 and satisfies $p\left(t_{j}\right)=c_{j}, j=$ 1,2 , 3. This polynomial function is $p=c_{1} p_{1}+c_{2} p_{2}+c_{3} p_{3}$.

Now let us discuss the relationship between linear functionals and subspaces. If $f$ is a non-zero linear functional, then the rank of $f$ is 1 because the range of $f$ is a non-zero subspace of the scalar field and must (therefore) be the scalar field. If the underlying space $V$ is finite-dimensional, the rank plus nullity theorem (Theorem 2) tells us that the null space $N_{f}$ has dimension

$$
\operatorname{dim} N_{f}=\operatorname{dim} V-1 .
$$

In a vector space of dimension $n$, a subspace of dimension $n-1$ is called a hyperspace. Such spaces are sometimes called hyperplanes or subspaces of codimension 1. Is every hyperspace the null space of a linear functional? The answer is easily seen to be yes. It is not much more difficult to show that each $d$-dimensional subspace of an $n$-dimensional space is the intersection of the null spaces of $(n-d)$ linear functionals (Theorem 16 below).

Definition. If $\mathrm{V}$ is a vector space over the field $\mathrm{F}$ and $\mathrm{S}$ is a subset of $\mathrm{V}$, the annihilator of $\mathrm{S}$ is the set $\mathrm{S}^{0}$ of linear functionals $\mathrm{f}$ on $\mathrm{V}$ such that $\mathrm{f}(\alpha)=0$ for every $\alpha$ in $\mathrm{S}$.

It should be clear to the reader that $S^{0}$ is a subspace of $V^{*}$, whether $S$ is a subspace of $V$ or not. If $S$ is the set consisting of the zero vector alone, then $S^{0}=V^{*}$. If $S=V$, then $S^{\bullet}$ is the zero subspace of $V^{*}$. (This is easy to see when $V$ is finite-dimensional.)

Theorem 16. Let $\mathrm{V}$ be a finite-dimensional vector space over the field $\mathrm{F}$, and let $\mathrm{W}$ be a subspace of $\mathrm{V}$. Then

$$
\operatorname{dim} \mathrm{W}+\operatorname{dim} \mathrm{W}^{0}=\operatorname{dim} \mathrm{V} .
$$

Proof. Let $k$ be the dimension of $W$ and $\left\{\alpha_{1}, \ldots, \alpha_{k}\right\}$ a basis for $W$. Choose vectors $\alpha_{k+1}, \ldots, \alpha_{n}$ in $V$ such that $\left\{\alpha_{1}, \ldots, \alpha_{n}\right\}$ is a basis for $V$. Let $\left\{f_{1}, \ldots, f_{n}\right\}$ be the basis for $V^{*}$ which is dual to this basis for $V$. The claim is that $\left\{f_{k+1}, \ldots, f_{n}\right\}$ is a basis for the annihilator $W^{0}$. Certainly $f_{i}$ belongs to $W^{0}$ for $i \geq k+1$, because

$$
f_{i}\left(\alpha_{j}\right)=\delta_{i j}
$$

and $\delta_{i j}=0$ if $i \geq k+1$ and $j \leq k$; from this it follows that, for $i \geq k+1$, $f_{i}(\alpha)=0$ whenever $\alpha$ is a linear combination of $\alpha_{1}, \ldots, \alpha_{k}$. The functionals $f_{k+1}, \ldots, f_{n}$ are independent, so all we must show is that they span $W^{0}$. Suppose $f$ is in $V^{*}$. Now

$$
f=\sum_{i=1}^{n} f\left(\alpha_{i}\right) f_{i}
$$

so that if $f$ is in $W^{\circ}$ we have $f\left(\alpha_{i}\right)=0$ for $i \leq k$ and

$$
f=\sum_{i=k+1}^{n} f\left(\alpha_{i}\right) f_{i} .
$$

We have shown that if $\operatorname{dim} W=k$ and $\operatorname{dim} V=n$ then $\operatorname{dim} W^{\bullet}=$ $n-k$.

Corollary. If $\mathrm{W}$ is a $\mathrm{k}$-dimensional subspace of an $\mathrm{n}$-dimensional vector space $\mathrm{V}$, then $\mathrm{W}$ is the intersection of $(\mathrm{n}-\mathrm{k})$ hyperspaces in $\mathrm{V}$.

Proof. This is a corollary of the proof of Theorem 16 rather than its statement. In the notation of the proof, $W$ is exactly the set of vectors $\alpha$ such that $f_{i}(\alpha)=0, i=k+1, \ldots, n$. In case $k=n-1, W$ is the null space of $f_{n}$.

Corollary. If $\mathrm{W}_{1}$ and $\mathrm{W}_{2}$ are subspaces of a finite-dimensional vector space, then $\mathrm{W}_{1}=\mathrm{W}_{2}$ if and only if $\mathrm{W}_{1}^{0}=\mathrm{W}_{2}^{0}$.

Proof. If $W_{1}=W_{2}$, then of course $W_{1}^{0}=W_{2}^{0}$. If $W_{1} \neq W_{2}$, then one of the two subspaces contains a vector which is not in the other. Suppose there is a vector $\alpha$ which is in $W_{2}$ but not in $W_{1}$. By the previous corollaries (or the proof of Theorem 16) there is a linear functional $f$ such that $f(\beta)=0$ for all $\beta$ in $W$, but $f(\alpha) \neq 0$. Then $f$ is in $W_{1}^{0}$ but not in $W_{2}^{0}$ and $W_{1}^{0} \neq W_{2}^{0}$.

In the next section we shall give different proofs for these two corollaries. The first corollary says that, if we select some ordered basis for the space, each $k$-dimensional subspace can be described by specif ying $(n-k)$ homogeneous linear conditions on the coordinates relative to that basis.

Let us look briefly at systems of homogeneous linear equations from the point of view of linear functionals. Suppose we have a system of linear equations,

$$
\begin{gathered}
A_{11} x_{1}+\cdots+A_{1 n} x_{n}=0 \\
\vdots \\
A_{m 1} x_{1}+\cdots+A_{m n} x_{n}=0
\end{gathered}
$$

for which we wish to find the solutions. If we let $f_{i}, i=1, \ldots, m$, be the linear functional on $F^{n}$ defined by

$$
f_{i}\left(x_{1}, \ldots, x_{n}\right)=A_{i 1} x_{1}+\cdots+A_{i n} x_{n}
$$

then we are seeking the subspace of $F^{n}$ of all $\alpha$ such that

$$
f_{i}(\alpha)=0, \quad i=1, \ldots, m .
$$

In other words, we are seeking the subspace annihilated by $f_{1}, \ldots, f_{m}$. Row-reduction of the coefficient matrix provides us with a systematic method of finding this subspace. The $n$-tuple $\left(A_{i 1}, \ldots, A_{i n}\right)$ gives the coordinates of the linear functional $f_{i}$ relative to the basis which is dual to the standard basis for $F^{n}$. The row space of the coefficient matrix may thus be regarded as the space of linear functionals spanned by $f_{1}, \ldots, f_{m}$. The solution space is the subspace annihilated by this space of functionals.

Now one may look at the system of equations from the 'dual' point of view. That is, suppose that we are given $m$ vectors in $F^{n}$

$$
\alpha_{i}=\left(A_{i 1}, \ldots, A_{i n}\right)
$$

and we wish to find the annihilator of the subspace spanned by these vectors. Since a typical linear functional on $F^{n}$ has the form

$$
f\left(x_{1}, \ldots, x_{n}\right)=c_{1} x_{1}+\cdots+c_{n} x_{n}
$$

the condition that $f$ be in this annihilator is that

$$
\sum_{j=1}^{n} A_{i j} c_{j}=0, \quad i=1, \ldots, m
$$

that is, that $\left(c_{1}, \ldots, c_{n}\right)$ be a solution of the system $A X=0$. From this point of view, row-reduction gives us a systematic method of finding the annihilator of the subspace spanned by a given finite set of vectors in $F^{n}$.

EXAMPLE 23. Here are three linear functionals on $R^{4}$ :

$$
\begin{aligned}
& f_{1}\left(x_{1}, x_{2}, x_{3}, x_{4}\right)=x_{1}+2 x_{2}+2 x_{3}+x_{4} \\
& f_{2}\left(x_{1}, x_{2}, x_{3}, x_{4}\right)=2 x_{2}+x_{4} \\
& f_{3}\left(x_{1}, x_{2}, x_{3}, x_{4}\right)=-2 x_{1}-4 x_{3}+3 x_{4} .
\end{aligned}
$$

The subspace which they annihilate may be found explicitly by finding the row-reduced echelon form of the matrix

$$
A=\left[\begin{array}{rrrr}
1 & 2 & 2 & 1 \\
0 & 2 & 0 & 1 \\
-2 & 0 & -4 & 3
\end{array}\right]
$$

A short calculation, or a peek at Example 21 of Chapter 2, shows that

$$
R=\left[\begin{array}{llll}
1 & 0 & 2 & 0 \\
0 & 1 & 0 & 0 \\
0 & 0 & 0 & 1
\end{array}\right]
$$

Therefore, the linear functionals

$$
\begin{aligned}
& g_{1}\left(x_{1}, x_{2}, x_{3}, x_{4}\right)=x_{1}+2 x_{3} \\
& g_{2}\left(x_{1}, x_{2}, x_{3}, x_{4}\right)=x_{2} \\
& g_{3}\left(x_{1}, x_{2}, x_{3}, x_{4}\right)=x_{4}
\end{aligned}
$$

span the same subspace of $\left(R^{4}\right)^{*}$ and annihilate the same subspace of $R^{4}$ as do $f_{1}, f_{2}, f_{3}$. The subspace annihilated consists of the vectors with

$$
\begin{aligned}
& x_{1}=-2 x_{3} \\
& x_{2}=x_{4}=0 .
\end{aligned}
$$

EXample 24. Let $W$ be the subspace of $R^{5}$ which is spanned by the vectors

$$
\begin{array}{llrl}
\alpha_{1} & =(2,-2,3,4,-1), & \alpha_{3} & =(0,0,-1,-2,3) \\
\alpha_{2} & =(-1,1,2,5,2), & \alpha_{4} & =(1,-1,2,3,0) .
\end{array}
$$

How does one describe $W^{0}$, the annihilator of $W$ ? Let us form the $4 \times 5$ matrix $A$ with row vectors $\alpha_{1}, \alpha_{2}, \alpha_{3}, \alpha_{4}$, and find the row-reduced echelon matrix $R$ which is row-equivalent to $A$ :

$$
A=\left[\begin{array}{rrrrr}
2 & -2 & 3 & 4 & -1 \\
-1 & 1 & 2 & 5 & 2 \\
0 & 0 & -1 & -2 & 3 \\
1 & -1 & 2 & 3 & 0
\end{array}\right] \rightarrow R=\left[\begin{array}{rrrrr}
1 & -1 & 0 & -1 & 0 \\
0 & 0 & 1 & 2 & 0 \\
0 & 0 & 0 & 0 & 1 \\
0 & 0 & 0 & 0 & 0
\end{array}\right]
$$

If $f$ is a linear functional on $R^{5}$ :

$$
f\left(x_{1}, \ldots, x_{5}\right)=\sum_{j=1}^{5} c_{j} x_{j}
$$

then $f$ is in $W^{0}$ if and only if $f\left(\alpha_{i}\right)=0, i=1,2,3,4$, i.e., if and only if

$$
\sum_{j=1}^{5} A_{i j} c_{j}=0, \quad 1 \leq i \leq 4
$$

This is equivalent to

$$
\sum_{j=1}^{5} R_{i j} c_{j}=0, \quad 1 \leq i \leq 3
$$

or

$$
\begin{aligned}
c_{1}-c_{2}-c_{4} & =0 \\
c_{3}+2 c_{4} & =0 \\
c_{5} & =0 .
\end{aligned}
$$

We obtain all such linear functionals $f$ by assigning arbitrary values to $c_{2}$ and $c_{4}$, say $c_{2}=a$ and $c_{4}=b$, and then finding the corresponding $c_{1}=$ $a+b, c_{3}=-2 b, c_{5}=0$. So $W^{0}$ consists of all linear functionals $f$ of the form

$$
f\left(x_{1}, x_{2}, x_{3}, x_{4}, x_{5}\right)=(a+b) x_{1}+a x_{2}-2 b x_{3}+b x_{4}
$$

The dimension of $W^{\circ}$ is 2 and a basis $\left\{f_{1}, f_{2}\right\}$ for $W^{0}$ can be found by first taking $a=1, b=0$ and then $a=0, b=1$ :

$$
\begin{aligned}
& f_{1}\left(x_{1}, \ldots, x_{5}\right)=x_{1}+x_{2} \\
& f_{2}\left(x_{1}, \ldots, x_{5}\right)=x_{1}-2 x_{3}+x_{4} .
\end{aligned}
$$

The above general $f$ in $W^{0}$ is $f=a f_{1}+b f_{2}$.

\section{Exercises}

1. In $R^{3}$, let $\alpha_{1}=(1,0,1), \alpha_{2}=(0,1,-2), \alpha_{3}=(-1,-1,0)$.

(a) If $f$ is a linear functional on $R^{3}$ such that

$$
f\left(\alpha_{1}\right)=1, \quad f\left(\alpha_{2}\right)=-1, \quad f\left(\alpha_{3}\right)=3,
$$

and if $\alpha=(\boldsymbol{a}, b, c)$, find $f(\alpha)$.

(b) Describe explicitly a linear functional $f$ on $R^{3}$ such that

$$
f\left(\alpha_{1}\right)=f\left(\alpha_{2}\right)=0 \text { but } f\left(\alpha_{3}\right) \neq 0 .
$$

(c) Let $f$ be any linear functional such that

$$
f\left(\alpha_{1}\right)=f\left(\alpha_{2}\right)=0 \text { and } f\left(\alpha_{3}\right) \neq 0 .
$$

If $\alpha=(2,3,-1)$, show that $f(\alpha) \neq 0$.

2. Let $B=\left\{\alpha_{1}, \alpha_{2}, \alpha_{3}\right\}$ be the basis for $C^{3}$ defined by

$$
\alpha_{1}=(1,0,-1), \quad \alpha_{2}=(1,1,1), \quad \alpha_{3}=(2,2,0) .
$$

Find the dual basis of $\Theta$.

3. If $A$ and $B$ are $n \times n$ matrices over the field $F$, show that trace $(A B)=\operatorname{trace}$ $(B A)$. Now show that similar matrices have the same trace.

4. Let $V$ be the vector space of all polynomial functions $p$ from $R$ into $R$ which have degree 2 or less:

$$
p(x)=c_{0}+c_{1} x+c_{2} x^{2} .
$$

Define three linear functionals on $V$ by

$$
f_{1}(p)=\int_{0}^{1} p(x) d x, \quad f_{2}(p)=\int_{0}^{2} p(x) d x, \quad f_{3}(p)=\int_{0}^{-1} p(x) d x .
$$

Show that $\left\{f_{1}, f_{2}, f_{3}\right\}$ is a basis for $V^{*}$ by exhibiting the basis for $V$ of which it is the dual.

5. If $A$ and $B$ are $n \times n$ complex matrices, show that $A B-B A=I$ is impossible.

6. Let $m$ and $n$ be positive integers and $F$ a field. Let $f_{1}, \ldots, f_{m}$ be linear functionals on $F^{n}$. For $\alpha$ in $F^{n}$ define

$$
T \alpha=\left(f_{1}(\alpha), \ldots, f_{m}(\alpha)\right) .
$$

Show that $T$ is a linear transformation from $F^{n}$ into $F^{m}$. Then show that every linear transformation from $F^{n}$ into $F^{m}$ is of the above form, for some $f_{1}, \ldots, f_{m}$. 7. Let $\alpha_{1}=(1,0,-1,2)$ and $\alpha_{2}=(2,3,1,1)$, and let $W$ be the subspace of $R^{4}$ spanned by $\alpha_{1}$ and $\alpha_{2}$. Which linear functionals $f$ : 

$$
f\left(x_{1}, x_{2}, x_{3}, x_{4}\right)=c_{1} x_{1}+c_{2} x_{2}+c_{3} x_{3}+c_{4} x_{4}
$$

are in the annihilator of $W$ ?

8. Let $W$ be the subspace of $R^{5}$ which is spanned by the vectors

$$
\begin{aligned}
& \alpha_{1}=\epsilon_{1}+2 \epsilon_{2}+\epsilon_{3}, \quad \alpha_{2}=\epsilon_{2}+3 \epsilon_{3}+3 \epsilon_{4}+\epsilon_{5} \\
& \alpha_{3}=\epsilon_{1}+4 \epsilon_{2}+6 \epsilon_{3}+4 \epsilon_{4}+\epsilon_{5 .}
\end{aligned}
$$

Find a basis for $W^{0}$.

9. Let $V$ be the vector space of all $2 \times 2$ matrices over the field of real numbers, and let

$$
B=\left[\begin{array}{rr}
2 & -2 \\
-1 & 1
\end{array}\right] \text {. }
$$

Let $W$ be the subspace of $V$ consisting of all $A$ such that $A B=0$. Let $f$ be a linear functional on $V$ which is in the annihilator of $W$. Suppose that $f(I)=0$ and $f(C)=3$, where $I$ is the $2 \times 2$ identity matrix and

$$
C=\left[\begin{array}{ll}
0 & 0 \\
0 & 1
\end{array}\right] \text {. }
$$

Find $f(B)$.

10. Let $F$ be a subfield of the complex numbers. We define $n$ linear functionals on $F^{n}(n \geq 2)$ by

$$
f_{k}\left(x_{1}, \ldots, x_{n}\right)=\sum_{j=1}^{n}(k-j) x_{j}, \quad 1 \leq k \leq n .
$$

What is the dimension of the subspace annihilated by $f_{1}, \ldots, f_{n}$ ?

11. Let $W_{1}$ and $W_{2}$ be subspaces of a finite-dimensional vector space $V$.

(a) Prove that $\left(W_{1}+W_{2}\right)^{0}=W_{1}^{\bullet} \cap W_{2}^{0}$.

(b) Prove that $\left(W_{1} \cap W_{2}\right)^{0}=W_{1}^{0}+W_{2}^{0}$.

12. Let $V$ be a finite-dimensional vector space over the field $F$ and let $W$ be a subspace of $V$. If $f$ is a linear functional on $W$, prove that there is a linear functional $g$ on $V$ such that $g(\alpha)=f(\alpha)$ for each $\alpha$ in the subspace $W$.

13. Let $F$ be a subfield of the field of complex numbers and let $V$ be any vector space over $F$. Suppose that $f$ and $g$ are linear functionals on $V$ such that the function $h$ defined by $h(\alpha)=f(\alpha) g(\alpha)$ is also a linear functional on $V$. Prove that either $f=0$ or $g=0$.

14. Let $F$ be a field of characteristic zero and let $V$ be a finite-dimensional vector space over $F$. If $\alpha_{1}, \ldots, \alpha_{m}$ are finitely many vectors in $V$, each different from the zero vector, prove that there is a linear functional $f$ on $V$ such that

$$
f\left(\alpha_{i}\right) \neq 0, \quad i=1, \ldots, m .
$$

15. According to Exercise 3, similar matrices have the same trace. Thus we can define the trace of a linear operator on a finite-dimensional space to be the trace of any matrix which represents the operator in an ordered basis. This is welldefined since all such representing matrices for one operator are similar.

Now let $V$ be the space of all $2 \times 2$ matrices over the field $F$ and let $P$ be a fixed $2 \times 2$ matrix. Let $T$ be the linear operator on $V$ defined by $T(A)=P A$. Prove that trace $(T)=2$ trace $(P)$. 16. Show that the trace functional on $n \times n$ matrices is unique in the following sense. If $W$ is the space of $n \times n$ matrices over the field $F$ and if $f$ is a linear functional on $W$ such that $f(A B)=f(B A)$ for each $A$ and $B$ in $W$, then $f$ is a scalar multiple of the trace function. If, in addition, $f(I)=n$, then $f$ is the trace function.

17. Let $W$ be the space of $n \times n$ matrices over the field $F$, and let $W_{0}$ be the subspace spanned by the matrices $C$ of the form $C=A B-B A$. Prove that $W_{0}$ is exactly the subspace of matrices which have trace zero. (Hint: What is the dimension of the space of matrices of trace zero? Use the matrix 'units,' i.e., matrices with exactly one non-zero entry, to construct enough linearly independent matrices of the form $A B-B A$.)

\subsection{The Double Dual}

One question about dual bases which we did not answer in the last section was whether every basis for $V^{*}$ is the dual of some basis for $V$. One way to answer that question is to consider $V^{* *}$, the dual space of $V^{*}$.

If $\alpha$ is a vector in $V$, then $\alpha$ induces a linear functional $L_{\alpha}$ on $V^{*}$ defined by

$$
L_{\alpha}(f)=f(\alpha), \quad f \text { in } V^{*} \text {. }
$$

The fact that $L_{\alpha}$ is linear is just a reformulation of the definition of linear operations in $V^{*}$ :

$$
\begin{aligned}
L_{\alpha}(c f+g) & =(c f+g)(\alpha) \\
& =(c f)(\alpha)+g(\alpha) \\
& =c f(\alpha)+g(\alpha) \\
& =c L_{\alpha}(f)+L_{\alpha}(g) .
\end{aligned}
$$

If $V$ is finite-dimensional and $\alpha \neq 0$, then $L_{\alpha} \neq 0$; in other words, there exists a linear functional $f$ such that $f(\alpha) \neq 0$. The proof is very simple and was given in Section 3.5: Choose an ordered basis $B=\left\{\alpha_{1}, \ldots, \alpha_{n}\right\}$ for $V$ such that $\alpha_{1}=\alpha$ and let $f$ be the linear functional which assigns to each vector in $V$ its first coordinate in the ordered basis $B$.

Theorem 17. Let $\mathrm{V}$ be a finite-dimensional vector space over the field $\mathrm{F}$. For each vector $\alpha$ in $\mathrm{V}$ define

$$
\mathrm{L}_{\alpha}(\mathrm{f})=\mathrm{f}(\alpha), \quad \mathrm{f} \text { in } \mathrm{V}^{*} \text {. }
$$

The mapping $\alpha \rightarrow \mathrm{L}_{\alpha}$ is then an isomorphism of $\mathrm{V}$ onto $\mathrm{V}^{* *}$.

Proof. We showed that for each $\alpha$ the function $L_{\alpha}$ is linear. Suppose $\alpha$ and $\beta$ are in $V$ and $c$ is in $F$, and let $\gamma=c \alpha+\beta$. Then for each $f$ in $V^{*}$

$$
\begin{aligned}
L_{\gamma}(f) & =f(\gamma) \\
& =f(c \alpha+\beta) \\
& =c f(\alpha)+f(\beta) \\
& =c L_{\alpha}(f)+L_{\beta}(f)
\end{aligned}
$$

and so

$$
L_{\gamma}=c L_{\alpha}+L_{\beta}
$$

This shows that the mapping $\alpha \rightarrow L_{\alpha}$ is a linear transformation from $V$ into $V^{* *}$. This transformation is non-singular; for, according to the remarks above $L_{\alpha}=0$ if and only if $\alpha=0$. Now $\alpha \rightarrow L_{\alpha}$ is a non-singular linear transformation from $V$ into $V^{* *}$, and since

$$
\operatorname{dim} V^{* *}=\operatorname{dim} V^{*}=\operatorname{dim} V
$$

Theorem 9 tells us that this transformation is invertible, and is therefore an isomorphism of $V$ onto $V^{* *}$.

Corollary. Let $\mathrm{V}$ be a finite-dimensional vector space over the field $\mathrm{F}$. If $\mathrm{L}$ is a linear functional on the dual space $\mathrm{V}^{*}$ of $\mathrm{V}$, then there is a unique vector $\alpha$ in $\mathrm{V}$ such that

$$
L(f)=f(\alpha)
$$

for every $\mathrm{f}$ in $\mathrm{V}^{*}$.

Corollary. Let $\mathrm{V}$ be a finite-dimensional vector space over the field $\mathrm{F}$. Each basis for $\mathrm{V}^{*}$ is the dual of some basis for $\mathrm{V}$.

Proof. Let $a^{*}=\left\{f_{1}, \ldots, f_{n}\right\}$ be a basis for $V^{*}$. By Theorem 15 , there is a basis $\left\{L_{1}, \ldots, L_{n}\right\}$ for $V^{* *}$ such that

$$
L_{i}\left(f_{j}\right)=\delta_{i j} .
$$

Using the corollary above, for each $i$ there is a vector $\alpha_{i}$ in $V$ such that

$$
L_{i}(f)=f\left(\alpha_{i}\right)
$$

for every $f$ in $V^{*}$, i.e., such that $L_{i}=L_{\alpha_{i}}$. It follows immediately that $\left\{\alpha_{1}, \ldots, \alpha_{n}\right\}$ is a basis for $V$ and that $Q^{*}$ is the dual of this basis.

In view of Theorem 17 , we usually identify $\alpha$ with $L_{\alpha}$ and say that $V$ 'is' the dual space of $V^{*}$ or that the spaces $V, V^{*}$ are naturally in duality with one another. Each is the dual space of the other. In the last corollary we have an illustration of how that can be useful. Here is a further illustration.

If $E$ is a subset of $V^{*}$, then the annihilator $E^{0}$ is (technically) a subset of $V^{* *}$. If we choose to identify $V$ and $V^{* *}$ as in Theorem 17 , then $E^{0}$ is a subspace of $V$, namely, the set of all $\alpha$ in $V$ such that $f(\alpha)=0$ for all $f$ in $E$. In a corollary of Theorem 16 we noted that each subspace $W$ is determined by its annihilator $W^{0}$. How is it determined? The answer is that $W$ is the subspace annihilated by all $f$ in $W^{0}$, that is, the intersection of the null spaces of all $f$ 's in $W^{0}$. In our present notation for annihilators, the answer may be phrased very simply: $W=\left(W^{0}\right)^{0}$.

Theorem 18. If $\mathrm{S}$ is any subset of a finite-dimensional vector space $\mathrm{V}$, then $\left(\mathrm{S}^{0}\right)^{0}$ is the subspace spanned by $\mathrm{S}$. Proof. Let $W$ be the subspace spanned by $S$. Clearly $W^{0}=S^{0}$. Therefore, what we are to prove is that $W=W^{00}$. We have given one proof. Here is another. By Theorem 16

$$
\begin{aligned}
\operatorname{dim} W+\operatorname{dim} W^{0} & =\operatorname{dim} V \\
\operatorname{dim} W^{0}+\operatorname{dim} W^{00} & =\operatorname{dim} V^{*}
\end{aligned}
$$

and since $\operatorname{dim} V=\operatorname{dim} V^{*}$ we have

$$
\operatorname{dim} W=\operatorname{dim} W^{00} \text {. }
$$

Since $W$ is a subspace of $W^{00}$, we see that $W=W^{00}$.

The results of this section hold for arbitrary vector spaces; however, the proofs require the use of the so-called Axiom of Choice. We want to avoid becoming embroiled in a lengthy discussion of that axiom, so we shall not tackle annihilators for general vector spaces. But, there are two results about linear functionals on arbitrary vector spaces which are so fundamental that we should include them.

Let $V$ be a vector space. We want to define hyperspaces in $V$. Unless $V$ is finite-dimensional, we cannot do that with the dimension of the hyperspace. But, we can express the idea that a space $N$ falls just one dimension short of filling out $V$, in the following way:

1. $N$ is a proper subspace of $V$;

2. if $W$ is a subspace of $V$ which contains $N$, then either $W=N$ or $W=V$.

Conditions (1) and (2) together say that $N$ is a proper subspace and there is no larger proper subspace, in short, $N$ is a maximal proper subspace.

Definition. If $\mathrm{V}$ is a vector space, a hyperspace in $\mathrm{V}$ is a maximal proper subspace of $\mathrm{V}$.

Theorem 19. If $\mathrm{f}$ is a non-zero linear functional on the vector space $\mathrm{V}$, then the null space of $\mathrm{f}$ is a hyperspace in $\mathrm{V}$. Conversely, every hyperspace in $\mathrm{V}$ is the null space of a (not unique) non-zero linear functional on V.

Proof. Let $f$ be a non-zero linear functional on $V$ and $N_{f}$ its null space. Let $\alpha$ be a vector in $V$ which is not in $N_{f}$, i.e., a vector such that $f(\alpha) \neq 0$. We shall show that every vector in $V$ is in the subspace spanned by $N_{f}$ and $\alpha$. That subspace consists of all vectors

$$
\boldsymbol{\gamma}+c \alpha, \quad \boldsymbol{\gamma} \text { in } N_{f}, c \text { in } F .
$$

Let $\beta$ be in $V$. Define

$$
c=\frac{f(\beta)}{f(\alpha)}
$$

which makes sense because $f(\alpha) \neq 0$. Then the vector $\gamma=\beta-c \alpha$ is in $N_{\zeta}$ since

$$
\begin{aligned}
f(\gamma) & =f(\beta-c \alpha) \\
& =f(\beta)-c f(\alpha) \\
& =0 .
\end{aligned}
$$

So $\beta$ is in the subspace spanned by $N_{f}$ and $\alpha$.

Now let $N$ be a hyperspace in $V$. Fix some vector $\alpha$ which is not in $N$. Since $N$ is a maximal proper subspace, the subspace spanned by $N$ and $\alpha$ is the entire space $V$. Therefore each vector $\beta$ in $V$ has the form

$$
\beta=\gamma+c \alpha, \quad \gamma \text { in } N, c \text { in } F .
$$

The vector $\gamma$ and the scalar $c$ are uniquely determined by $\beta$. If we have also then

$$
\beta=\gamma^{\prime}+c^{\prime} \alpha, \quad \gamma^{\prime} \text { in } N, c^{\prime} \text { in } F .
$$

$$
\left(c^{\prime}-c\right) \alpha=\gamma-\gamma^{\prime} .
$$

If $c^{\prime}-c \neq 0$, then $\alpha$ would be in $N$; hence, $c^{\prime}=c$ and $\gamma^{\prime}=\gamma$. Another way to phrase our conclusion is this: If $\beta$ is in $V$, there is a unique scalar $c$ such that $\beta-c \alpha$ is in $N$. Call that scalar $g(\beta)$. It is easy to see that $g$ is a linear functional on $V$ and that $N$ is the null space of $g$.

Lemma. If $\mathrm{f}$ and $\mathrm{g}$ are linear functionals on a vector space $\mathrm{V}$, then $\mathrm{g}$ is a scalar multiple of $\mathrm{f}$ if and only if the null space of $\mathrm{g}$ contains the null space of $\mathrm{f}$, that is, if and only if $\mathrm{f}(\alpha)=0$ implies $\mathrm{g}(\alpha)=0$.

Proof. If $f=0$ then $g=0$ as well and $g$ is trivially a scalar multiple of $f$. Suppose $f \neq 0$ so that the null space $N_{f}$ is a hyperspace in $V$. Choose some vector $\alpha$ in $V$ with $f(\alpha) \neq 0$ and let

$$
c=\frac{g(\alpha)}{f(\alpha)} .
$$

The linear functional $h=g-\mathrm{cf}$ is 0 on $N_{f}$, since both $f$ and $g$ are 0 there, and $h(\alpha)=g(\alpha)-c f(\alpha)=0$. Thus $h$ is 0 on the subspace spanned by $N_{f}$ and $\alpha$-and that subspace is $V$. We conclude that $h=0$, i.e., that $g=$ $c f$.

Theorem 20. Let $\mathrm{g}, \mathrm{f}_{1}, \ldots, \mathrm{f}_{\mathrm{r}}$ be linear functionals on a vector space $\mathrm{V}$ with respective null spaces $\mathrm{N}, \mathrm{N}_{1}, \ldots, \mathrm{N}_{\mathrm{r}}$. Then $\mathrm{g}$ is a linear combination of $\mathrm{f}_{1}, \ldots, \mathrm{f}_{\mathrm{r}}$ if and only if $\mathrm{N}$ contains the intersection $\mathrm{N}_{1} \cap \cdots \cap \mathrm{N}_{\mathrm{r}}$.

Proof. If $g=c_{1} f_{1}+\cdots+c_{r} f_{r}$ and $f_{i}(\alpha)=0$ for each $i$, then clearly $g(\alpha)=0$. Therefore, $N$ contains $N_{1} \cap \cdots \cap N_{r}$.

We shall prove the converse (the 'if' half of the theorem) by induction on the number $r$. The preceding lemma handles the case $r=1$. Suppose we know the result for $r=k-1$, and let $f_{1}, \ldots, f_{k}$ be linear functionals with null spaces $N_{1}, \ldots, N_{k}$ such that $N_{1} \cap \cdots \cap N_{k}$ is contained in $N$, the null space of $g$. Let $g^{\prime}, f_{1}^{\prime}, \ldots, f_{k-1}^{\prime}$ be the restrictions of $g, f_{1}, \ldots, f_{k-1}$ to the subspace $N_{k}$. Then $g^{\prime}, f_{1}^{\prime}, \ldots, f_{k-1}^{\prime}$ are linear functionals on the vector space $N_{k}$. Furthermore, if $\alpha$ is a vector in $N_{k}$ and $f_{i}^{\prime}(\alpha)=0, i=1, \ldots$, $k-1$, then $\alpha$ is in $N_{1} \cap \cdots \cap N_{k}$ and so $g^{\prime}(\alpha)=0$. By the induction hypothesis (the case $r=k-1$ ), there are scalars $c_{i}$ such that

Now let

$$
g^{\prime}=c_{1} f_{1}^{\prime}+\cdots+c_{k-1} f_{k-1}^{\prime} .
$$

$$
h=g-\sum_{i=1}^{k-1} c_{i} f_{i} .
$$

Then $h$ is a linear functional on $V$ and (3-16) tells us that $h(\boldsymbol{\alpha})=0$ for every $\alpha$ in $N_{k}$. By the preceding lemma, $h$ is a scalar multiple of $f_{k}$. If $h=$ $c_{k} f_{k}$, then

$$
g=\sum_{i=1}^{k} c_{i} f_{i}
$$

\section{Exercises}

1. Let $n$ be a positive integer and $F$ a field. Let $W$ be the set of all vectors $\left(x_{1}, \ldots, x_{n}\right)$ in $F^{n}$ such that $x_{1}+\cdots+x_{n}=0$.

(a) Prove that $W^{\bullet}$ consists of all linear functionals $f$ of the form

$$
f\left(x_{1}, \ldots, x_{n}\right)=c \sum_{j=1}^{n} x_{j} .
$$

(b) Show that the dual space $W^{*}$ of $W$ can be 'naturally' identified with the linear functionals

$$
f\left(x_{1}, \ldots, x_{n}\right)=c_{1} x_{1}+\cdots+c_{n} x_{n}
$$

on $F^{n}$ which satisfy $c_{1}+\cdots+c_{n}=0$.

2. Use Theorem 20 to prove the following. If $W$ is a subspace of a finite-dimensional vector space $V$ and if $\left\{g_{1}, \ldots, g_{r}\right\}$ is any basis for $W^{0}$, then

$$
W=\bigcap_{i=1}^{r} N_{\theta_{i}} .
$$

3. Let $S$ be a set, $F$ a field, and $V(S ; F)$ the space of all functions from $S$ into $F$ :

$$
\begin{aligned}
(f+g)(x) & =f(x)+g(x) \\
(c f)(x) & =c f(x) .
\end{aligned}
$$

Let $W$ be any $n$-dimensional subspace of $V(S ; F)$. Show that there exist points $x_{1}, \ldots, x_{n}$ in $S$ and functions $f_{1}, \ldots, f_{n}$ in $W$ such that $f_{i}\left(x_{j}\right)=\delta_{i j}$.

\subsection{The Transpose of a Linear Transformation}

Suppose that we have two vector spaces over the field $F, V$, and $W$, and a linear transformation $T$ from $V$ into $W$. Then $T$ induces a linear transformation from $W^{*}$ into $V^{*}$, as follows. Suppose $g$ is a linear functional on $W$, and let

$$
f(\alpha)=g(T \alpha)
$$

for each $\alpha$ in $V$. Then (3-17) defines a function $f$ from $V$ into $F$, namely, the composition of $T$, a function from $V$ into $W$, with $g$, a function from $W$ into $F$. Since both $T$ and $g$ are linear, Theorem 6 tells us that $f$ is also linear, i.e., $f$ is a linear functional on $V$. Thus $T$ provides us with a rule $T^{t}$ which associates with each linear functional $g$ on $W$ a linear functional $f=T^{t} g$ on $V$, defined by (3-17). Note also that $T^{t}$ is actually a linear transformation from $W^{*}$ into $V^{*}$; for, if $g_{1}$ and $g_{2}$ are in $W^{*}$ and $c$ is a scalar

$$
\begin{aligned}
{\left[T^{t}\left(c g_{1}+g_{2}\right)\right](\alpha) } & =\left(c g_{1}+g_{2}\right)(T \alpha) \\
& =c g_{1}\left(T^{\prime} \alpha\right)+g_{2}(T \alpha) \\
& =c\left(T^{t} g_{1}\right)(\alpha)+\left(T^{t} g_{2}\right)(\alpha)
\end{aligned}
$$

so that $T^{t}\left(c g_{1}+g_{2}\right)=c T^{t} g_{1}+T^{t} g_{2}$. Let us summarize.

Theorem 21. Let $\mathrm{V}$ and $\mathrm{W}$ be vector spaces over the field $\mathrm{F}$. For each linear transformation $\mathrm{T}$ from $\mathrm{V}$ into $\mathrm{W}$, there is a unique linear transformation $\mathrm{T}^{\mathrm{t}}$ from $\mathrm{W}^{*}$ into $\mathrm{V}^{*}$ such that

$$
\left(T^{\operatorname{tg}}\right)(\alpha)=\mathrm{g}(\mathrm{T} \alpha)
$$

for every $\mathrm{g}$ in $\mathrm{W}^{*}$ and $\alpha$ in $\mathrm{V}$.

We shall call $T^{t}$ the transpose of $T$. This transformation $T^{t}$ is of ten called the adjoint of $T$; however, we shall not use this terminology.

Theorem 22. Let $\mathrm{V}$ and $\mathrm{W}$ be vector spaces over the field $\mathrm{F}$, and let $\mathrm{T}$ be a linear transformation from $\mathrm{V}$ into $\mathrm{W}$. The null space of $\mathrm{T}^{\mathrm{t}}$ is the annihi lator of the range of $\mathrm{T}$. If $\mathrm{V}$ and $\mathrm{W}$ are finite-dimensional, then

(i) $\operatorname{rank}\left(\mathrm{T}^{t}\right)=\operatorname{rank}(\mathrm{T})$

(ii) the range of $\mathrm{T}^{t}$ is the annihilator of the null space of $\mathrm{T}$.

Proof. If $g$ is in $W^{*}$, then by definition

$$
\left(T^{t} g\right)(\alpha)=g(T \alpha)
$$

for each $\alpha$ in $V$. The statement that $g$ is in the null space of $T^{t}$ means that $g(T \alpha)=0$ for every $\alpha$ in $V$. Thus the null space of $T^{t}$ is precisely the annihilator of the range of $T$.

Suppose that $V$ and $W$ are finite-dimensional, say $\operatorname{dim} V=n$ and $\operatorname{dim} W=m$. For (i) : Let $r$ be the rank of $T$, i.e., the dimension of the range of $T$. By Theorem 16, the annihilator of the range of $T$ then has dimension $(m-r)$. By the first statement of this theorem, the nullity of $T^{t}$ must be $(m-r)$. But then since $T^{t}$ is a linear transformation on an $m$-dimensional space, the rank of $T^{t}$ is $m-(m-r)=r$, and so $T$ and $T^{t}$ have the same rank. For (ii): Let $N$ be the null space of $T$. Every functional in the range of $T^{t}$ is in the annihilator of $N$; for, suppose $f=T^{t} g$ for some $g$ in $W^{*}$; then, if $\alpha$ is in $N$

$$
f(\alpha)=\left(T^{t} g\right)(\alpha)=g(T \alpha)=g(0)=0 .
$$

Now the range of $T^{t}$ is a subspace of the space $N^{0}$, and

$$
\operatorname{dim} N^{0}=n-\operatorname{dim} N=\operatorname{rank}(T)=\operatorname{rank}\left(T^{t}\right)
$$

so that the range of $T^{t}$ must be exactly $N^{0}$.

Theorem 23. Let $\mathrm{V}$ and $\mathrm{W}$ be finite-dimensional vector spaces over the field $\mathrm{F}$. Let $B$ be an ordered basis for $\mathrm{V}$ with dual basis $B^{*}$, and let $B^{\prime}$ be an ordered basis for $\mathrm{W}$ with dual basis $\mathbb{B}^{\prime *}$. Let $\mathrm{T}$ be a linear transformation from $\mathrm{V}$ into $\mathrm{W}$; let $\mathrm{A}$ be the matrix of $\mathrm{T}$ relative to $\mathrm{B}, \mathrm{O}^{\prime}$ and let $\mathrm{B}$ be the matrix of $\mathrm{T}^{t}$ relative to $\mathrm{B}^{\prime *}, \mathrm{~B}^{*}$. Then $\mathrm{B}_{\mathrm{i} \mathrm{j}}=\mathrm{A}_{\mathrm{j} \mathrm{i} \text {. }}$.

Proof. Let

$$
\begin{aligned}
& \mathbb{B}=\left\{\alpha_{1}, \ldots, \alpha_{n}\right\}, \quad \mathbb{B}^{\prime}=\left\{\beta_{1}, \ldots, \beta_{m}\right\}, \\
& Q^{*}=\left\{f_{1}, \ldots, f_{n}\right\}, \quad Q^{\prime *}=\left\{g_{1}, \ldots, g_{m}\right\} .
\end{aligned}
$$

By definition,

$$
\begin{aligned}
T \alpha_{j}=\sum_{i=1}^{m} A_{i j} \beta_{i}, & j=1, \ldots, n \\
T^{t} g_{j}=\sum_{i=1}^{n} B_{i j} f_{i}, & j=1, \ldots, m .
\end{aligned}
$$

On the other hand,

$$
\begin{aligned}
\left(T^{t} g_{j}\right)\left(\alpha_{i}\right) & =g_{j}\left(T_{\alpha_{i}}\right) \\
& =g_{j}\left(\sum_{k=1}^{m} A_{k i} \beta_{k}\right) \\
& =\sum_{k=1}^{m} A_{k i} g_{j}\left(\beta_{k}\right) \\
& =\sum_{k=1}^{m} A_{k i} \delta_{j k} \\
& =A_{j i} .
\end{aligned}
$$

For any linear functional $f$ on $V$

$$
f=\sum_{i=1}^{m} f\left(\alpha_{i}\right) f_{i} .
$$

If we apply this formula to the functional $f=T^{t} g_{j}$ and use the fact that $\left(T^{t} g_{j}\right)\left(\alpha_{i}\right)=A_{j i}$, we have

$$
T^{t} g_{j}=\sum_{i=1}^{n} A_{j i} f_{i}
$$

from which it immediately follows that $B_{i j}=A_{j i}$. Definition. If $\mathrm{A}$ is an $\mathrm{m} \times \mathrm{n}$ matrix over the field $\mathrm{F}$, the transpose of $\mathrm{A}$ is the $\mathrm{n} \times \mathrm{m}$ matrix $\mathrm{A}^{\mathrm{t}}$ defined by $\mathrm{A}_{1 \mathrm{j}}^{\mathrm{t}}=\mathrm{A}_{\mathrm{j} \mathbf{i} \text {. }}$.

Theorem 23 thus states that if $T$ is a linear transformation from $V$ into $W$, the matrix of which in some pair of bases is $A$, then the transpose transformation $T^{t}$ is represented in the dual pair of bases by the transpose matrix $A^{t}$.

Theorem 24. Let $\mathrm{A}$ be any $\mathrm{m} \times \mathrm{n}$ matrix over the field $\mathrm{F}$. Then the row rank of $\mathrm{A}$ is equal to the column rank of $\mathrm{A}$.

Proof. Let $Q$ be the standard ordered basis for $F^{n}$ and $B^{\prime}$ the standard ordered basis for $F^{m}$. Let $T$ be the linear transformation from $F^{n}$ into $F^{m}$ such that the matrix of $T$ relative to the pair $B, B^{\prime}$ is $A$, i.e.,

where

$$
T\left(x_{1}, \ldots, x_{n}\right)=\left(y_{1}, \ldots, y_{m}\right)
$$

$$
y_{i}=\sum_{j=1}^{n} A_{i j} x_{j} .
$$

The column rank of $A$ is the rank of the transformation $T$, because the range of $T$ consists of all $m$-tuples which are linear combinations of the column vectors of $A$.

Relative to the dual bases $Q^{\prime *}$ and $B^{*}$, the transpose mapping $T^{t}$ is represented by the matrix $A^{t}$. Since the columns of $A^{t}$ are the rows of $A$, we see by the same reasoning that the row rank of $A$ (the column rank of $A^{t}$ ) is equal to the rank of $T^{t}$. By Theorem $22, T$ and $T^{t}$ have the same rank, and hence the row $\operatorname{rank}$ of $A$ is equal to the column rank of $A$.

Now we see that if $A$ is an $m \times n$ matrix over $F$ and $T$ is the linear transformation from $F^{n}$ into $F^{m}$ defined above, then

$$
\operatorname{rank}(T)=\operatorname{row} \operatorname{rank}(A)=\operatorname{column} \operatorname{rank}(A)
$$

and we shall call this number simply the $\operatorname{rank}$ of $A$.

Example 25. This example will be of a general nature-more discussion than example. Let $V$ be an $n$-dimensional vector space over the field $F$, and let $T$ be a linear operator on $V$. Suppose $B=\left\{\alpha_{1}, \ldots, \alpha_{n}\right\}$ is an ordered basis for $V$. The matrix of $T$ in the ordered basis $B$ is defined to be the $n \times n$ matrix $A$ such that

$$
T \alpha_{j}=\sum_{j=1}^{n} A_{i j} \alpha_{i}
$$

in other words, $A_{i j}$ is the $i$ th coordinate of the vector $T \alpha_{j}$ in the ordered basis $B$. If $\left\{f_{1}, \ldots, f_{n}\right\}$ is the dual basis of $Q$, this can be stated simply

$$
A_{i j}=f_{i}\left(T \alpha_{j}\right) .
$$

Let us see what happens when we change basis. Suppose

$$
\mathbb{B}^{\prime}=\left\{\alpha_{1}^{\prime}, \ldots, \alpha_{n}^{\prime}\right\}
$$

is another ordered basis for $V$, with dual basis $\left\{f_{1}^{\prime}, \ldots, f_{n}^{\prime}\right\}$. If $B$ is the matrix of $T$ in the ordered basis $Q^{\prime}$, then

$$
B_{i j}=f_{i}^{\prime}\left(T \alpha_{j}^{\prime}\right) .
$$

Let $U$ be the invertible linear operator such that $U \alpha_{j}=\alpha_{j}^{\prime}$. Then the transpose of $U$ is given by $U^{t} f_{i}^{\prime}=f_{i}$. It is easy to verify that since $U$ is invertible, so is $U^{t}$ and $\left(U^{t}\right)^{-1}=\left(U^{-1}\right)^{t}$. Thus $f_{i}^{\prime}=\left(U^{-1}\right)^{t} f_{i}, i=1, \ldots, n$. Therefore,

$$
\begin{aligned}
B_{i j} & =\left[\left(U^{-1}\right)^{t} f_{i}\right]\left(T \alpha_{j}^{\prime}\right) \\
& =f_{i}\left(U^{-1} T \alpha_{j}^{\prime}\right) \\
& =f_{i}\left(U^{-1} T U \alpha_{j}\right) .
\end{aligned}
$$

Now what does this say? Well, $f_{i}\left(U^{-1} T^{\prime} U \alpha_{j}\right)$ is the $i, j$ entry of the matrix of $U^{-1} T U$ in the ordered basis $B$. Our computation above shows that this scalar is also the $i, j$ entry of the matrix of $T$ in the ordered basis $\Omega^{\prime}$. In other words

$$
\begin{aligned}
& [T]_{\mathscr{Q}^{\prime}}=\left[U^{-1} T U\right]_{\mathscr{Q}}
\end{aligned}
$$

and this is precisely the change-of-basis formula which we derived earlier.

\section{Exercises}

1. Let $F$ be a field and let $f$ be the linear functional on $F^{2}$ defined by $f\left(x_{1}, x_{2}\right)=$ $a x_{1}+b x_{2}$. For each of the following linear operators $T$, let $g=T^{t} f$, and find $g\left(x_{1}, x_{2}\right)$.

(a) $T\left(x_{1}, x_{2}\right)=\left(x_{1}, 0\right)$;

(b) $T\left(x_{1}, x_{2}\right)=\left(-x_{2}, x_{1}\right)$;

(c) $T\left(x_{1}, x_{2}\right)=\left(x_{1}-x_{2}, x_{1}+x_{2}\right)$.

2. Let $V$ be the vector space of all polynomial functions over the field of real numbers. Let $a$ and $b$ be fixed real numbers and let $f$ be the linear functional on $V$ defined by

$$
f(p)=\int_{a}^{b} p(x) d x .
$$

If $D$ is the differentiation operator on $V$, what is $D^{t} f$ ?

3. Let $V$ be the space of all $n \times n$ matrices over a field $F$ and let $B$ be a fixed $n \times n$ matrix. If $T$ is the linear operator on $V$ defined by $T(A)=A B-B A$, and if $f$ is the trace function, what is $T^{t} f$ ?

4. Let $V$ be a finite-dimensional vector space over the field $F$ and let $T$ be a linear operator on $V$. Let $c$ be a scalar and suppose there is a non-zero vector $\alpha$ in $V$ such that $T \alpha=c \alpha$. Prove that there is a non-zero linear functional $f$ on $V$ such that $T^{t} f=c f$.

![](https://cdn.mathpix.com/cropped/2023_01_16_0f6afdf132d84aaf6bdeg-124.jpg?height=43&width=288&top_left_y=982&top_left_x=514)

![](https://cdn.mathpix.com/cropped/2023_01_16_0f6afdf132d84aaf6bdeg-124.jpg?height=45&width=274&top_left_y=1028&top_left_x=517)

5. Let $A$ be an $m \times n$ matrix with real entries. Prove that $A=0$ if and only if trace $\left(A^{t} A\right)=0$.

6. Let $n$ be a positive integer and let $V$ be the space of all polynomial functions over the field of real numbers which have degree at most $n$, i.e., functions of the form

$$
f(x)=c_{0}+c_{1} x+\cdots+c_{n} x^{n} .
$$

Let $D$ be the differentiation operator on $V$. Find a basis for the null space of the transpose operator $D^{t}$.

7. Let $V$ be a finite-dimensional vector space over the field $F$. Show that $T \rightarrow T^{t}$ is an isomorphism of $L(V, V)$ onto $L\left(V^{*}, V^{*}\right)$.

8. Let $V$ be the vector space of $n \times n$ matrices over the field $F$.

(a) If $B$ is a fixed $n \times n$ matrix, define a function $f_{B}$ on $V$ by $f_{B}(A)=$ trace $\left(B^{t} A\right)$. Show that $f_{B}$ is a linear functional on $V$.

(b) Show that every linear functional on $V$ is of the above form, i.e., is $f_{B}$ for some $B$.

(c) Show that $B \rightarrow f_{B}$ is an isomorphism of $V$ onto $V^{*}$. 

\section{Polynomials}

\subsection{Algebras}

The purpose of this chapter is to establish a few of the basic properties of the algebra of polynomials over a field. The discussion will be facilitated if we first introduce the concept of a linear algebra over a field.

Definition. Let $\mathrm{F}$ be a field. A linear algebra over the field $\mathrm{F}$ is a vector space $Q$ over $F$ with an additional operation called multiplication of vectors which associates with each pair of vectors $\alpha, \beta$ in $\boldsymbol{a}$ a vector $\alpha \boldsymbol{\beta}$ in Q called the product of $\alpha$ and $\beta$ in such a way that

(a) multiplication is associative,

$$
\alpha(\beta \gamma)=(\alpha \beta) \gamma
$$

(b) multiplication is distributive with respect to addition,

$$
\alpha(\beta+\gamma)=\alpha \beta+\alpha \gamma \text { and }(\alpha+\beta) \gamma=\alpha \gamma+\beta \gamma
$$

(c) for each scalar c in $\mathrm{F}$,

$$
\mathrm{c}(\alpha \beta)=(\mathrm{c} \alpha) \beta=\alpha(\mathrm{c} \beta) .
$$

If there is an element 1 in $Q$ such that $1 \alpha=\alpha 1=\alpha$ for each $\alpha$ in $Q$, we call $Q$ a linear algebra with identity over $F$, and call 1 the identity of $Q$. The algebra $Q$ is called commutative if $\alpha \beta=\beta \alpha$ for all $\alpha$ and $\beta$ in $Q$.

Example 1. The set of $n \times n$ matrices over a field, with the usual operations, is a linear algebra with identity; in particular the field itself is an algebra with identity. This algebra is not commutative if $n \geq 2$. The field itself is (of course) commutative. EXample 2. The space of all linear operators on a vector space, with composition as the product, is a linear algebra with identity. It is commutative if and only if the space is one-dimensionai.

The reader may have had some experience with the dot product and cross product of vectors in $R^{3}$. If so, he should observe that neither of these products is of the type described in the definition of a linear algebra. The dot product is a 'scalar product,' that is, it associates with a pair of vectors a scalar, and thus it is certainly not the type of product we are presently discussing. The cross product does associate a vector with each pair of vectors in $R^{3}$; however, this is not an associative multiplication.

The rest of this section will be devoted to the construction of an algebra which is significantly different from the algebras in either of the preceding examples. Let $F$ be a field and $S$ the set of non-negative integers. By Example 3 of Chapter 2, the set of all functions from $S$ into $F$ is a vector space over $F$. We shall denote this vector space by $F^{\infty}$. The vectors in $F^{\infty}$ are therefore infinite sequences $f=\left(f_{0}, f_{1}, f_{2}, \ldots\right)$ of scalars $f_{i}$ in $F$. If $g=\left(g_{0}, g_{1}, g_{2}, \ldots\right), g_{i}$ in $F$, and $a, b$ are scalars in $F$, af $+b g$ is the infinite sequence given by

$$
a f+b g=\left(a f_{0}+b g_{0}, a f_{1}+b g_{1}, a f_{2}+b g_{2}, \ldots\right) .
$$

We define a product in $F^{\infty}$ by associating with each pair of vectors $f$ and $g$ in $F^{\infty}$ the vector $f g$ which is given by

$$
(f g)_{n}=\sum_{i=0}^{n} f_{i} g_{n-i}, \quad n=0,1,2, \ldots
$$

Thus

and as

$$
f g=\left(f_{0} g_{0}, f_{0} g_{1}+f_{1} g_{0}, f_{0} g_{2}+f_{1} g_{1}+f_{2} g_{0}, \ldots\right)
$$

$$
(g f)_{n}=\sum_{i=0}^{n} g_{i} f_{n-i}=\sum_{i=0}^{n} f_{i} g_{n-i}=(f g)_{n}
$$

for $n=0,1,2, \ldots$, it follows that multiplication is commutative, $f g=g f$. If $h$ also belongs to $F^{\infty}$, then

$$
\begin{aligned}
{[(f g) h]_{n} } & =\sum_{i=0}^{n}(f g)_{i} h_{n-i} \\
& =\sum_{i=0}^{n}\left(\sum_{j=0}^{i} f_{i} g_{i-j}\right) h_{n-i} \\
& =\sum_{i=0}^{n} \sum_{j=0}^{i} f_{i} g_{i-j} h_{n-i} \\
& =\sum_{j=0}^{n} f_{j} \sum_{i=0}^{n-j} g_{i} h_{n-i-j} \\
& =\sum_{j=0}^{n} f_{j}(g h)_{n-j}=[f(g h)]_{n}
\end{aligned}
$$

for $n=0,1,2, \ldots$, so that

$$
(f g) h=f(g h) .
$$

We leave it to the reader to verify that the multiplication defined by (4-2) satisfies (b) and (c) in the definition of a linear algebra, and that the vector $1=(1,0,0, \ldots)$ serves as an identity for $F^{\infty}$. Then $F^{\infty}$, with the operations defined above, is a commutative linear algebra with identity over the field $F$.

The vector $(0,1,0, \ldots, 0, \ldots)$ plays a distinguished role in what follows and we shall consistently denote it by $x$. Throughout this chapter $x$ will never be used to denote an element of the field $F$. The product of $x$ with itself $n$ times will be denoted by $x^{n}$ and we shall put $x^{0}=1$. Then

$$
x^{2}=(0,0,1,0, \ldots), \quad x^{3}=(0,0,0,1,0, \ldots)
$$

and in general for each integer $k \geq 0,\left(x^{k}\right)_{k}=1$ and $\left(x^{k}\right)_{n}=0$ for all nonnegative integers $n \neq k$. In concluding this section we observe that the set consisting of $1, x, x^{2}, \ldots$ is both independent and infinite. Thus the algebra $F^{\infty}$ is not finite-dimensional.

The algebra $F^{\infty}$ is sometimes called the algebra of formal power series over $F$. The element $f=\left(f_{0}, f_{1}, f_{2}, \ldots\right)$ is frequently written

$$
f=\sum_{n=0}^{\infty} f_{n} x^{n} .
$$

This notation is very convenient for dealing with the algebraic operations. When used, it must be remembered that it is purely formal. There are no 'infinite sums' in algebra, and the power series notation (4-4) is not intended to suggest anything about convergence, if the reader knows what that is. By using sequences, we were able to define carefully an algebra in which the operations behave like addition and multiplication of formal power series, without running the risk of confusion over such things as infinite sums.

\subsection{The Algebra of Polynomials}

We are now in a position to define a polynomial over the field $F$.

Definition. Let $\mathrm{F}[\mathrm{x}]$ be the subspace of $\mathrm{F}^{\infty}$ spanned by the vectors $1, \mathrm{x}, \mathrm{x}^{2}, \ldots$. An element of $\mathrm{F}[\mathrm{x}]$ is called a polynomial over $\mathrm{F}$.

Since $F[x]$ consists of all (finite) linear combinations of $x$ and its powers, a non-zero vector $f$ in $F^{\infty}$ is a polynomial if and only if there is an integer $n \geq 0$ such that $f_{n} \neq 0$ and such that $f_{k}=0$ for all integers $k>n$; this integer (when it exists) is obviously unique and is called the degree of $f$. We denote the degree of a polynomial $f$ by $\operatorname{deg} f$, and do not assign a degree to the 0 -polynomial. If $f$ is a non-zero polynomial of degree $n$ it follows that

$$
f=f_{0} x^{0}+f_{1} x+f_{2} x^{2}+\cdots+f_{n} x^{n}, \quad f_{n} \neq 0 .
$$

The scalars $f_{0}, f_{1}, \ldots, f_{n}$ are sometimes called the coefficients of $f$, and we may say that $f$ is a polynomial with coefficients in $F$. We shall call polynomials of the form $c x^{0}$ scalar polynomials, and frequently write $c$ for $c x^{0}$. A non-zero polynomial $f$ of degree $n$ such that $f_{n}=1$ is said to be a monic polynomial.

The reader should note that polynomials are not the same sort of objects as the polynomial functions on $F$ which we have discussed on several occasions. If $F$ contains an infinite number of elements, there is a natural isomorphism between $F[x]$ and the algebra of polynomial functions on $F$. We shall discuss that in the next section. Let us verify that $F[x]$ is an algebra.

Theorem 1. Let $\mathrm{f}$ and $\mathrm{g}$ be non-zero polynomials over $\mathrm{F}$. Then

(i) fg is a non-zero polynomial;

(ii) $\operatorname{deg}(\mathrm{fg})=\operatorname{deg} \mathrm{f}+\operatorname{deg} \mathrm{g}$;

(iii) $\mathrm{fg}$ is a monic polynomial if both $\mathrm{f}$ and $\mathrm{g}$ are monic polynomials;

(iv) $\mathrm{fg}$ is a scalar polynomial if and only if both $\mathrm{f}$ and $\mathrm{g}$ are scalar polynomials;

(v) if $\mathrm{f}+\mathrm{g} \neq 0$

$$
\operatorname{deg}(\mathrm{f}+\mathrm{g}) \leq \max (\operatorname{deg} \mathrm{f}, \operatorname{deg} \mathrm{g}) .
$$

Proof. Suppose $f$ has degree $m$ and that $g$ has degree $n$. If $k$ is a non-negative integer,

$$
(f g)_{m+n+k}=\sum_{i=0}^{m+n+k} f_{i} g_{m+n+k-i} .
$$

In order that $f_{i} g_{m+n+k-i} \neq 0$, it is necessary that $i \leq m$ and $m+n+$ $k-i \leq n$. Hence it is necessary that $m+k \leq i \leq m$, which implies $k=0$ and $i=m$. Thus

$$
(f g)_{m+n}=f_{m} g_{n}
$$

and

$$
(f g)_{m+n+k}=0, \quad k>0 .
$$

The statements (i), (ii), (iii) follow immediately from (4-6) and (4-7), while (iv) is a consequence of (i) and (ii). We leave the verification of (v) to the reader.

Corollary 1. The set of all polynomials over a given field $\mathrm{F}$ equipped with the operations (4-1) and (4-2) is a commutative linear algebra with identity over $\mathrm{F}$. Proof. Since the operations (4-1) and (4-2) are those defined in the algebra $F^{\infty}$ and since $F[x]$ is a subspace of $F^{\infty}$, it suffices to prove that the product of two polynomials is again a polynomial. This is trivial when one of the factors is 0 and otherwise follows from (i).

Corollary 2. Suppose $\mathrm{f}, \mathrm{g}$, and $\mathrm{h}$ are polynomials over the field $\mathrm{F}$ such that $\mathrm{f} \neq 0$ and $\mathrm{fg}=$ fh. Then $\mathrm{g}=\mathrm{h}$.

Proof. Since $f g=f h, f(g-h)=0$, and as $f \neq 0$ it follows at once from (i) that $g-h=0$.

Certain additional facts follow rather easily from the proof of Theorem 1 , and we shall mention some of these.

Suppose

$$
f=\sum_{i=0}^{m} f_{i} x^{i} \quad \text { and } \quad g=\sum_{j=0}^{n} g_{j} x^{i} .
$$

Then from (4-7) we obtain,

$$
f g=\sum_{s=0}^{m+n}\left(\sum_{r=0}^{s} f_{r} g_{s-r}\right) x^{s} .
$$

The reader should verify, in the special case $f=c x^{m}, g=d x^{n}$ with $c, d$ in $F$, that (4-8) reduces to

$$
\left(c x^{m}\right)\left(d x^{n}\right)=c d x^{m+n} .
$$

Now from (4-9) and the distributive laws in $F[x]$, it follows that the product in (4-8) is also given by

$$
\sum_{i, j} f_{i} g_{j} x^{i+j}
$$

where the sum is extended over all integer pairs $i, j$ such that $0 \leq i \leq m$, and $0 \leq j \leq n$.

Definition. Let $\mathrm{Q}$ be a linear algebra with identity over the field $\mathrm{F}$. We shall denote the identity of $Q$ by 1 and make the convention that $\alpha^{0}=1$ for each $\alpha$ in $Q$. Then to each polynomial $\mathrm{f}=\sum_{\mathrm{i}=0}^{n} \mathrm{f}_{\mathrm{i}} \mathrm{x}^{\mathrm{i}}$ over $\mathrm{F}$ and $\alpha$ in $Q$ we associate an element $\mathrm{f}(\alpha)$ in $Q$ by the rule

$$
f(\alpha)=\sum_{i=0}^{n} f_{i} \alpha^{i} .
$$

Example 3. Let $C$ be the field of complex numbers and let $f=x^{2}+2$.

(a) If $Q=C$ and $z$ belongs to $C, f(z)=z^{2}+2$, in particular $f(2)=6$ and

$$
f\left(\frac{1+i}{1-i}\right)=1 \text {. }
$$

(b) If $Q$ is the algebra of all $2 \times 2$ matrices over $C$ and if

then

$$
B=\left[\begin{array}{rr}
1 & 0 \\
-1 & 2
\end{array}\right]
$$

$$
f(B)=2\left[\begin{array}{ll}
1 & 0 \\
0 & 1
\end{array}\right]+\left[\begin{array}{rr}
1 & 0 \\
-1 & 2
\end{array}\right]^{2}=\left[\begin{array}{rr}
3 & 0 \\
-3 & 6
\end{array}\right] \text {. }
$$

(c) If $Q$ is the algebra of all linear operators on $C^{3}$ and $T$ is the element of $Q$ given by

$$
T\left(c_{1}, c_{2}, c_{3}\right)=\left(i \sqrt{2} c_{1}, c_{2}, i \sqrt{2} c_{3}\right)
$$

then $f(T)$ is the linear operator on $C^{3}$ defined by

$$
f(T)\left(c_{1}, c_{2}, c_{3}\right)=\left(0,3 c_{2}, 0\right) .
$$

(d) If $Q$ is the algebra of all polynomials over $C$ and $g=x^{4}+3 i$, then $f(g)$ is the polynomial in $Q$ given by

$$
f(g)=-7+6 i x^{4}+x^{8} .
$$

The observant reader may notice in connection with this last example that if $f$ is a polynomial over any field and $x$ is the polynomial $(0,1,0, \ldots)$ then $f=f(x)$, but he is advised to forget this fact.

Theorem 2. Let $\mathrm{F}$ be a field and $\mathrm{Q}$ be a linear algebra with identity over F. Suppose f and $\mathrm{g}$ are polynomials over F, that $\alpha$ is an element of $\alpha$, and that $\mathrm{c}$ belongs to $\mathrm{F}$. Then

(i) $(\mathrm{cf}+\mathrm{g})(\alpha)=\operatorname{cf}(\alpha)+\mathrm{g}(\alpha)$

(ii) $(\mathrm{fg})(\alpha)=\mathrm{f}(\alpha) \mathrm{g}(\alpha)$.

Proof. As (i) is quite easy to establish, we shall only prove (ii). Suppose

By (4-10),

$$
f=\sum_{i=0}^{m} f_{i} x^{i} \text { and } g=\sum_{j=0}^{n} g_{j} x^{j} \text {. }
$$

and hence by (i),

$$
f g=\sum_{i, j} f_{i} g_{j} x^{i+i}
$$

$$
\begin{aligned}
(f g)(\alpha) & =\sum_{i, j} f_{i} g_{j} \alpha^{i+i} \\
& =\left(\sum_{i=0}^{m} f_{i} \alpha^{i}\right)\left(\sum_{j=0}^{n} g_{j} \alpha^{i}\right) \\
& =f(\alpha) g(\alpha) .
\end{aligned}
$$

\section{Exercises}

1. Let $F$ be a subfield of the complex numbers and let $A$ be the following $2 \times 2$ matrix over $F$

$$
A=\left[\begin{array}{rr}
2 & 1 \\
-1 & 3
\end{array}\right]
$$

For each of the following polynomials $f$ over $F$, compute $f(A)$.
(a) $f=x^{2}-x+2$;
(b) $f=x^{3}-1$;
(c) $f=x^{2}-5 x+7$.

2. Let $T$ be the linear operator on $R^{3}$ defined by

$$
T\left(x_{1}, x_{2}, x_{3}\right)=\left(x_{1}, x_{3},-2 x_{2}-x_{3}\right) .
$$

Let $f$ be the polynomial over $R$ defined by $f=-x^{3}+2$. Find $f(T)$.

3. Let $A$ be an $n \times n$ diagonal matrix over the field $F$, i.e., a matrix satisfying $A_{i j}=0$ for $i \neq j$. Let $f$ be the polynomial over $F$ defined by

$$
f=\left(x-A_{11}\right) \cdots\left(x-A_{n n}\right) .
$$

What is the matrix $f(A)$ ?

4. If $f$ and $g$ are independent polynomials over a field $F$ and $h$ is a non-zero polynomial over $F$, show that $f h$ and $g h$ are independent.

5. If $F$ is a field, show that the product of two non-zero elements of $F^{\infty}$ is non-zero.

6. Let $S$ be a set of non-zero polynomials over a field $F$. If no two elements of $S$ have the same degree, show that $S$ is an independent set in $F[x]$.

7. If $a$ and $b$ are elements of a field $F$ and $a \neq 0$, show that the polynomials 1 , $a x+b,(a x+b)^{2},(a x+b)^{3}, \ldots$ form a basis of $F[x]$.

8. If $F$ is a field and $h$ is a polynomial over $F$ of degree $\geq 1$, show that the mapping $f \rightarrow f(h)$ is a one-one linear transformation of $F[x]$ into $F[x]$. Show that this transformation is an isomorphism of $F[x]$ onto $F[x]$ if and only if deg $h=1$.

9. Let $F$ be a subfield of the complex numbers and let $T, D$ be the transformations on $F[x]$ defined by

and

$$
T\left(\sum_{i=0}^{n} c_{i} x^{i}\right)=\sum_{i=0}^{n} \frac{c_{i}}{1+i} x^{i+1}
$$

$$
D\left(\sum_{i=0}^{n} c_{i} x^{i}\right)=\sum_{i=1}^{n} i_{i} x^{i-1} .
$$

(a) Show that $T$ is a non-singular linear operator on $F[x]$. Show also that $T$ is not invertible.

(b) Show that $D$ is a linear operator on $F[x]$ and find its null space.

(c) Show that $D T=I$, and $T D \neq I$.

(d) Show that $T[(T f) g]=(T f)(T g)-T[f(T g)]$ for all $f, g$ in $F[x]$.

(e) State and prove a rule for $D$ similar to the one given for $T$ in (d).

(f) Suppose $V$ is a non-zero subspace of $F[x]$ such that $T f$ belongs to $V$ for each $f$ in $V$. Show that $V$ is not finite-dimensional.

(g) Suppose $V$ is a finite-dimensional subspace of $F[x]$. Prove there is an integer $m \geq 0$ such that $D^{m} f=0$ for each $f$ in $V$. 

\subsection{Lagrange Interpolation}

Throughout this section we shall assume $F$ is a fixed field and that $t_{0}, t_{1}, \ldots, t_{n}$ are $n+1$ distinct elements of $F$. Let $V$ be the subspace of $F[x]$ consisting of all polynomials of degree less than or equal to $n$ (together with the 0 -polynomial), and let $L_{i}$ be the function from $V$ into $F$ defined for $f$ in $V$ by

$$
L_{i}(f)=f\left(t_{i}\right), \quad 0 \leq i \leq n .
$$

By part (i) of Theorem 2, each $L_{i}$ is a linear functional on $V$, and one of the things we intend to show is that the set consisting of $L_{0}, L_{1}, \ldots, L_{n}$ is a basis for $V^{*}$, the dual space of $V$.

Of course in order that this be so, it is sufficient (cf. Theorem 15 of Chapter 3) that $\left\{L_{0}, L_{1}, \ldots, L_{n}\right\}$ be the dual of a basis $\left\{P_{0}, P_{1}, \ldots, P_{n}\right\}$ of $V$. There is at most one such basis, and if it exists it is characterized by

$$
L_{j}\left(P_{i}\right)=P_{i}\left(t_{j}\right)=\delta_{i j} .
$$

The polynomials

$$
\begin{aligned}
P_{i} & =\frac{\left(x-t_{0}\right) \cdots\left(x-t_{i-1}\right)\left(x-t_{i+1}\right) \cdots\left(x-t_{n}\right)}{\left(t_{i}-t_{0}\right) \cdots\left(t_{i}-t_{i-1}\right)\left(t_{i}-t_{i+1}\right) \cdots\left(t_{i}-t_{n}\right)} \\
& =\prod_{j \neq i}\left(\frac{x-t_{j}}{t_{i}-t_{j}}\right)
\end{aligned}
$$

are of degree $n$, hence belong to $V$, and by Theorem 2 , they satisfy (4-11). If $f=\sum_{i} c_{i} P_{i}$, then for each $j$

$$
f\left(t_{j}\right)={\underset{i}{i}}_{\boldsymbol{\Sigma}_{i}} P_{i}\left(t_{j}\right)=c_{j} .
$$

Since the 0 -polynomial has the property that $0(t)=0$ for each $t$ in $F$, it follows from (4-13) that the polynomials $P_{0}, P_{1}, \ldots, P_{n}$ are linearly independent. The polynomials $1, x, \ldots, x^{n}$ form a basis of $V$ and hence the dimension of $V$ is $(n+1)$. So, the independent set $\left\{P_{0}, P_{1}, \ldots, P_{n}\right\}$ must also be a basis for $V$. Thus for each $f$ in $V$

$$
f=\sum_{i=0}^{n} f\left(t_{i}\right) P_{i} .
$$

The expression (4-14) is called Lagrange's interpolation formula. Setting $f=x^{j}$ in (4-14) we obtain

$$
x^{j}=\sum_{i=0}^{n}\left(t_{i}\right)^{j} P_{i} .
$$

Now from Theorem 7 of Chapter 2 it follows that the matrix

$$
\left[\begin{array}{ccccc}
1 & t_{0} & t_{0}^{2} & \cdots & t_{0}^{n} \\
1 & t_{1} & t_{1}^{2} & \cdots & t_{1}^{n} \\
\vdots & \vdots & \vdots & \vdots & \vdots \\
1 & t_{n} & t_{n}^{2} & \cdots & t_{n}^{n}
\end{array}\right]
$$

is invertible. The matrix in (4-15) is called a Vandermonde matrix; it is an interesting exercise to show directly that such a matrix is invertible, when $t_{0}, t_{1}, \ldots, t_{n}$ are $n+1$ distinct elements of $F$.

If $f$ is any polynomial over $F$ we shall, in our present discussion, denote by $f^{\sim}$ the polynomial function from $F$ into $F$ taking each $t$ in $F$ into $f(t)$. By definition (cf. Example 4, Chapter 2 ) every polynomial function arises in this way; however, it may happen that $f^{\sim}=g^{\sim}$ for two polynomials $f$ and $g$ such that $f \neq g$. Fortunately, as we shall see, this unpleasant situation only occurs in the case where $F$ is a field having only a finite number of distinct elements. In order to describe in a precise way the relation between polynomials and polynomial functions, we need to define the product of two polynomial functions. If $f, g$ are polynomials over $F$, the product of $f^{\sim}$ and $g^{\sim}$ is the function $f^{\sim} g^{\sim}$ from $F$ into $F$ given by

$$
\left(f^{\sim} g^{\sim}\right)(t)=f^{\sim}(t) g^{\sim}(t), \quad t \text { in } F .
$$

By part (ii) of Theorem 2, $(f g)(t)=f(t) g(t)$, and hence

$$
(f g)^{\sim}(t)=f^{\sim}(t) g^{\sim}(t)
$$

for each $t$ in $F$. Thus $f^{\sim} g^{\sim}=(f g)^{\sim}$, and is a polynomial function. At this point it is a straightforward matter, which we leave to the reader, to verify that the vector space of polynomial functions over $F$ becomes a linear algebra with identity over $F$ if multiplication is defined by (4-16).

Definition. Let $\mathrm{F}$ be a field and let $\mathrm{a}$ and $\mathrm{a}^{\sim}$ be lincar algebras over $\mathrm{F}$. The algebras $Q$ and $Q^{\sim}$ are said to be isomorphic if there is a one-to-one mapping $\alpha \rightarrow \alpha^{\sim}$ of $Q$ onto $Q^{\sim}$ such that

$$
\begin{aligned}
& \text { (a) } \quad(\mathrm{c} \alpha+\mathrm{d} \beta)^{\sim}=\mathrm{c} \alpha^{\sim}+\mathrm{d} \beta^{\sim} \\
& (\alpha \beta)^{\sim}=\alpha^{\sim} \beta^{\sim}
\end{aligned}
$$

for all $\alpha, \beta$ in $Q$ and all scalars $\mathrm{c}, \mathrm{d}$ in $\mathrm{F}$. The mapping $\alpha \rightarrow \alpha^{\sim}$ is called an isomorphism of Q onto $Q^{\sim}$. An isomorphism of $Q$ onto $Q^{\sim}$ is thus a vectorspace isomorphism of a onto $\mathrm{Q}^{\sim}$ which has the additional property (b) of 'preserving' products.

Example 4. Let $V$ be an $n$-dimensional vector space over the field $F$. By Theorem 13 of Chapter 3 and subsequent remarks, each ordered basis Q of $V$ determines an isomorphism $T \rightarrow[T]_{Q}$ of the algebra of linear operators on $V$ onto the algebra of $n \times n$ matrices over $F$. Suppose now that $U$ is a fixed linear operator on $V$ and that we are given a polynomial

$$
f=\sum_{i=0}^{n} c_{i} x^{i}
$$

with coefficients $c_{i}$ in $F$. Then

$$
f(U)=\sum_{i=0}^{n} c_{i} U^{i}
$$

and since $T \rightarrow[T]_{\Theta}$ is a linear mapping

$$
[f(U)]_{\mathscr{B}}=\sum_{i=0}^{n} c_{i}\left[U^{i}\right]_{B} .
$$

Now from the additional fact that

$$
\left[T_{1} T_{2}\right]_{\Theta}=\left[T_{1}\right]_{\Theta}\left[T_{2}\right]_{\Theta}
$$

for all $T_{1}, T_{2}$ in $L(V, V)$ it follows that

$$
\left[U^{i}\right]_{Q}=\left([U]_{Q}\right)^{i}, \quad 2 \leq i \leq n .
$$

As this relation is also valid for $i=0,1$ we obtain the result that

$$
[f(U)]_{\mathbb{Q}}=f\left([U]_{\mathbb{B}}\right) .
$$

In words, if $U$ is a linear operator on $V$, the matrix of a polynomial in $U$, in a given basis, is the same polynomial in the matrix of $U$.

Theorem 3. If $\mathrm{F}$ is a field containing an infinite number of distinct elements, the mapping $\mathrm{f} \rightarrow \mathrm{f}^{\sim}$ is an isomorphism of the algebra of polynomials over $\mathrm{F}$ onto the algebra of polynomial functions over $\mathrm{F}$.

Proof. By definition, the mapping is onto, and if $f, g$ belong to $F[x]$ it is evident that

$$
(c f+d g)^{\sim}=d f^{\sim}+d g^{\sim}
$$

for all scalars $c$ and $d$. Since we have already shown that $(f g)^{\sim}=f^{\sim} g^{\sim}$, we need only show that the mapping is one-to-one. To do this it suffices by linearity to show that $f^{\sim}=0$ implies $f=0$. Suppose then that $f$ is a polynomial of degree $n$ or less such that $f^{\prime}=0$. Let $t_{0}, t_{\mathrm{l}}, \ldots, t_{n}$ be any $n+1$ distinct elements of $F$. Since $f^{\sim}=0, f\left(t_{i}\right)=0$ for $i=0,1, \ldots, n$, and it is an immediate consequence of (4-14) that $f=0$.

From the results of the next section we shall obtain an altogether different proof of this theorem.

\section{Exercises}

1. Use the Lagrange interpolation formula to find a polynomial $f$ with real coefficients such that $f$ has degree $\leq 3$ and $f(-1)=-6, f(0)=2, f(1)=-2$, $f(2)=6$.

2. Let $\alpha, \beta, \gamma, \delta$ be real numbers. We ask when it is possible to find a polynomial $f$ over $R$, of degree not more than 2, such that $f(-1)=\alpha, f(1)=\beta, f(3)=\gamma$ and $f(0)=\delta$. Prove that this is possible if and only if

$$
3 \alpha+6 \beta-\gamma-8 \delta=0 .
$$

3. Let $F$ be the field of real numbers, Sec. $4.4$

$$
\begin{aligned}
A & =\left[\begin{array}{llll}
2 & 0 & 0 & 0 \\
0 & 2 & 0 & 0 \\
0 & 0 & 3 & 0 \\
0 & 0 & 0 & 1
\end{array}\right] \\
p & =(x-2)(x-3)(x-1) .
\end{aligned}
$$

(a) Show that $p(A)=0$.

(b) Let $P_{1}, P_{2}, P_{3}$ be the Lagrange polynomials for $t_{1}=2, t_{2}=3, t_{3}=1$. Compute $E_{i}=P_{i}(A), i=1,2,3$.

(c) Show that $E_{1}+E_{2}+E_{3}=I, E_{i} E_{j}=0$ if $i \neq j, E_{i}^{2}=E_{i}$.

(d) Show that $A=2 E_{1}+3 E_{2}+E_{3}$.

4. Let $p=(x-2)(x-3)(x-1)$ and let $T$ be any linear operator on $R^{4}$ such that $p(T)=0$. Let $P_{1}, P_{2}, P_{3}$ be the Lagrange polynomials of Exercise 3 , and let $E_{i}=P_{i}(T), i=1,2,3$. Prove that

$$
\begin{gathered}
E_{1}+E_{2}+E_{3}=I, \quad E_{i} E_{j}=0 \text { if } i \neq j, \\
E_{i}^{2}=E_{i}, \quad \text { and } \quad T=2 E_{1}+3 E_{2}+E_{3} .
\end{gathered}
$$

5. Let $n$ be a positive integer and $F$ a field. Suppose $A$ is an $n \times n$ matrix over $F$ and $P$ is an invertible $n \times n$ matrix over $F$. If $f$ is any polynomial over $F$, prove that

$$
f\left(P^{-1} A P\right)=P^{-1} f(A) P .
$$

6. Let $F$ be a field. We have considered certain special linear functionals on $F[x]$ obtained via 'evaluation at $t$ ':

$$
L(f)=f(t) .
$$

Such functionals are not only linear but also have the property that $L(f g)=$ $L(f) L(g)$. Prove that if $L$ is any linear functional on $F[x]$ such that

$$
L(f g)=L(f) L(g)
$$

for all $f$ and $g$, then either $L=0$ or there is a $t$ in $F$ such that $L(f)=f(t)$ for all $f$.

\subsection{Polynomial Ideals}

In this section we are concerned with results which depend primarily on the multiplicative structure of the algebra of polynomials over a field.

Lemma. Suppose f and d are non-zero polynomials over a field F such that deg $\mathrm{d} \leq \operatorname{deg} \mathrm{f}$. Then there exists a polynomial $\mathrm{g}$ in $\mathrm{F}[\mathrm{x}]$ such that either

$$
\mathrm{f}-\operatorname{dg}=0 \quad \text { or } \quad \operatorname{deg}(\mathrm{f}-\mathrm{dg})<\operatorname{deg} \mathrm{f} .
$$

Proof. Suppose

$$
f=a_{m} x^{m}+\sum_{i=0}^{m-1} a_{i} x^{i}, \quad a_{m} \neq 0
$$

and that

$$
d=b_{n} x^{n}+\sum_{i=0}^{n-1} b_{i} x^{i}, \quad b_{n} \neq 0
$$

Then $m \geq n$, and

$$
f-\left(\frac{a_{m}}{b_{n}}\right) x^{m-n} d=0 \quad \text { or } \quad \operatorname{deg}\left[f-\left(\frac{\boldsymbol{a}_{m}}{b_{n}}\right) x^{m-n} d\right]<\operatorname{deg} f
$$

Thus we may take $g=\left(\frac{a_{m}}{b_{n}}\right) x^{m-n}$.

Using this lemma we can show that the familiar process of 'long division' of polynomials with real or complex coefficients is possible over any field.

Theorem 4. If $\mathrm{f}, \mathrm{d}$ are polynomials over a field $\mathrm{F}$ and $\mathrm{d}$ is different from 0 then there exist polynomials $\mathrm{q}, \mathrm{r}$ in $\mathrm{F}[\mathrm{x}]$ such that

(i) $f=d q+r$.

(ii) either $\mathrm{r}=0$ or $\operatorname{deg} \mathrm{r}<\operatorname{deg} \mathrm{d}$.

The polynomials $\mathrm{q}, \mathrm{r}$ satisfying (i) and (ii) are unique.

Proof. If $f$ is 0 or $\operatorname{deg} f<\operatorname{deg} d$ we may take $q=0$ and $r=f$. In case $f \neq 0$ and $\operatorname{deg} f \geq \operatorname{deg} d$, the preceding lemma shows we may choose a polynomial $g$ such that $f-d g=0$ or $\operatorname{deg}(f-d g)<\operatorname{deg} f$. If $f-$ $d g \neq 0$ and $\operatorname{deg}(f-d g) \geq \operatorname{deg} d$ we choose a polynomial $h$ such that $(f-d g)-d h=0$ or

$$
\operatorname{deg}[f-d(g+h)]<\operatorname{deg}(f-d g) .
$$

Continuing this process as long as necessary, we ultimately obtain polynomials $q, r$ such that $r=0$ or $\operatorname{deg} r<\operatorname{deg} d$, and $f=d q+r$. Now suppose we also have $f=d q_{1}+r_{1}$ where $r_{1}=0$ or $\operatorname{deg} r_{1}<\operatorname{deg} d$. Then $d q+r=d q_{1}+r_{1}$, and $d\left(q-q_{1}\right)=r_{1}-r$. If $q-q_{1} \neq 0$ then $d\left(q-q_{1}\right) \neq$ 0 and

$$
\operatorname{deg} d+\operatorname{deg}\left(q-q_{1}\right)=\operatorname{deg}\left(r_{1}-r\right) .
$$

But as the degree of $r_{1}-r$ is less than the degree of $d$, this is impossible and $q-q_{1}=0$. Hence also $r_{1}-r=0$.

Definition. Let d be a non-zero polynomial over the field $\mathrm{F}$. If $\mathrm{f}$ is in $\mathrm{F}[\mathrm{x}]$, the preceding theorem shows there is at most one polynomial $\mathrm{q}$ in $\mathrm{F}[\mathrm{x}]$ such that $\mathrm{f}=\mathrm{dq}$. If such $a \mathrm{q}$ exists we say that $\mathrm{d}$ divides $\mathrm{f}$, that $\mathrm{f}$ is divisible by $\mathrm{d}$, that $\mathrm{f}$ is a multiple of $\mathrm{d}$, and call $\mathrm{q}$ the quotient of $\mathrm{f}$ and $\mathrm{d}$. We also write $\mathrm{q}=\mathrm{f} / \mathrm{d}$.

Corollary 1. Let $\mathrm{f}$ be a polynomial over the field $\mathrm{F}$, and let $\mathrm{c}$ be an element of $\mathrm{F}$. Then $\mathrm{f}$ is divisible by $\mathrm{x}-\mathrm{c}$ if and only if $\mathrm{f}(\mathrm{c})=0$.

Proof. By the theorem, $f=(x-c) q+r$ where $r$ is a scalar polynomial. By Theorem 2 ,

$$
f(c)=0 q(c)+r(c)=r(c) .
$$

Hence $r=0$ if and only if $f(c)=0$.

Definition. Let $\mathrm{F}$ be a field. An element $\mathrm{c}$ in $\mathrm{F}$ is said to be a root or $a$ zero of a given polynomial $\mathrm{f}$ over $\mathrm{F}$ if $\mathrm{f}(\mathrm{c})=0$.

Corollary 2. A polynomial $\mathrm{f}$ of degree $\mathrm{n}$ over a field $\mathrm{F}$ has at most $\mathrm{n}$ roots in $\mathrm{F}$.

Proof. The result is obviously true for polynomials of degree 0 and degree 1 . We assume it to be true for polynomials of degree $n-1$. If $a$ is a root of $f, f=(x-a) q$ where $q$ has degree $n-1$. Since $f(b)=0$ if and only if $a=b$ or $q(b)=0$, it follows by our inductive assumption that $f$ has at most $n$ roots.

The reader should observe that the main step in the proof of Theorem 3 follows immediately from this corollary.

The formal derivatives of a polynomial are useful in discussing multiple roots. The derivative of the polynomial

$$
f=c_{0}+c_{1} x+\cdots+c_{n} x^{n}
$$

is the polynomial

$$
f^{\prime}=c_{1}+2 c_{2} x+\cdots+n c_{n} x^{n-1} .
$$

We also use the notation $D f=f^{\prime}$. Differentiation is linear, that is, $D$ is a linear operator on $F[x]$. We have the higher order formal derivatives $f^{\prime \prime}=D^{2} f, f^{(3)}=D^{3} f$, and so on.

Theorem 5 (Taylor's Formula). Let $\mathrm{F}$ be a field of characteristic zer $\mathrm{c}$ an element of $\mathrm{F}$, and $\mathrm{n}$ a positive integer. If $\mathrm{f}$ is a polynomial over $\mathrm{f}$ with $\operatorname{deg} \mathrm{f} \leq \mathrm{n}$, then

$$
\mathrm{f}=\sum_{\mathrm{k}=0}^{\mathrm{n}} \frac{\left(D^{\mathrm{k}} \mathrm{f}\right)}{\mathrm{k} !}(\mathrm{c})(\mathrm{x}-\mathrm{c})^{\mathrm{k}} .
$$

Proof. Taylor's formula is a consequence of the binomial theorem and the linearity of the operators $D, D^{2}, \ldots, D^{n}$. The binomial theorem is easily proved by induction and asserts that

where

$$
(a+b)^{m}=\sum_{k=0}^{m}\left(\begin{array}{l}
m \\
k
\end{array}\right) a^{m-k} b^{k}
$$

$$
\left(\begin{array}{l}
m \\
k
\end{array}\right)=\frac{m !}{k !(m-k) !}=\frac{m(m-1) \cdots(m-k+1)}{1 \cdot 2 \cdots k}
$$

is the familiar binomial coefficient giving the number of combinations of $m$ objects taken $k$ at a time. By the binomial theorem

$$
\begin{aligned}
x^{m} & =[c+(x-c)]^{m} \\
& =\sum_{k=0}^{m}\left(\begin{array}{l}
m \\
k
\end{array}\right) c^{m-k}(x-c)^{k} \\
& =c^{m}+m c^{m-1}(x-c)+\cdots+(x-c)^{m}
\end{aligned}
$$

and this is the statement of Taylor's formula for the case $f=x^{m}$. If

$$
f=\sum_{m=0}^{n} a_{m} x^{m}
$$

then

$$
D^{k} f(c)=\sum_{m} a_{m}\left(D^{k} x^{m}\right)(c)
$$

and

$$
\begin{aligned}
\sum_{k=0}^{n} \frac{D^{k} f(c)}{k !}(x-c)^{k} & =\sum_{k} \sum_{m} a_{m} \frac{\left(D^{k} x^{m}\right)}{k !}(c)(x-c)^{k} \\
& =\sum_{m} a_{m} \sum_{k} \frac{\left(D^{k} x^{m}\right)}{k !}(c)(x-c)^{k} \\
& =\sum_{m} a_{m} x^{m} \\
& =f .
\end{aligned}
$$

It should be noted that because the polynomials $1,(x-c), \ldots$, $(x-c)^{n}$ are linearly independent (cf. Exercise 6, Section 4.2) Taylor's formula provides the unique method for writing $f$ as a linear combination of the polynomials $(x-c)^{k}(0 \leq k \leq n)$.

Although we shall not give any details, it is perhaps worth mentioning at this point that with the proper interpretation Taylor's formula is also valid for polynomials over fields of finite characteristic. If the field $F$ has finite characteristic (the sum of some finite number of 1 's in $F$ is 0 ) then we may have $k !=0$ in $F$, in which case the division of $\left(D^{k} f\right)$ (c) by $k !$ is meaningless. Nevertheless, sense can be made out of the division of $D^{k} f$ by $k$ !, because every coefficient of $D^{k} f$ is an element of $F$ multiplied by an integer divisible by $k$ ! If all of this seems confusing, we advise the reader to restrict his attention to fields of characteristic 0 or to subfields of the complex numbers.

If $c$ is a root of the polynomial $f$, the multiplicity of $c$ as a root of $f$ is the largest positive integer $r$ such that $(x-c)^{r}$ divides $f$.

The multiplicity of a root is clearly less than or equal to the degree of $f$. For polynomials over fields of characteristic zero, the multiplicity of $c$ as a root of $f$ is related to the number of derivatives of $f$ that are 0 at $c$.

Theorem 6. Let $\mathrm{F}$ be a field of characteristic zero and $\mathrm{f}$ a polynomial over $\mathrm{F}$ with $\operatorname{deg} \mathrm{f} \leq \mathrm{n}$. Then the scalar $\mathrm{c}$ is a root of $\mathrm{f}$ of multiplicity $\mathrm{r}$ if and only if

$$
\begin{gathered}
\left(D^{k} f\right)(c)=0, \quad 0 \leq k \leq r-1 \\
\left(D^{r} f\right)(c) \neq 0 .
\end{gathered}
$$

Proof. Suppose that $r$ is the multiplicity of $c$ as a root of $f$. Then there is a polynomial $g$ such that $f=(x-c)^{r} g$ and $g(c) \neq 0$. For other- wise $f$. would be divisible by $(x-c)^{r+1}$, by Corollary 1 of Theorem 4. By Taylor's formula applied to $g$

$$
\begin{aligned}
f & =(x-c)^{r}\left[\sum_{m=0}^{n-r} \frac{\left(D^{m} g\right)}{m !}(c)(x-c)^{m}\right] \\
& =\sum_{m=0}^{n-r} \frac{\left(D^{m} g\right)}{m !}(x-c)^{r+m}
\end{aligned}
$$

Since there is only one way to write $f$ as a linear combination of the powers $(x-c)^{k}(0 \leq k \leq n)$ it follows that

![](https://cdn.mathpix.com/cropped/2023_01_16_0f6afdf132d84aaf6bdeg-140.jpg?height=173&width=546&top_left_y=581&top_left_x=339)

Therefore, $D^{k} f(c)=0$ for $0 \leq k \leq r-1$, and $D^{r} f(c)=g(c) \neq 0$. Conversely, if these conditions are satisfied, it follows at once from Taylor's formula that there is a polynomial $g$ such that $f=(x-c)^{r} g$ and $g(c) \neq 0$. Now suppose that $r$ is not the largest positive integer such that $(x-c)^{r}$ divides $f$. Then there is a polynomial $h$ such that $f=(x-c)^{r+1} h$. But this implies $g=(x-c) h$, by Corollary 2 of Theorem 1 ; hence $g(c)=0$, a contradiction.

Definition. Let $\mathrm{F}$ be a field. An ideal in $\mathrm{F}[\mathrm{x}]$ is a subspace $\mathrm{M}$ of $\mathrm{F}[\mathrm{x}]$ such that $\mathrm{fg}$ belongs to $\mathrm{M}$ whenever $\mathrm{f}$ is in $\mathrm{F}[\mathrm{x}]$ and $\mathrm{g}$ is in $\mathrm{M}$.

Example 5. If $F$ is a field and $d$ is a polynomial over $F$, the set $M=d F[x]$, of all multiples $d f$ of $d$ by arbitrary $f$ in $F[x]$, is an ideal. For $M$ is non-empty, $M$ in fact contains $d$. If $f, g$ belong to $F[x]$ and $c$ is a scalar, then

$$
c(d f)-d g=d(c f-g)
$$

belongs to $M$, so that $M$ is a subspace. Finally $M$ contains $(d f) g=d(f g)$ as well. The ideal $M$ is called the principal ideal generated by $d$.

EXAMPLE 6 . Let $d_{1}, \ldots, d_{n}$ be a finite number of polynomials over $F$. Then the sum $M$ of the subspaces $d_{i} F[x]$ is a subspace and is also an ideal. For suppose $p$ belongs to $M$. Then there exist polynomials $f_{1}, \ldots, f_{n}$ in $F[x]$ such that $p=d_{1} f_{1}+\cdots+d_{n} f_{n}$. If $g$ is an arbitrary polynomial over $F$, then

$$
p g=d_{1}\left(f_{1} g\right)+\cdots+d_{n}\left(f_{n} g\right)
$$

so that $p g$ also belongs to $M$. Thus $M$ is an ideal, and we say that $M$ is the ideal generated by the polynomials, $d_{1}, \ldots, d_{n}$.

Example 7. Let $F$ be a subfield of the complex numbers, and consider the ideal

$$
M=(x+2) F[x]+\left(x^{2}+8 x+16\right) F[x] .
$$

We assert that $M=F[x]$. For $M$ contains

$$
x^{2}+8 x+16-x(x+2)=6 x+16
$$

and hence $M$ contains $6 x+16-6(x+2)=4$. Thus the scalar polynomial 1 belongs to $M$ as well as all its multiples.

Theorem 7. If $\mathrm{F}$ is a field, and $\mathrm{M}$ is any non-zero ideal in $\mathrm{F}[\mathrm{x}]$, there is a unique monic polynomial $\mathrm{d}$ in $\mathrm{F}[\mathrm{x}]$ such that $\mathrm{M}$ is the principal ideal generated by $\mathrm{d}$.

Proof. By assumption, $M$ contains a non-zero polynomial; among all non-zero polynomials in $M$ there is a polynomial $d$ of minimal degree. We may assume $d$ is monic, for otherwise we can multiply $d$ by a scalar to make it monic. Now if $f$ belongs to $M$, Theorem 4 shows that $f=d q+r$ where $r=0$ or $\operatorname{deg} r<\operatorname{deg} d$. Since $d$ is in $M, d q$ and $f-d q=r$ also belong to $M$. Because $d$ is an element of $M$ of minimal degree we cannot have $\operatorname{deg} r<\operatorname{deg} d$, so $r=0$. Thus $M=d F[x]$. If $g$ is another monic polynomial such that $M=g F[x]$, then there exist non-zero polynomials $p, q$ such that $d=g p$ and $g=d q$. Thus $d=d p q$ and

$$
\operatorname{deg} d=\operatorname{deg} d+\operatorname{deg} p+\operatorname{deg} q .
$$

Hence $\operatorname{deg} p=\operatorname{deg} q=0$, and as $d, g$ are monic, $p=q=1$. Thus $d=g$.

It is worth observing that in the proof just given we have used a special case of a more general and rather useful fact; namely, if $p$ is a nonzero polynomial in an ideal $M$ and if $f$ is a polynomial in $M$ which is not divisible by $p$, then $f=p q+r$ where the 'remainder' $r$ belongs to $M$, is different from 0 , and has smaller degree than $p$. We have already made use of this fact in Example 7 to show that the scalar polynomial 1 is the monic generator of the ideal considered there. In principle it is always possible to find the monic polynomial generating a given non-zero ideal. For one can ultimately obtain a polynomial in the ideal of minimal degree by a finite number of successive divisions.

Corollary. If $\mathrm{p}_{1}, \ldots, \mathrm{p}_{\mathrm{n}}$ are polynomials over a field $\mathrm{F}$, not all of which are 0, there is a unique monic polynomial $\mathrm{d}$ in $\mathrm{F}[\mathrm{x}]$ such that

(a) $\mathrm{d}$ is in the ideal generated by $\mathrm{p}_{\mathrm{l}}, \ldots, \mathrm{p}_{\mathrm{n}}$;

(b) d divides each of the polynomials $\mathrm{p}_{\mathbf{i}}$.

Any polynomial satisfying (a) and (b) necessarily satisfies

(c) d is divisible by every polynomial which divides each of the polynomials $\mathrm{p}_{\mathrm{l}}, \ldots, \mathrm{p}_{\mathrm{n}}$.

Proof. Let $d$ be the monic generator of the ideal

$$
p_{1} F[x]+\cdots+p_{n} F[x] .
$$

Every member of this ideal is divisible by $d$; thus each of the polynomials $p_{i}$ is divisible by $d$. Now suppose $f$ is a polynomial which divides each of the polynomials $p_{1}, \ldots, p_{n}$. Then there exist polynomials $g_{1}, \ldots, g_{n}$ such that $p_{i}=f \boldsymbol{g}_{i}, 1 \leq i \leq n$. Also, since $d$ is in the ideal

$$
p_{1} F[x]+\cdots+p_{n} F[x],
$$

there exist polynomials $q_{1}, \ldots, q_{n}$ in $F[x]$ such that

Thus

$$
d=p_{1} q_{1}+\cdots+p_{n} q_{n} .
$$

$$
d=f\left[g_{1} q_{1}+\cdots+g_{n} q_{n}\right] .
$$

We have shown that $d$ is a monic polynomial satisfying (a), (b), and (c). If $d^{\prime}$ is any polynomial satisfying (a) and (b) it follows, from (a) and the definition of $d$, that $d^{\prime}$ is a scalar multiple of $d$ and satisfies (c) as well. Finally, in case $d^{\prime}$ is a monic polynomial, we have $d^{\prime}=d$.

Definition. If $\mathrm{p}_{\mathrm{l}}, \ldots, \mathrm{p}_{\mathrm{n}}$ are polynomials over a field $\mathrm{F}$, not all of which are 0 , the monic generator $\mathrm{d}$ of the ideal

$$
\mathrm{p}_{1} \mathrm{~F}[\mathrm{x}]+\cdots+\mathrm{p}_{\mathrm{n}} \mathrm{F}[\mathrm{x}]
$$

is called the greatest common divisor (g.c.d.) of $\mathrm{p}_{1}, \ldots, \mathrm{p}_{\mathrm{n}}$. This terminology is justified by the preceding corollary. We say that the polynomials $\mathrm{p}_{1}, \ldots, \mathrm{p}_{\mathrm{n}}$ are relatively prime if their greatest common divisor is 1 , or equivalently if the ideal they generate is all of $\mathrm{F}[\mathrm{x}]$.

Example 8. Let $C$ be the field of complex numbers. Then

(a) g.c.d. $\left(x+2, x^{2}+8 x+16\right)=1$ (see Example 7);

(b) g.c.d. $\left((x-2)^{2}(x+i),(x-2)\left(x^{2}+1\right)\right)=(x-2)(x+i)$. For, the ideal

$$
(x-2)^{2}(x+i) F[x]+(x-2)\left(x^{2}+1\right) F[x]
$$

contains

$$
(x-2)^{2}(x+i)-(x-2)\left(x^{2}+1\right)=(x-2)(x+i)(i-2) .
$$

Hence it contains $(x-2)(x+i)$, which is monic and divides both

$$
(x-2)^{2}(x+i) \text { and }(x-2)\left(x^{2}+1\right) .
$$

Example 9 . Let $F$ be the field of rational numbers and in $F[x]$ let $M$ be the ideal generated by

$$
(x-1)(x+2)^{2}, \quad(x+2)^{2}(x-3), \quad \text { and } \quad(x-3) .
$$

Then $M$ contains

$$
\frac{1}{2}(x+2)^{2}[(x-1)-(x-3)]=(x+2)^{2}
$$

and since

$$
(x+2)^{2}=(x-3)(x+7)-17
$$

$M$ contains the scalar polynomial 1 . Thus $M=F[x]$ and the polynomials

$$
(x-1)(x+2)^{2}, \quad(x+2)^{2}(x-3), \quad \text { and } \quad(x-3)
$$

are relatively prime.

\section{Exercises}

1. Let $Q$ be the field of rational numbers. Determine which of the following subsets of $Q[x]$ are ideals. When the set is an ideal, find its monic generator.

(a) all $f$ of even degree;

(b) all $f$ of degree $\geq 5$;

(c) all $f$ such that $f(0)=0$;

(d) all $f$ such that $f(2)=f(4)=0$;

(e) all $f$ in the range of the linear operator $T$ defined by

$$
T\left(\sum_{i=0}^{n} c_{i} x^{i}\right)=\sum_{i=0}^{n} \frac{c_{i}}{i+1} x^{i+1} .
$$

2. Find the g.c.d. of each of the following pairs of polynomials
(a) $2 x^{5}-x^{3}-3 x^{2}-6 x+4, x^{4}+x^{3}-x^{2}-2 x-2$;
(b) $3 x^{4}+8 x^{2}-3, x^{3}+2 x^{2}+3 x+6$ :
(c) $x^{4}-2 x^{3}-2 x^{2}-2 x-3, x^{3}+6 x^{2}+7 x+1$.

3. Let $A$ be an $n \times n$ matrix over a field $F$. Show that the set of all polynomials $f$ in $F[x]$ such that $f(A)=0$ is an ideal.

4. Let $F$ be a subfield of the complex numbers, and let

$$
A=\left[\begin{array}{rr}
1 & -2 \\
0 & 3
\end{array}\right] \text {. }
$$

Find the monic generator of the ideal of all polynomials $f$ in $F[x]$ such that $f(A)=0$.

5. Let $F$ be a field. Show that the intersection of any number of ideals in $F[x]$ is an ideal.

6. Let $F$ be a field. Show that the ideal generated by a finite number of polynomials $f_{1}, \ldots, f_{n}$ in $F[x]$ is the intersection of all ideals containing $f_{1}, \ldots, f_{n}$.

7. Let $K$ be a subfield of a field $F$, and suppose $f, g$ are polynomials in $K[x]$. Let $M_{K}$ be the ideal generated by $f$ and $g$ in $K[x]$ and $M_{F}$ be the ideal they generate in $F[x]$. Show that $M M_{K}$ and $M_{F}$ have the same monic generator.

\subsection{The Prime Factorization of a Polynomial}

In this section we shall prove that each polynomial over the field $F$ can be written as a product of 'prime' polynomials. This factorization provides us with an effective tool for finding the greatest common divisor of a finite number of polynomials, and in particular, provides an effective means for deciding when the polynomials are relatively prime.

Definition. Let $\mathrm{F}$ be a field. A polynomial $\mathrm{f}$ in $\mathrm{F}[\mathrm{x}]$ is said to be reducible over $\mathrm{F}$ if there exist polynomials $\mathrm{g}, \mathrm{h}$ in $\mathrm{F}[\mathrm{x}]$ of degree $\geq 1$ such that $\mathrm{f}=\mathrm{gh}$, and if not, $\mathrm{f}$ is said to be irreducible over $\mathrm{F}$. A non-scalar irreducible polynomial over $\mathrm{F}$ is called a prime polynomial over $\mathrm{F}$, and we sometimes say it is a prime in $\mathrm{F}[\mathrm{x}]$.

Example 10 . The polynomial $x^{2}+1$ is reducible over the field $C$ of complex numbers. For

$$
x^{2}+1=(x+i)(x-i)
$$

and the polynomials $x+i, x-i$ belong to $C[x]$. On the other hand, $x^{2}+1$ is irreducible over the field $R$ of real numbers. For if

$$
x^{2}+1=(a x+b)\left(a^{\prime} x+b^{\prime}\right)
$$

with $a, a^{\prime}, b, b^{\prime}$ in $R$, then

$$
a a^{\prime}=1, \quad a b^{\prime}+b a^{\prime}=0, \quad b b^{\prime}=1 .
$$

These relations imply $a^{2}+b^{2}=0$, which is impossible with real numbers $a$ and $b$, unless $a=b=0$.

Theorem 8. Let p, f, and $\mathrm{g}$ be polynomials over the field F. Suppose that $\mathrm{p}$ is a prime polynomial and that $\mathrm{p}$ divides the product $\mathrm{fg}$. Then either $\mathrm{p}$ divides $\mathrm{f}$ or $\mathrm{p}$ divides $\mathrm{g}$.

Proof. It is no loss of generality to assume that $p$ is a monic prime polynomial. The fact that $p$ is prime then simply says that the only monic divisors of $p$ are 1 and $p$. Let $\boldsymbol{d}$ be the g.c.d. of $f$ and $p$. Then either $d=1$ or $d=p$, since $d$ is a monic polynomial which divides $p$. If $d=p$, then $p$ divides $f$ and we are done. So suppose $d=1$, i.e., suppose $f$ and $p$ are relatively prime. We shall prove that $p$ divides $g$. Since $(f, p)=1$, there are polynomials $f_{0}$ and $p_{0}$ such that $1=f_{0} f+p_{0} p$. Multiplying by $g$, we obtain

$$
\begin{aligned}
\boldsymbol{g} & =f_{0} f g+p_{0} p g \\
& =(f g) f_{0}+p\left(p_{0} g\right) .
\end{aligned}
$$

Since $p$ divides $f g$ it divides $(f g) f_{0}$, and certainly $p$ divides $p\left(p_{0} g\right)$. Thus $p$ divides $g$.

Corollary. If $\mathrm{p}$ is a prime and divides a product $\mathrm{f}_{1} \cdots \mathrm{f}_{\mathrm{n}}$, then $\mathrm{p}$ divides one of the polynomials $\mathrm{f}_{1}, \ldots, \mathrm{f}_{\mathrm{n}}$.

Proof. The proof is by induction. When $n=2$, the result is simply the statement of Theorem 6. Suppose we have proved the corollary for $n=k$, and that $p$ divides the product $f_{1} \cdots f_{k+1}$ of some $(k+1)$ poly- nomials. Since $p$ divides $\left(f_{1} \cdots f_{k}\right) f_{k+1}$, either $p$ divides $f_{k+1}$ or $p$ divides $f_{1} \cdots f_{k}$. By the induction hypothesis, if $p$ divides $f_{1} \cdots f_{k}$, then $p$ divides $f_{j}$ for some $j, 1 \leq j \leq k$. So we see that in any case $p$ must divide some $f_{j}$, $1 \leq j \leq k+1$.

Theorem 9. If $\mathrm{F}$ is a field, a non-scalar monic polynomial in $\mathrm{F}[\mathrm{x}]$ can be factored as a product of monic primes in $\mathrm{F}[\mathrm{x}]$ in one and, except for order, only one way.

Proof. Suppose $f$ is a non-scalar monic polynomial over $F$. As polynomials of degree one are irreducible, there is nothing to prove if $\operatorname{deg} f=1$. Suppose $f$ has degree $n>1$. By induction we may assume the theorem is true for all non-scalar monic polynomials of degree less than $n$. If $f$ is irreducible, it is already factored as a product of monic primes, and otherwise $f=g h$ where $g$ and $h$ are non-scalar monic polynomials of degree less than $n$. Thus $g$ and $h$ can be factored as products of monic primes in $F[x]$ and hence so can $f$. Now suppose

$$
f=p_{1} \cdots p_{m}=q_{1} \cdots q_{n}
$$

where $p_{1}, \ldots, p_{m}$ and $q_{1}, \ldots, q_{n}$ are monic primes in $F[x]$. Then $p_{m}$ divides the product $q_{1} \cdots q_{n}$. By the above corollary, $p_{m}$ must divide some $q_{i}$. Since $q_{i}$ and $p_{m}$ are both monic primes, this means that

$$
q_{i}=p_{m} .
$$

From (4-16) we see that $m=n=1$ if either $m=1$ or $n=1$. For

$$
\operatorname{deg} f=\sum_{i=1}^{m} \operatorname{deg} p_{i}=\sum_{j=1}^{n} \operatorname{deg} q_{j} .
$$

In this case there is nothing more to prove, so we may assume $m>1$ and $n>1$. By rearranging the $q$ 's we can then assume $p_{m}=q_{n}$, and that

$$
p_{1} \cdots p_{m-1} p_{m}=q_{1} \cdots q_{n-1} p_{m} .
$$

Now by Corollary 2 of Theorem 1 it follows that

$$
p_{1} \cdots p_{m-1}=q_{1} \cdots q_{n-1} \text {. }
$$

As the polynomial $p_{1} \cdots p_{m-1}$ has degree less than $n$, our inductive assumption applies and shows that the sequence $q_{1}, \ldots, q_{n-1}$ is at most a rearrangement of the sequence $p_{1}, \ldots, p_{m-1}$. This together with (4-16) shows that the factorization of $f$ as a product of monic primes is unique up to the order of the factors.

In the above factorization of a given non-scalar monic polynomial $f$, some of the monic prime factors may be repeated. If $p_{1}, p_{2}, \ldots, p_{r}$ are the distinct monic primes occurring in this factorization of $f$, then

$$
f=p_{1}^{n_{1}} p_{2}^{n_{2}} \cdots p_{r_{r}}^{n_{r}},
$$

the exponent $n_{i}$ being the number of times the prime $p_{i}$ occurs in the factorization. This decomposition is also clearly unique, and is called the primary decomposition of $f$. It is easily verified that every monic divisor of $f$ has the form

$$
p_{1}^{m_{i}} p_{2}^{m_{1}} \cdots p_{r}^{m_{r}}, \quad 0 \leq m_{i} \leq n_{i} .
$$

From (4-18) it follows that the g.c.d. of a finite number of non-scalar monic polynomials $f_{1}, \ldots, f_{s}$ is obtained by combining all those monic primes which occur simultaneously in the factorizations of $f_{1}, \ldots, f_{s}$. The exponent to which each prime is to be taken is the largest for which the corresponding prime power is a factor of each $f_{i}$. If no (non-trivial) prime power is a factor of each $f_{i}$, the polynomials are relatively prime.

Example 11. Suppose $F$ is a field, and let $a, b, c$ be distinct elements of $F$. Then the polynomials $x-a, x-b, x-c$ are distinct monic primes in $F[x]$. If $m, n$, and $s$ are positive integers, $(x-c)^{s}$ is the g.c.d. of the polynomials.

$$
(x-b)^{n}(x-c)^{s} \text { and }(x-a)^{m}(x-c)^{s}
$$

whereas the three polynomials

$$
(x-b)^{n}(x-c)^{s}, \quad(x-a)^{m}(x-c)^{s}, \quad(x-a)^{m}(x-b)^{n}
$$

are relatively prime.

Theorem 10. Let $\mathrm{f}$ be a non-scalar monic polynomial over the field $\mathrm{F}$ and let

$$
\mathrm{f}=\mathrm{p}_{1}^{\mathrm{n}_{1}} \cdots \mathrm{p}_{\mathrm{k}_{\mathrm{k}}}^{\mathrm{n}_{\mathrm{k}}}
$$

be the prime factorization of f. For each $\mathrm{j}, 1 \leq \mathrm{j} \leq \mathrm{k}$, let

$$
\mathrm{f}_{\mathrm{j}}=\mathrm{f} / \mathrm{p}_{\mathrm{j}}^{\mathrm{n}_{\mathrm{j}}}=\prod_{\mathrm{i} \neq \mathrm{j}} \mathrm{p}^{\mathbf{h}_{\mathrm{i}}} \text {. }
$$

Then $\mathrm{f}_{1}, \ldots, \mathrm{f}_{\mathrm{k}}$ are relatively prime.

Proof. We leave the (easy) proof of this to the reader. We have stated this theorem largely because we wish to refer to it later.

Theorem 11. Let $\mathrm{f}$ be a polynomial over the field $\mathrm{F}$ with derivative $\mathrm{f}^{\prime}$. Then $\mathrm{f}$ is a product of distinct irreducible polynomials over $\mathrm{F}$ if and only if $\mathrm{f}$ and $\mathrm{f}^{\prime}$ are relatively prime.

Proof. Suppose in the prime factorization of $f$ over the field $F$ that some (non-scalar) prime polynomial $p$ is repeated. Then $f=p^{2} h$ for some $h$ in $F[x]$. Then

$$
f^{\prime}=p^{2} h^{\prime}+2 p p^{\prime} h
$$

and $p$ is also a divisor of $f^{\prime}$. Hence $f$ and $f^{\prime}$ are not relatively prime.

Now suppose $f=p_{1} \cdots p_{k}$, where $p_{1}, \ldots, p_{k}$ are distinct non-scalar irreducible polynomials over $F$. Let $f_{j}=f / p_{j}$. Then

$$
f^{\prime}=p_{1}^{\prime} f_{1}+p_{2}^{\prime} f_{2}+\cdots+p_{k}^{\prime} f_{k} .
$$

Let $p$ be a prime polynomial which divides both $f$ and $f^{\prime}$. Then $p=p_{i}$ for some $i$. Now $p_{i}$ divides $f_{j}$ for $j \neq i$, and since $p_{i}$ also divides

$$
f^{\prime}=\sum_{j=1}^{k} p_{j}^{\prime} f_{j}
$$

we see that $p_{i}$ must divide $p_{i}^{\prime} f_{i}$. Therefore $p_{i}$ divides either $f_{i}$ or $p_{i}^{\prime}$. But $p_{i}$ does not divide $f_{i}$ since $p_{1}, \ldots, p_{k}$ are distinct. So $p_{i}$ divides $p_{i}^{\prime}$. This is not possible, since $p_{i}^{\prime}$ has degree one less than the degree of $p_{i}$. We conclude that no prime divides both $f$ and $f^{\prime}$, or that, $f$ and $f^{\prime}$ are relatively prime.

Definition. The field $\mathrm{F}$ is called algebraically closed if every prime polynomial over $\mathrm{F}$ has degree 1.

To say that $F$ is algebraically closed means every non-scalar irreducible monic polynomial over $F$ is of the form $(x-c)$. We have already observed that each such polynomial is irreducible for any $F$. Accordingly, an equivalent definition of an algebraically closed field is a field $F$ such that each non-scalar polynomial $f$ in $F[x]$ can be expressed in the form

$$
f=c\left(x-c_{1}\right)^{n_{1}} \cdots\left(x-c_{k}\right)^{n_{k}}
$$

where $c$ is a scalar, $c_{1}, \ldots, c_{k}$ are distinct elements of $F$, and $n_{1}, \ldots, n_{k}$ are positive integers. Still another formulation is that if $f$ is a non-scalar polynomial over $F$, then there is an element $c$ in $F$ such that $f(c)=0$.

The field $R$ of real numbers is not algebraically closed, since the polynomial $\left(x^{2}+1\right)$ is irreducible over $R$ but not of degree 1 , or, because there is no real number $c$ such that $c^{2}+1=0$. The so-called Fundamental Theorem of Algebra states that the field $C$ of complex numbers is algebraically closed. We shall not prove this theorem, although we shall use it somewhat later in this book. The proof is omitted partly because of the limitations of time and partly because the proof depends upon a 'non-algebraic' property of the system of real numbers. For one possible proof the interested reader may consult the book by Schreier and Sperner in the Bibliography.

The Fundamental Theorem of Algebra also makes it clear what the possibilities are for the prime factorization of a polynomial with real coefficients. If $f$ is a polynomial with real coefficients and $c$ is a complex root of $f$, then the complex conjugate $\bar{c}$ is also a root of $f$. Therefore, those complex roots which are not real must occur in conjugate pairs, and the entire set of roots has the form $\left\{t_{1}, \ldots, t_{k}, c_{1}, \bar{c}_{1}, \ldots, c_{r}, \bar{c}_{r}\right\}$ where $t_{1}, \ldots, t_{k}$ are real and $c_{1}, \ldots, c_{r}$ are non-real complex numbers. Thus $f$ factors

$$
f=c\left(x-t_{1}\right) \cdots\left(x-t_{k}\right) p_{1} \cdots p_{r}
$$

where $p_{i}$ is the quadratic polynomial

$$
p_{i}=\left(x-c_{i}\right)\left(x-\bar{c}_{i}\right) .
$$

These polynomials $p_{i}$ have real coefficients. We conclude that every irreducible polynomial over the real number field has degree 1 or 2 . Each polynomial over $R$ is the product of certain linear factors, obtained from the real roots of $f$, and certain irreducible quadratic polynomials.

\section{Exercises}

1. Let $p$ be a monic polynomial over the field $F$, and let $f$ and $g$ be relatively prime polynomials over $F$. Prove that the g.c.d. of $p f$ and $p g$ is $p$.

2. Assuming the Fundamental Theorem of Algebra, prove the following. If $f$ and $g$ are polynomials over the field of complex numbers, then g.c.d. $(f, g)=1$ if and only if $f$ and $g$ have no common root.

3. Let $D$ be the differentiation operator on the space of polynomials over the field of complex numbers. Let $f$ be a monic polynomial over the field of complex numbers. Prove that

$$
f=\left(x-c_{1}\right) \cdots\left(x-c_{k}\right)
$$

where $c_{1}, \ldots, c_{k}$ are distinct complex numbers if and only if $f$ and $D f$ are relatively prime. In other words, $f$ has no repeated root if and only if $f$ and $D f$ have no common root. (Assume the Fundamental Theorem of Algebra.)

4. Prove the following generalization of Taylor's formula. Let $f, g$, and $h$ be polynomials over a subfield of the complex numbers, with $\operatorname{deg} f \leq n$. Then

$$
f(g)=\sum_{k=0}^{n} \frac{1}{k !} f^{(k)}(h)(g-h)^{k} .
$$

(Here $f(g)$ denotes ' $f$ of $g$. ')

For the remaining exercises, we shall need the following definition. If $f, g$, and $p$ are polynomials over the field $F$ with $p \neq 0$, we say that $f$ is congruent to $g$ modulo $p$ if $(f-g)$ is divisible by $p$. If $f$ is congruent to $g$ modulo $p$, we write

$$
f \equiv g \bmod p .
$$

5. Prove, for any non-zero polynomial $p$, that congruence modulo $p$ is an equivalence relation.

(a) It is reflexive: $f \equiv f \bmod p$.

(b) It is symmetric: if $f \equiv g \bmod p$, then $g \equiv f \bmod p$.

(c) It is transitive: if $f \equiv g \bmod p$ and $g=h \bmod p$, then $f \equiv h \bmod p$.

6. Suppose $f \equiv g \bmod p$ and $f_{1} \equiv g_{1} \bmod p$.

(a) Prove that $f+f_{1} \equiv g+g_{1} \bmod p$.

(b) Prove that $f f_{1} \equiv g g_{1} \bmod p$.

7. Use Exercise 7 to prove the following. If $f, g, h$, and $p$ are polynomials over the field $F$ and $p \neq 0$, and if $f=g \bmod p$, then $h(f) \equiv h(g) \bmod p$.

8. If $p$ is an irreducible polynomial and $f g \equiv 0 \bmod p$, prove that either $f \equiv 0 \bmod p$ or $g \equiv 0 \bmod p$. Give an example which shows that this is false if $p$ is not irreducible. 

\section{Determinants}

\subsection{Commutative Rings}

In this chapter we shall prove the essential facts about determinants of square matrices. We shall do this not only for matrices over a field, but also for matrices with entries which are 'scalars' of a more general type. There are two reasons for this generality. First, at certain points in the next chapter, we shall find it necessary to deal with determinants of matrices with polynomial entries. Second, in the treatment of determinants which we present, one of the axioms for a field plays no role, namely, the axiom which guarantees a multiplicative inverse for each non-zero element. For these reasons, it is appropriate to develop the theory of determinants for matrices, the entries of which are elements from a commutative ring with identity.

Definition. A ring is a set $\mathrm{K}$, together with two operations $(\mathrm{x}, \mathrm{y}) \rightarrow$ $\mathrm{x}+\mathrm{y}$ and $(\mathrm{x}, \mathrm{y}) \rightarrow \mathrm{xy}$ satisfying

(a) $\mathrm{K}$ is a commutative group under the operation $(\mathrm{x}, \mathrm{y}) \rightarrow \mathrm{x}+\mathrm{y}(\mathrm{K}$ is a commutative group under addition);

(b) $(\mathrm{xy}) \mathrm{z}=\mathrm{x}(\mathrm{yz})$ (multiplication is associative)

(c) $\mathrm{x}(\mathrm{y}+\mathrm{z})=\mathrm{xy}+\mathrm{xz} ;(\mathrm{y}+\mathrm{z}) \mathrm{x}=\mathrm{yx}+\mathrm{zx}$ (the two distributive laws hold).

If $\mathrm{xy}=\mathrm{y} \mathrm{x}$ for all $\mathrm{x}$ and $\mathrm{y}$ in $\mathrm{K}$, we say that the ring $\mathrm{K}$ is commutative. If there is an element 1 in $\mathrm{K}$ such that $\mathrm{x}=\mathrm{x} 1=\mathrm{x}$ for each $\mathrm{x}, \mathrm{K}$ is said to be $a$ ring with identity, and 1 is called the identity for $\mathrm{K}$. We are interested here in commutative rings with identity. Such a ring can be described briefly as a set $K$, together with two operations which satisf y all the axioms for a field given in Chapter 1, except possibly for axiom (8) and the condition $1 \neq 0$. Thus, a field is a commutative ring with non-zero identity such that to each non-zero $x$ there corresponds an element $x^{-1}$ with $x x^{-1}=1$. The set of integers, with the usual operations, is a commutative ring with identity which is not a field. Another commutative ring with identity is the set of all polynomials over a field, together with the addition and multiplication which we have defined for polynomials.

If $K$ is a commutative ring with identity, we define an $m \times n$ matrix over $K$ to be a function $A$ from the set of pairs $(i, j)$ of integers, $1 \leq i \leq m$, $1 \leq j \leq n$, into $K$. As usual we represent such a matrix by a rectangular array having $m$ rows and $n$ columns. The sum and product of matrices over $K$ are defined as for matrices over a field

$$
\begin{aligned}
(A+B)_{i j} & =A_{i j}+B_{i j} \\
(A B)_{i j} & =\sum_{k} A_{i k} B_{k j}
\end{aligned}
$$

the sum being defined when $A$ and $B$ have the same number of rows and the same number of columns, the product being defined when the number of columns of $A$ is equal to the number of rows of $B$. The basic algebraic properties of these operations are again valid. For example,

$$
A(B+C)=A B+A C, \quad(A B) C=A(B C), \quad \text { etc. }
$$

As in the case of fields, we shall refer to the elements of $K$ as scalars. We may then define linear combinations of the rows or columns of a matrix as we did earlier. Roughly speaking, all that we previously did for matrices over a field is valid for matrices over $K$, excluding those results which depended upon the ability to 'divide' in $K$.

\subsection{Determinant Functions}

Let $K$ be a commutative ring with identity. We wish to assign to each $n \times n$ (square) matrix over $K$ a scalar (element of $K$ ) to be known as the determinant of the matrix. It is possible to define the determinant of a square matrix $A$ by simply writing down a formula for this determinant in terms of the entries of $A$. One can then deduce the various properties of determinants from this formula. However, such a formula is rather complicated, and to gain some technical advantage we shall proceed as follows. We shall define a 'determinant function' on $K^{n \times n}$ as a function which assigns to each $n \times n$ matrix over $K$ a scalar, the function having these special properties. It is linear as a function of each of the rows of the matrix: its value is 0 on any matrix having two equal rows; and its value on the $n \times n$ identity matrix is 1 . We shall prove that such a function exists, and then that it is unique, i.e., that there is precisely one such function. As we prove the uniqueness, an explicit formula for the determinant will be obtained, along with many of its useful properties.

This section will be devoted to the definition of 'determinant function' and to the proof that at least one such function exists.

Definition. Let $\mathrm{K}$ be a commutative ring with identity, $\mathrm{n}$ a positive integer, and let $\mathrm{D}$ be a function which assigns to each $\mathrm{n} \times \mathrm{n}$ matrix $\mathrm{A}$ over $\mathrm{K}$ a scalar $\mathrm{D}$ (A) in $\mathrm{K}$. We say that $\mathrm{D}$ is n-linear if for each $\mathrm{i}, 1 \leq \mathrm{i} \leq \mathrm{n}$, $\mathrm{D}$ is a linear function of the $\mathrm{i}$ th row when the other $(\mathrm{n}-1)$ rows are held fixed.

This definition requires some clarification. If $D$ is a function from $K^{n \times n}$ into $K$, and if $\alpha_{1}, \ldots, \alpha_{n}$ are the rows of the matrix $A$, let us also write

$$
D(A)=D\left(\alpha_{1}, \ldots, \alpha_{n}\right)
$$

that is, let us also think of $D$ as the function of the rows of $A$. The statement that $D$ is $n$-linear then means

$$
\begin{aligned}
& D\left(\alpha_{1}, \ldots, c \alpha_{i}+\alpha_{l}^{\prime}, \ldots, \alpha_{n}\right)=c D\left(\alpha_{1}, \ldots, \alpha_{i}, \ldots, \alpha_{n}\right) \\
&+D\left(\alpha_{1}, \ldots, \alpha_{1}^{\prime}, \ldots, \alpha_{n}\right) .
\end{aligned}
$$

If we fix all rows except row $i$ and regard $D$ as a function of the $i$ th row, it is of ten convenient to write $D\left(\alpha_{i}\right)$ for $D(A)$. Thus, we may abbreviate (5-1) to

$$
D\left(c \alpha_{i}+\alpha_{i}^{\prime}\right)=c D\left(\alpha_{i}\right)+D\left(\alpha_{i}^{\prime}\right)
$$

so long as it is clear what the meaning is.

Example 1 . Let $k_{1}, \ldots, k_{n}$ be positive integers, $1 \leq k_{i} \leq n$, and let $a$ be an element of $K$. For each $n \times n$ matrix $A$ over $K$, define

$$
D(A)=a A\left(1, k_{1}\right) \cdots A\left(n, k_{n}\right) .
$$

Then the function $D$ defined by $(5-2)$ is $n$-linear. For, if we regard $D$ as a function of the $i$ th row of $A$, the others being fixed, we may write

$$
D\left(\alpha_{i}\right)=A\left(i, k_{i}\right) b
$$

where $b$ is some fixed element of $K$. Let $\alpha_{i}^{\prime}=\left(A_{i 1}^{\prime}, \ldots, A_{i n}^{\prime}\right)$. Then we have

$$
\begin{aligned}
D\left(c \alpha_{i}+\alpha_{i}\right) & =\left[c A\left(i, k_{i}\right)+A^{\prime}\left(i, k_{i}\right)\right] b \\
& =c D\left(\alpha_{i}\right)+D\left(\alpha_{i}^{\prime}\right) .
\end{aligned}
$$

Thus $D$ is a linear function of each of the rows of $A$.

A particular $n$-linear function of this type is

$$
D(A)=A_{11} A_{22} \cdots A_{n n} .
$$

In other words, the 'product of the diagonal entries' is an $n$-linear function on $K^{n \times n}$.

Example 2. Let us find all 2-linear functions on $2 \times 2$ matrices over $K$. Let $D$ be such a function. If we denote the rows of the $2 \times 2$ identity matrix by $\epsilon_{1}, \epsilon_{2}$, we have

$$
D(A)=D\left(A_{11} \epsilon_{1}+A_{12} \epsilon_{2}, A_{21} \epsilon_{1}+A_{22 \epsilon_{2}}\right) .
$$

Using the fact that $D$ is 2 -linear, (.5-1), we have

$$
\begin{aligned}
D(A) & =A_{11} D\left(\epsilon_{1}, A_{21} \epsilon_{1}+A_{22} \epsilon_{2}\right)+A_{12} D\left(\epsilon_{2}, A_{21} \epsilon_{1}+A_{22} \epsilon_{2}\right) \\
= & A_{11} A_{21} D\left(\epsilon_{1}, \epsilon_{1}\right)+A_{11} A_{22} D\left(\epsilon_{1}, \epsilon_{2}\right) \\
\quad & \quad+A_{12} A_{21} D\left(\epsilon_{2}, \epsilon_{1}\right)+A_{12} A_{22} D\left(\epsilon_{2}, \epsilon_{2}\right) .
\end{aligned}
$$

Thus $D$ is completely determined by the four scalars

$$
D\left(\epsilon_{1}, \epsilon_{1}\right), \quad D\left(\epsilon_{1}, \epsilon_{2}\right), \quad D\left(\epsilon_{2}, \epsilon_{1}\right), \quad \text { and } \quad D\left(\epsilon_{2}, \epsilon_{2}\right) \text {. }
$$

The reader should find it easy to verify the following. If $a, b, c, d$ are any four scalars in $K$ and if we define

$$
D(A)=A_{11} A_{21} a+A_{11} A_{22} b+A_{12} A_{21} c+A_{12} A_{22} d
$$

then $D$ is a 2-linear function on $2 \times 2$ matrices over $K$ and

$$
\begin{array}{ll}
D\left(\epsilon_{1}, \epsilon_{1}\right)=a, & D\left(\epsilon_{1}, \epsilon_{2}\right)=b \\
D\left(\epsilon_{2}, \epsilon_{1}\right)=c, & D\left(\epsilon_{2}, \epsilon_{2}\right)=d .
\end{array}
$$

Lemma. A linear combination of n-linear functions is n-linear.

Proof. It suffices to prove that a linear combination of two $n$-linear functions is $n$-linear. Let $D$ and $E$ be $n$-linear functions. If $a$ and $b$ belong to $K$, the linear combination $a D+b E$ is of course defined by

$$
(a D+b E)(A)=a D(A)+b E(A) .
$$

Hence, if w e fix all rows except row $i$

$$
\begin{aligned}
(a D+b E)\left(c \alpha_{i}+\alpha_{i}^{\prime}\right) & =a D\left(c \alpha_{i}+\alpha_{i}^{\prime}\right)+b E\left(c \alpha_{i}+\alpha_{i}^{\prime}\right) \\
& =a c D\left(\alpha_{i}\right)+a D\left(\alpha_{i}^{\prime}\right)+b c E\left(\alpha_{i}\right)+b E\left(\alpha_{i}^{\prime}\right) \\
& =c(a D+b E)\left(\alpha_{i}\right)+(a D+b E)\left(\alpha_{i}^{\prime}\right) .
\end{aligned}
$$

If $K$ is a field and $V$ is the set of $n \times n$ matrices over $K$, the above lemma says the following. The set of $n$-linear functions on $V$ is a subspace of the space of all functions from $V$ into $K$.

EXAMPle 3. Let $D$ be the function defined on $2 \times 2$ matrices over $K$ by

$$
D(A)=A_{11} A_{22}-A_{12} A_{21} .
$$

Now $D$ is the sum of two functions of the type described in Example 1:

$$
\begin{aligned}
D & =D_{1}+D_{2} \\
D_{1}(A) & =A_{11} A_{22} \\
D_{2}(A) & =-A_{12} A_{21} .
\end{aligned}
$$

By the above lemma, $D$ is a 2-linear function. The reader who has had any experience with determinants will not find this surprising, since he will recognize (5-3) as the usual definition of the determinant of a $2 \times 2$ matrix. Of course the function $D$ we have just defined is not a typical 2-linear function. It has many special properties. Let us note some of these properties. First, if $I$ is the $2 \times 2$ identity matrix, then $D(I)=1$, i.e., $D\left(\epsilon_{1}, \epsilon_{2}\right)=1$. Second, if the two rows of $A$ are equal, then

$$
D(A)=A_{11} A_{12}-A_{12} A_{11}=0 .
$$

Third, if $A^{\prime}$ is the matrix obtained from a $2 \times 2$ matrix $A$ by interchanging its rows, then $D\left(A^{\prime}\right)=-D(A)$; for

$$
\begin{aligned}
D\left(A^{\prime}\right) & =A_{11}^{\prime} A_{22}^{\prime}-A_{12}^{\prime} A_{21}^{\prime} \\
& =A_{21} A_{12}-A_{22} A_{11} \\
& =-D(A) .
\end{aligned}
$$

Definition. Let $\mathrm{D}$ be an $\mathrm{n}$-linear function. We say $\mathrm{D}$ is alternating (or alternate) if the following two conditions are satisfied:

(a) $\mathrm{D}(\mathrm{A})=0$ whenever two rows of $\mathrm{A}$ are equal.

(b) If $\mathrm{A}^{\prime}$ is a matrix obtained from $\mathrm{A}$ by interchanging two rows of $\mathrm{A}$, then $\mathrm{D}\left(\mathrm{A}^{\prime}\right)=-\mathrm{D}(\mathrm{A})$.

We shall prove below that any $n$-linear function $D$ which satisfies (a) automatically satisfies (b). We have put both properties in the definition of alternating $n$-linear function as a matter of convenience. The reader will probably also note that if $D$ satisfies (b) and $A$ is a matrix with two equal rows, then $D(A)=-D(A)$. It is tempting to conclude that $D$ satisfies condition (a) as well. This is true, for example, if $K$ is a field in which $1+1 \neq 0$, but in general (a) is not a consequence of (b).

Definition. Let $\mathrm{K}$ be a commutative ring with identity, and let $\mathrm{n}$ be a positive integer. Suppose $\mathrm{D}$ is a function from $\mathrm{n} \times \mathrm{n}$ matrices over $\mathrm{K}$ into $\mathrm{K}$. We say that $\mathrm{D}$ is a determinant function if $\mathrm{D}$ is n-linear, alternating, and $\mathrm{D}(\mathrm{I})=1$.

As we stated earlier, we shall ultimately show that there is exactly one determinant function on $n \times n$ matrices over $K$. This is easily seen for $1 \times 1$ matrices $A=[a]$ over $K$. The function $D$ given by $D(A)=a$ is a determinant function, and clearly this is the only determinant function on $1 \times 1$ matrices. We are also in a position to dispose of the case $n=2$. The function

$$
D(A)=A_{11} A_{22}-A_{12} A_{21}
$$

was shown in Example 3 to be a determinant function. Furthermore, the formula exhibited in Example 2 shows that $D$ is the only determinant function on $2 \times 2$ matrices. For we showed that for any 2 -linear function $D$ $D(A)=A_{11} A_{21} D\left(\epsilon_{1}, \epsilon_{1}\right)+A_{11} A_{22} D\left(\epsilon_{1}, \epsilon_{2}\right)$

If $D$ is alternating, then

$$
+A_{12} A_{21} D\left(\epsilon_{2}, \epsilon_{1}\right)+A_{12} A_{22} D\left(\epsilon_{2}, \epsilon_{2}\right) .
$$

$$
D\left(\epsilon_{1}, \epsilon_{1}\right)=D\left(\epsilon_{2}, \epsilon_{2}\right)=0
$$

and

$$
D\left(\epsilon_{2}, \epsilon_{1}\right)=-D\left(\epsilon_{1}, \epsilon_{2}\right)=-D(I) .
$$

If $D$ also satisfies $D(I)=1$, then

$$
D(A)=A_{11} A_{22}-A_{12} A_{21} .
$$

EXAMPLE 4 . Let $F$ be a field and let $D$ be any alternating 3-linear function on $3 \times 3$ matrices over the polynomial ring $F[x]$.

Let

$$
A=\left[\begin{array}{ccc}
x & 0 & -x^{2} \\
0 & 1 & 0 \\
1 & 0 & x^{3}
\end{array}\right] .
$$

If we denote the rows of the $3 \times 3$ identity matrix by $\epsilon_{1}, \epsilon_{2}$, $\epsilon_{3}$, then

$$
D(A)=D\left(x \epsilon_{1}-x^{2} \epsilon_{3}, \epsilon_{2}, \epsilon_{1}+x^{3} \epsilon_{3}\right) .
$$

Since $D$ is linear as a function of each row,

$$
\begin{aligned}
D(A) & =x D\left(\epsilon_{1}, \epsilon_{2}, \epsilon_{1}+x^{3} \epsilon_{3}\right)-x^{2} D\left(\epsilon_{3}, \epsilon_{2}, \epsilon_{1}+x^{3} \epsilon_{3}\right) \\
& =x D\left(\epsilon_{1}, \epsilon_{2}, \epsilon_{1}\right)+x^{4} D\left(\epsilon_{1}, \epsilon_{2}, \epsilon_{3}\right)-x^{2} D\left(\epsilon_{3}, \epsilon_{2}, \epsilon_{1}\right)-x^{5} D\left(\epsilon_{3}, \epsilon_{2}, \epsilon_{3}\right) .
\end{aligned}
$$

Because $D$ is alternating it follows that

$$
D(A)=\left(x^{4}+x^{2}\right) D\left(\epsilon_{1}, \epsilon_{2}, \epsilon_{3}\right) .
$$

Lemma. Let $\mathrm{D}$ be a 2-linear function with the property that $\mathrm{D}(\mathrm{A})=0$ for all $2 \times 2$ matrices $\mathrm{A}$ over $\mathrm{K}$ having equal rows. Then $\mathrm{D}$ is alternating.

Proof. What we must show is that if $A$ is a $2 \times 2$ matrix and $A^{\prime}$ is obtained by interchanging the rows of $A$, then $D\left(A^{\prime}\right)=-D(A)$. If the rows of $A$ are $\boldsymbol{\alpha}$ and $\boldsymbol{\beta}$, this means we must show that $D(\beta, \alpha)=-D(\alpha, \boldsymbol{\beta})$. Since $D$ is 2 -linear,

$$
D(\alpha+\beta, \alpha+\beta)=D(\alpha, \alpha)+D(\alpha, \beta)+D(\beta, \alpha)+D(\beta, \beta) .
$$

By our hypothesis $D(\alpha+\beta, \alpha+\beta)=D(\alpha, \alpha)=D(\beta, \beta)=0$. So

$$
\mathbf{0}=D(\alpha, \beta)+D(\beta, \boldsymbol{\alpha}) \text {. }
$$

Lemma. Let $\mathrm{D}$ be an $\mathrm{n}$-linear function on $\mathrm{n} \times \mathrm{n}$ matrices over $\mathrm{K}$. Suppose $\mathrm{D}$ has the property that $\mathrm{D}(\mathrm{A})=0$ whenever two adjacent rows of A are equal. Then $\mathrm{D}$ is alternating.

Proof. We must show that $D(A)=0$ when any two rows of $A$ are equal, and that $D\left(A^{\prime}\right)=-D(A)$ if $A^{\prime}$ is obtained by interchanging some two rows of $A$. First, let us suppose that $A^{\prime}$ is obtained by interchanging two adjacent rows of $A$. The reader should see that the argument used in the proof of the preceding lemma extends to the present case and gives us $D\left(A^{\prime}\right)=-D(A)$.

Now let $B$ be obtained by interchanging rows $i$ and $j$ of $A$, where $i<j$. We can obtain $B$ from $A$ by a succession of interchanges of pairs of adjacent rows. We begin by interchanging row $i$ with row $(i+1)$ and continue until the rows are in the order

$$
\alpha_{1}, \ldots, \alpha_{i-1}, \alpha_{i+1}, \ldots, \alpha_{j}, \alpha_{i}, \alpha_{j+1}, \ldots, \alpha_{n} .
$$

This requires $k=j-i$ interchanges of adjacent rows. We now move $\alpha_{j}$ to the $i$ th position using $(k-1)$ interchanges of adjacent rows. We have thus obtained $B$ from $A$ by $k+(k-1)=2 k-1$ interchanges of adjacent rows. 'Thus

$$
D(B)=(-1)^{2 k-1} D(A)=-D(A) .
$$

Suppose $A$ is any $n \times n$ matrix with two equal rows, say $\alpha_{i}=\alpha_{j}$ with $i<j$. If $j=i+1$, then $A$ has two equal and adjacent rows and $D(A)=0$. If $j>i+1$, we interchange $\alpha_{i+1}$ and $\alpha_{j}$ and the resulting matrix $B$ has two equal and adjacent rows, so $D(B)=0$. On the other hand, $D(B)=-D(A)$, hence $D(A)=0$.

Definition. If $\mathrm{n}>1$ and $\mathrm{A}$ is an $\mathrm{n} \times \mathrm{n}$ matrix over $\mathrm{K}$, we let $\mathrm{A}(\mathrm{i} \mid \mathrm{j})$ denote the $(\mathrm{n}-1) \times(\mathrm{n}-1)$ matrix obtained by deleting the ith row and $\mathrm{j}$ th column of $\mathrm{A}$. If $\mathrm{D}$ is an $(\mathrm{n}-1)$-linear function and $\mathrm{A}$ is an $\mathrm{n} \times \mathrm{n}$ matrix, we put $\mathrm{D}_{\mathrm{i} j}(\mathrm{~A})=\mathrm{D}[\mathrm{A}(\mathrm{i} \mid \mathrm{j})]$.

Theorem 1. Let $\mathrm{n}>1$ and let $\mathrm{D}$ be an alternating $(\mathrm{n}-1)$-linear function on $(n-1) \times(n-1)$ matrices over $K$. For each $j, 1 \leq j \leq n$, the function $\mathrm{E}_{\mathrm{j}}$ defined by

$$
E_{j}(A)=\sum_{i=1}^{n}(-1)^{i+i} A_{i j} D_{i j}(A)
$$

is an alternating $\mathrm{n}$-linear function on $\mathrm{n} \times \mathrm{n}$ matrices $\mathrm{A}$. If $\mathrm{D}$ is a determinant function, so is each $\mathrm{E}_{\mathbf{j}}$.

Proof. If $A$ is an $n \times n$ matrix, $D_{i j}(A)$ is independent of the $i$ th row of $A$. Since $D$ is $(n-1)$-linear, it is clear that $D_{i j}$ is linear as a function of any row except row $i$. Therefore $A_{i j} D_{i j}(A)$ is an $n$-linear function of $A$. A linear combination of $n$-linear functions is $n$-linear; hence, $E_{j}$ is $n$-linear. To prove that $E_{j}$ is alternating, it will suffice to show that $E_{j}(A)=0$ whenever $A$ has two equal and adjacent rows. Suppose $\alpha_{k}=$ $\alpha_{k+1}$. If $i \neq k$ and $i \neq k+1$, the matrix $A(i \mid j)$ has two equal rows, and thus $D_{i j}(A)=0$. Therefore

$$
E_{j}(A)=(-1)^{k+i} A_{k j} D_{k j}(A)+(-1)^{k+1+j} A_{(k+1) j} D_{(k+1) j}(A) .
$$

Since $\alpha_{k}=\alpha_{k+1}$

$$
A_{k j}=A_{(k+1) j} \text { and } A(k \mid j)=A(k+1 \mid j) .
$$

Clearly then $E_{j}(A)=0$.

Now suppose $D$ is a determinant function. If $I^{(n)}$ is the $n \times n$ identity matrix, then $I^{(n)}(j \mid j)$ is the $(n-1) \times(n-1)$ identity matrix $I^{(n-1)}$. Since $I_{i j}^{(n)}=\delta_{i j}$, it follows from (5-4) that

$$
E_{j}\left(I^{(n)}\right)=D\left(I^{(n-1)}\right) .
$$

Now $D\left(I^{(n-1)}\right)=1$, so that $E_{j}\left(I^{(n)}\right)=1$ and $E_{j}$ is a determinant function.

Corollary. Let $\mathrm{K}$ be a commutative ring with identity and let $\mathrm{n}$ be a positive integer. There exists at least one determinant function on $\mathrm{K}^{\mathrm{n} \times \mathrm{n}}$.

Proof. We have shown the existence of a determinant function on $1 \times 1$ matrices over $K$, and even on $2 \times 2$ matrices over $K$. Theorem 1 tells us explicitly how to construct a determinant function on $n \times n$ matrices, given such a function on $(n-1) \times(n-1)$ matrices. The corollary follows by induction.

Example 5 . If $B$ is a $2 \times 2$ matrix over $K$, we let

$$
|B|=B_{11} B_{22}-B_{12} B_{21} \text {. }
$$

Then $|B|=D(B)$, where $D$ is the determinant function on $2 \times 2$ matrices. We showed that this function on $K^{2 \times 2}$ is unique. Let

$$
A=\left[\begin{array}{lll}
A_{11} & A_{12} & A_{13} \\
A_{21} & A_{22} & A_{23} \\
A_{31} & A_{32} & A_{33}
\end{array}\right]
$$

be a $3 \times 3$ matrix over $K$. If we define $E_{1}, E_{2}, E_{3}$ as in (5-4), then

$$
\begin{aligned}
& E_{1}(A)=A_{11}\left|\begin{array}{ll}
A_{22} & A_{23} \\
A_{32} & A_{33}
\end{array}\right|-A_{21}\left|\begin{array}{ll}
A_{12} & A_{13} \\
A_{32} & A_{33}
\end{array}\right|+A_{31}\left|\begin{array}{ll}
A_{12} & A_{13} \\
A_{22} & A_{23}
\end{array}\right| \\
& E_{2}(A)=-A_{12}\left|\begin{array}{ll}
A_{21} & A_{23} \\
A_{31} & A_{33}
\end{array}\right|+A_{22}\left|\begin{array}{ll}
A_{11} & A_{13} \\
A_{31} & A_{33}
\end{array}\right|-A_{32}\left|\begin{array}{ll}
A_{11} & A_{13} \\
A_{21} & A_{23}
\end{array}\right| \\
& E_{3}(A)=A_{13}\left|\begin{array}{ll}
A_{21} & A_{22} \\
A_{31} & A_{32}
\end{array}\right|-A_{23}\left|\begin{array}{ll}
A_{11} & A_{12} \\
A_{31} & A_{32}
\end{array}\right|+A_{33}\left|\begin{array}{ll}
A_{11} & A_{12} \\
A_{21} & A_{22}
\end{array}\right| .
\end{aligned}
$$

It follows from Theorem 1 that $E_{1}, E_{2}$, and $E_{3}$ are determinant functions. Actually, as we shall show later, $E_{1}=E_{2}=E_{3}$, but this is not yet apparent even in this simple case. It could, however, be verified directly, by expanding each of the above expressions. Instead of doing this we give some specific examples.

(a) Let $K=R[x]$ and

$$
A=\left[\begin{array}{ccc}
x-1 & x^{2} & x^{3} \\
0 & x-2 & 1 \\
0 & 0 & x-3
\end{array}\right] .
$$

Then

$$
\begin{aligned}
E_{1}(A) & =(x-1)\left|\begin{array}{cc}
x-2 & 1 \\
0 & x-3
\end{array}\right|=(x-1)(x-2)(x-3) \\
E_{2}(A) & =-x^{2}\left|\begin{array}{cc}
0 & 1 \\
0 & x-3
\end{array}\right|+(x-2)\left|\begin{array}{cc}
x-1 & x^{3} \\
0 & x-3
\end{array}\right| \\
& =(x-1)(x-2)(x-3)
\end{aligned}
$$

and

$$
\begin{aligned}
E_{3}(A) & =x^{8}\left|\begin{array}{lc}
0 & x-2 \\
0 & 0
\end{array}\right|-\left|\begin{array}{cc}
x-1 & x^{2} \\
0 & 0
\end{array}\right|+(x-3)\left|\begin{array}{cc}
x-1 & x^{2} \\
0 & x-2
\end{array}\right| \\
& =(x-1)(x-2)(x-3) .
\end{aligned}
$$

(b) Let $K=R$ and

Then

$$
A=\left[\begin{array}{lll}
0 & 1 & 0 \\
0 & 0 & 1 \\
1 & 0 & 0
\end{array}\right] .
$$

$$
\begin{aligned}
& E_{1}(A)=\left|\begin{array}{ll}
1 & 0 \\
0 & 1
\end{array}\right|=1 \\
& E_{2}(A)=-\left|\begin{array}{ll}
0 & 1 \\
1 & 0
\end{array}\right|=1 \\
& E_{3}(A)=-\left|\begin{array}{ll}
0 & 1 \\
1 & 0
\end{array}\right|=1 .
\end{aligned}
$$

\section{Exercises}

1. Each of the following expressions defines a function $D$ on the set of $3 \times 3$ matrices over the field of real numbers. In which of these cases is $D$ a 3-linear function?

(a) $D(A)=A_{11}+A_{22}+A_{33}$;

(b) $D(A)=\left(A_{11}\right)^{2}+3 A_{11} A_{22}$;

(c) $D(A)=A_{11} A_{12} A_{33}$;

(d) $D(A)=A_{13} A_{22} A_{32}+5 A_{12} A_{22} A_{32}$;

(e) $D(A)=0$;

(f) $D(A)=1$.

2. Verify directly that the three functions $E_{1}, E_{2}, E_{3}$ defined by (5-6), (5-7), and (5-8) are identical.

3. Let $K$ be a commutative ring with identity. If $A$ is a $2 \times 2$ matrix over $K$, the classical adjoint of $A$ is the $2 \times 2$ matrix adj $A$ defined by

$$
\operatorname{adj} A=\left[\begin{array}{rr}
A_{22} & -A_{12} \\
-A_{21} & A_{11}
\end{array}\right] \text {. }
$$

If det denotes the unique determinant function on $2 \times 2$ matrices over $K$, show that (a) $(\operatorname{adj} A) A=A(\operatorname{adj} A)=(\operatorname{det} A) I$

(b) $\operatorname{det}(\operatorname{adj} A)=\operatorname{det}(A)$;

(c) $\operatorname{adj}\left(A^{t}\right)=(\operatorname{adj} A)^{t}$.

$\left(A^{t}\right.$ denotes the transpose of $\left.A.\right)$

4. Let $A$ be a $2 \times 2$ matrix over a field $F$. Show that $A$ is invertible if and only if $\operatorname{det} A \neq 0$. When $A$ is invertible, give a formula for $A^{-1}$.

5. Let $A$ be a $2 \times 2$ matrix over a field $F$, and suppose that $A^{2}=0$. Show for each scalar $c$ that $\operatorname{det}(c I-A)=c^{2}$.

6. Let $K$ be a subfield of the complex numbers and $n$ a positive integer. Let $j_{1}, \ldots, j_{n}$ and $k_{1}, \ldots, k_{n}$ be positive integers not exceeding $n$. For an $n \times n$ matrix $A$ over $K$ define

$$
D(A)=A\left(j_{1}, k_{1}\right) A\left(j_{2}, k_{2}\right) \cdots A\left(j_{n}, k_{n}\right) .
$$

Prove that $D$ is $n$-linear if and only if the integers $j_{1}, \ldots, j_{n}$ are distinct.

7. Let $K$ be a commutative ring with identity. Show that the determinant function on $2 \times 2$ matrices $A$ over $K$ is alternating and 2-linear as a function of the columns of $A$.

8. Let $K$ be a commutative ring with identity. Define a function $D$ on $3 \times 3$ matrices over $K$ by the rule

$$
D(A)=A_{11} \operatorname{det}\left[\begin{array}{ll}
A_{22} & A_{22} \\
A_{32} & A_{33}
\end{array}\right]-A_{12} \operatorname{det}\left[\begin{array}{ll}
A_{21} & A_{23} \\
A_{31} & A_{33}
\end{array}\right]+A_{13} \operatorname{det}\left[\begin{array}{ll}
A_{21} & A_{22} \\
A_{31} & A_{32}
\end{array}\right] .
$$

Show that $D$ is alternating and 3 -linear as a function of the columns of $A$.

9. Let $K$ be a commutative ring with identity and $D$ an alternating $n$-linear function on $n \times n$ matrices over $K$. Show that

(a) $D(A)=0$, if one of the rows of $A$ is 0 .

(b) $D(B)=D(A)$, if $B$ is obtained from $A$ by adding a scalar multiple of one row of $A$ to another.

10. Let $F$ be a field, $A$ a $2 \times 3$ matrix over $F$, and $\left(c_{1}, c_{2}, c_{3}\right)$ the vector in $F^{3}$ defined by

Show that

$$
c_{1}=\left|\begin{array}{ll}
A_{12} & A_{13} \\
A_{22} & A_{23}
\end{array}\right|, \quad c_{2}=\left|\begin{array}{ll}
A_{13} & A_{11} \\
A_{23} & A_{21}
\end{array}\right|, \quad c_{3}=\left|\begin{array}{ll}
A_{11} & A_{12} \\
A_{21} & A_{22}
\end{array}\right| .
$$

(a) $\operatorname{rank}(A)=2$ if and only if $\left(c_{1}, c_{2}, c_{3}\right) \neq 0$;

(b) if $A$ has rank 2 , then $\left(c_{1}, c_{2}, c_{3}\right)$ is a basis for the solution space of the system of equations $A X=0$.

11. Let $K$ be a commutative ring with identity, and let $D$ be an alternating 2 -linear function on $2 \times 2$ matrices over $K$. Show that $D(A)=(\operatorname{det} A) D(I)$ for all $A$. Now use this result (no computations with the entries allowed) to show that $\operatorname{det}(A B)=(\operatorname{det} A)(\operatorname{det} B)$ for any $2 \times 2$ matrices $A$ and $B$ over $K$.

12. Let $F$ be a field and $D$ a function on $n \times n$ matrices over $F$ (with values in $F$ ). Suppose $D(A B)=D(A) D(B)$ for all $A, B$. Show that either $D(A)=0$ for all $A$, or $D(I)=1$. In the latter case show that $D(A) \neq 0$ whenever $A$ is invertible.

13. Let $R$ be the field of real numbers, and let $D$ be a function on $2 \times 2$ matrices over $R$, with values in $R$, such that $D(A B)=D(A) D(B)$ for all $A, B$. Suppose also that

Prove the following.

$$
D\left(\left[\begin{array}{ll}
0 & 1 \\
1 & 0
\end{array}\right]\right) \neq D\left(\left[\begin{array}{ll}
1 & 0 \\
0 & 1
\end{array}\right]\right)
$$

(a) $D(0)=0$

(b) $D(A)=0$ if $A^{2}=0$

(c) $D(B)=-D(A)$ if $B$ is obtained by interchanging the rows (or columns) of $A$;

(d) $D(A)=0$ if one row (or one column) of $A$ is 0 ;

(e) $D(A)=0$ whenever $A$ is singular.

14. Let $A$ be a $2 \times 2$ matrix over a field $F$. Then the set of all matrices of the form $f(A)$, where $f$ is a polynomial over $F$, is a commutative ring $K$ with identity. If $B$ is a $2 \times 2$ matrix over $K$, the determinant of $B$ is then a $2 \times 2$ matrix over $F$, of the form $f(A)$. Suppose $I$ is the $2 \times 2$ identity matrix over $F$ and that $B$ is the $2 \times 2$ matrix over $K$

$$
B=\left[\begin{array}{cc}
A-A_{11} I & -A_{12} I \\
-A_{21} I & A-A_{22} I
\end{array}\right] .
$$

Show that $\operatorname{det} B=f(A)$, where $f=x^{2}-\left(A_{11}+A_{22}\right) x+\operatorname{det} A$, and also that $f(A)=0$.

\subsection{Permutations and the Uniqueness of Determinants}

In this section we prove the uniqueness of the determinant function on $n \times n$ matrices over $K$. The proof will lead us quite naturally to consider permutations and some of their basic properties.

Suppose $D$ is an alternating $n$-linear function on $n \times n$ matrices over $K$. Let $A$ be an $n \times n$ matrix over $K$ with rows $\alpha_{1}, \alpha_{2}, \cdots, \alpha_{n}$. If we de:note the rows of the $n \times n$ identity matrix over $K$ by $\epsilon_{1}, \epsilon_{2}, \cdots, \epsilon_{n}$, then

$$
\alpha_{i}=\sum_{j=1}^{n} A(i, j) \epsilon_{j}, \quad 1 \leq i \leq n .
$$

Hence

$$
\begin{aligned}
D(A) & =D\left(\sum_{j} A(1, j) \epsilon_{j}, \alpha_{2}, \ldots, \alpha_{n}\right) \\
& =\sum_{j} A(1, j) D\left(\epsilon_{j}, \alpha_{2}, \ldots, \alpha_{n}\right) .
\end{aligned}
$$

If we now replace $\alpha_{2}$ by $\sum_{k} A(2, k) \epsilon_{k}$, we see that

Thus

$$
D\left(\epsilon_{j}, \alpha_{2}, \ldots, \alpha_{n}\right)=\sum_{k} A(2, k) D\left(\epsilon_{j}, \epsilon_{k}, \ldots, \alpha_{n}\right) .
$$

$$
D(A)=\sum_{j, k} A(1, j) A(2, k) D\left(\epsilon_{j}, \epsilon_{k}, \ldots, \alpha_{n}\right)
$$

In $D\left(\epsilon_{j}, \epsilon_{k}, \ldots, \alpha_{n}\right)$ we next replace $\alpha_{3}$ by $\Sigma A(3, l) \epsilon_{l}$ and so on. We finally obtain a complicated but theoretically important expression for $D(A)$, namely

$$
D(A)=\underset{k_{1}, k_{2}, \ldots, k_{n}}{\boldsymbol{\Sigma}} A\left(1, k_{1}\right) A\left(2, k_{2}\right) \cdots A\left(n, k_{n}\right) D\left(\epsilon_{k_{1}}, \epsilon_{k_{2}}, \ldots, \epsilon_{k_{n}}\right) .
$$

In (5-10) the sum is extended over all sequences $\left(k_{1}, k_{2}, \ldots, k_{n}\right)$ of positive integers not exceeding $n$. This shows that $D$ is a finite sum of functions of the type described by (5-2). It should be noted that (5-10) is a consequence just of assumption that $D$ is $n$-linear, and that a special case of (5-10) was obtained in Example 2. Since $D$ is alternating,

$$
D\left(\epsilon_{k_{1}}, \epsilon_{k_{2}}, \ldots, \epsilon_{k_{n}}\right)=0
$$

whenever two of the indices $k_{i}$ are equal. A sequence $\left(k_{1}, k_{2}, \ldots, k_{n}\right)$ of positive integers not exceeding $n$, with the property that no two of the $k_{i}$ are equal, is called a permutation of degree $n$. In (5-10) we need therefore sum only over those sequences which are permutations of degree $n$.

Since a finite sequence, or $n$-tuple, is a function defined on the first $n$ positive integers, a permutation of degree $n$ may be defined as a one-one function from the set $\{1,2, \ldots, n\}$ onto itself. Such a function $\sigma$ corresponds to the $n$-tuple $(\sigma 1, \sigma 2, \ldots, \sigma n)$ and is thus simply a rule for ordering $1,2, \ldots, n$ in some well-defined way.

If $D$ is an alternating $n$-linear function and $A$ is an $n \times n$ matrix over $K$, we then have

$$
D(A)=\sum_{\sigma} A(1, \sigma 1) \cdots A(n, \sigma n) D\left(\epsilon_{\sigma 1}, \ldots, \epsilon_{\sigma n}\right)
$$

where the sum is extended over the distinct permutations $\sigma$ of degree $n$.

Next we shall show that

$$
D\left(\epsilon_{o 1}, \ldots, \epsilon_{\sigma n}\right)=\pm D\left(\epsilon_{1}, \ldots, \epsilon_{n}\right)
$$

where the sign $\pm$ depends only on the permutation $\sigma$. The reason for this is as follows. The sequence $(\sigma 1, \sigma 2, \ldots, \sigma n)$ can be obtained from the sequence $(1,2, \ldots, n)$ by a finite number of interchanges of pairs of elements. For example, if $\sigma 1 \neq 1$, we can transpose 1 and $\sigma 1$, obtaining $(\sigma 1, \ldots, 1, \ldots)$. Proceeding in this way we shall arrive at the sequence $(\sigma 1, \ldots, \sigma n)$ after $n$ or less such interchanges of pairs. Since $D$ is alternating, the sign of its value changes each time that we interchange two of the rows $\epsilon_{i}$ and $\epsilon_{j}$. Thus, if we pass from $(1,2, \ldots, n)$ to $(\sigma 1, \sigma 2, \ldots, \sigma n)$ by means of $m$ interchanges of pairs $(i, j)$, we shall have

$$
D\left(\epsilon_{\sigma 1}, \ldots, \epsilon_{\sigma n}\right)=(-1)^{m} D\left(\epsilon_{1}, \ldots, \epsilon_{n}\right) .
$$

In particular, if $D$ is a determinant function

$$
D\left(\epsilon_{\sigma 1}, \ldots, \epsilon_{\sigma n}\right)=(-1)^{m}
$$

where $m$ depends only upon $\sigma$, not upon $D$. Thus all determinant functions assign the same value to the matrix with rows $\epsilon_{\sigma 1}, \ldots, \epsilon_{\sigma_{n}}$, and this value is either 1 or $-1$.

Now a basic fact about permutations is the following. If $\sigma$ is a permutation of degree $n$, one can pass from the sequence $(1,2, \ldots, n)$ to the sequence $(\sigma 1, \sigma 2, \ldots, \sigma n)$ by a succession of interchanges of pairs, and this can be done in a variety of ways; however, no matter how it is done, the number of interchanges used is either always even or always odd. The permutation is then called even or odd, respectively. One defines the sign of a permutation by

$$
\operatorname{sgn} \sigma=\left\{\begin{aligned}
1, & \text { if } \sigma \text { is even } \\
-1, & \text { if } \sigma \text { is odd }
\end{aligned}\right.
$$

the symbol ' 1 ' denoting here the integer 1 .

We shall show below that this basic property of permutations can be deduced from what we already know about determinant functions. Let us assume this for the time being. Then the integer $m$ occurring in (5-13) is always even if $\sigma$ is an even permutation, and is always odd if $\sigma$ is an odd permutation. For any alternating $n$-linear function $D$ we then have

$$
D\left(\epsilon_{\sigma 1}, \ldots, \epsilon_{\sigma n}\right)=(\operatorname{sgn} \sigma) D\left(\epsilon_{1}, \ldots, \epsilon_{n}\right)
$$

and using (5-11)

$$
D(A)=\left[\sum_{\sigma}(\operatorname{sgn} \sigma) A(1, \sigma 1) \cdots A(n, \sigma n)\right] D(I) .
$$

Of course $I$ denotes the $n \times n$ identity matrix.

From (5-14) we see that there is precisely one determinant function on $n \times n$ matrices over $K$. If we denote this function by det, it is given by

$$
\operatorname{det}(A)=\sum_{\sigma}(\operatorname{sgn} \sigma) A(1, \sigma 1) \cdots A(n, \sigma n)
$$

the sum being extended over the distinct permutations $\sigma$ of degree $n$. We can formally summarize as follows.

Theorem 2. Let $\mathrm{K}$ be a commutative ring with identity and let $\mathrm{n}$ be a positive integer. There is precisely one determinant function on the set of $\mathrm{n} \times \mathrm{n}$ matrices over $\mathrm{K}$, and it is the function det defined by (5-15). If $\mathrm{D}$ is any alternating $\mathrm{n}$-linear function on $\mathrm{K}^{\mathrm{n} \times \mathrm{n}}$, then for each $\mathrm{n} \times \mathrm{n}$ matrix $\mathrm{A}$

$$
\mathrm{D}(\mathbf{A})=(\operatorname{det} \mathbf{A}) \mathrm{D}(\mathbf{I}) .
$$

This is the theorem we have been seeking, but we have left a gap in the proof. That gap is the proof that for a given permutation $\sigma$, when we pass from $(1,2, \ldots, n)$ to $(\sigma 1, \sigma 2, \ldots, \sigma n)$ by interchanging pairs, the number of interchanges is always even or always odd. This basic combinatorial fact can be proved without any reference to determinants; however, we should like to point out how it follows from the existence of a determinant function on $n \times n$ matrices.

Let us take $K$ to be the ring of integers. Let $D$ be a determinant function on $n \times n$ matrices over $K$. Let $\sigma$ be a permutation of degree $n$, . and suppose we pass from $(1,2, \ldots, n)$ to $(\sigma 1, \sigma 2, \ldots, \sigma n)$ by $m$ interchanges of pairs $(i, j), i \neq j$. As we showed in $(5-13)$

$$
(-1)^{m}=D\left(\epsilon_{\sigma 1}, \ldots, \epsilon_{\sigma n}\right)
$$

that is, the number $(-1)^{m}$ must be the value of $D$ on the matrix with rows $\epsilon_{\sigma 1}, \ldots, \epsilon_{\sigma n}$. If

then $m$ must be even. If

$$
D\left(\epsilon_{\sigma 1}, \ldots, \epsilon_{\sigma n}\right)=1
$$

then $m$ must be odd.

$$
D\left(\epsilon_{\sigma 1}, \ldots, \epsilon_{\sigma n}\right)=-1
$$

Since we have an explicit formula for the determinant of an $n \times n$ matrix and this formula involves the permutations of degree $n$, let us conclude this section by making a few more observations about permutations. First, let us note that there are precisely $n !=1 \cdot 2 \cdots n$ permutations of degree $n$. For, if $\sigma$ is such a permutation, there are $n$ possible choices for $\sigma 1$; when this choice has been made, there are $(n-1)$ choices for $\sigma 2$, then $(n-2)$ choices for $\sigma 3$, and so on. So there are

$$
n(n-1)(n-2) \cdots 2 \cdot 1=n !
$$

permutations $\sigma$. The formula $(5-15)$ for $\operatorname{det}(A)$ thus gives $\operatorname{det}(A)$ as a sum of $n$ ! terms, one for each permutation of degree $n$. A given term is a product

$$
A(1, \sigma 1) \cdots A(n, \sigma n)
$$

of $n$ entries of $A$, one entry from each row and one from each column, and is prefixed by a 't' or '-' $\operatorname{sign}$ according as $\sigma$ is an even or odd permutation.

When permutations are regarded as one-one functions from the set $\{1,2, \ldots, n\}$ onto itself, one can define a product of permutations. The product of $\sigma$ and $\tau$ will simply be the composed function $\sigma \tau$ defined by

$$
(\sigma \tau)(i)=\sigma(\tau(i)) .
$$

If $\epsilon$ denotes the identity permutation, $\epsilon(i)=i$, then each $\sigma$ has an inverse $\sigma^{-1}$ such that

$$
\sigma \sigma^{-1}=\sigma^{-1} \sigma=\epsilon .
$$

One can summarize these observations by saying that, under the operation of composition, the set of permutations of degree $n$ is a group. This group is usually called the symmetric group of degree $\boldsymbol{n}$.

From the point of view of products of permutations, the basic property of the sign of a permutation is that

$$
\operatorname{sgn}(\sigma \tau)=(\operatorname{sgn} \sigma)(\operatorname{sgn} \tau) .
$$

In other words, $\sigma \tau$ is an even permutation if $\sigma$ and $\tau$ are either both even or both odd, while $\sigma \tau$ is odd if one of the two permutations is odd and the other is even. One can see this from the definition of the sign in terms of successive interchanges of pairs $(i, j)$. It may also be instructive if we point out how $\operatorname{sgn}(\sigma \tau)=(\operatorname{sgn} \sigma)(\operatorname{sgn} \tau)$ follows from a fundamental property of determinants.

Let $K$ be the ring of integers and let $\sigma$ and $\tau$ be permutations of degree $n$. Let $\epsilon_{1}, \ldots, \epsilon_{n}$ be the rows of the $n \times n$ identity matrix over $K$, let $A$ be the matrix with rows $\epsilon_{r 1}, \ldots, \epsilon_{\tau n}$, and let $B$ be the matrix with rows $\epsilon_{\sigma 1}, \ldots, \epsilon_{\sigma_{n}}$. The $i$ th row of $A$ contains exactly one non-zero entry, namely the 1 in column $\tau i$. From this it is easy to see that $\epsilon_{\sigma r i}$ is the $i$ th row of the product matrix $A B$. Now

$\operatorname{det}(A)=\operatorname{sgn} \tau, \quad \operatorname{det}(B)=\operatorname{sgn} \sigma, \quad$ and $\quad \operatorname{det}(A B)=\operatorname{sgn}(\sigma \tau)$. So we shall have $\operatorname{sgn}(\sigma \tau)=(\operatorname{sgn} \sigma)(\operatorname{sgn} \tau)$ as soon as we prove the following.

Theorem 3. Let $\mathrm{K}$ be a commutative ring with identity, and let $\mathrm{A}$ and $\mathrm{B}$ be $\mathrm{n} \times \mathrm{n}$ matrices over $\mathrm{K}$. Then

$$
\operatorname{det}(\mathrm{AB})=(\operatorname{det} \mathrm{A})(\operatorname{det} \mathrm{B}) .
$$

Proof. Let $B$ be a fixed $n \times n$ matrix over $K$, and for each $n \times n$ matrix $A$ define $D(A)=\operatorname{det}(A B)$. If we denote the rows of $A$ by $\alpha_{1}, \ldots$, $\alpha_{n}$, then

$$
D\left(\alpha_{1}, \ldots, \alpha_{n}\right)=\operatorname{det}\left(\alpha_{1} B, \ldots, \alpha_{n} B\right) .
$$

Here $\alpha_{j} B$ denotes the $1 \times n$ matrix which is the product of the $1 \times n$ matrix $\alpha_{j}$ and the $n \times n$ matrix $B$. Since

$$
\left(c \alpha_{i}+\alpha_{i}^{\prime}\right) B=c \alpha_{i} B+\alpha_{i}^{\prime} B
$$

and det is $n$-linear, it is easy to see that $D$ is $n$-linear. If $\alpha_{i}=\alpha_{j}$, then $\alpha_{i} B=\alpha_{j} B$, and since det is alternating,

$$
D\left(\alpha_{1}, \ldots, \alpha_{n}\right)=0 .
$$

Hence, $D$ is alternating. Now $D$ is an alternating $n$-linear function, and by Theorem 2

$$
D(A)=(\operatorname{det} A) D(I) .
$$

But $D(I)=\operatorname{det}(I B)=\operatorname{det} B$, so

$$
\operatorname{det}(A B)=D(A)=(\operatorname{det} A)(\operatorname{det} B) .
$$

The fact that $\operatorname{sgn}(\sigma \tau)=(\operatorname{sgn} \sigma)(\operatorname{sgn} \tau)$ is only one of many corollarics to Theorem 3. We shall consider some of these corollaries in the next section. 

\section{Exercises}

1. If $K$ is a commutative ring with identity and $A$ is the matrix over $K$ given by

$$
A=\left[\begin{array}{rrr}
0 & a & b \\
-a & 0 & c \\
-b & -c & 0
\end{array}\right]
$$

show that $\operatorname{det} A=0$.

2. Prove that the determinant of the Vandermonde matrix

$$
\left[\begin{array}{lll}
1 & a & a^{2} \\
1 & b & b^{2} \\
1 & c & c^{2}
\end{array}\right]
$$

is $(b-a)(c-a)(c-b)$.

3. List explicitly the six permutations of degree 3 , state which are odd and which are even, and use this to give the complete formula (5-15) for the determinant of a $3 \times 3$ matrix.

4. Let $\sigma$ and $\tau$ be the permutations of degree 4 defined by $\sigma 1=2, \sigma 2=3$, $\sigma 3=4, \sigma 4=1, \tau 1=3, \tau 2=1, \tau 3=2, \tau 4=4$.

(a) Is $\sigma$ odd or even? Is $\tau$ odd or even?

(b) Find $\sigma \tau$ and $\tau \sigma$.

5. If $A$ is an invertible $n \times n$ matrix over a field, show that $\operatorname{det} A \neq 0$.

6. Let $A$ be a $2 \times 2$ matrix over a field. Prove that $\operatorname{det}(I+A)=1+\operatorname{det} A$ if and only if trace $(A)=0$.

7. An $n \times n$ matrix $A$ is called triangular if $A_{i j}=0$ whenever $i>j$ or if $A_{i j}=0$ whenever $i<j$. Prove that the determinant of a triangular matrix is the product $A_{11} A_{22} \cdots A_{n n}$ of its diagonal entries.

8. Let $A$ be a $3 \times 3$ matrix over the field of complex numbers. We form the matrix $x I-A$ with polynomial entries, the $i, j$ entry of this matrix being the polynomial $\delta_{i j} x-A_{i j}$. If $f=\operatorname{det}(x I-A)$, show that $f$ is a monic polynomial of degree 3 . If we write

$$
f=\left(x-c_{1}\right)\left(x-c_{2}\right)\left(x-c_{3}\right)
$$

with complex numbers $c_{1}, c_{2}$, and $c_{3}$, prove that

$$
c_{1}+c_{2}+c_{3}=\operatorname{trace}(A) \text { and } c_{1} c_{2} c_{3}=\operatorname{det} A \text {. }
$$

9. Let $n$ be a positive integer and $F$ a field. If $\sigma$ is a permutation of degree $n$, prove that the function

$$
T\left(x_{1}, \ldots, x_{n}\right)=\left(x_{o 1}, \ldots, x_{\sigma n}\right)
$$

is an invertible linear operator on $F^{n}$.

10. Let $F$ be a field, $n$ a positive integer, and $S$ the set of $n \times n$ matrices over $F$. Let $V$ be the vector space of all functions from $S$ into $F$. Let $W$ be the set of alternating $n$-linear functions on $S$. Prove that $W$ is a subspace of $V$. What is the dimension of $W$ ? 11. Let $T$ be a linear operator on $F^{n}$. Define

$$
D_{T}\left(\alpha_{1}, \ldots, \alpha_{n}\right)=\operatorname{det}\left(T \alpha_{1}, \ldots, T \alpha_{n}\right)
$$

(a) Show that $D_{T}$ is an alternating $n$-linear function.

(b) If

$$
c=\operatorname{det}\left(T \epsilon_{1}, \ldots, T \epsilon_{n}\right)
$$

show that for any $n$ vectors $\alpha_{1}, \ldots, \alpha_{n}$ we have

$$
\operatorname{det}\left(T \alpha_{1}, \ldots, T \alpha_{n}\right)=c \operatorname{det}\left(\alpha_{1}, \ldots, \alpha_{n}\right) \text {. }
$$

(c) If $B$ is any ordered basis for $F^{n}$ and $A$ is the matrix of $T$ in the ordered basis $Q$, show that $\operatorname{det} A=c$.

(d) What do you think is a reasonable name for the scalar $c$ ?

12. If $\sigma$ is a permutation of degree $n$ and $A$ is an $n \times n$ matrix over the field $F$ with row vectors $\alpha_{1}, \ldots, \alpha_{n}$, let $\sigma(A)$ denote the $n \times n$ matrix with row vectors $\alpha_{o 1}, \ldots, \alpha_{o n}$.

(a) Prove that $\sigma(A B)=\sigma(A) B$, and in particular that $\sigma(A)=\sigma(I) A$.

(b) If $T$ is the linear operator of Exercise 9 , prove that the matrix of $T$ in the standard ordered basis is $\sigma(I)$.

(c) Is $\sigma^{-1}(I)$ the inverse matrix of $\sigma(I)$ ?

(d) Is it true that $\sigma(A)$ is similar to $A$ ?

13. Prove that the sign function on permutations is unique in the following sense. If $f$ is any function which assigns to each permutation of degree $n$ an integer, and if $f(\sigma \tau)=f(\sigma) f(\tau)$, then $f$ is identically 0 , or $f$ is identically 1 , or $f$ is the sign function.

\subsection{Additional Properties of Determinants}

In this section we shall relate some of the useful properties of the determinant function on $n \times n$ matrices. Perhaps the first thing we should point out is the following. In our discussion of $\operatorname{det} A$, the rows of $A$ have played a privileged role. Since there is no fundamental difference between rows and columns, one might very well expect that $\operatorname{det} A$ is an alternating $n$-linear function of the columns of $A$. This is the case, and to prove it, it suffices to show that

$$
\operatorname{det}\left(A^{t}\right)=\operatorname{det}(A)
$$

where $A^{t}$ denotes the transpose of $A$.

If $\sigma$ is a permutation of degree $n$,

$$
A^{t}(i, \sigma i)=A(\sigma i, i) .
$$

From the expression (5-15) one then has

$$
\operatorname{det}\left(A^{t}\right)=\sum_{\sigma}(\operatorname{sgn} \sigma) A(\sigma 1,1) \cdots A(\sigma n, n) \text {. }
$$

When $i=\sigma^{-1} j, A(\sigma i, i)=A\left(j, \sigma^{-1} j\right)$. Thus

$$
A(\sigma 1,1) \cdots A(\sigma n, n)=A\left(1, \sigma^{-1} 1\right) \cdots A\left(n, \sigma^{-1} n\right) \text {. }
$$

Since $\sigma \sigma^{-1}$ is the identity permutation,

$$
(\operatorname{sgn} \sigma)\left(\operatorname{sgn} \sigma^{-1}\right)=1 \text { or } \operatorname{sgn}\left(\sigma^{-1}\right)=\operatorname{sgn}(\sigma) .
$$

Furthermore, as $\sigma$ varies over all permutations of degree $n$, so does $\sigma^{-1}$. Therefore

$$
\begin{aligned}
\operatorname{det}\left(A^{t}\right) & =\sum_{\sigma}\left(\operatorname{sgn} \sigma^{-1}\right) A\left(1, \sigma^{-1} 1\right) \cdots A\left(n, \sigma^{-1} n\right) \\
& =\operatorname{det} A
\end{aligned}
$$

proving (5-17).

On certain occasions one needs to compute specific determinants. When this is necessary, it is frequently useful to take advantage of the following fact. If $B$ is obtained from $A$ by adding a multiple of one row of $A$ to another (or a multiple of one column to another), then

$$
\operatorname{det} B=\operatorname{det} A \text {. }
$$

We shall prove the statement about rows. Let $B$ be obtained from $A$ by adding $c \alpha_{j}$ to $\alpha_{i}$, where $i<j$. Since det is linear as a function of the $i$ th row

$$
\begin{aligned}
\operatorname{det} B & =\operatorname{det} A+c \operatorname{det}\left(\alpha_{1}, \ldots, \alpha_{j}, \ldots, \alpha_{j}, \ldots, \alpha_{n}\right) \\
& =\operatorname{det} A .
\end{aligned}
$$

Another useful fact is the following. Suppose we have a $n \times n$ matrix of the block form

$$
\left[\begin{array}{ll}
A & B \\
0 & C
\end{array}\right]
$$

where $A$ is an $r \times r$ matrix, $C$ is an $s \times s$ matrix, $B$ is $r \times s$, and 0 denotes the $s \times r$ zero matrix. Then

$$
\operatorname{det}\left[\begin{array}{ll}
A & B \\
0 & C
\end{array}\right]=(\operatorname{det} A)(\operatorname{det} C) .
$$

To prove this, define

$$
D(A, B, C)=\operatorname{det}\left[\begin{array}{ll}
A & B \\
0 & C
\end{array}\right] .
$$

If we fix $A$ and $B$, then $D$ is alternating and s-linear as a function of the rows of $C$. Thus, by Theorem 2

$$
D(A, B, C)=(\operatorname{det} C) D(A, \boldsymbol{B}, I)
$$

where $I$ is the $s \times s$ identity matrix. By subtracting multiples of the rows of $I$ from the rows of $B$ and using the statement above (5-18), we obtain

$$
D(A, B, I)=D(A, 0, I) .
$$

Now $D(A, 0, I)$ is clearly alternating and $r$-linear as a function of the rows of $A$. Thus

$$
D(A, 0, I)=(\operatorname{det} A) D(I, 0, I) .
$$

But $D(I, 0, I)=1$, so

$$
\begin{aligned}
D(A, B, C) & =(\operatorname{det} C) D(A, B, I) \\
& =(\operatorname{det} C) D(A, 0, I) \\
& =(\operatorname{det} C)(\operatorname{det} A) .
\end{aligned}
$$

By the same sort of argument, or by taking transposes

$$
\operatorname{det}\left[\begin{array}{cc}
A & 0 \\
B & C
\end{array}\right]=(\operatorname{det} A)(\operatorname{det} C) .
$$

Example 6. Suppose $K$ is the field of rational numbers and we wish to compute the determinant of the $4 \times 4$ matrix

$$
A=\left[\begin{array}{rrrr}
1 & -1 & 2 & 3 \\
2 & 2 & 0 & 2 \\
4 & 1 & -1 & -1 \\
1 & 2 & 3 & 0
\end{array}\right] .
$$

By subtracting suitable multiples of row 1 from rows 2, 3, and 4, we obtain the matrix

$$
\left[\begin{array}{rrrr}
1 & -1 & 2 & 3 \\
0 & 4 & -4 & -4 \\
0 & 5 & -9 & -13 \\
0 & 3 & 1 & -3
\end{array}\right]
$$

which we know by (5-18) will have the same determinant as $A$. If we subtract $\frac{5}{4}$ of row 2 from row 3 and then subtract $\frac{3}{4}$ of row 2 from row 4 , we obtain

$$
B=\left[\begin{array}{rrrr}
1 & -1 & 2 & 3 \\
0 & 4 & -4 & -4 \\
0 & 0 & -4 & -8 \\
0 & 0 & 4 & 0
\end{array}\right]
$$

and again $\operatorname{det} B=\operatorname{det} A$. The block form of $B$ tells us that

$$
\operatorname{d} \iota t A=\operatorname{det} B=\left|\begin{array}{rr}
1 & -1 \\
0 & 4
\end{array}\right|\left|\begin{array}{rr}
-4 & -8 \\
4 & 0
\end{array}\right|=4(32)=128 .
$$

Now let $n>1$ and let $A$ be an $n \times n$ matrix over $K$. In Theorem 1 , we showed how to construct a determinant function on $n \times n$ matrices, given one on $(n-1) \times(n-1)$ matrices. Now that we have proved the uniqueness of the determinant function, the formula (5-4) tells us the following. If we fix any column index $j$,

$$
\operatorname{det} A=\sum_{i=1}^{n}(-1)^{i+i} A_{i j} \operatorname{det} A(i \mid j) .
$$

The scalar $(-1)^{i+j} \operatorname{det} A(i \mid j)$ is usually called the $i, j$ cofactor of $A$ or the cofactor of the $i, j$ entry of $A$. The above formula for $\operatorname{det} A$ is then called the expansion of $\operatorname{det} A$ by cofactors of the $j$ th column (or sometimes the expansion by minors of the $j$ th column). If we set

$$
C_{i j}=(-1)^{i+j} \operatorname{det} A(i \mid j)
$$

then the above formula says that for each $j$

$$
\operatorname{det} A=\sum_{i=1}^{n} A_{i j} C_{i j}
$$

where the cofactor $C_{i j}$ is $(-1)^{i+i}$ times the determinant of the $(n-1) \times$ $(n-1)$ matrix obtained by deleting the $i$ th row and $j$ th column of $A$.

If $j \neq k$, then

$$
\sum_{i=1}^{n} A_{i k} C_{i j}=0 .
$$

For, replace the $j$ th column of $A$ by its $k$ th column, and call the resulting matrix $B$. Then $B$ has two equal columns and so $\operatorname{det} B=0$. Since $B(i \mid j)=$ $A(i \mid j)$, we have

$$
\begin{aligned}
0 & =\operatorname{det} B \\
& =\sum_{i=1}^{n}(-1)^{i+i} B_{i j} \operatorname{det} B(i \mid j) \\
& =\sum_{i=1}^{n}(-1)^{i+j} A_{i k} \operatorname{det} A(i \mid j) \\
& =\sum_{i=1}^{n} A_{i k} C_{i j} .
\end{aligned}
$$

These properties of the cofactors can be summarized by

$$
\sum_{i=1}^{n} A_{i k} C_{i j}=\delta_{j k} \operatorname{det} A .
$$

The $n \times n$ matrix adj $A$, which is the transpose of the matrix of cofactors of $A$, is called the classical adjoint of $A$. Thus

$$
(\operatorname{adj} A)_{i j}=C_{j i}=(-1)^{i+j} \operatorname{det} A(j \mid i) .
$$

The formulas (5-21) can be summarized in the matrix equation

$$
(\operatorname{adj} A) A=(\operatorname{det} A) I \text {. }
$$

We wish to see that $A(\operatorname{adj} A)=(\operatorname{det} A) I$ also. Since $A^{t}(i \mid j)=A(j \mid i)^{t}$, we have

$$
(-1)^{i+j} \operatorname{det} A^{t}(i \mid j)=(-1)^{i+i} \operatorname{det} A(j \mid i)
$$

which simply says that the $i, j$ cofactor of $A^{t}$ is the $j, i$ cofactor of $A$. Thus

$$
\operatorname{adj}\left(A^{t}\right)=(\operatorname{adj} A)^{t}
$$

By applying (5-23) to $A^{t}$, we obtain

$$
\left(\operatorname{adj} A^{t}\right) A^{t}=\left(\operatorname{det} A^{t}\right) I=(\operatorname{det} A) I
$$

and transposing

$$
A\left(\operatorname{adj} A^{t}\right)^{t}=(\operatorname{det} A) I .
$$

Using (5-24), we have what we want:

$$
A(\operatorname{adj} A)=(\operatorname{det} A) I .
$$

As for matrices over a field, an $n \times n$ matrix $A$ over $K$ is called invertible over $K$ if there is an $n \times n$ matrix $A^{-1}$ with entries in $K$ such that $A A^{-1}=A^{-1} A=I$. If such an inverse matrix exists it is unique; for the same argument used in Chapter 1 shows that when $B A=A C=I$ we have $B=C$. The formulas (5-23) and (5-25) tell us the following about invertibility of matrices over $K$. If the element $\operatorname{det} A$ has a multiplicative inverse in $K$, then $A$ is invertible and $A^{-1}=(\operatorname{det} A)^{-1}$ adj $A$ is the unique inverse of $A$. Conversely, it is easy to see that if $A$ is invertible over $K$, the element $\operatorname{det} A$ is invertible in $K$. For, if $B A=I$ we have

$$
1=\operatorname{det} I=\operatorname{det}(A B)=(\operatorname{det} A)(\operatorname{det} B) .
$$

What we have proved is the following.

Theorem 4. Let $\mathrm{A}$ be an $\mathrm{n} \times \mathrm{n}$ matrix over $\mathrm{K}$. Then $\mathrm{A}$ is invertible over $\mathrm{K}$ if and only if det $\mathrm{A}$ is invertible in $\mathrm{K}$. When $\mathrm{A}$ is invertible, the unique inverse for $\mathrm{A}$ is

$$
\mathrm{A}^{-1}=(\operatorname{det} \mathrm{A})^{-1} a d j \mathrm{~A} .
$$

In particular, an $\mathrm{n} \times \mathrm{n}$ matrix over a field is invertible if and only if its determinant is different from zero.

We should point out that this determinant criterion for invertibility proves that an $n \times n$ matrix with either a left or right inverse is invertible. This proof is completely independent of the proof which we gave in Chapter 1 for matrices over a field. We should also like to point out what invertibility means for matrices with polynomial entries. If $K$ is the polynomial ring $F[x]$, the only elements of $K$ which are invertible are the non-zero scalar polynomials. For if $f$ and $g$ are polynomials and $f g=1$, we have $\operatorname{deg} f+\operatorname{deg} g=0$ so that $\operatorname{deg} f=\operatorname{deg} g=0$, i.e., $f$ and $g$ are scalar polynomials. So an $n \times n$ matrix over the polynomial ring $F[x]$ is invertible over $F[x]$ if and only if its determinant is a non-zero scalar polynomial.

Example 7. Let $K=R[x]$, the ring of polynomials over the field of real numbers. Let

$$
A=\left[\begin{array}{cc}
x^{2}+x & x+1 \\
x-1 & 1
\end{array}\right], \quad B=\left[\begin{array}{cc}
x^{2}-1 & x+2 \\
x^{2}-2 x+3 & x
\end{array}\right] .
$$

Then, by a short computation, $\operatorname{det} A=x+1$ and $\operatorname{det} B=-6$. Thus $A$ is not invertible over $K$, whereas $B$ is invertible over $K$. Note that

$\operatorname{adj} A=\left[\begin{array}{cc}1 & -x-1 \\ -x+1 & x^{2}+x\end{array}\right], \quad \operatorname{adj} B=\left[\begin{array}{cc}x & -x-2 \\ -x^{2}+2 x-3 & x^{2}-1\end{array}\right]$ and $(\operatorname{adj} A) A=(x+1) I,(\operatorname{adj} B) B=-6 I$. Of course,

$$
B^{-1}=-\frac{1}{6}\left[\begin{array}{cc}
x & -x-2 \\
-x^{2}+2 x-3 & 1-x^{2}
\end{array}\right] \text {. }
$$

Example 8 . Let $K$ be the ring of integers and

$$
A=\left[\begin{array}{ll}
1 & 2 \\
3 & 4
\end{array}\right]
$$

Then $\operatorname{det} A=-2$ and

$$
\operatorname{adj} A=\left[\begin{array}{rr}
4 & -2 \\
-3 & 1
\end{array}\right]
$$

Thus $A$ is not invertible as a matrix over the ring of integers; however, we can also regard $A$ as a matrix over the field of rational numbers. If we do, then $A$ is invertible and

$$
A^{-1}=-\frac{1}{2}\left[\begin{array}{rr}
4 & -2 \\
-3 & 1
\end{array}\right]=\left[\begin{array}{rr}
-2 & 1 \\
\frac{3}{2} & \frac{1}{2}
\end{array}\right]
$$

In connection with invertible matrices, we should like to mention one further elementary fact. Similar matrices have the same determinant, that is, if $P$ is invertible over $K$ and $B=P^{-1} A P$, then $\operatorname{det} B=\operatorname{det} A$. This is clear since

$$
\operatorname{det}\left(P^{-1} A P\right)=\left(\operatorname{det} P^{-1}\right)(\operatorname{det} A)(\operatorname{det} P)=\operatorname{det} A .
$$

This simple observation makes it possible to define the determinant of a linear operator on a finite dimensional vector space. If $T$ is a linear operator on $V$, we define the determinant of $T$ to be the determinant of any $n \times n$ matrix which represents $T$ in an ordered basis for $V$. Since all such matrices are similar, they have the same determinant and our definition makes sense. In this connection, see Exercise 11 of section 5.3.

We should like now to discuss Cramer's rule for solving systems of linear equations. Suppose $A$ is an $n \times n$ matrix over the field $F$ and we wish to solve the system of linear equations $A X=Y$ for some given $n$-tuple $\left(y_{1}, \ldots, y_{n}\right)$. If $A X=Y$, then

$$
(\operatorname{adj} A) A X=(\operatorname{adj} A) Y
$$

and so

$$
(\operatorname{det} A) X=(\operatorname{adj} A) Y .
$$

Thus

$$
\begin{aligned}
(\operatorname{det} A) x_{j} & =\sum_{i=1}^{n}(\operatorname{adj} A)_{j i} y_{i} \\
& =\sum_{i=1}^{n}(-1)^{i+j} y_{i} \operatorname{det} A(i \mid j) .
\end{aligned}
$$

This last expression is the determinant of the $n \times n$ matrix obtained by replacing the $j$ th column of $A$ by $Y$. If $\operatorname{det} A=0$, all this tells us nothing; however, if $\operatorname{det} A \neq 0$, we have what is known as Cramer's rule. Let $A$ be an $n \times n$ matrix over the field $F$ such that $\operatorname{det} A \neq 0$. If $y_{1}, \ldots, y_{n}$ are any scalars in $F$, the unique solution $X=A^{-1} Y$ of the system of equations $A X=Y$ is given by

$$
x_{j}=\frac{\operatorname{det} B_{j}}{\operatorname{det} A}, \quad j=1, \ldots, n
$$

where $B_{j}$ is the $n \times n$ matrix obtained from $A$ by replacing the $j$ th column of $A$ by $Y$.

In concluding this chapter, we should like to make some comments which serve to place determinants in what we believe to be the proper perspective. From time to time it is necessary to compute specific determinants, and this section has been partially devoted to techniques which will facilitate such work. However, the principal role of determinants in this book is theoretical. There is no disputing the beauty of facts such as Cramer's rule. But Cramer's rule is an inefficient tool for solving systems of linear equations, chiefly because it involves too many computations. So one should concentrate on what Cramer's rule says, rather than on how to compute with it. Indeed, while reflecting on this entire chapter, we hope that the reader will place more emphasis on understanding what the determinant function is and how it behaves than on how to compute determinants of specific matrices.

\section{Exercises}

1. Use the classical adjoint formula to compute the inverses of each of the following $3 \times 3$ real matrices.

$$
\left[\begin{array}{rrr}
-2 & 3 & 2 \\
6 & 0 & 3 \\
4 & 1 & -1
\end{array}\right], \quad\left[\begin{array}{ccc}
\cos \theta & 0 & -\sin \theta \\
0 & 1 & 0 \\
\sin \theta & 0 & \cos \theta
\end{array}\right]
$$

2. Use Cramer's rule to solve each of the following systems of linear equations over the field of rational numbers.

(a) $x+y+z=11$

$2 x-6 y-z=0$

$3 x+4 y+2 z=0$.

(b) $3 x-2 y=7$

$3 y-2 z=6$

$3 z-2 x=-1$.

3. An $n \times n$ matrix $A$ over a field $F$ is skew-symmetric if $A^{t}=-A$. If $A$ is a skew-symmetric $n \times n$ matrix with complex entries and $n$ is odd, prove that $\operatorname{det} A=0$.

4. An $n \times n$ matrix $A$ over a field $F$ is called orthogonal if $A A^{t}=I$. If $A$ is orthogonal, show that $\operatorname{det} A=\pm 1$. Give an example of an orthogonal matrix for which $\operatorname{det} A=-1$. 5. An $n \times n$ matrix $A$ over the field of complex numbers is said to be unitary if $A A^{*}=I\left(A^{*}\right.$ denotes the conjugate transpose of $\left.A\right)$. If $A$ is unitary, show that $|\operatorname{det} A|=1$.

6. Let $T$ and $U$ be linear operators on the finite dimensional vector space $V$. Prove

(a) $\operatorname{det}(T U)=(\operatorname{det} T)(\operatorname{det} U)$;

(b) $T$ is invertible if and only if $\operatorname{det} T \neq 0$.

7. Let $A$ be an $n \times n$ matrix over $K$, a commutative ring with identity. Suppose $A$ has the block form

$$
A=\left[\begin{array}{cccc}
A_{1} & 0 & \cdots & 0 \\
0 & A_{2} & \cdots & 0 \\
\vdots & \vdots & & \vdots \\
0 & 0 & \cdots & A_{k}
\end{array}\right]
$$

where $A_{i}$ is an $r_{i} \times r_{j}$ matrix. Prove

$$
\operatorname{det} A=\left(\operatorname{det} A_{1}\right)\left(\operatorname{det} A_{2}\right) \cdots\left(\operatorname{det} A_{k}\right) .
$$

8. Let $V$ be the vector space of $n \times n$ matrices over the field $F$. Let $B$ be a fixed element of $V$ and let $T_{B}$ be the linear operator on $V$ defined by $T_{B}(A)=A B-B A$. Show that $\operatorname{det} T_{B}=0$.

9. Let $A$ be an $n \times n$ matrix over a field, $A \neq 0$. If $r$ is any positive integer between 1 and $n$, an $r \times r$ submatrix of $A$ is any $r \times r$ matrix obtained by deleting $(n-r)$ rows and $(n-r)$ columns of $A$. The determinant rank of $A$ is the largest positive integer $r$ such that some $r \times r$ submatrix of $A$ has a non-zero determinant. Prove that the determinant rank of $A$ is equal to the row rank of $A$ (= column $\operatorname{rank} A)$.

10. Let $A$ be an $n \times n$ matrix over the field $F$. Prove that there are at most $n$ distinct scalars $c$ in $F$ such that $\operatorname{det}(c I-A)=0$.

11. Let $A$ and $B$ be $n \times n$ matrices over the field $F$. Show that if $A$ is invertible there are at most $n$ scalars $c$ in $F$ for which the matrix $c A+B$ is not invertible.

12. If $V$ is the vector space of $n \times n$ matrices over $F$ and $B$ is a fixed $n \times n$ matrix over $F$, let $L_{B}$ and $R_{B}$ be the linear operators on $V$ defined by $L_{B}(A)=B A$ and $R_{B}(A)=A B$. Show that

(a) $\operatorname{det} L_{B}=(\operatorname{det} B)^{n}$;

(b) $\operatorname{det} R_{B}=(\operatorname{det} B)^{n}$.

13. Let $V$ be the vector space of all $n \times n$ matrices over the field of complex numbers, and let $B$ be a fixed $n \times n$ matrix over $C$. Define a linear operator $M_{B}$ on $V$ by $M_{B}(A)=B A B^{*}$, where $B^{*}=\overline{B^{t}}$. Show that

$$
\operatorname{det} M_{B}=|\operatorname{det} B|^{2 n} .
$$

Now let $H$ be the set of all Hermitian matrices in $V, A$ being Hermitian if $A=A^{*}$. Then $H$ is a vector space over the field of real numbers. Show that the function $T_{B}$ defined by $T_{B}(A)=B A B^{*}$ is a linear operator on the real vector space $H$, and then show that $\operatorname{det} T_{B}=|\operatorname{det} B|^{2 n}$. (Hint: In computing $\operatorname{det} T_{B}$, show that $V$ has a basis consisting of Hermitian matrices and then show that $\left.\operatorname{det} T_{B}=\operatorname{det} M_{B} \cdot\right)$ 14. Let $A, B, C, D$ be commuting $n \times n$ matrices over the field $F$. Show that the determinant of the $2 n \times 2 n$ matrix

is det $(A D-B C)$.

$$
\left[\begin{array}{ll}
A & B \\
C & D
\end{array}\right]
$$

\subsection{Modules}

If $K$ is a commutative ring with identity, a module over $K$ is an algebraic system which behaves like a vector space, with $K$ playing the role of the scalar field. To be precise, we say that $V$ is a nodule over $K$ (or a $K$-module) if

1. there is an addition $(\alpha, \beta) \rightarrow \alpha+\beta$ on $V$, under which $V$ is a commutative group;

2. there is a multiplication $(c, \alpha) \rightarrow c \alpha$ of elements $\alpha$ in $V$ and $c$ in $K$ such that

$$
\begin{aligned}
\left(c_{1}+c_{2}\right) \alpha & =c_{1} \alpha+c_{2} \alpha \\
c\left(\alpha_{1}+\alpha_{2}\right) & =c \alpha_{1}+c \alpha_{2} \\
\left(c_{1} c_{2}\right) \alpha & =c_{1}\left(c_{2} \alpha\right) \\
1 \alpha & =\alpha .
\end{aligned}
$$

For us, the most important $K$-modules will be the $n$-tuple modules $K^{n}$. The matrix modules $K^{m \times n}$ will also be important. If $V$ is any module, we speak of linear combinations, linear dependence and linear independence, just as we do in a vector space. We must be careful not to apply to $V$ any vector space results which depend upon division by non-zero scalars, the one field operation which may be lacking in the ring $K$. For example, if $\alpha_{1}, \ldots, \alpha_{k}$ are linearly dependent, we cannot conclude that some $\alpha_{i}$ is a linear combination of the others. This makes it more difficult to find bases in modules.

A basis for the module $V$ is a linearly independent subset which spans (or generates) the module. This is the same definition which we gave for vector spaces; and, the important property of a basis $B$ is that each element of $V$ can be expressed uniquely as a linear combination of (some finite number of) elements of $Q$. If one admits into mathematics the Axiom of Choice (see Appendix), it can be shown that every vector space has a basis. The reader is well aware that a basis exists in any vector space which is spanned by a finite number of vectors. But this is not the case for modules. Therefore we need special names for modules which have bases and for modules which are spanned by finite numbers of elements.

Definition. The $\mathrm{K}$-module $\mathrm{V}$ is called a free module if it has a basis. If $\mathrm{V}$ has a finite basis containing $\mathrm{n}$ elements, then $\mathrm{V}$ is called a free $\mathbf{K}-\mathbf{m o d u l e}$ with $\mathbf{n}$ generators. Definition. The module $\mathrm{V}$ is finitely generated if it contains a finite subset which spans V. The rank of a finitely generated module is the smallest integer $\mathrm{k}$ such that some $\mathrm{k}$ elements span $\mathrm{V}$.

We repeat that a module may be finitely generated without having a finite basis. If $V$ is a free $K$-module with $n$ generators, then $V$ is isomorphic to the module $K^{n}$. If $\left\{\beta_{1}, \ldots, \beta_{n}\right\}$ is a basis for $V$, there is an isomorphism which sends the vector $c_{1} \beta_{1}+\cdots+c_{n} \beta_{n}$ onto the $n$-tuple $\left(c_{1}, \ldots, c_{n}\right)$ in $K^{n}$. It is not immediately apparent that the same module $V$ could not also be a free module on $k$ generators, with $k \neq n$. In other words, it is not obvious that any two bases for $V$ must contain the same number of elements. The proof of that fact is an interesting application of determinants.

Theorem 5. Let $\mathrm{K}$ be a commutative ring with identity. If $\mathrm{V}$ is a free $\mathrm{K}$-module with $\mathrm{n}$ generators, then the rank of $\mathrm{V}$ is $\mathrm{n}$.

Proof. We are to prove that $V$ cannot be spanned by less than $n$ of its elements. Since $V$ is isomorphic to $K^{n}$, we must show that, if $m<n$, the module $K^{n}$ is not spanned by $n$-tuples $\alpha_{1}, \ldots, \alpha_{m}$. Let $A$ be the matrix with rows $\alpha_{1}, \ldots, \alpha_{m}$. Suppose that each of the standard basis vectors $\epsilon_{1}, \ldots, \epsilon_{n}$ is a linear combination of $\alpha_{1}, \ldots, \alpha_{m}$. Then there exists a matrix $P$ in $K^{n \times m}$ such that

$$
P A=I
$$

where $I$ is the $n \times n$ identity matrix. Let $\tilde{A}$ be the $n \times n$ matrix obtained by adjoining $n-m$ rows of 0 's to the bottom of $A$, and let $\tilde{P}$ be any $n \times n$ matrix which has the columns of $P$ as its first $n$ columns. Then

$$
\tilde{P} \tilde{A}=I \text {. }
$$

Therefore $\operatorname{det} \tilde{A} \neq 0$. But, since $m<n$, at least one row of $\tilde{A}$ has all 0 entries. This contradiction shows that $\alpha_{1}, \ldots, \alpha_{m}$ do not $\operatorname{span} K^{n}$.

It is interesting to note that Theorem 5 establishes the uniqueness of the dimension of a (finite-dimensional) vector space. The proof, based upon the existence of the determinant function, is quite different from the proof we gave in Chapter 2 . From Theorem 5 we know that 'free module of rank $n$ ' is the same as 'free module with $n$ generators.'

If $V$ is a module over $K$, the dual module $V^{*}$ consists of all linear functions $f$ from $V$ into $K$. If $V$ is a free module of rank $n$, then $V^{*}$ is also a free module of rank $n$. The proof is just the same as for vector spaces. If $\left\{\beta_{1}, \ldots, \beta_{n}\right\}$ is an ordered basis for $V$, there is an associated dual basis $\left\{f_{1}, \ldots, f_{n}\right\}$ for the module $V^{*}$. The function $f_{i}$ assigns to each $\alpha$ in $V$ its $i$ th coordinate relative to $\left\{\beta_{1}, \ldots, \beta_{n}\right\}$ :

$$
\alpha=f_{1}(\alpha) \beta_{1}+\cdots+f_{n}(\alpha) \beta_{n} .
$$

If $f$ is a linear function on $V$, then

$$
f=f\left(\beta_{1}\right) f_{1}+\cdots+f\left(\beta_{n}\right) f_{n} .
$$



\subsection{Multilinear Functions}

The purpose of this section is to place our discussion of determinants in what we believe to be the proper perspective. We shall treat alternating multilinear forms on modules. These forms are the natural generalization of determinants as we presented them. The reader who has not read (or does not wish to read) the brief account of modules in Section $5.5$ can still study this section profitably by consistently reading 'vector space over $F$ of dimension $n$ ' for 'free module over $K$ of rank $n$.'

Let $K$ be a commutative ring with identity and let $V$ be a module over $K$. If $r$ is a positive integer, a function $L$ from $V^{r}=V \times V \times \cdots \times V$ into $K$ is called multilinear if $L\left(\alpha_{1}, \ldots, \alpha_{r}\right)$ is linear as a function of each $\alpha_{i}$ when the other $\alpha_{j}$ 's are held fixed, that is, if for each $i$

$L\left(\alpha_{1}, \ldots, c \alpha_{i}+\boldsymbol{\beta}_{i}, \ldots, \alpha_{r}\right)=c L\left(\alpha_{1}, \ldots, \alpha_{i}, \ldots, \alpha_{r}+\right.$ $L\left(\alpha_{1}, \ldots, \beta_{i}, \ldots, \alpha_{r}\right)$

A multilinear function on $V^{r}$ will also be called an $r$-linear form on $V$ or a multilinear form of degree $r$ on $V$. Such functions are sometimes called $r$-tensors on $V$. The collection of all multilinear functions on $V^{r}$ will be denoted by $M^{r}(V)$. If $L$ and $M$ are in $M^{r}(V)$, then the sum $L+M$ :

$$
(L+M)\left(\alpha_{1}, \ldots, \alpha_{r}\right)=L\left(\alpha_{1}, \ldots, \alpha_{r}\right)+M\left(\alpha_{1}, \ldots, \alpha_{r}\right)
$$

is also multilinear; and, if $c$ is an element of $K$, the product $c L$ :

$$
(c L)\left(\alpha_{1}, \ldots, \alpha_{r}\right)=c L\left(\alpha_{1}, \ldots, \alpha_{r}\right)
$$

is multilinear. Therefore $M^{r}(V)$ is a $K$-module-a submodule of the module of all functions from $V^{r}$ into $K$.

If $r=1$ we have $M^{1}(V)=V^{*}$, the dual module of linear functions on $V$. Linear functions can also be used to construct examples of multilinear forms of higher order. If $f_{1}, \ldots, f_{r}$ are linear functions on $V$, define

$$
L\left(\alpha_{1}, \ldots, \alpha_{r}\right)=f_{1}\left(\alpha_{1}\right) f_{2}\left(\alpha_{2}\right) \cdots f_{r}\left(\alpha_{r}\right) .
$$

Clearly $L$ is an $r$-linear form on $V$.

EXAmple 9 . If $V$ is a module, a 2-linear form on $V$ is usually called a bilinear form on $V$. Let $A$ be an $n \times n$ matrix with entries in $K$. Then

$$
L(X, Y)=Y^{t} A X
$$

defines a bilinear form $L$ on the module $K^{n \times 1}$. Similarly,

$$
M(\alpha, \beta)=\alpha A \beta^{t}
$$

defines a bilinear form $M$ on $K^{n}$.

EXAMPle 10. The determinant function associates with each $n \times n$ matrix $A$ an element $\operatorname{det} A$ in $K$. If $\operatorname{det} A$ is considered as a function of the rows of $A$ :

$$
\operatorname{det} A=D\left(\alpha_{1}, \ldots, \alpha_{n}\right)
$$

then $D$ is an $n$-linear form on $K^{n}$.

Example 11. It is easy to obtain an algebraic expression for the general $r$-linear form on the module $K^{n}$. If $\alpha_{1}, \ldots, \alpha_{r}$ are vectors in $V$ and $A$ is the $r \times n$ matrix with rows $\alpha_{1}, \ldots, \alpha_{r}$, then for any function $L$ in $M^{r}\left(K^{n}\right)$,

$$
\begin{aligned}
L\left(\alpha_{1}, \ldots, \alpha_{r}\right) & =L\left(\sum_{j=1}^{n} A_{1 j} \epsilon_{j}, \alpha_{2}, \ldots, \alpha_{r}\right) \\
& =\sum_{j=1}^{n} A_{1 j} L\left(\epsilon_{j}, \alpha_{2}, \ldots, \alpha_{r}\right) \\
& =\sum_{j=1}^{n} A_{1 j} L\left(\epsilon_{j}, \sum_{j=1}^{n} A_{2 k} \epsilon_{k}, \ldots, \alpha_{r}\right) \\
& =\sum_{j=1}^{n} \sum_{k=1}^{n} A_{1 j} A_{2 k} L\left(\epsilon_{j}, \epsilon_{k}, \alpha_{3}, \ldots, \alpha_{r}\right) \\
& =\sum_{j, k=1}^{n} A_{1 j} A_{2 k} L\left(\epsilon_{j}, \epsilon_{k}, \alpha_{3}, \ldots, \alpha_{r}\right) .
\end{aligned}
$$

If we replace $\alpha_{3}, \ldots, \alpha_{r}$ in turn b y their expressions as linear combinations of the standard basis vectors, and if we write $A(i, j)$ for $A_{i j}$, we obtain the following:

$$
L\left(\alpha_{1}, \ldots, \alpha_{r}\right)=\sum_{j_{1}, \ldots, j_{r}=1}^{n} A\left(1, j_{1}\right) \cdots A\left(r, j_{r}\right) L\left(\epsilon_{j_{1}}, \ldots \epsilon_{j_{r}}\right) .
$$

In (5-26), there is one term for each $r$-tuple $J=\left(j_{1}, \ldots, j_{r}\right)$ of positive integers between 1 and $n$. There are $n^{r}$ such $r$-tuples. Thus $L$ is completely determined by $(5-26)$ and the particular values:

$$
c_{J}=L\left(\epsilon_{j_{1}}, \ldots, \epsilon_{j_{r}}\right)
$$

assigned to the $n^{r}$ elements $\left(\epsilon_{j_{1}}, \ldots, \epsilon_{j_{r}}\right)$. It is also easy to see that if for each $r$-tuple $J$ we choose an element $c_{J}$ of $K$ then

$$
L\left(\alpha_{1}, \ldots, \alpha_{r}\right)=\sum_{J} A\left(1, j_{1}\right) \cdots A\left(r, j_{r}\right) c_{J}
$$

defines an $r$-linear form on $K^{n}$.

Suppose that $L$ is a multilinear function on $V^{r}$ and $M$ is a multilinear function on $V^{s}$. We define a function $L \otimes M$ on $V^{r+s}$ by

$$
(L \otimes M)\left(\alpha_{1}, \ldots, \alpha_{r+s}\right)=L\left(\alpha_{1}, \ldots, \alpha_{r}\right) M\left(\alpha_{r+1}, \ldots, \alpha_{r+\varepsilon}\right) .
$$

If we think of $V^{r+s}$ as $V^{r} \times V^{s}$, then for $\alpha$ in $V^{r}$ and $\beta$ in $V^{s}$

$$
(L \otimes M)(\alpha, \beta)=L(\alpha) M(\beta) .
$$

It is clear that $L \otimes M$ is multilinear on $V^{r+s}$. The function $L \otimes M$ is called the tensor product of $L$ and $M$. The tensor product is not commutative. In fact, $M \otimes L \neq L \otimes M$ unless $L=0$ or $M=0$; however, the tensor product does relate nicely to the module operations in $M^{r}$ and $M^{8}$.

Lemma. Let $\mathrm{L}$, $\mathrm{L}_{1}$ be $\mathrm{r}$-linear forms on $\mathrm{V}$, let $\mathrm{M}, \mathrm{M}_{1}$ be s-linear forms on $\mathrm{V}$ and let $\mathrm{c}$ be an element of $\mathrm{K}$.

(a) $\left(\mathrm{cL}+\mathrm{L}_{1}\right) \otimes \mathrm{M}=\mathrm{c}(\mathrm{L} \otimes \mathrm{M})+\mathrm{L}_{1} \otimes \mathrm{M}$;

(b) $\mathrm{L} \otimes\left(\mathrm{cM}+\mathrm{M}_{1}\right)=\mathrm{c}(\mathrm{L} \otimes \mathrm{M})+\mathrm{L} \otimes \mathrm{M}_{1}$.

Proof. Exercise.

Tensoring is associative, i.e., if $L, M$ and $N$ are (respectively) $r$-, $s$ and $t$-linear forms on $V$, then

$$
(L \otimes M) \otimes N=L \otimes(M \otimes N) .
$$

This is immediate from the fact that the multiplication in $K$ is associative. Therefore, if $L_{1}, L_{2}, \ldots, L_{k}$ are multilinear functions on $V^{r_{1}}, \ldots, V^{r_{k}}$, then the tensor product

$$
L=L_{1} \otimes \cdots \otimes L_{k}
$$

is unambiguously defined as a multilinear function on $V^{r}$, where $r=$ $r_{1}+\cdots+r_{k}$. We mentioned a particular case of this earlier. If $f_{1}, \ldots, f_{r}$ are linear functions on $V$, then the tensor product

is given by

$$
L=f_{1} \otimes \cdots \otimes f_{r}
$$

$$
L\left(\alpha_{1}, \ldots, \alpha_{r}\right)=f_{1}\left(\alpha_{1}\right) \cdots f_{r}\left(\alpha_{r}\right) .
$$

Theorem 6. Let $\mathrm{K}$ be a commutative ring with identity. If $\mathrm{V}$ is a free $\mathrm{K}$-module of rank $\mathrm{n}$ then $\mathrm{M}^{\mathrm{r}}(\mathrm{V})$ is a free $\mathrm{K}$-module of rank $\mathrm{n}^{\mathrm{r}}$; in fact, if $\left\{\mathrm{f}_{1}, \ldots, \mathrm{f}_{\mathrm{n}}\right\}$ is a basis for the dual module $\mathrm{V}^{*}$, the $\mathrm{n}^{\mathrm{r}}$ tensor products

$$
\mathrm{f}_{\mathrm{j}_{1}} \otimes \cdots \otimes \mathrm{f}_{\mathrm{r}_{\mathrm{r}}}, \quad 1 \leq \mathrm{j}_{1} \leq \mathrm{n}, \ldots, 1 \leq \mathrm{j}_{\mathrm{r}} \leq \mathrm{n}
$$

form a basis for $\mathrm{M}^{\mathrm{r}}(\mathrm{V})$.

Proof. Let $\left\{f_{1}, \ldots, f_{n}\right\}$ be an ordered basis for $V^{*}$ which is dual to the basis $\left\{\beta_{1}, \ldots, \beta_{n}\right\}$ for $V$. For each vector $\alpha$ in $V$ we have

$$
\alpha=f_{1}(\alpha) \beta_{1}+\cdots+f_{n}(\alpha) \beta_{n} .
$$

We now make the calculation carried out in Example 11. If $L$ is an $r$-linear form on $V$ and $\alpha_{1}, \ldots, \alpha_{r}$ are elements of $V$, then by (5-26)

$$
L\left(\alpha_{1}, \ldots, \alpha_{r}\right)=\underset{j_{1}, \ldots, j_{r}}{\Sigma} f_{j_{1}}\left(\alpha_{1}\right) \cdots f_{j_{r}}\left(\alpha_{r}\right) L\left(\beta_{j_{1}}, \ldots, \beta_{j_{r}}\right) .
$$

In other words,

$$
L=\underset{j_{1}, \ldots j_{r}}{\Sigma} L\left(\beta_{j_{1}}, \ldots, \beta_{j_{r}}\right) f_{j_{1}} \otimes \cdots \otimes f_{j_{r}}
$$

This shows that the $n^{r}$ tensor products

$$
E_{J}=f_{j_{1}} \otimes \cdots \otimes f_{j_{r}}
$$

given by the $r$-tuples $J=\left(j_{1}, \ldots, j_{r}\right)$ span the module $M^{r}(V)$. We see that the various $r$-forms $E_{J}$ are independent, as follows. Suppose that for each $J$ we have an element $c_{J}$ in $K$ and we form the multilinear function

$$
L=\sum_{J} c_{J} E_{J} .
$$

Notice that if $I=\left(i_{1}, \ldots, i_{r}\right)$, then

$$
E_{J}\left(\beta_{i \mathfrak{i}}, \ldots, \beta_{i r}\right)= \begin{cases}0, & I \neq J \\ 1, & I=J .\end{cases}
$$

Therefore we see from (5-31) that

$$
c_{I}=L\left(\beta_{i \mathrm{i}}, \ldots, \beta_{i_{r}}\right) .
$$

In particular, if $L=0$ then $c_{I}=0$ for each $r$-tuple $I$.

Definition. Let L be an r-linear form on a $\mathrm{K}$-module $\mathrm{V}$. We say that $\mathrm{L}$ is alternating if $\mathrm{L}\left(\alpha_{1}, \ldots, \alpha_{\mathrm{r}}\right)=0$ whenever $\alpha_{\mathrm{i}}=\alpha_{\mathrm{j}}$ with $\mathrm{i} \neq \mathrm{j}$.

If $L$ is an alternating multilinear function on $V^{r}$, then

$$
L\left(\alpha_{1}, \ldots, \alpha_{i}, \ldots, \alpha_{j}, \ldots, \alpha_{r}\right)=-L\left(\alpha_{1}, \ldots, \alpha_{j}, \ldots, \alpha_{i}, \ldots, \alpha_{r}\right) .
$$

In other words, if we transpose two of the vectors (with different indices) in the $r$-tuple $\left(\alpha_{1}, \ldots, \alpha_{r}\right)$ the associated value of $L$ changes sign. Since every permutation $\sigma$ is a product of transpositions, we see that $L\left(\alpha_{\sigma 1}, \ldots\right.$, $\left.\alpha_{\sigma r}\right)=(\operatorname{sgn} \sigma) L\left(\alpha_{1}, \ldots, \alpha_{r}\right)$.

We denote by $\Lambda^{\tau}(V)$ the collection of all alternating $r$-linear forms on $V$. It should be clear that $\Lambda^{r}(V)$ is a submodule of $M^{r}(V)$.

Example 12. Earlier in this chapter, we showed that on the module $K^{n}$ there is precisely one alternating $n$-linear form $D$ with the property that $D\left(\epsilon_{1}, \ldots, \epsilon_{n}\right)=1$. We also showed in Theorem 2 that if $L$ is any form in $\Lambda^{n}\left(K^{n}\right)$ then

$$
L=L\left(\epsilon_{1}, \ldots, \epsilon_{n}\right) D .
$$

In other words, $\Lambda^{n}\left(K^{n}\right)$ is a free $K$-module of rank 1 . We also developed an explicit formula (5-15) for $D$. In terms of the notation we are now using, that formula may be written

$$
D=\sum_{\sigma}(\operatorname{sgn} \sigma) f_{\sigma 1} \otimes \cdots \otimes f_{\sigma n} .
$$

where $f_{1}, \ldots, f_{n}$ are the standard coordinate functions on $K^{n}$ and the sum is extended over the $n$ ! different permutations $\sigma$ of the set $\{1, \ldots, n\}$. If we write the determinant of a matrix $A$ as

$$
\operatorname{det} A=\boldsymbol{\Sigma}_{\sigma}(\operatorname{sgn} \sigma) A(\sigma 1,1) \cdots A(\sigma n, n)
$$

then we obtain a different expression for $D$ :

$$
\begin{aligned}
D\left(\alpha_{1}, \ldots, \alpha_{n}\right) & =\boldsymbol{\Sigma}_{\sigma}(\operatorname{sgn} \sigma) f_{1}\left(\alpha_{\sigma 1}\right) \cdots f_{n}\left(\alpha_{\sigma n}\right) \\
& =\sum_{\sigma}(\operatorname{sgn} \sigma) L\left(\alpha_{\sigma 1}, \ldots, \alpha_{\sigma n}\right)
\end{aligned}
$$

where $L=f_{1} \otimes \cdots \otimes f_{n}$.

There is a general method for associating an alternating form with a multilinear form. If $L$ is an $r$-linear form on a module $V$ and if $\sigma$ is a permutation of $\{1, \ldots, r\}$, we obtain another $r$-linear function $L_{\sigma}$ by defining

$$
L_{\sigma}\left(\alpha_{1}, \ldots, \alpha_{r}\right)=L\left(\alpha_{\sigma 1}, \ldots, \alpha_{\sigma r}\right) .
$$

If $L$ happens to be alternating, then $L_{\sigma}=(\operatorname{sgn} \sigma) L$. Now, for each $L$ in $M^{r}(V)$ we define a function $\pi_{r} L$ in $M^{r}(V)$ by

$$
\pi_{r} L=\sum_{\sigma}(\operatorname{sgn} \sigma) L_{\sigma}
$$

that is,

$$
\left(\pi_{r} L\right)\left(\alpha_{1}, \ldots, \alpha_{r}\right)=\sum_{\sigma}(\operatorname{sgn} \sigma) L\left(\alpha_{\sigma 1}, \ldots, \alpha_{\sigma r}\right)
$$

Lemma. $\pi_{\mathrm{r}}$ is a linear transformation from $\mathrm{M}^{\mathrm{r}}(\mathrm{V})$ into $\Lambda^{\mathrm{r}}(\mathrm{V})$. If $\mathrm{L}$ is in $\Lambda^{\mathrm{r}}(\mathrm{V})$ then $\pi_{\mathrm{r}} \mathrm{L}=\mathrm{r} ! \mathrm{L}$.

Proof. Let $\tau$ be any permutation of $\{1, \ldots, r\}$. Then

$$
\begin{aligned}
\left(\pi_{r} L\right)\left(\alpha_{\tau 1}, \ldots, \alpha_{r r}\right) & =\sum_{\sigma}(\operatorname{sgn} \sigma) L\left(\alpha_{\tau \sigma 1}, \ldots, \alpha_{\tau \sigma r}\right) \\
& =(\operatorname{sgn} \tau) \sum_{\sigma}(\operatorname{sgn} \tau \sigma) L\left(\alpha_{\tau \sigma 1}, \ldots, \alpha_{\tau \sigma r}\right) .
\end{aligned}
$$

As $\sigma$ runs (once) over all permutations of $\{1, \ldots, r\}$, so does $\tau \sigma$. Therefore,

$$
\left(\pi_{r} L\right)\left(\alpha_{r 1}, \ldots, \alpha_{r r}\right)=(\operatorname{sgn} \tau)\left(\pi_{r} L\right)\left(\alpha_{1}, \ldots, \alpha_{r}\right) .
$$

Thus $\pi_{r} L$ is an alternating form.

If $L$ is in $\Lambda^{r}(V)$, then $L\left(\alpha_{\sigma 1}, \ldots, \alpha_{\sigma_{r}}\right)=(\operatorname{sgn} \sigma) L\left(\alpha_{1}, \ldots, \alpha_{r}\right)$ for each $\sigma$; hence $\pi_{r} L=r ! L$.

In (5-33) we showed that the determinant function $D$ in $\Lambda^{n}\left(K^{n}\right)$ is

$$
D=\pi_{n}\left(f_{1} \otimes \cdots \otimes f_{n}\right)
$$

where $f_{1}, \ldots, f_{n}$ are the standard coordinate functions on $K^{n}$. There is an important remark we should make in connection with the last lemma. If $K$ is a field of characteristic zero, such that $r$ ! is invertible in $K$, then $\pi$ maps $M^{r}(V)$ onto $\Lambda^{r}(V)$. In fact, in that case it is more natural from one point of view to use the map $\pi_{1}=(1 / r !) \pi$ rather than $\pi$, because $\pi_{1}$ is a projection of $M^{r}(V)$ onto $\Lambda^{r}(V)$, i.e., a linear map of $M^{r}(V)$ onto $\Lambda^{r}(V)$ such that $\pi_{1}(L)=L$ if and only if $L$ is in $\Lambda^{r}(V)$. Theorem 7. Let $\mathrm{K}$ be a commutative ring with identity and let $\mathrm{V}$ be a free $\mathrm{K}$-module of rank $\mathrm{n}$. If $\mathrm{r}>\mathrm{n}$, then $\Lambda^{\mathrm{r}}(\mathrm{V})=\{0\}$. If $1 \leq \mathrm{r} \leq \mathrm{n}$, then $\Lambda^{\mathrm{r}}(\mathrm{V})$ is a free $\mathrm{K}$-module of rank $\left(\begin{array}{l}\mathrm{n} \\ \mathrm{r}\end{array}\right)$.

Proof. Let $\left\{\beta_{1}, \ldots, \beta_{n}\right\}$ be an ordered basis for $V$ with dual basis $\left\{f_{1}, \ldots, f_{n}\right\}$. If $L$ is in $M^{r}(V)$, we have

$$
L=\sum_{J} L\left(\beta_{j_{1}}, \ldots, \beta_{j_{r}}\right) f_{j_{1}} \otimes \cdots \otimes f_{j_{r}}
$$

where the sum extends over all $r$-tuples $J=\left(j_{1}, \ldots, j_{r}\right)$ of integers between 1 and $n$. If $L$ is alternating, then

$$
L\left(\beta_{j_{1}}, \ldots, \beta_{j_{r}}\right)=0
$$

whenever two of the subscripts $j_{i}$ are the same. If $r>n$, then in each $r$-tuple $J$ some integer must be repeated. Thus $\Lambda^{r}(V)=\{0\}$ if $r>n$.

Now suppose $1 \leq r \leq n$. If $L$ is in $\Lambda^{r}(V)$, the sum in (5-37) need be extended only over the $r$-tuples $J$ for which $j_{1}, \ldots, j_{r}$ are distinct, because all other terms are 0 . Each $r$-tuple of distinct integers between 1 and $n$ is a permutation of an $r$-tuple $J=\left(j_{1}, \ldots, j_{r}\right)$ such that $j_{1}<\cdots<j_{r}$. This special type of $r$-tuple is called an $r$-shuffle of $\{1, \ldots, n\}$. There are

such shuffles.

$$
\left(\begin{array}{l}
n \\
r
\end{array}\right)=\frac{n !}{r !(n-r) !}
$$

Suppose we fix an $r$-shuffle $J$. Let $L_{J}$ be the sum of all the terms in (5-37) corresponding to permutations of the shuffle $J$. If $\sigma$ is a permutation of $\{1, \ldots, r\}$, then

Thus

$$
L\left(\beta_{j_{11}}, \ldots, \beta_{j_{\sigma_{r}}}\right)=(\operatorname{sgn} \sigma) L\left(\beta_{j_{1}}, \ldots, \beta_{j_{r}}\right) .
$$

$$
L_{J}=L\left(\beta_{j_{t}}, \ldots, \beta_{j_{r}}\right) D_{J}
$$

where

$$
\begin{aligned}
D_{J} & =\sum_{\sigma}(\operatorname{sgn} \sigma) f_{j^{n} 1} \otimes \cdots \otimes f_{j_{\sigma r}} \\
& =\pi_{r}\left(f_{j_{1}} \otimes \cdots \otimes f_{j_{r}}\right) .
\end{aligned}
$$

We see from (5-39) that each $D_{J}$ is alternating and that

$$
L=\sum_{\text {shuffes } J} L\left(\beta_{j i}, \ldots, \beta_{j_{r}}\right) D_{J}
$$

for every $L$ in $\Lambda^{r}(V)$. The assertion is that the $\left(\begin{array}{l}n \\ r\end{array}\right)$ forms $D_{J}$ constitute a basis for $\Lambda^{r}(V)$. We have seen that they span $\Lambda^{r}(V)$. It is easy to see that they are independent, as follows. If $I=\left(i_{1}, \ldots, i_{r}\right)$ and $J=\left(j_{1}, \ldots, j_{r}\right)$ are shuffles, then

$$
D_{J}\left(\beta_{i \mathfrak{i}}, \ldots, \beta_{i_{r}}\right)=\left\{\begin{array}{ll}
1, & I=J \\
0, & I \neq J
\end{array} .\right.
$$

Suppose we have a scalar $c_{J}$ for each shuffle and we define

$$
L=\sum_{J} c_{J} D_{J} .
$$

From (5-40) and (5-41) we obtain

$$
c_{I}=L\left(\beta_{i 1}, \ldots, \beta_{i r}\right) .
$$

In particular, if $L=0$ then $c_{I}=0$ for each shuffle $I$.

Corollary. If $\mathrm{V}$ is a free $\mathrm{K}$-module of rank $\mathrm{n}$, then $\Lambda^{\mathrm{n}}(\mathrm{V})$ is a free $\mathrm{K}$-module of rank 1 . If $\mathrm{T}$ is a linear operator on $\mathrm{V}$, there is a unique element $\mathrm{c}$ in $\mathrm{K}$ such that

$$
\mathrm{L}\left(\mathrm{T} \alpha_{1}, \ldots, \mathrm{T} \alpha_{\mathrm{n}}\right)=\mathrm{cL}\left(\alpha_{1}, \ldots, \alpha_{\mathrm{n}}\right)
$$

for every alternating $\mathrm{n}$-linear form $\mathrm{L}$ on $\mathrm{V}$.

Proof. If $L$ is in $\Lambda^{n}(V)$, then clearly

$$
L_{T}\left(\alpha_{1}, \ldots, \alpha_{n}\right)=L\left(T \alpha_{1}, \ldots, T \alpha_{n}\right)
$$

defines an alternating $n$-linear form $L_{T}$. Let $M$ be a generator for the rank 1 module $\Lambda^{n}(V)$. Each $L$ in $\Lambda^{n}(V)$ is uniquely expressible as $L=a M$ for some $a$ in $K$. In particular, $M_{T}=c M$ for a certain $c$. For $L=a M$ we have

$$
\begin{aligned}
L_{T} & =(a M)_{T} \\
& =a M_{T} \\
& =a(c M) \\
& =c(a M) \\
& =c L .
\end{aligned}
$$

Of course, the element $c$ in the last corollary is called the determinant of $T$. From (5-39) for the case $r=n$ (when there is only one shuffe $J=(1, \ldots, n))$ we see that the determinant of $T$ is the determinant of the matrix which represents $T$ in any ordered basis $\left\{\beta_{1}, \ldots, \beta_{n}\right\}$. Let us see why. The representing matrix has $i, j$ entry

so that

$$
A_{i j}=f_{j}\left(T \beta_{i}\right)
$$

On the other hand,

$$
\begin{aligned}
D_{J}\left(T \beta_{1}, \ldots, T \beta_{n}\right) & =\sum_{\sigma}(\operatorname{sgn} \sigma) A(1, \sigma 1) \cdots A(n, \sigma n) \\
& =\operatorname{det} A .
\end{aligned}
$$

$$
\begin{aligned}
D_{J}\left(T \beta_{1}, \ldots, T \beta_{n}\right) & =(\operatorname{det} T) D_{J}\left(\beta_{1}, \ldots, \beta_{n}\right) \\
& =\operatorname{det} T .
\end{aligned}
$$

The point of these remarks is that via Theorem 7 and its corollary we obtain a definition of the determinant of a linear operator which does not presume knowledge of determinants of matrices. Determinants of matrices can be defined in terms of determinants of operators instead of the other way around. We want to say a bit more about the special alternating $r$-linear forms $D_{J}$, which we associated with a basis $\left\{f_{1}, \ldots, f_{n}\right\}$ for $V^{*}$ in (5-39). It is important to understand that $D_{J}\left(\alpha_{1}, \ldots, \alpha_{r}\right)$ is the determinant of a certain $r \times r$ matrix. If

that is, if

$$
A_{i j}=f_{j}\left(\alpha_{i}\right), \quad 1 \leq i \leq r, 1 \leq j \leq n,
$$

$$
\alpha_{i}=A_{i 1} \beta_{1}+\cdots+A_{i n} \beta_{n}, \quad 1 \leq i \leq r
$$

and $J$ is the $r$-shuffle $\left(j_{1}, \ldots, j_{\tau}\right)$, then

$$
\begin{aligned}
D_{J}\left(\alpha_{1}, \ldots, \alpha_{r}\right) & =\sum_{\sigma}(\operatorname{sgn} \sigma) A\left(1, j_{\sigma 1}\right) \cdots A\left(n, j_{\sigma_{n}}\right) \\
& =\operatorname{det}\left[\begin{array}{ccc}
A\left(1, j_{1}\right) & \cdots & A\left(1, j_{r}\right) \\
\vdots & \vdots \\
A\left(r, j_{1}\right) & \cdots & A\left(r, j_{r}\right)
\end{array}\right] .
\end{aligned}
$$

Thus $D_{J}\left(\alpha_{1}, \ldots, \alpha_{r}\right)$ is the determinant of the $r \times r$ matrix formed from columns $j_{1}, \ldots, j_{r}$ of the $r \times n$ matrix which has (the coordinate $n$-tuples of) $\alpha_{1}, \ldots, \alpha_{r}$ as its rows. Another notation which is sometimes used for this determinant is

$$
D_{J}\left(\alpha_{1}, \ldots, \alpha_{r}\right)=\frac{\partial\left(\alpha_{1}, \ldots, \alpha_{r}\right)}{\partial\left(\beta_{j_{1}}, \ldots, \beta_{j_{r}}\right)}
$$

In this notation, the proof of Theorem 7 shows that every alternating $r$-linear form $L$ can be expressed relative to a basis $\left\{\beta_{1}, \ldots, \beta_{n}\right\}$ by the equation

$$
L\left(\alpha_{1}, \ldots, \alpha_{r}\right)=\sum_{j_{1}<\cdots<j_{r}} \frac{\partial\left(\alpha_{1}, \ldots, \alpha_{r}\right)}{\partial\left(\beta_{j_{1}}, \ldots, \beta_{j_{r}}\right)} L\left(\beta_{j_{1}}, \ldots, \beta_{j_{r}}\right) .
$$

\subsection{The Grassman Ring}

Many of the important properties of determinants and alternating multilinear forms are best described in terms of a multiplication operation on forms, called the exterior product. If $L$ and $M$ are, respectively, alternating $r$ and $s$-linear forms on the module $V$, we have an associated product of $L$ and $M$, the tensor product $L \otimes M$. This is not an alternating form unless $L=0$ or $M=0$; however, we have a natural way of projecting it into $\Lambda^{r+g}(V)$. It appears that

$$
L \cdot M=\pi_{r+s}(L \otimes M)
$$

should be the 'natural' multiplication of alternating forms. But, is it?

Let us take a specific example. Suppose that $V$ is the module $K^{n}$ and $f_{1}, \ldots, f_{n}$ are the standard coordinate functions on $K^{n}$. If $i \neq j$, then

$$
f_{i} \cdot f_{j}=\pi_{2}\left(f_{i} \otimes f_{j}\right)
$$

is the (determinant) function

$$
D_{i j}=f_{i} \otimes f_{j}-f_{j} \otimes f_{i}
$$

given by (5-39). Now suppose $k$ is an index different from $i$ and $j$. Then

$$
\begin{aligned}
D_{i j} \cdot f_{k} & =\pi_{3}\left[\left(f_{i} \otimes f_{j}-f_{j} \otimes f_{i}\right) \otimes f_{k}\right] \\
& =\pi_{3}\left(f_{i} \otimes f_{i} \otimes f_{k}\right)-\pi_{3}\left(f_{j} \otimes f_{i} \otimes f_{k}\right) .
\end{aligned}
$$

The proof of the lemma following equation (5-36) shows that for any $r$-linear form $L$ and any permutation $\sigma$ of $\{1, \ldots, r\}$

$$
\pi_{r}\left(L_{\sigma}\right)=\operatorname{sgn} \sigma \pi_{r}(L)
$$

Hence, $D_{i j} \cdot f_{k}=2 \pi_{3}\left(f_{i} \otimes f_{j} \otimes f_{k}\right)$. By a similar computation, $f_{i} \cdot D_{j k}=$ $2 \pi_{3}\left(f_{i} \otimes f_{j} \otimes f_{k}\right)$. Thus we have

$$
\left(f_{i} \cdot f_{j}\right) \cdot f_{k}=f_{i} \cdot\left(f_{j} \cdot f_{k}\right)
$$

and all of this looks very promising. But there is a catch. Despite the computation that we have just completed, the putative multiplication in (5-45) is not associative. In fact, if $l$ is an index different from $i, j, k$, then one can calculate that

$$
D_{i j} \cdot D_{k l}=4 \pi_{4}\left(f_{i} \otimes f_{j} \otimes f_{k} \otimes f_{l}\right)
$$

and that

$$
\left(D_{i j} \cdot f_{k}\right) \cdot f_{l}=6 \pi_{4}\left(f_{i} \otimes f_{j} \otimes f_{k} \otimes f_{l}\right) .
$$

Thus, in general

$$
\left(f_{i} \cdot f_{j}\right) \cdot\left(f_{k} \cdot f_{l}\right) \neq\left[\left(f_{i} \cdot f_{j}\right) \cdot f_{l}\right] \cdot f_{l}
$$

and we see that our first attempt to find a multiplication has produced a non-associative operation.

The reader should not be surprised if he finds it rather tedious to give a direct verification of the two equations showing non-associativity. This is typical of the subject, and it is also typical that there is a general fact which considerably simplifies the work.

Suppose $L$ is an $r$-linear form and that $M$ is an $s$-linear form on the module $V$. Then

$$
\begin{aligned}
& \pi_{r+\varepsilon}\left(\left(\pi_{r} L\right) \otimes\left(\pi_{s} M\right)\right)=\pi_{r+8}\left(\sum_{\sigma, \tau}(\operatorname{sgn} \sigma)(\operatorname{sgn} \tau) L_{\sigma} \otimes M_{\tau}\right) \\
&=\sum_{\sigma, \tau}(\operatorname{sgn} \sigma)(\operatorname{sgn} \tau) \pi_{r+8}\left(L_{\sigma} \otimes M_{r}\right)
\end{aligned}
$$

where $\sigma$ varies over the symmetric group, $S_{r}$, of all permutations of $\{1, \ldots, r\}$, and $\tau$ varies over $S_{s}$. Each pair $\sigma, \tau$ defines an element $(\sigma, \tau)$ of $S_{r+s}$ which permutes the first $r$ elements of $\{1, \ldots, r+s\}$ according to $\sigma$ and the last $s$ elements according to $\tau$. It is clear that

and that

$$
\operatorname{sgn}(\sigma, \tau)=(\operatorname{sgn} \sigma)(\operatorname{sgn} \tau)
$$

$$
(L \otimes M)_{(\sigma, \tau)}=L_{\sigma} \otimes L_{r}
$$

Therefore

![](https://cdn.mathpix.com/cropped/2023_01_16_0f6afdf132d84aaf6bdeg-184.jpg?height=82&width=850&top_left_y=249&top_left_x=235)

Now we have already observed that

$$
\operatorname{sgn}(\sigma, \tau) \pi_{r+s}\left[(L \otimes M)_{(\sigma, \tau)}\right]=\pi_{r+s}(L \otimes M) .
$$

Thus, it follows that

$$
\pi_{r_{s} s}\left[\left(\pi_{r} L\right) \otimes\left(\pi_{s} M\right)\right]=r ! s ! \pi_{r+s}(L \otimes M) .
$$

This formula simplifies a number of computations. For example, suppose we have an $r$-shuffle $I=\left(i_{1}, \ldots, i_{r}\right)$ and $s$-shuffle $J=\left(j_{1}, \ldots, j_{s}\right)$. To make things simple, assume, in addition, that

$$
i_{1}<\cdots<i_{r}<j_{1}<\cdots<j_{8} .
$$

Then we have the associated determinant functions

$$
\begin{aligned}
& D_{I}=\pi_{r}\left(E_{I}\right) \\
& D_{J}=\pi_{s}\left(E_{J}\right)
\end{aligned}
$$

where $E_{I}$ and $E_{J}$ are given by (5-30). Using (5-46), we see immediately that

$$
\begin{aligned}
D_{I} \cdot D_{J} & =\pi_{r+s}\left[\pi_{r}\left(E_{I}\right) \otimes \pi_{s}\left(E_{J}\right)\right] \\
& =r ! s ! \pi_{r+s}\left(E_{I} \otimes E_{J}\right) .
\end{aligned}
$$

Since $E_{I} \otimes E_{J}=E_{I \cup J}$, it follows that

$$
D_{I} \cdot D_{J}=r ! s ! D_{I \cup J} .
$$

This suggests that the lack of associativity for the multiplication (5-45) results from the fact that $D_{I} \cdot D_{J} \neq D_{I \cup J}$. After all, the product of $D_{I}$ and $D_{J}$ ought to be $D_{I \cup J}$. To repair the situation, we should define a new product, the exterior product (or wedge product) of an alternating $r$-linear form $L$ and an alternating $s$-linear form $M$ by

$$
L \wedge M=\frac{1}{r ! s !} \pi_{r+8}(L \otimes M) .
$$

We then have

$$
D_{I} \wedge D_{J}=D_{I \cup J}
$$

for the determinant functions on $K^{n}$, and, if there is any justice at all, we must have found the proper multiplication of alternating multilinear forms. Unfortunately, (5-47) fails to make sense for the most general case under consideration, since we may not be able to divide by $r ! s !$ in the ring $K$. If $K$ is a field of characteristic zero, then (5-47) is meaningful, and one can proceed quite rapidly to show that the wedge product is associative.

Theorem 8. Let $\mathrm{K}$ be a field of characteristic zero and $\mathrm{V}$ a vector space over $\mathrm{K}$. Then the exterior product is an associative operation on the alternating multilinear forms on $\mathrm{V}$. In other words, if $\mathrm{L}, \mathrm{M}$, and $\mathrm{N}$ are alternating multilinear forms on $\mathrm{V}$ of degrees $\mathrm{r}, \mathrm{s}$, and $\mathrm{t}$, respectively, then $(\mathrm{L} \wedge \mathrm{M}) \wedge \mathrm{N}=\mathrm{L} \wedge(\mathrm{M} \wedge \mathrm{N})$.

Proof. It follows from (5-47) that $c d(L \wedge M)=c L \wedge d M$ for any scalars $c$ and $d$. Hence

$$
r ! s ! t ![(L \wedge M) \wedge N]=r ! s !(L \wedge M) \wedge t ! N
$$

and since $\pi_{t}(N)=t ! N$, it results that

$$
\begin{aligned}
r ! s ! t ![(L \wedge M) \wedge N] & =\pi_{r+s}(L \otimes M) \wedge \pi_{t}(N) \\
& =\frac{1}{(r+s) !} \frac{1}{t !} \pi_{r_{+s+t}}\left[\pi_{r+s}(L \otimes M) \otimes \pi_{t}(N)\right] .
\end{aligned}
$$

From (5-46) we now see that

$$
r ! s ! t[(L \wedge M) \wedge N]=\pi_{r+s+t}(L \otimes M \otimes N) .
$$

By a similar computation

$$
r ! s ! t ![L \wedge(M \wedge N)]=\pi_{r+s+t}(L \otimes M \otimes N)
$$

and therefore, $(L \wedge M) \wedge N=L \wedge(M \wedge N)$.

Now we return to the general case, in which it is only assumed that $K$ is a commutative ring with identity. Our first problem is to replace (5-47) by an equivalent definition which works in general. If $L$ and $M$ are alternating multilinear forms of degrees $r$ and $s$ respectively, we shall construct a canonical alternating multilinear form $L \wedge M$ of degree $r+s$ such that

$$
r ! s !(L \wedge M)=\pi_{r+s}(L \otimes M) .
$$

Let us recall how we define $\pi_{r+s}(L \otimes M)$. With each permutation $\sigma$ of $\{1, \ldots, r+s\}$ we associate the multilinear function

$$
(\operatorname{sgn} \sigma)(L \otimes M)_{\sigma}
$$

where

$$
(L \otimes M)_{\sigma}\left(\alpha_{1}, \ldots, \alpha_{r+s}\right)=(L \otimes M)\left(\alpha_{\sigma 1}, \ldots, \alpha_{\sigma(r+s)}\right)
$$

and we sum the functions (5-48) over all permutations $\sigma$. There are $(r+s)$ ! permutations; however, since $L$ and $M$ are alternating, many of the functions (5-48) are the same. In fact there are at most

$$
\frac{(r+s) !}{r ! s !}
$$

distinct functions (5-48). Let us see why. Let $S_{r+s}$ be the set of permutations of $\{1, \ldots, r+s\}$, i.e., let $S_{r+s}$ be the symmetric group of degree $r+s$. As in the proof of (5-46), we distinguish the subset $G$ that consists of the permutations $\sigma$ which permute the sets $\{1, \ldots, r\}$ and $\{r+1, \ldots$, $r+s\}$ within themselves. In other words, $\sigma$ is in $G$ if $1 \leq \sigma i \leq r$ for each $i$ between 1 and $r$. (It necessarily follows that $r+1 \leq \sigma j \leq r+s$ for each $j$ between $r+1$ and $r+$ s.) Now $G$ is a subgroup of $S_{r+s}$, that is, if $\sigma$ and $\tau$ are in $G$ then $\sigma \tau^{-1}$ is in $G$. Evidently $G$ has $r ! s !$ members. We have a map

defined by

$$
S_{r+s} \stackrel{\psi}{\rightarrow} M^{r+s}(V)
$$

$$
\psi \cdot(\sigma)=(\operatorname{sgn} \sigma)(L \otimes M)_{\sigma} .
$$

Since $L$ and $M$ are alternating,

$$
\psi(\gamma)=L \otimes M
$$

for every $\gamma$ in $G$. Therefore, since $(N \sigma) \tau=N \tau \sigma$ for any $(r+s)$-linear form $N$ on $V$, we have

$$
\psi(\tau \gamma)=\psi(\tau), \quad \tau \text { in } S_{r+s}, \gamma \text { in } G .
$$

This says that the map $\psi$ is constant on each (left) coset $\tau G$ of the subgroup $G$. If $\tau_{1}$ and $\tau_{2}$ are in $S_{r+s}$, the cosets $\tau_{1} G$ and $\tau_{2} G$ are either identical or disjoint, according as $\tau_{2}^{-1} \tau_{1}$ is in $G$ or is not in $G$. Each coset contains $r ! s !$ elements; hence, there are

$$
\frac{(r+s) !}{r ! s !}
$$

distinct cosets. If $S_{r+s} / G$ denotes the collection of cosets then $\psi$ defines a function on $S_{r+s} / G$, i.e., by what we have shown, there is a function $\tilde{\psi}$ on that set so that

$$
\psi(\tau)=\tilde{\psi}(\tau G)
$$

for every $\tau$ in $S_{r+s}$. If $H$ is a left coset of $G$, then $\tilde{\psi}(H)=\psi(\tau)$ for every $\tau$ in $H$.

We now define the exterior product of the alternating multilinear forms $L$ and $M$ of degrees $r$ and $s$ by setting

$$
L \wedge M=\sum_{H} \tilde{\psi}(H)
$$

where $H$ varies over $S_{r+s} / G$. Another way to phrase the definition of $L \wedge M$ is the following. Let $S$ be any set of permutations of $\{1, \ldots, r+s\}$ which contains exactly one element from each left coset of $G$. Then

$$
L \wedge M=\sum_{\sigma}(\operatorname{sgn} \sigma)(L \otimes M)_{\sigma}
$$

where $\sigma$ varies over $S$. Clearly

$$
r ! s ! L \wedge M=\pi_{r+s}(L \otimes M)
$$

so that the new definition is equivalent to $(5-47)$ when $K$ is a field of characteristic zero.

Theorem 9. Let $\mathrm{K}$ be a commutative ring with identity and let $\mathrm{V}$ be a module over $\mathrm{K}$. Then the exterior product is an associative operation on the alternating multilinear forms on $\mathrm{V}$. In other words, if $\mathrm{L}, \mathrm{M}$, and $\mathrm{N}$ are alternating multilinear forms on $\mathrm{V}$ of degrees $\mathrm{r}, \mathrm{s}$, and $\mathrm{t}$, respectively, then

$(\mathrm{L} \wedge \mathrm{M}) \wedge \mathrm{N}=\mathrm{L} \wedge(\mathrm{M} \wedge \mathrm{N})$. Proof. Although the proof of Theorem 8 does not apply here, it does suggest how to handle the general case. Let $G(r, s, t)$ be the subgroup of $S_{r+s+t}$ that consists of the permutations which permute the sets

$$
\{1, \ldots, r\},\{r+1, \ldots, r+s\},\{r+s+1, \ldots, r+s+t\}
$$

within themselves. Then $(\operatorname{sgn} \mu)(L \otimes M \otimes N)_{\mu}$ is the same multilinear function for all $\mu$ in a given left coset of $G(r, s, t)$. Choose one element from each left coset of $G(r, s, t)$, and let $E$ be the sum of the corresponding terms $(\operatorname{sgn} \mu)(L \otimes M \otimes N)_{\mu}$. Then $E$ is independent of the way in which the representatives $\mu$ are chosen, and

$$
r ! s ! t ! E=\pi_{r+s+t}(L \otimes M \otimes N) .
$$

We shall show that $(L \wedge M) \wedge N$ and $L \wedge(M \wedge N)$ are both equal to $E$. Let $G(r+s, t)$ be the subgroup of $S_{r+s+t}$ that permutes the sets

$$
\{1, \ldots, r+s\},\{r+s+1, \ldots, r+s+t\}
$$

within themselves. Let $T$ be any set of permutations of $\{1, \ldots, r+s+t\}$ which contains exactly one element from each left coset of $G(r+s, t)$. By $(5-50)$

$$
(L \wedge M) \wedge N=\sum_{\tau}(\operatorname{sgn} \tau)[(L \wedge M) \otimes N]_{\tau}
$$

where the sum is extended over the permutations $\tau$ in $T$. Now let $G(r, s)$ be the subgroup of $S_{r+s}$ that permutes the sets

$$
\{1, \ldots, r\},\{r+1, \ldots, r+s\}
$$

within themselves. Let $S$ be any set of permutations of $\{1, \ldots, r+s\}$ which contains exactly one element from each left coset of $G(r, s)$. From (5-50) and what we have shown above, it follows that

$$
(L \wedge M) \wedge N=\sum_{\sigma, \tau}(\operatorname{sgn} \sigma)(\operatorname{sgn} \tau)\left[(L \otimes M)_{\sigma} \otimes N\right]_{r}
$$

where the sum is extended over all pairs $\sigma, \tau$ in $S \times T$. If we agree to identify each $\sigma$ in $S_{r+s}$ with the element of $S_{r+s+t}$ which agrees with $\sigma$ on $\{1, \ldots, r+s\}$ and is the identity on $\{r+s+1, \ldots, r+s+t\}$, then we may write

But,

$$
(L \wedge M) \wedge N=\sum_{\sigma, \tau} \operatorname{sgn}(\sigma \tau)\left[(L \otimes M \otimes N)_{\sigma}\right]_{\tau} .
$$

$$
\left[(L \otimes M \otimes N)_{\sigma}\right]_{\tau}=(L \otimes M \otimes N)_{r \sigma}
$$

Therefore

$$
(L \wedge M) \wedge N=\sum_{\sigma, \tau} \operatorname{sgn}(\tau \sigma)(L \otimes M \otimes N)_{\tau \sigma} .
$$

Now suppose we have

$$
\tau_{1} \sigma_{1}=\tau_{2} \sigma_{2} \gamma
$$

with $\sigma_{i}$ in $S, \tau_{i}$ in $T$, and $\gamma$ in $G(r, s, t)$. Then $\tau_{2}^{-1} \tau_{1}=\sigma_{2} \gamma \sigma_{1}^{-1}$, and since $\dot{\sigma}_{2} \gamma \sigma_{1}^{-1}$ lies in $G(r+s, t)$, it follows that $\tau_{1}$ and $\tau_{2}$ are in the same left coset of $G(r+s, t)$. Therefore, $\tau_{1}=\tau_{2}$, and $\sigma_{1}=\sigma_{2} \gamma$. But this implies that $\sigma_{1}$ and $\sigma_{2}$ (regarded as elements of $\left.S_{r+s}\right)$ lie in the same coset of $G(r, s)$; hence $\sigma_{1}=\sigma_{2}$. Therefore, the products $\tau \sigma$ corresponding to the

$$
\frac{(r+s+t) !}{(r+s) ! t !} \frac{(r+s) !}{r ! s !}
$$

pairs $(r, \sigma)$ in $T \times S$ are all distinct and lie in distinct cosets of $G(r, s, t)$. Since there are exactly

$$
\frac{(r+s+t) !}{r ! s ! t !}
$$

left cosets of $G(r, s, t)$ in $S_{r+s+t}$, it follows that $(L \wedge M) \wedge N=E$. By an analogous argument, $L \wedge(M \wedge N)=E$ as well.

Example 13. The exterior product is closely related to certain formulas for evaluating determinants known as the Laplace expansions. Let $K$ be a commutative ring with identity and $n$ a positive integer. Suppose that $1 \leq r<n$, and let $L$ be the alternating $r$-linear form on $K^{n}$ defined by

$$
L\left(\alpha_{1}, \ldots, \alpha_{r}\right)=\operatorname{det}\left[\begin{array}{ccc}
A_{11} & \cdots & A_{1 r} \\
\vdots & & \vdots \\
A_{r 1} & \cdots & A_{r r}
\end{array}\right] .
$$

If $s=n-r$ and $M$ is the alternating $s$-linear form

$$
M\left(\alpha_{1}, \ldots, \alpha_{s}\right)=\operatorname{det}\left[\begin{array}{ccc}
A_{1(r+1)} & \cdots & A_{1 n} \\
\vdots & & \vdots \\
A_{s(r+1)} & \cdots & A_{s n}
\end{array}\right]
$$

then $L \wedge M=D$, the determinant function on $K^{n}$. This is immediate from the fact that $L \wedge M$ is an alternating $n$-linear form and (as can be seen)

$$
(L \wedge M)\left(\epsilon_{1}, \ldots, \epsilon_{n}\right)=1 \text {. }
$$

If we now describe $L \wedge M$ in the correct way, we obtain one Laplace expansion for the determinant of an $n \times n$ matrix over $K$.

In the permutation group $S_{n}$, let $G$ be the subgroup which permutes the sets $\{1, \ldots, r\}$ and $\{r+1, \ldots, n\}$ within themselves. Each left coset of $G$ contains precisely one permutation $\sigma$ such that $\sigma 1<\sigma 2<\ldots<$ $\sigma r$ and $\sigma(r+1)<\ldots<\sigma n$. The sign of this permutation is given by

$$
\operatorname{sgn} \sigma=(-1)^{\sigma 1+\cdots+\sigma r+(r(r-1) / 2)} \text {. }
$$

The wedge product $L \wedge M$ is given by

$$
(L \wedge M)\left(\alpha_{1}, \ldots, \alpha_{n}\right)=\Sigma(\operatorname{sgn} \sigma) L\left(\alpha \sigma_{1}, \ldots, \alpha_{\sigma r}\right) M\left(\alpha_{\sigma(r+1)}, \ldots, \alpha_{\sigma_{n}}\right)
$$

where the sum is taken over a collection of $\sigma$ 's, one from each coset of $G$. Therefore, where

$$
(L \wedge M)\left(\alpha_{1}, \ldots, \alpha_{n}\right)=\sum_{j_{1}<\cdots<j_{r}}^{\Sigma} e_{J} L\left(\alpha_{j_{1}}, \ldots, \alpha_{j_{r}}\right) M\left(\alpha_{k_{1}}, \ldots, \alpha_{k_{2}}\right)
$$

$$
\begin{aligned}
& e_{J}=(-1)^{j_{1}+\cdots+j_{r}+(r(r-1) / 2)} \\
& k_{i}=\sigma(r+i) .
\end{aligned}
$$

In other words,

$$
\operatorname{det} A=\sum_{j_{1}<\cdots<j_{r}}^{\Sigma} e_{J}\left|\begin{array}{ccc|ccc}
A_{j_{1}, 1} & \cdots & A_{j, r} \\
\vdots & & \vdots \\
A_{j_{r}, 1} & \cdots & A_{j, r}
\end{array}\right|\left|\begin{array}{ccc}
A_{k 1, r+1} & \cdots & A_{k 1, n} \\
\vdots & & \vdots \\
A_{k_{r}, r+1} & \cdots & A_{k_{r}, n}
\end{array}\right|
$$

This is one Laplace expansion. Others may be obtained by replacing the sets $\{1, \ldots, r\}$ and $\{r+1, \ldots, n\}$ by two different complementary sets of indices.

If $V$ is a $K$-module, we may put the various form modules $\Lambda^{r}(V)$ together and use the exterior product to define a ring. For simplicity, we shall do this only for the case of a free $K$-module of rank $n$. The modules $\Lambda^{r}(V)$ are then trivial for $r>n$. We define

$$
\Lambda(V)=\Lambda^{0}(V) \oplus \Lambda^{1}(V) \oplus \cdots \oplus \Lambda^{n}(V) .
$$

This is an external direct sum-something which we have not discussed previously. The elements of $\Lambda(V)$ are the $(n+1)$-tuples $\left(L_{0}, \ldots, L_{n}\right)$ with $L_{r}$ in $\Lambda^{\tau}(V)$. Addition and multiplication by elements of $K$ are defined as one would expect for $(n+1)$-tuples. Incidentally, $\Lambda^{0}(V)=K$. If we identify $\Lambda^{r}(K)$ with the $(n+1)$-tuples $(0, \ldots, 0, L, 0, \ldots, 0)$ where $L$ is in $\Lambda^{r}(K)$, then $\Lambda^{r}(K)$ is a submodule of $\Lambda(V)$ and the direct sum decomposition

$$
\Lambda(V)=\Lambda^{0}(V) \oplus \cdots \oplus \Lambda^{n}(V)
$$

holds in the usual sense. Since $\Lambda^{r}(V)$ is a free $K$-module of $\operatorname{rank}\left(\begin{array}{l}n \\ r\end{array}\right)$, we see that $\Lambda(V)$ is a free $K$-module and

$$
\begin{aligned}
\operatorname{rank} \Lambda(V) & =\sum_{r=0}^{n}\left(\begin{array}{l}
n \\
r
\end{array}\right) \\
& =2^{n} .
\end{aligned}
$$

The exterior product defines a multiplication in $\Lambda(V)$ : Use the exterior product on forms and extend it linearly to $\Lambda(V)$. It distributes over the addition of $\Lambda(V)$ and gives $\Lambda(V)$ the structure of a ring. This ring is the Grassman ring over $V^{*}$. It is not a commutative ring, e.g., if $L, M$ are respectively in $\Lambda^{r}$ and $\Lambda^{s}$, then

$$
L \wedge M=(-1)^{r s} M \wedge L .
$$

But, the Grassman ring is important in several parts of mathematics. 

\section{Elementary}

\section{Canonical Forms}

\subsection{Introduction}

We have mentioned earlier that our principal aim is to study linear transformations on finite-dimensional vector spaces. By this time, we have seen many specific examples of linear transformations, and we have proved a few theorems about the general linear transformation. In the finitedimensional case we have utilized ordered bases to represent such transformations by matrices, and this representation adds to our insight into their behavior. We have explored the vector space $L(V, W)$, consisting of the linear transformations from one space into another, and we have explored the linear algebra $L(V, V)$, consisting of the linear transformations of a space into itself.

In the next two chapters, we shall be preoccupied with linear operators. Our program is to select a single linear operator $T$ on a finite-dimensional vector space $V$ and to 'take it apart to see what makes it tick.' At this early stage, it is easiest to express our goal in matrix language: Given the linear operator $T$, find an ordered basis for $V$ in which the matrix of $T$ assumes an especially simple form.

Here is an illustration of what we have in mind. Perhaps the simplest matrices to work with, beyond the scalar multiples of the identity, are the diagonal matrices:

$$
D=\left[\begin{array}{ccccc}
c_{1} & 0 & 0 & \cdots & 0 \\
0 & c_{2} & 0 & \cdots & 0 \\
0 & 0 & c_{3} & \cdots & 0 \\
\vdots & \vdots & \vdots & & \vdots \\
0 & 0 & 0 & \cdots & c_{n}
\end{array}\right] .
$$

Let $T$ be a linear operator on an $n$-dimensional space $V$. If we could find an ordered basis $B=\left\{\alpha_{1}, \ldots, \alpha_{n}\right\}$ for $V$ in which $T$ were represented by a diagonal matrix $D$ (6-1), we would gain considerable information about $T$. For instance, simple numbers associated with $T$, such as the rank of $T$ or the determinant of $T$, could be determined with little more than a glance at the matrix $D$. We could describe explicitly the range and the null space of $T$. Since $[T]_{\mathfrak{B}}=D$ if and only if

$$
T \boldsymbol{\alpha}_{k}=c_{k} \alpha_{k}, \quad k=1, \ldots, n
$$

the range would be the subspace spanned by those $\alpha_{k}$ 's for which $c_{k} \neq 0$ and the null space would be spanned by the remaining $\alpha_{k}$ 's. Indeed, it seems fair to say that, if we knew a basis $B$ and a diagonal matrix $D$ such that $[T]_{c}=D$, we could answer readily any question about $T$ which might arise.

Can each linear operator $T$ be represented by a diagonal matrix in some ordered basis? If not, for which operators $T$ does such a basis exist? How can we find such a basis if there is one? If no such basis exists, what is the simplest type of matrix by which we can represent $T$ ? These are some of the questions which we shall attack in this (and the next) chapter. The form of our questions will become more sophisticated as we learn what some of the difficulties are.

\subsection{Characteristic Values}

The introductory remarks of the previous section provide us with a starting point for our attempt to analyze the general linear operator $T$. We take our cue from (6-2), which suggests that we should study vectors which are sent by $T$ into scalar multiples of themselves.

Definition. Let $\mathrm{V}$ be a vector space over the field $\mathrm{F}$ and let $\mathrm{T}$ be a linear operator on $\mathrm{V}$. A characteristic value of $\mathrm{T}$ is a scalar $\mathrm{c}$ in $\mathrm{F}$ such that there is a non-zero vector $\alpha$ in $\mathrm{V}$ with $\mathrm{T} \alpha=\mathrm{c} \alpha$. If $\mathrm{c}$ is a characteristic value of $\mathrm{T}$, then

(a) any $\alpha$ such that $\mathrm{T} \alpha=\mathrm{c} \alpha$ is called a characteristic vector of $\mathrm{T}$ associated with the characteristic value c;

(b) the collection of all $\alpha$ such that $\mathrm{T} \alpha=\mathrm{c} \alpha$ is called the characteristic space associated with c.

Characteristic values are of ten called characteristic roots, latent roots, eigenvalues, proper values, or spectral values. In this book we shall use only the name 'characteristic values.'

If $T$ is any linear operator and $c$ is any scalar, the set of vectors $\alpha$ such that $T \alpha=c \alpha$ is a subspace of $V$. It is the null space of the linear trans- formation $(T-c I)$. We call $c$ a characteristic value of $T$ if this subspace is different from the zero subspace, i.e., if $(T-c I)$ fails to be $1: 1$. If the underlying space $V$ is finite-dimensional, $(T-c I)$ fails to be $1: 1$ precisely when its determinant is different from 0 . Let us summarize.

Theorem 1. Let $\mathrm{T}$ be a linear operator on a finite-dimensional space $\mathrm{V}$ and let $\mathrm{c}$ be a scalar. The following are equivalent.

(i) $\mathrm{c}$ is a characteristic value of $\mathrm{T}$.

(ii) The operator $(\mathrm{T}-\mathrm{cI}$ ) is singular (not invertible).

(iii) $\operatorname{det}(\mathrm{T}-\mathrm{cI})=0$.

The determinant criterion (iii) is very important because it tells us where to look for the characteristic values of $T$. Since $\operatorname{det}(T-c I)$ is a polynomial of degree $n$ in the variable $c$, we will find the characteristic values as the roots of that polynomial. Let us explain carefully.

If $B$ is any ordered basis for $V$ and $A=[T]_{\Pi}$, then $(T-c I)$ is invertible if and only if the matrix $(A-c I)$ is invertible. Accordingly, we make the following definition.

Definition. If A is an $\mathrm{n} \times \mathrm{n}$ matrix over the field $\mathrm{F}$, a characteristic value of $\mathrm{A}$ in $\mathrm{F}$ is a scalar $\mathrm{c}$ in $\mathrm{F}$ such that the matrix ( $\mathrm{A}-\mathrm{cI}$ ) is singular (not invertible).

Since $c$ is a characteristic value of $A$ if and only if $\operatorname{det}(A-c I)=0$, or equivalently if and only if $\operatorname{det}(c I-A)=0$, we form the matrix $(x I-A)$ with polynomial entries, and consider the polynomial $f=$ $\operatorname{det}(x I-A)$. Clearly the characteristic values of $A$ in $F$ are just the scalars $c$ in $F$ such that $f(c)=0$. For this reason $f$ is called the characteristic polynomial of $A$. It is important to note that $f$ is a monic polynomial which has degree exactly $n$. This is easily seen from the formula for the determinant of a matrix in terms of its entries.

Lemma. Similar matrices have the same characteristic polynomial.

Proof. If $B=P^{-1} A P$, then

$$
\begin{aligned}
\operatorname{det}(x I-B) & =\operatorname{det}\left(x I-P^{-1} A P\right) \\
& =\operatorname{det}\left(P^{-1}(x I-A) P\right) \\
& =\operatorname{det} P^{-1} \cdot \operatorname{det}(x I-A) \cdot \operatorname{det} P \\
& =\operatorname{det}(x I-A) .
\end{aligned}
$$

This lemma enables us to define sensibly the characteristic polynomial of the operator $T$ as the characteristic polynomial of any $n \times n$ matrix which represents $T$ in some ordered basis for $V$. Just as for matrices, the characteristic values of $T$ will be the roots of the characteristic polynomial for $T$. In particular, this shows us that $T$ cannot have more than $n$ distinct characteristic values. It is important to point out that $T$ may not have any characteristic values.

Example 1 . Let $T$ be the linear operator on $R^{2}$ which is represented in the standard ordered basis by the matrix

$$
A=\left[\begin{array}{rr}
0 & -1 \\
1 & 0
\end{array}\right] \text {. }
$$

The characteristic polynomial for $T$ (or for $A$ ) is

$$
\operatorname{det}(x I-A)=\left|\begin{array}{rr}
x & 1 \\
-1 & x
\end{array}\right|=x^{2}+1 .
$$

Since this polynomial has no real roots, $T$ has no characteristic values. If $U$ is the linear operator on $C^{2}$ which is represented by $A$ in the standard ordered basis, then $U$ has two characteristic values, $i$ and $-i$. Here we see a subtle point. In discussing the characteristic values of a matrix $A$, we must be careful to stipulate the field involved. The matrix $A$ above has no characteristic values in $R$, but has the two characteristic values $i$ and $-i$ in $C$.

Example 2 . Let $A$ be the (real) $3 \times 3$ matrix

$$
\left[\begin{array}{rrr}
3 & 1 & -1 \\
2 & 2 & -1 \\
2 & 2 & 0
\end{array}\right]
$$

Then the characteristic polynomial for $A$ is

$$
\left|\begin{array}{ccc}
x-3 & -1 & 1 \\
-2 & x-2 & 1 \\
-2 & -2 & x
\end{array}\right|=x^{3}-5 x^{2}+8 x-4=(x-1)(x-2)^{2}
$$

Thus the characteristic values of $A$ are 1 and 2.

Suppose that $T$ is the linear operator on $R^{3}$ which is represented by $A$ in the standard basis. Let us find the characteristic vectors of $T$ associated with the characteristic values, 1 and 2 . Now

$$
A-I=\left[\begin{array}{lll}
2 & 1 & -1 \\
2 & 1 & -1 \\
2 & 2 & -1
\end{array}\right]
$$

It is obvious at a glance that $A-I$ has rank equal to 2 (and hence $T-I$ has nullity equal to 1 ). So the space of characteristic vectors associated with the characteristic value 1 is one-dimensional. The vector $\alpha_{1}=(1,0,2)$ spans the null space of $T-I$. Thus $T \alpha=\alpha$ if and only if $\alpha$ is a scalar multiple of $\alpha_{1}$. Now consider

$$
A-2 I=\left[\begin{array}{lll}
1 & 1 & -1 \\
2 & 0 & -1 \\
2 & 2 & -2
\end{array}\right]
$$

Evidently $A-2 I$ also has rank 2 , so that the space of characteristic vectors associated with the characteristic value 2 has dimension 1 . Evidently $T \alpha=2 \alpha$ if and only if $\alpha$ is a scalar multiple of $\alpha_{2}=(1,1,2)$.

Definition. Let $\mathrm{T}$ be a linear operator on the finite-dimensional space . V. We say that $\mathrm{T}$ is diagonalizable if there is a basis for $\mathrm{V}$ each vector of which is a characteristic vector of $\mathrm{T}$.

The reason for the name should be apparent; for, if there is an ordered basis $B=\left\{\alpha_{1}, \ldots, \alpha_{n}\right\}$ for $V$ in which each $\alpha_{i}$ is a characteristic vector of $T$, then the matrix of $T$ in the ordered basis $B$ is diagonal. If $T \alpha_{i}=c_{i} \alpha_{i}$, then

$$
[T]_{\mathfrak{B}}=\left[\begin{array}{cccc}
c_{1} & 0 & \cdots & 0 \\
0 & c_{2} & \cdots & 0 \\
\vdots & \vdots & & \vdots \\
0 & 0 & \cdots & c_{n}
\end{array}\right] .
$$

We certainly do not require that the scalars $c_{1}, \ldots, c_{n}$ be distinct; indeed, they may all be the same scalar (when $T$ is a scalar multiple of the identity operator).

One could also define $T$ to be diagonalizable when the characteristic vectors of $T$ span $V$. This is only superficially different from our definition, since we can select a basis out of any spanning set of vectors.

For Examples 1 and 2 we purposely chose linear operators $T$ on $R^{n}$ which are not diagonalizable. In Example 1, we have a linear operator on $R^{2}$ which is not diagonalizable, because it has no characteristic values. In Example 2, the operator $T$ has characteristic values; in fact, the characteristic polynomial for $T$ factors completely over the real number field: $f=(x-1)(x-2)^{2}$. Nevertheless $T$ fails to be diagonalizable. There is only a one-dimensional space of characteristic vectors associated with each of the two characteristic values of $T$. Hence, we cannot possibly form a basis for $R^{3}$ which consists of characteristic vectors of $T$.

Suppose that $T$ is a diagonalizable linear operator. Let $c_{1}, \ldots, c_{k}$ be the distinct characteristic values of $T$. Then there is an ordered basis $B$ in which $T$ is represented by a diagonal matrix which has for its diagonal entries the scalars $c_{i}$, each repeated a certain number of times. If $c_{i}$ is repeated $d_{i}$ times, then (we may arrange that) the matrix has the block form

$$
[T]_{\mathscr{Q}}=\left[\begin{array}{cccc}
c_{1} I_{1} & 0 & \cdots & 0 \\
0 & c_{2} I_{2} & \cdots & 0 \\
\vdots & \vdots & & \vdots \\
0 & 0 & \cdots & c_{k} I_{k}
\end{array}\right]
$$

where $I_{j}$ is the $d_{j} \times d_{j}$ identity matrix. From that matrix we see two things. First, the characteristic polynomial for $T$ is the product of (possibly repeated) linear factors: 

$$
f=\left(x-c_{1}\right)^{d_{1}} \cdots\left(x-c_{k}\right)^{d_{k}} .
$$

If the scalar field $F$ is algebraically closed, e.g., the field of complex numbers, every polynomial over $F$ can be so factored (see Section 4.5); however, if $F$ is not algebraically closed, we are citing a special property of $T$ when we say that its characteristic polynomial has such a factorization. The second thing we see from (6-3) is that $d_{i}$, the number of times which $c_{i}$ is repeated as root of $f$, is equal to the dimension of the space of characteristic vectors associated with the characteristic value $c_{i}$. That is because the nullity of a diagonal matrix is equal to the number of zeros which it has on its main diagonal, and the matrix $\left[T-c_{i} I\right]_{\infty}$ has $d_{i}$ zeros on its main diagonal. This relation between the dimension of the characteristic space and the multiplicity of the characteristic value as a root of $f$ does not seem exciting at first; however, it will provide us with a simpler way of determining whether a given operator is diagonalizable.

Lemma. Suppose that $\mathrm{T} \alpha=\mathrm{c} \alpha$. If $\mathrm{f}$ is any polynomial, then $\mathrm{f}(\mathrm{T}) \alpha=$ $\mathrm{f}(\mathrm{c}) \alpha$.

Proof. Exercise.

Lemma. Let $\mathrm{T}$ be a linear operator on the finite-dimensional space $\mathrm{V}$. Let $\mathrm{c}_{1}, \ldots, \mathrm{c}_{\mathbf{k}}$ be the distinct characteristic values of $\mathrm{T}$ and let $\mathrm{W}_{\mathrm{i}}$ be the space of characteristic vectors associated with the characteristic value $\mathrm{c}_{\mathbf{i}}$. If $\mathrm{W}=$ $\mathrm{W}_{1}+\cdots+\mathrm{W}_{\mathrm{k}}$, then

$$
\operatorname{dim} \mathrm{W}=\operatorname{dim} \mathrm{W}_{1}+\cdots+\operatorname{dim} \mathrm{W}_{\mathrm{k}} .
$$

In fact, if $B_{\mathfrak{i}}$ is an ordered basis for $\mathrm{W}_{\mathrm{i}}$, then $B=\left(B_{1}, \ldots, B_{\mathrm{k}}\right)$ is an ordered basis for $\mathrm{W}$.

Proof. The space $W=W_{1}+\cdots+W_{k}$ is the subspace spanned by all of the characteristic vectors of $T$. Usually when one forms the sum $W$ of subspaces $W_{i}$, one expects that $\operatorname{dim} W<\operatorname{dim} W_{1}+\cdots+\operatorname{dim} W_{k}$ because of linear relations which may exist between vectors in the various spaces. This lemma states that the characteristic spaces associated with different characteristic values are independent of one another.

Suppose that (for cach $i$ ) we have a vector $\beta_{i}$ in $W_{i}$, and assume that $\beta_{1}+\cdots+\beta_{k}=0$. We shall show that $\beta_{i}=0$ for each $i$. Let $f$ be any polynomial. Since $T \beta_{i}=c_{i} \beta_{i}$, the preceding lemma tells us that

$$
\begin{aligned}
0=f(T) 0 & =f(T) \beta_{1}+\cdots+f(T) \beta_{k} \\
& =f\left(c_{1}\right) \beta_{1}+\cdots+f\left(c_{k}\right) \beta_{k} .
\end{aligned}
$$

Choose polynomials $f_{1}, \ldots, f_{k}$ such that

$$
f_{i}\left(c_{j}\right)=\delta_{i j}= \begin{cases}1, & i=j \\ 0, & i \neq j .\end{cases}
$$

Then

$$
\begin{aligned}
0=f_{i}(T) 0 & =\sum_{j} \delta_{i j} \beta_{j} \\
& =\beta_{i} .
\end{aligned}
$$

Now, let $B_{i}$ be an ordered basis for $W_{i}$, and let $B$ be the sequence $B=\left(\mathbb{B}_{1}, \ldots, \mathbb{B}_{k}\right)$. Then $B$ spans the subspace $W=W_{1}+\cdots+W_{k}$. Also, $Q$ is a linearly independent sequence of vectors, for the following reason. Any linear relation between the vectors in $B$ will have the form $\beta_{1}+\cdots+\beta_{k}=0$, where $\beta_{i}$ is some linear combination of the vectors in $Q_{i}$. From what we just did, we know that $\beta_{i}=0$ for each $i$. Since each $B_{i}$ is linearly independent, we see that we have only the trivial linear relation between the vectors in $Q$.

Theorem 2. Let $\mathrm{T}$ be a linear operator on a finite-dimensional space $\mathrm{V}$. Let $\mathrm{c}_{1}, \ldots, \mathrm{c}_{\mathrm{k}}$ be the distinct characteristic values of $\mathrm{T}$ and let $\mathrm{W}_{\mathrm{i}}$ be the null space of $\left(\mathrm{T}-\mathrm{c}_{\mathrm{i}} \mathrm{I}\right)$. The following are equivalent.

(i) $\mathrm{T}$ is diagonalizable.

(ii) The characteristic polynomial for $\mathrm{T}$ is

$$
f=\left(x-c_{1}\right)^{d 1} \cdots\left(x-c_{k}\right)^{d k}
$$

and $\operatorname{dim} \mathrm{W}_{\mathbf{i}}=\mathrm{d}_{\mathrm{i}}, \mathrm{i}=1, \ldots, \mathrm{k}$.

(iii) $\operatorname{dim} \mathrm{W}_{1}+\cdots+\operatorname{dim} \mathrm{W}_{\mathbf{k}}=\operatorname{dim} \mathrm{V}$.

Proof. We have observed that (i) implies (ii). If the characteristic polynomial $f$ is the product of linear factors, as in (ii), then $d_{1}+\cdots+$ $d_{k}=\operatorname{dim} V$. For, the sum of the $d_{i}$ 's is the degree of the characteristic polynomial, and that degree is $\operatorname{dim} V$. Therefore (ii) implies (iii). Suppose (iii) holds. By the lemma, we must have $V=W_{1}+\cdots+W_{k}$, i.e., the characteristic vectors of $T$ span $V$.

The matrix analogue of Theorem 2 may be formulated as follows. Let $A$ be an $n \times n$ matrix with entries in a field $F$, and let $c_{1}, \ldots, c_{k}$ be the distinct characteristic values of $A$ in $F$. For each $i$, let $W_{i}$ be the space of column matrices $X$ (with entries in $F$ ) such that

$$
\left(A-c_{i} I\right) X=0 \text {, }
$$

and let $B_{i}$ be an ordered basis for $W_{i}$. The bases $Q_{1}, \ldots, B_{k}$ collectively string together to form the sequence of columns of a matrix $P$ :

$$
P=\left[P_{1}, P_{2}, \ldots\right]=\left(B_{1}, \ldots, B_{k}\right) .
$$

The matrix $A$ is similar over $F$ to a diagonal matrix if and only if $P$ is a square matrix. When $P$ is square, $P$ is invertible and $P^{-1} A P$ is diagonal.

Example 3. Let $T$ be the linear operator on $R^{3}$ which is represented in the standard ordered basis by the matrix 

$$
A=\left[\begin{array}{rrr}
5 & -6 & -6 \\
-1 & 4 & 2 \\
3 & -6 & -4
\end{array}\right]
$$

Let us indicate how one might compute the characteristic polynomial, using various row and column operations:

$$
\begin{aligned}
\left|\begin{array}{ccc}
x-5 & 6 & 6 \\
1 & x-4 & -2 \\
-3 & 6 & x+4
\end{array}\right| & =\left|\begin{array}{ccc}
x-5 & 0 & 6 \\
1 & x-2 & -2 \\
-3 & 2-x & x+4
\end{array}\right| \\
& =(x-2)\left|\begin{array}{ccc}
x-5 & 0 & 6 \\
1 & 1 & -2 \\
-3 & -1 & x+4
\end{array}\right| \\
& =(x-2)\left|\begin{array}{ccc}
x-5 & 0 & 6 \\
1 & 1 & -2 \\
-2 & 0 & x+2
\end{array}\right| \\
& =(x-2)\left|\begin{array}{ll}
x-5 & 6 \\
-2 & x+2
\end{array}\right| \\
& =(x-2)\left(x^{2}-3 x+2\right) \\
& =(x-2)^{2}(x-1) .
\end{aligned}
$$

What are the dimensions of the spaces of characteristic vectors associated with the two characteristic values? We have

$$
\begin{aligned}
& A-I=\left[\begin{array}{rrr}
4 & -6 & -6 \\
-1 & 3 & 2 \\
3 & -6 & -5
\end{array}\right] \\
& A-2 I=\left[\begin{array}{rrr}
3 & -6 & -6 \\
-1 & 2 & 2 \\
3 & -6 & -6
\end{array}\right]
\end{aligned}
$$

We know that $A-I$ is singular and obviously $\operatorname{rank}(A-I) \geq 2$. Therefore, rank $(A-I)=2$. It is evident that rank $(A-2 I)=1$.

Let $W_{1}, W_{2}$ be the spaces of characteristic vectors associated with the characteristic values 1,2 . We know that $\operatorname{dim} W_{1}=1$ and $\operatorname{dim} W_{2}=2$. By Theorem 2, $T$ is diagonalizable. It is easy to exhibit a basis for $R^{3}$ in which $T$ is represented by a diagonal matrix. The null space of $(T-I)$ is spanned by the vector $\alpha_{1}=(3,-1,3)$ and so $\left\{\alpha_{1}\right\}$ is a basis for $W_{1}$. The null space of $T-2 I$ (i.e., the space $\left.W_{2}\right)$ consists of the vectors $\left(x_{1}, x_{2}, x_{3}\right)$ with $x_{1}=$ $2 x_{2}+2 x_{3}$. Thus, one example of a basis for $W_{2}$ is

$$
\begin{aligned}
& \alpha_{2}=(2,1,0) \\
& \alpha_{3}=(2,0,1) .
\end{aligned}
$$

If $\Theta=\left\{\alpha_{1}, \alpha_{2}, \alpha_{3}\right\}$, then $[T]_{Q}$ is the diagonal matrix 

$$
D=\left[\begin{array}{lll}
1 & 0 & 0 \\
0 & 2 & 0 \\
0 & 0 & 2
\end{array}\right] \text {. }
$$

The fact that $T$ is diagonalizable means that the original matrix $A$ is similar (over $R$ ) to the diagonal matrix $D$. The matrix $P$ which enables us to change coordinates from the basis $\&$ to the standard basis is (of course) the matrix which has the transposes of $\alpha_{1}, \alpha_{2}, \alpha_{3}$ as its column vectors:

$$
P=\left[\begin{array}{rrr}
3 & 2 & 2 \\
-1 & 1 & 0 \\
3 & 0 & 1
\end{array}\right]
$$

Furthermore, $A P=P D$, so that

$$
P^{-1} A P=D .
$$

\section{Exercises}

1. In each of the following cases, let $T$ be the linear operator on $R^{2}$ which is represented by the matrix $A$ in the standard ordered basis for $R^{2}$, and let $U$ be the linear operator on $C^{2}$ represented by $A$ in the standard ordered basis. Find the characteristic polynomial for $T$ and that for $U$, find the characteristic values of each operator, and for each such characteristic value $c$ find a basis for the corresponding space of characteristic vectors.

$$
A=\left[\begin{array}{ll}
1 & 0 \\
0 & 0
\end{array}\right], \quad A=\left[\begin{array}{rr}
2 & 3 \\
-1 & 1
\end{array}\right], \quad A=\left[\begin{array}{ll}
1 & 1 \\
1 & 1
\end{array}\right] .
$$

2. Let $V$ be an $n$-dimensional vector space over $F$. What is the characteristic polynomial of the identity operator on $V$ ? What is the characteristic polynomial for the zero operator?

3. Let $A$ be an $n \times n$ triangular matrix over the field $F$. Prove that the characteristic values of $A$ are the diagonal entries of $A$, i.e., the scalars $A_{i i}$.

4. Let $T$ be the linear operator on $R^{3}$ which is represented in the standard ordered basis by the matrix

$$
\left[\begin{array}{rrr}
-9 & 4 & 4 \\
-8 & 3 & 4 \\
-16 & 8 & 7
\end{array}\right] \text {. }
$$

Prove that $T$ is diagonalizable by exhibiting a basis for $R^{3}$, each vector of which is a characteristic vector of $T$.

5. Let

$$
A=\left[\begin{array}{rrr}
6 & -3 & -2 \\
4 & -1 & -2 \\
10 & -5 & -3
\end{array}\right] \text {. }
$$

Is $A$ similar over the field $R$ to a diagonal matrix? Is $A$ similar over the field $C$ to a diagonal matrix? 6. Let $T$ be the linear operator on $R^{4}$ which is represented in the standard ordered basis by the matrix

$$
\left[\begin{array}{llll}
0 & 0 & 0 & 0 \\
a & 0 & 0 & 0 \\
0 & b & 0 & 0 \\
0 & 0 & c & 0
\end{array}\right] .
$$

Under what conditions on $a, b$, and $c$ is $T$ diagonalizable?

7. Let $T$ be a linear operator on the $n$-dimensional vector space $V$, and suppose that $T$ has $n$ distinct characteristic values. Prove that $T$ is diagonalizable.

8. Let $A$ and $B$ be $n \times n$ matrices over the field $F$. Prove that if $(I-A B)$ is invertible, then $I-B A$ is invertible and

$$
(I-B A)^{-1}=I+B(I-A B)^{-1} A .
$$

9. Use the result of Exercise 8 to prove that, if $A$ and $B$ are $n \times n$ matrices over the field $F$, then $A B$ and $B A$ have precisely the same characteristic values in $F$.

10. Suppose that $A$ is a $2 \times 2$ matrix with real entries which is symmetric $\left(A^{t}=A\right)$. Prove that $A$ is similar over $R$ to a diagonal matrix.

11. Let $N$ be a $2 \times 2$ complex matrix such that $N^{2}=0$. Prove that either $N=0$ or $N$ is similar over $C$ to

$$
\left[\begin{array}{ll}
0 & 0 \\
1 & 0
\end{array}\right] \text {. }
$$

12. Use the result of Exercise 11 to prove the following: If $A$ is a $2 \times 2$ matrix with complex entries, then $A$ is similar over $C$ to a matrix of one of the two types

$$
\left[\begin{array}{ll}
a & 0 \\
0 & b
\end{array}\right] \quad\left[\begin{array}{ll}
a & 0 \\
1 & a
\end{array}\right] .
$$

13. Let $V$ be the vector space of all functions from $R$ into $R$ which are continuous, i.e., the space of continuous real-valued functions on the real line. Let $T$ be the linear operator on $V$ defined by

$$
(T f)(x)=\int_{0}^{x} f(t) d t .
$$

Prove that $T$ has no characteristic values.

14. Let $A$ be an $n \times n$ diagonal matrix with characteristic polynomial

$$
\left(x-c_{1}\right)^{d_{1} \cdots}\left(x-c_{k}\right)^{d_{k}},
$$

where $c_{1}, \ldots, c_{k}$ are distinct. Let $V$ be the space of $n \times n$ matrices $B$ such that $A B=B A$. Prove that the dimension of $V$ is $d_{1}^{2}+\cdots+d_{k \text { }}^{2}$

15. Let $V$ be the space of $n \times n$ matrices over $F$. Let $A$ be a fixed $n \times n$ matrix over $F$. Let $T$ be the linear operator 'left multiplication by $A$ ' on $V$. Is it true that $A$ and $T$ have the same characteristic values?

\subsection{Annihilating Polynomials}

In attempting to analyze a linear operator $T$, one of the most useful things to know is the class of polynomials which annihilate $T$. Specifically, suppose $T$ is a linear operator on $V$, a vector space over the field $F$. If $p$ is a polynomial over $F$, then $p(T)$ is again a linear operator on $V$. If $q$ is another polynomial over $F$, then

$$
\begin{aligned}
(p+q)(T) & =p(T)+q(T) \\
(p q)(T) & =p(T) q(T) .
\end{aligned}
$$

Therefore, the collection of polynomials $p$ which annihilate $T$, in the sense that

$$
p(T)=0,
$$

is an ideal in the polynomial algebra $F[x]$. It may be the zero ideal, i.e., it may be that $T$ is not annihilated by any non-zero polynomial. But, that cannot happen if the space $V$ is finite-dimensional.

Suppose $T$ is a linear operator on the $n$-dimensional space $V$. Look at the first $\left(n^{2}+1\right)$ powers of $T$ :

$$
I, T, T^{2}, \ldots, T^{n^{2}} \text {. }
$$

This is a sequence of $n^{2}+1$ operators in $L(V, V)$, the space of linear operators on $V$. The space $L(V, V)$ has dimension $n^{2}$. Therefore, that sequence of $n^{2}+1$ operators must be linearly dependent, i.e., we have

$$
c_{0} I+c_{1} T+\cdots+c_{n^{2}} T^{n^{2}}=0
$$

for some scalars $c_{i}$, not all zero. So, the ideal of polynomials which annihilate $T$ contains a non-zero polynomial of degree $n^{2}$ or less.

According to Theorem 5 of Chapter 4, every polynomial ideal consists of all multiples of some fixed monic polynomial, the generator of the ideal. Thus, there corresponds to the operator $T$ a monic polynomial $p$ with this property: If $f$ is a polynomial over $F$, then $f(T)=0$ if and only if $f=p g$, where $g$ is some polynomial over $F$.

Definition. Let $\mathrm{T}$ be a linear operator on a finite-dimensional vector space $\mathrm{V}$ over the field $\mathrm{F}$. The minimal polynomial for $\mathrm{T}$ is the (unique) monic generator of the ideal of polynomials over $\mathrm{F}$ which annihilate $\mathrm{T}$.

The name 'minimal polynomial' stems from the fact that the generator of a polynomial ideal is characterized by being the monic polynomial of minimum degree in the ideal. That means that the minimal polynomial $p$ for the linear operator $T$ is uniquely determined by these three properties:

(1) $p$ is a monic polynomial over the scalar field $F$.

(2) $p(T)=0$.

(3) No polynomial over $F$ which annihilates $T$ has smaller degree than $p$ has.

If $A$ is an $n \times n$ matrix over $F$, we define the minimal polynomial for $A$ in an analogous way, as the unique monic generator of the ideal of all polynomials over $F$ which annihilate $A$. If the operator $T$ is represented in some ordered basis by the matrix $A$, then $T$ and $A$ have the same minimal polynomial. That is because $f(T)$ is represented in the basis by the matrix $f(A)$, so that $f(T)=0$ if and only if $f(A)=0$.

From the last remark about operators and matrices it follows that similar matrices have the same minimal polynomial. That fact is also clear from the definitions because

$$
f\left(P^{-1} A P\right)=P^{-1} f(A) P
$$

for every polynomial $f$.

There is another basic remark which we should make about minimal polynomials of matrices. Suppose that $A$ is an $n \times n$ matrix with entries in the field $F$. Suppose that $F_{1}$ is a field which contains $F$ as a subfield. (For example, $A$ might be a matrix with rational entries, while $F_{1}$ is the field of real numbers. Or, $A$ might be a matrix with real entries, while $F_{1}$ is the field of complex numbers.) We may regard $A$ either as an $n \times n$ matrix over $F$ or as an $n \times n$ matrix over $F_{1}$. On the surface, it might appear that we obtain two different minimal polynomials for $A$. Fortunately that is not the case; and we must see why. What is the definition of the minimal polynomial for $A$, regarded as an $n \times n$ matrix over the field $F$ ? We consider all monic polynomials with coefficients in $F$ which annihilate $A$, and we choose the one of least degree. If $f$ is a monic polynomial over $F$ :

$$
f=x^{k}+\sum_{j=0}^{k-1} a_{j} x^{j}
$$

then $f(A)=0$ merely says that we have a linear relation between the powers of $A$ :

$$
A^{k}+a_{k-1} A^{k-1}+\cdots+a_{1} A+a_{0} I=0 .
$$

The degree of the minimal polynomial is the least positive integer $k$ such that there is a linear relation of the form (6-5) between the powers $I$, $A, \ldots, A^{k}$. Furthermore, by the uniqueness of the minimal polynomial, there is for that $k$ one and only one relation of the form (6-5); i.e., once the minimal $k$ is determined, there are unique scalars $a_{0}, \ldots, a_{k-1}$ in $F$ such that (6-5) holds. They are the coefficients of the minimal polynomial.

Now (for each $k$ ) we have in (6-5) a system of $n^{2}$ linear equations for the 'unknowns' $a_{0}, \ldots, a_{k-1}$. Since the entries of $A$ lie in $F$, the coefficients of the system of equations (6-5) are in $F$. Therefore, if the system has a solution with $a_{0}, \ldots, a_{k-1}$ in $F_{1}$ it has a solution with $a_{0}, \ldots, a_{k-1}$ in $F$. (See the end of Section 1.4.) It should now be clear that the two minimal polynomials are the same.

What do we know thus far about the minimal polynomial for a linear operator on an $n$-dimensional space? Only that its degree does not exceed $n^{2}$. That turns out to be a rather poor estimate, since the degree cannot exceed $n$. We shall prove shortly that the operator is annihilated by its characteristic polynomial. First, let us observe a more elementary fact. Theorem 3. Let $\mathrm{T}$ be a linear operator on an $\mathrm{n}$-dimensional vector space $\mathrm{V}$ [or, let $\mathrm{A}$ be an $\mathrm{n} \times \mathrm{n}$ matrix]. The characteristic and minimal polynomials for $\mathrm{T}$ [for $\mathrm{A}]$ have the same roots, except for multiplicities.

Proof. Let $p$ be the minimal polynomial for $T$. Let $c$ be a scalar. What we want to show is that $\boldsymbol{p}(c)=0$ if and only if $c$ is a characteristic value of $T$.

First, suppose $p(c)=0$. Then

$$
p=(x-c) q
$$

where $q$ is a polynomial. Since $\operatorname{deg} q<\operatorname{deg} p$, the definition of the minimal polynomial $p$ tells us that $q(T) \neq 0$. Choose a vector $\beta$ such that $q(T) \beta \neq 0$. Let $\alpha=q(T) \beta$. Then

$$
\begin{aligned}
0 & =p(T) \beta \\
& =(T-c I) q(T) \beta \\
& =(T-c I) \alpha
\end{aligned}
$$

and thus, $c$ is a characteristic value of $T$.

Now, suppose that $c$ is a characteristic value of $T$, say, $T \alpha=c \alpha$ with $\alpha \neq 0$. As we noted in a previous lemma,

$$
p(T) \alpha=p(c) \alpha .
$$

Since $p(T)=0$ and $\alpha \neq 0$, we have $p(c)=0$.

Iet $T$ be a diagonalizable linear operator and let $c_{1}, \ldots, c_{k}$ be the distinct characteristic values of $T$. Then it is easy to see that the minimal polynomial for $T$ is the polynomial

$$
p=\left(x-c_{1}\right) \cdots\left(x-c_{k}\right) .
$$

If $\alpha$ is a characteristic vector, then one of the operators $T-c_{1} I, \ldots$, $T-c_{k} I$ sends $\alpha$ into 0 . Therefore

$$
\left(T-c_{1} I\right) \cdots\left(T-c_{k} I\right) \alpha=0
$$

for every characteristic vector $\alpha$. There is a basis for the underlying space which consists of characteristic vectors of $T$; hence

$$
p(T)=\left(T-c_{1} I\right) \cdots\left(T-c_{k} I\right)=0 .
$$

What we have concluded is this. If $T$ is a diagonalizable linear operator, then the minimal polynomial for $T$ is a product of distinct linear factors. As we shall soon see, that property characterizes diagonalizable operators.

Example 4. Let's try to find the minimal polynomials for the operators in Examples 1, 2, and 3. We shall discuss them in reverse order. The operator in Example 3 was found to be diagonalizable with characteristic polynomial

$$
f=(x-1)(x-2)^{2} .
$$

From the preceding paragraph, we know that the minimal polynomial for $T$ is

$$
p=(x-1)(x-2) .
$$

The reader might find it reassuring to verify directly that

$$
(A-I)(A-2 I)=0 .
$$

In Example 2, the operator $T$ also had the characteristic polynomial $f=(x-1)(x-2)^{2}$. But, this $T$ is not diagonalizable, so we don't know that the minimal polynomial is $(x-1)(x-2)$. What do we know about the minimal polynomial in this case? From Theorem 3 we know that its roots are 1 and 2, with some multiplicities allowed. Thus we search for $p$ among polynornials of the form $(x-1)^{k}(x-2)^{l}, k \geq 1, l \geq 1$. Try $(x-1)$ $(x-2)$ :

$$
\begin{aligned}
(A-I)(A-2 I) & =\left[\begin{array}{lll}
2 & 1 & -1 \\
2 & 1 & -1 \\
2 & 2 & -1
\end{array}\right]\left[\begin{array}{lll}
1 & 1 & -1 \\
2 & 0 & -1 \\
2 & 2 & -2
\end{array}\right] \\
& =\left[\begin{array}{lll}
2 & 0 & -1 \\
2 & 0 & -1 \\
4 & 0 & -2
\end{array}\right] .
\end{aligned}
$$

Thus, the minimal polynomial has degree at least 3 . So, next we should try either $(x-1)^{2}(x-2)$ or $(x-1)(x-2)^{2}$. The second, being the characteristic polynomial, would seem a less random choice. One can readily compute that $(A-I)(A-2 I)^{2}=0$. Thus the minimal polynomial for $T$ is its characteristic polynomial.

In Example 1 we discussed the linear operator $T$ on $R^{2}$ which is represented in the standard basis by the matrix

$$
A=\left[\begin{array}{rr}
0 & -1 \\
1 & 0
\end{array}\right] .
$$

The characteristic polynomial is $x^{2}+1$, which has no real roots. To determine the minimal polynomial, forget about $T$ and concentrate on $A$. As a complex $2 \times 2$ matrix, $A$ has the characteristic values $i$ and $-i$. Both roots must appear in the minimal polynomial. Thus the minimal polynomial is divisible by $x^{2}+1$. It is trivial to verify that $A^{2}+I=0$. So the minimal polynomial is $x^{2}+1$.

Theorem 4 (Cayley-Hamilton). Let $\mathrm{T}$ be a linear operator on a finite dimensional vector space $\mathrm{V}$. If $\mathrm{f}$ is the characteristic polynomial for $\mathrm{T}$, then $\mathrm{f}(\mathrm{T})=0$; in other words, the minimal polynomial divides the characteristic polynomial for $\mathrm{T}$.

Proof. Later on we shall give two proofs of this result independent of the one to be given here. The present proof, although short, may be difficult to understand. Aside from brevity, it has the virtue of providing an illuminating and far from trivial application of the general theory of determinants developed in Chapter 5.

Let $K$ be the commutative ring with identity consisting of all polynomials in $T$. Of course, $K$ is actually a commutative algebra with identity over the scalar field. Choose an ordered basis $\left\{\alpha_{1}, \ldots, \alpha_{n}\right\}$ for $V$, and let $A$ be the matrix which represents $T$ in the given basis. Then

$$
T \alpha_{i}=\sum_{j=1}^{n} A_{j i} \alpha_{j}, \quad 1 \leq i \leq n .
$$

These equations may be written in the equivalent form

$$
\sum_{j=1}^{n}\left(\delta_{i j} T-A_{j i} I\right) \alpha_{j}=0, \quad 1 \leq i \leq n .
$$

Let $B$ denote the element of $K^{n \times n}$ with entries

When $n=2$

$$
B_{i j}=\delta_{i j} T-A_{j i} I .
$$

$$
B=\left[\begin{array}{ll}
T-A_{11} I & -A_{21} I \\
-A_{12} I & T-A_{22} I
\end{array}\right]
$$

and

$$
\begin{aligned}
\operatorname{det} B & =\left(T-A_{11} I\right)\left(T-A_{22} I\right)-A_{12} A_{21} I \\
& =T^{2}-\left(A_{11}+A_{22}\right) T+\left(A_{11} A_{22}-A_{12} A_{21}\right) I \\
& =f(T)
\end{aligned}
$$

where $f$ is the characteristic polynomial:

$$
f=x^{2}-(\text { trace } A) x+\operatorname{det} A .
$$

For the case $n>2$, it is also clear that

$$
\operatorname{det} B=f(T)
$$

since $f$ is the determinant of the matrix $x I-A$ whose entries are the polynomials

$$
(x I-A)_{i j}=\delta_{i j} x-A_{j i} .
$$

We wish to show that $f(T)=0$. In order that $f(T)$ be the zero operator, it is necessary and sufficient that $(\operatorname{det} B) \alpha_{k}=0$ for $k=1, \ldots, n$. By the definition of $B$, the vectors $\alpha_{1}, \ldots, \alpha_{n}$ satisfy the equations

$$
\sum_{j=1}^{n} B_{i j} \alpha_{j}=0, \quad 1 \leq i \leq n .
$$

When $n=2$, it is suggestive to write (6-6) in the form

$$
\left[\begin{array}{ll}
T-A_{11} I & -A_{21} I \\
-A_{12} I & T-A_{22} I
\end{array}\right]\left[\begin{array}{l}
\alpha_{1} \\
\alpha_{2}
\end{array}\right]=\left[\begin{array}{l}
0 \\
0
\end{array}\right] .
$$

In this case, the classical adjoint, adj $B$ is the matrix

$$
\tilde{B}=\left[\begin{array}{ll}
T-A_{22} I & A_{21} I \\
A_{12} I & T-A_{11} I
\end{array}\right]
$$

and

Hence, we have

$$
\tilde{B} B=\left[\begin{array}{ll}
\operatorname{det} B & 0 \\
0 & \operatorname{det} B
\end{array}\right] .
$$

$$
\begin{aligned}
(\operatorname{det} B)\left[\begin{array}{l}
\alpha_{1} \\
\alpha_{2}
\end{array}\right] & =(\tilde{B} B)\left[\begin{array}{l}
\alpha_{1} \\
\alpha_{2}
\end{array}\right] \\
& =\tilde{B}\left(B\left[\begin{array}{l}
\alpha_{1} \\
\alpha_{2}
\end{array}\right]\right) \\
& =\left[\begin{array}{l}
0 \\
0
\end{array}\right] .
\end{aligned}
$$

In the general case, let $\tilde{B}=\operatorname{adj} B$. Then by (6-6)

$$
\sum_{j=1}^{n} \tilde{B}_{k i} B_{i j} \alpha_{j}=0
$$

for each pair $k, i$, and summing on $i$, we have

$$
\begin{aligned}
0 & =\sum_{i=1}^{n} \sum_{j=1}^{n} \tilde{B}_{k i} B_{i j} \alpha_{j} \\
& =\sum_{j=1}^{n}\left(\sum_{i=1}^{n} \tilde{B}_{k i} B_{i j}\right) \alpha_{j} .
\end{aligned}
$$

Now $\tilde{B} B=(\operatorname{det} B) I$, so that

Therefore

$$
\sum_{i=1}^{n} \tilde{B}_{k i} B_{i j}=\delta_{k j} \operatorname{det} B .
$$

$$
\begin{aligned}
0 & =\sum_{j=1}^{n} \delta_{k j}(\operatorname{det} B) \alpha_{j} \\
& =(\operatorname{det} B) \alpha_{k}, \quad 1 \leq k \leq n .
\end{aligned}
$$

The Cayley-Hamilton theorem is useful to us at this point primarily because it narrows down the search for the minimal polynomials of various operators. If we know the matrix $A$ which represents $T$ in some ordered basis, then we can compute the characteristic polynomial $f$. We know that the minimal polynomial $p$ divides $f$ and that the two polynomials have the same roots. There is no method for computing precisely the roots of a polynomial (unless its degree is small); however, if $f$ factors

$$
f=\left(x-c_{1}\right)^{d_{1}} \cdots\left(x-c_{k}\right)^{d_{k}}, \quad c_{1}, \ldots, c_{k} \text { distinct, } d_{i} \geq 1
$$

then

$$
p=\left(x-c_{1}\right)^{r_{1}} \cdots\left(x-c_{k}\right)^{r_{k}}, \quad 1 \leq r_{j} \leq d_{j} .
$$

That is all we can say in general. If $f$ is the polynomial (6-7) and has degree $n$, then for every polynomial $p$ as in (6-8) we can find an $n \times n$ matrix which has $f$ as its characteristic polynomial and $p$ as its minimal polynomial. We shall not prove this now. But, we want to emphasize the fact that the knowledge that the characteristic polynomial has the form (6-7) tells us that the minimal polynomial has the form (6-8), and it tells us nothing else about $p$.

Example 5. Let $A$ be the $4 \times 4$ (rational) matrix

$$
A=\left[\begin{array}{llll}
0 & 1 & 0 & 1 \\
1 & 0 & 1 & 0 \\
0 & 1 & 0 & 1 \\
1 & 0 & 1 & 0
\end{array}\right] .
$$

The powers of $A$ are easy to compute:

$$
\begin{aligned}
A^{2} & =\left[\begin{array}{llll}
2 & 0 & 2 & 0 \\
0 & 2 & 0 & 2 \\
2 & 0 & 2 & 0 \\
0 & 2 & 0 & 2
\end{array}\right] \\
A^{3} & =\left[\begin{array}{llll}
0 & 4 & 0 & 4 \\
4 & 0 & 4 & 0 \\
0 & 4 & 0 & 4 \\
4 & 0 & 4 & 0
\end{array}\right] .
\end{aligned}
$$

Thus $A^{3}=4 A$, i.e., if $p=x^{3}-4 x=x(x+2)(x-2)$, then $p(A)=0$. The minimal polynomial for $A$ must divide $p$. That minimal polynomial is obviously not of degree 1 , since that would mean that $A$ was a scalar multiple of the identity. Hence, the candidates for the minimal polynomial are: $p, x(x+2), x(x-2), x^{2}-4$. The three quadratic polynomials can be eliminated because it is obvious at a glance that $A^{2} \neq-2 A, A^{2} \neq 2 A$, $A^{2} \neq 4 I$. Therefore $p$ is the minimal polynomial for $A$. In particular 0,2 , and $-2$ are the characteristic values of $A$. One of the factors $x, x-2$, $x+2$ must be repeated twice in the characteristic polynomial. Evidently, rank $(A)=2$. Consequently there is a two-dimensional space of characteristic vectors associated with the characteristic value 0 . From Theorem 2 , it should now be clear that the characteristic polynomial is $x^{2}\left(x^{2}-4\right)$ and that $A$ is similar over the field of rational numbers to the matrix

$$
\left[\begin{array}{rrrr}
0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 \\
0 & 0 & 2 & 0 \\
0 & 0 & 0 & -2
\end{array}\right] \text {. }
$$

\section{Exercises}

1. Let $V$ be a finite-dimensional vector space. What is the minimal polynomial for the identity operator on $V$ ? What is the minimal polynomial for the zero operator? 2. Let $a, b$, and $c$ be elements of a field $F$, and let $A$ be the following $3 \times 3$ matrix over $F$ :

$$
A=\left[\begin{array}{lll}
0 & 0 & c \\
1 & 0 & b \\
0 & 1 & a
\end{array}\right] \text {. }
$$

Prove that the characteristic polynomial for $A$ is $x^{3}-a x^{2}-b x-c$ and that this is also the minimal polynomial for $A$.

3. Let $A$ be the $4 \times 4$ real matrix

$$
A=\left[\begin{array}{rrrr}
1 & 1 & 0 & 0 \\
-1 & -1 & 0 & 0 \\
-2 & -2 & 2 & 1 \\
1 & 1 & -1 & 0
\end{array}\right]
$$

Show that the characteristic polynomial for $A$ is $x^{2}(x-1)^{2}$ and that it is also the minimal polynomial.

4. Is the matrix $A$ of Exercise 3 similar over the field of complex numbers to a diagonal matrix?

5. Let $V$ be an $n$-dimensional vector space and let $T$ be a linear operator on $V$. Suppose that there exists some positive integer $k$ so that $T^{k}=0$. Prove that $T^{n}=0$.

6. Find a $3 \times 3$ matrix for which the minimal polynomial is $x^{2}$.

7. Let $n$ be a positive integer, and let $V$ be the space of polynomials over $R$ which have degree at most $n$ (throw in the 0 -polynomial). Let $D$ be the differentiation operator on $V$. What is the minimal polynomial for $D$ ?

8. Let $P$ be the operator on $R^{2}$ which projects each vector onto the $x$-axis, parallel to the $y$-axis: $P(x, y)=(x, 0)$. Show that $P$ is linear. What is the minimal polynomial for $P$ ?

9. Let $A$ be an $n \times n$ matrix with characteristic polynomial

Show that

$$
f=\left(x-c_{1}\right) d_{1} \cdots\left(x-c_{k}\right) d_{k}
$$

$$
c_{1} d_{1}+\cdots+c_{k} d_{k}=\operatorname{trace}(A) .
$$

10. Let $V$ be the vector space of $n \times n$ matrices over the field $F$. Let $A$ be a fixed $n \times n$ matrix. Let $T$ be the linear operator on $V$ defined by

$$
T(B)=A B .
$$

Show that the minimal polynomial for $T$ is the minimal polynomial for $A$.

11. Let $A$ and $B$ be $n \times n$ matrices over the field $F$. According to Exercise 9 of Section 6.1, the matrices $A B$ and $B A$ have the same characteristic values. Do they have the same characteristic polynomial? Do they have the same minimal polynomial?

\subsection{Invariant Subspaces}

In this section, we shall introduce a few concepts which are useful in attempting to analyze a linear operator. We shall use these ideas to obtain characterizations of diagonalizable (and triangulable) operators in terms of their minimal polynomials.

Definition. Let $\mathrm{V}$ be a vector space and $\mathrm{T}$ a linear operator on $\mathrm{V}$. If $\mathrm{W}$ is a subspace of $\mathrm{V}$, we say that $\mathrm{W}$ is invariant under $\mathrm{T}$ if for each vector $\alpha$ in $\mathrm{W}$ the vector $\mathrm{T} \alpha$ is in $\mathrm{W}$, i.e., if $\mathrm{T}(\mathrm{W})$ is contained in $\mathrm{W}$.

Example 6 . If $T$ is any linear operator on $V$, then $V$ is invariant under $T$, as is the zero subspace. The range of $T$ and the null space of $T$ are also invariant under $T$.

Example 7. Let $F$ be a field and let $D$ be the differentiation operator on the space $F[x]$ of polynomials over $F$. Let $n$ be a positive integer and let $W$ be the subspace of polynomials of degree not greater than $n$. Then $W$ is invariant under $D$. This is just another way of saying that $D$ is 'degree decreasing.'

Example 8. Here is a very useful generalization of Example 6 . Let $T$ be a linear operator on $V$. Let $U$ be any linear operator on $V$ which commutes with $T$, i.e., $T U=U T$. Let $W$ be the range of $U$ and let $N$ be the null space of $U$. Both $W$ and $N$ are invariant under $T$. If $\alpha$ is in the range of $U$, say $\alpha=U \beta$, then $T \alpha=T(U \beta)=U(T \beta)$ so that $T \alpha$ is in the range of $U$. If $\alpha$ is in $N$, then $U(T \alpha)=T(U \alpha)=T(0)=0$; hence, $T \alpha$ is in $N$.

A particular type of operator which commutes with $T$ is an operator $U=g(T)$, where $g$ is a polynomial. For instance, we might have $U=$ $T-c I$, where $c$ is a characteristic value of $T$. The null space of $U$ is familiar to us. We see that this example includes the (obvious) fact that the space of characteristic vectors of $T$ associated with the characteristic value $c$ is invariant under $T$.

Example 9 . Let $T$ be the linear operator on $R^{2}$ which is represented in the standard ordered basis by the matrix

$$
A=\left[\begin{array}{rr}
0 & -1 \\
1 & 0
\end{array}\right] \text {. }
$$

Then the only subspaces of $R^{2}$ which are invariant under $T$ are $R^{2}$ and the zero subspace. Any other invariant subspace would necessarily have dimension 1. But, if $W$ is the subspace spanned by some non-zero vector $\alpha$, the fact that $W$ is invariant under $T$ means that $\alpha$ is a characteristic vector, but $A$ has no real characteristic values.

When the subspace $W$ is invariant under the operator $T$, then $T$ induces a linear operator $T_{W}$ on the space $W$. The linear operator $T_{W}$ is defined by $T_{W}(\alpha)=T(\alpha)$, for $\alpha$ in $W$, but $T_{W}$ is quite a different object from $T$ since its domain is $W$ not $V$.

When $V$ is finite-dimensional, the invariance of $W$ under $T$ has a simple matrix interpretation, and perhaps we should mention it at this point. Suppose we choose an ordered basis $\boldsymbol{\beta}=\left\{\alpha_{1}, \ldots, \alpha_{n}\right\}$ for $V$ such that $\beta^{\prime}=\left\{\alpha_{1}, \ldots, \alpha_{r}\right\}$ is an ordered basis for $W(r=\operatorname{dim} W)$. Let $A=$ $[T]_{B}$ so that

$$
T \alpha_{j}=\sum_{i=1}^{n} A_{i j} \alpha_{i} .
$$

Since $W$ is invariant under $T$, the vector $T \alpha_{j}$ belongs to $W$ for $j \leq r$. This means that

$$
T \alpha_{j}=\sum_{i=1}^{\Gamma} A_{i j} \alpha_{i}, \quad j \leq r .
$$

In other words, $A_{i j}=0$ if $j \leq r$ and $i>r$.

Schematically, $A$ has the block form

$$
A=\left[\begin{array}{ll}
B & C^{-} \\
0 & D
\end{array}\right]
$$

where $B$ is an $r \times r$ matrix, $C$ is an $r \times(n-r)$ matrix, and $D$ is an $(n-r) \times(n-r)$ matrix. The reader should note that according to (6-9) the matrix $B$ is precisely the matrix of the induced operator $T_{w}$ in the ordered basis $\boldsymbol{B}^{\prime}$.

Most often, we shall carry out arguments about $T$ and $T_{W}$ without making use of the block form of the matrix $A$ in (6-10). But we should note how certain relations between $T_{W}$ and $T$ are apparent from that block form.

Lemma. Let $\mathrm{W}$ be an invariant subspace for T. The characteristic polynomial for the restriction operator $\mathrm{T}_{\mathrm{W}}$ divides the characteristic polynomial for $\mathrm{T}$. The minimal polynomial for $\mathrm{T}_{\mathrm{W}}$ divides the minimal polynomialfor $\mathrm{T}$.

Proof. We have

$$
A=\left[\begin{array}{ll}
B & C \\
0 & D
\end{array}\right]
$$

where $A=[T]_{\mathscr{B}}$ and $B=\left[T_{\boldsymbol{T}}\right]_{\boldsymbol{\omega}^{\prime}}$. Because of the block form of the matrix

$$
\operatorname{det}(x I-A)=\operatorname{det}(x I-B) \operatorname{det}(x I-D) \text {. }
$$

That proves the statement about characteristic polynomials. Notice that we used $I$ to represent identity matrices of three different sizes.

The $k$ th power of the matrix $A$ has the block form

$$
A^{k}=\left[\begin{array}{ll}
B^{k} & C_{k} \\
0 & D^{k}
\end{array}\right]
$$

where $C_{k}$ is some $r \times(n-r)$ matrix. Therefore, any polynomial which annihilates $A$ also annihilates $B$ (and $D$ too). So, the minimal polynomial for $B$ divides the minimal polynomial for $A$.

Example 10. Let $T$ be any linear operator on a finiterdimensional space $V$. Let $W$ be the subspace spanned by all of the characteristic vectors of $T$. Let $c_{1}, \ldots, c_{k}$ be the distinct characteristic values of $T$. For each $i$, let $W_{i}$ be the space of characteristic vectors associated with the characteristic value $c_{i}$, and let $\Theta_{i}$ be an ordered basis for $W_{i}$. The lemma before Theorem 2 tells us that $B^{\prime}=\left(B_{1}, \ldots, B_{k}\right)$ is an ordered basis for $W$. In particular,

$$
\operatorname{dim} W=\operatorname{dim} W_{1}+\cdots+\operatorname{dim} W_{k} .
$$

Let $\mathbb{Q}^{\prime}=\left\{\alpha_{1}, \ldots, \alpha_{r}\right\}$ so that the first few $\alpha^{\prime}$ 's form the basis $\Theta_{1}$, the next few $B_{2}$, and so on. Then

$$
T \alpha_{i}=t_{i} \alpha_{i}, \quad i=1, \ldots, r
$$

where $\left(t_{1}, \ldots, t_{r}\right)=\left(c_{1}, c_{1}, \ldots, c_{1}, \ldots, c_{k}, c_{k}, \ldots, c_{k}\right)$ with $c_{i}$ repeated $\operatorname{dim} W_{i}$ times.

Now $W$ is invariant under $T$, since for each $\alpha$ in $W$ we have

$$
\begin{aligned}
\alpha & =x_{1} \alpha_{1}+\cdots+x_{r} \alpha_{r} \\
T \alpha & =t_{1} x_{1} \alpha_{1}+\cdots+t_{r} x_{r} \alpha_{r} .
\end{aligned}
$$

Choose any other vectors $\alpha_{r+1}, \ldots, \alpha_{n}$ in $V$ such that $B=\left\{\alpha_{1}, \ldots, \alpha_{n}\right\}$ is a basis for $V$. The matrix of $T$ relative to $\&$ has the block form (6-10), and the matrix of the restriction operator $T_{W}$ relative to the basis $Q^{\prime}$ is

$$
B=\left[\begin{array}{cccc}
t_{1} & 0 & \cdots & 0 \\
0 & t_{2} & \cdots & 0 \\
\vdots & \vdots & & \vdots \\
0 & 0 & \cdots & t_{r}
\end{array}\right] .
$$

The characteristic polynomial of $B$ (i.e., of $T_{W}$ ) is

$$
g=\left(x-c_{1}\right)^{e_{1}} \cdots\left(x-c_{k}\right)^{e_{k}}
$$

where $e_{i}=\operatorname{dim} W_{i}$. Furthermore, $g$ divides $f$, the characteristic polynomial for $T$. Therefore, the multiplicity of $c_{i}$ as a root of $f$ is at least $\operatorname{dim} W_{i}$.

All of this should make Theorem 2 transparent. It merely says that $T$ is diagonalizable if and only if $r=n$, if and only if $e_{1}+\cdots+e_{k}=n$. It does not help us too much with the non-diagonalizable case, since we don't know the matrices $C$ and $D$ of (6-10).

Definition. Let $\mathrm{W}$ be an invariant subspace for $\mathrm{T}$ and let $\alpha$ be a vector in $\mathrm{V}$. The T-conductor of $\alpha$ into $\mathrm{W}$ is the set $\mathrm{S}_{\mathrm{T}}(\alpha ; \mathrm{W})$, which consists of all polynomials g (over the scalar field) such that $\mathrm{g}(\mathrm{T}) \alpha$ is in $\mathrm{W}$.

Since the operator $T$ will be fixed throughout most discussions, we shall usually drop the subscript $T$ and write $S(\alpha ; W)$. The authors usually call that collection of polynomials the 'stuffer' (das einstopfende Ideal). 'Conductor' is the more standard term, preferred by those who envision a less aggressive operator $\boldsymbol{g}(T)$, gently leading the vector $\alpha$ into $W$. In the special case $W=\{0\}$ the conductor is called the $T$-annihilator of $\alpha$. Lemma. If $\mathrm{W}$ is an invariant subspace for $\mathrm{T}$, then $\mathrm{W}$ is invariant under every polynomial in $\mathrm{T}$. Thus, for each $\alpha$ in $\mathrm{V}$, the conductor $\mathrm{S}(\alpha ; \mathrm{W})$ is an ideal in the polynomial algebra $\mathrm{F}[\mathrm{x}]$.

Proof. If $\beta$ is in $W$, then $T \beta$ is in $W$. Consequently, $T(T \beta)=T^{2} \beta$ is in $W$. By induction, $T^{k} \beta$ is in $W$ for each $k$. Take linear combinations to see that $f(T) \beta$ is in $W$ for every polynomial $f$.

The definition of $S(\alpha ; W)$ makes sense if $W$ is any subset of $V$. If $W$ is a subspace, then $S(\alpha ; W)$ is a subspace of $F[x]$, because

$$
(c f+g)(T)=c f(T)+g(T) .
$$

If $W$ is also invariant under $T$, let $g$ be a polynomial in $S(\alpha ; W)$, i.e., let $g(T) \boldsymbol{\alpha}$ be in $W$. If $f$ is any polynomial, then $f(T)[g(T) \alpha]$ will be in $W$. Since

$$
(f g)(T)=f(T) g(T)
$$

$f g$ is in $S(\alpha ; W)$. Thus the conductor absorbs multiplication by any polynomial.

The unique monic generator of the ideal $S(\alpha ; W)$ is also called the $T$-conductor of $\alpha$ into $W$ (the $T$-annihilator in case $W=\{0\}$ ). The $T$-conductor of $\alpha$ into $W$ is the monic polynomial $g$ of least degree such that $g(T) \alpha$ is in $W$. A polynomial $f$ is in $S(\alpha ; W)$ if and only if $g$ divides $f$. Note that the conductor $S(\alpha ; W)$ always contains the minimal polynomial for $T$; hence, every $\mathrm{T}$-conductor divides the minimal polynomial for $\mathrm{T}$.

As the first illustration of how to use the conductor $S(\alpha ; W)$, we shall characterize triangulable operators. The linear operator $T$ is called triangulable if there is an ordered basis in which $T$ is represented by a triangular matrix.

Lemma. Let $\mathrm{V}$ be a finite-dimensional vector space over the field $\mathrm{F}$. Let $\mathrm{T}$ be a linear operator on $\mathrm{V}$ such that the minimal polynomial for $\mathrm{T}$ is a product of linear factors

$$
\mathrm{p}=\left(\mathrm{x}-\mathrm{c}_{\mathrm{i}}\right)^{\mathbf{r}_{1}} \cdots\left(\mathrm{x}-\mathrm{c}_{\mathrm{k}}\right)^{\mathbf{r}_{\mathrm{k}}}, \quad \mathrm{c}_{\mathrm{i}} \text { in } \mathrm{F} \text {. }
$$

Let $\mathrm{W}$ be a proper $(\mathrm{W} \neq \mathrm{V})$ subspace of $\mathrm{V}$ which is invariant under $\mathrm{T}$. There exists a vector $\alpha$ in $\mathrm{V}$ such that

(a) $\alpha$ is not in $\mathrm{W}$;

(b) $(\mathrm{T}-\mathrm{cI}) \alpha$ is in $\mathrm{W}$, for some characteristic value c of the operator $\mathrm{T}$.

Proof. What (a) and (b) say is that the $T$-conductor of $\alpha$ into $W$ is a linear polynomial. Let $\beta$ be any vector in $V$ which is not in $W$. Let $g$ be the $T$-conductor of $\beta$ into $W$. Then $g$ divides $p$, the minimal polynomial for $T$. Since $\beta$ is not in $W$, the polynomial $g$ is not constant. Therefore,

$$
g=\left(x-c_{1}\right)^{e_{1}} \cdots\left(x-c_{k}\right)^{e_{k}}
$$

where at least one of the integers $e_{i}$ is positive. Choose $j$ so that $e_{j}>0$. Then $\left(x-c_{j}\right)$ divides $g$ :

$$
g=\left(x-c_{j}\right) h .
$$

By the definition of $g$, the vector $\alpha=h(T) \beta$ cannot be in $W$. But

$$
\begin{aligned}
\left(T-c_{j} I\right) \alpha & =\left(T-c_{j} I\right) h(T) \beta \\
& =g(T) \beta
\end{aligned}
$$

is in $W$.

Theorem 5. Let $\mathrm{V}$ be a finite-dimensional vector space over the field $\mathrm{F}$ and let $\mathrm{T}$ be a linear operator on $\mathrm{V}$. Then $\mathrm{T}$ is triangulable if and only if the minimal polynomial for $\mathrm{T}$ is a product of linear polynomials over $\mathrm{F}$.

Proof. Suppose that the minimal polynomial factors

$$
p=\left(x-c_{1}\right)^{r 1} \cdots\left(x-c_{k}\right)^{r_{k}} .
$$

By repeated application of the lemma above, we shall arrive at an ordered basis $B=\left\{\alpha_{1}, \ldots, \alpha_{n}\right\}$ in which the matrix representing $T$ is uppertriangular:

$$
[T]_{\Theta}=\left[\begin{array}{ccccc}
a_{11} & a_{12} & a_{13} & \cdots & a_{1 n} \\
0 & a_{22} & a_{23} & \cdots & a_{2 n} \\
0 & 0 & a_{33} & \cdots & a_{3 n} \\
\vdots & \vdots & \vdots & & \vdots \\
0 & 0 & 0 & \cdots & a_{n n}
\end{array}\right] .
$$

Now (6-11) merely says that

$$
T \alpha_{j}=a_{1 j} \alpha_{1}+\cdots+a_{j j} \alpha_{j}, \quad 1 \leq j \leq n
$$

that is, $T \alpha_{j}$ is in the subspace spanned by $\alpha_{1}, \ldots, \alpha_{j}$. To find $\alpha_{1}, \ldots, \alpha_{n}$, we start by applying the lemma to the subspace $W=\{0\}$, to obtain the vector $\alpha_{1}$. Then apply the lemma to $W_{1}$, the space spanned by $\alpha_{1}$, and we obtain $\alpha_{2}$. Next apply the lemma to $W_{2}$, the space spanned by $\alpha_{1}$ and $\alpha_{2}$. Continue in that way. One point deserves comment. After $\alpha_{1}, \ldots, \alpha_{i}$ have been found, it is the triangular-type relations (6-12) for $j=1, \ldots, i$ which ensure that the subspace spanned by $\alpha_{1}, \ldots, \alpha_{i}$ is invariant under $T$.

If $T$ is triangulable, it is evident that the characteristic polynomial for $T$ has the form

$$
f=\left(x-c_{1}\right)^{d_{1}} \cdots\left(x-c_{k}\right)^{d_{k}}, \quad c_{i} \text { in } F .
$$

Just look at the triangular matrix (6-11). The diagonal entries $a_{11}, \ldots, a_{1 n}$ are the characteristic values, with $c_{i}$ repeated $d_{i}$ times. But, if $f$ can be so factored, so can the minimal polynomial $p$, because it divides $f$.

Corollary. Let $\mathrm{F}$ be an algebraically closed ficld, e.g., the complex number field. Every $\mathrm{n} \times \mathrm{n}$ matrix over $\mathrm{F}$ is similar over $\mathrm{F}$ to a triangular matrix. Theorem 6. Let $\mathrm{V}$ be a finite-dimensional vector space over the field $\mathrm{F}$ and let $\mathrm{T}$ be a linear operator on $\mathrm{V}$. Then $\mathrm{T}$ is diagonalizable if and only if the minimal polynomial for $\mathrm{T}$ has the form

$$
\mathrm{p}=\left(\mathrm{x}-\mathrm{c}_{1}\right) \cdots\left(\mathrm{x}-\mathrm{c}_{\mathrm{k}}\right)
$$

where $\mathbf{c}_{1}, \ldots, \mathbf{c}_{\mathbf{k}}$ are distinct elements of $\mathrm{F}$.

Proof. We have noted earlier that, if $T$ is diagonalizable, its minimal polynomial is a product of distinct linear factors (see the discussion prior to Example 4). To prove the converse, let $W$ be the subspace spanned by all of the characteristic vectors of $T$, and suppose $W \neq V$. By the lemma used in the proof of Theorem 5, there is a vector $\alpha$ not in $W$ and a characteristic value $c_{j}$ of $T$ such that the vector

$$
\beta=\left(T-c_{j} I\right) \alpha
$$

lies in $W$. Since $\beta$ is in $W$,

$$
\beta=\beta_{1}+\cdots+\beta_{k}
$$

where $T \beta_{i}=c_{i} \beta_{i}, 1 \leq i \leq k$, and therefore the vector

$$
h(T) \beta=h\left(c_{1}\right) \beta_{1}+\cdots+h\left(c_{k}\right) \beta_{k}
$$

is in $W$, for every polynomial $h$.

Now $p=\left(x-c_{j}\right) q$, for some polynomial $q$. Also

We have

$$
q-q\left(c_{j}\right)=\left(x-c_{j}\right) h .
$$

$$
q(T) \alpha-q\left(c_{j}\right) \alpha=h(T)\left(T-c_{j} I\right) \alpha=h(T) \beta .
$$

But $h(T) \beta$ is in $W$ and, since

$$
0=p(T) \alpha=\left(T-c_{j} I\right) q(T) \alpha
$$

the vector $q(T) \alpha$ is in $W$. Therefore, $q\left(c_{j}\right) \alpha$ is in $W$. Since $\alpha$ is not in $W$, we have $q\left(c_{j}\right)=0$. That contradicts the fact that $p$ has distinct roots.

At the end of Section 6.7, we shall give a different proof of Theorem 6. In addition to being an elegant result, Theorem 6 is useful in a computational way. Suppose we have a linear operator $T$, represented by the matrix $A$ in some ordered basis, and we wish to know if $T$ is diagonalizable. We compute the characteristic polynomial $f$. If we can factor $f$ :

$$
f=\left(x-c_{1}\right)^{d_{1}} \cdots\left(x-c_{k}\right)^{d_{k}}
$$

we have two different methods for determining whether or not $T$ is diagonalizable. One method is to see whether (for each $i$ ) we can find $d_{i}$ independent characteristic vectors associated with the characteristic value $c_{i}$. The other method is to check whether or not $\left(T-c_{1} I\right) \cdots\left(T-c_{k} I\right)$ is the zero operator.

Theorem 5 provides a different proof of the Cayley-Hamilton theorem. That theorem is easy for a triangular matrix. Hence, via Theorem 5, we obtain the result for any matrix over an algebraically closed field. Any field is a subfield of an algebraically closed field. If one knows that result, one obtains a proof of the Cayley-Hamilton theorem for matrices over any field. If we at least admit into our discussion the Fundamental Theorem of Algebra (the complex number field is algebraically closed), then Theorem 5 provides a proof of the Cayley-Hamilton theorem for complex matrices, and that proof is independent of the one which we gave earlier.

\section{Exercises}

1. Let $T$ be the linear operator on $R^{2}$, the matrix of which in the standard ordered basis is

$$
A=\left[\begin{array}{rr}
1 & -1 \\
2 & 2
\end{array}\right] \text {. }
$$

(a) Prove that the only subspaces of $R^{2}$ invariant under $T$ are $R^{2}$ and the zero subspace.

(b) If $U$ is the linear operator on $C^{2}$, the matrix of which in the standard ordered basis is $A$, show that $U$ has 1-dimensional invariant subspaces.

2. Let $W$ be an invariant subspace for $T$. Prove that the minimal polynomial for the restriction operator $T_{W}$ divides the minimal polynomial for $T$, without referring to matrices.

3. Let $c$ be a characteristic value of $T$ and let $W$ be the space of characteristic vectors associated with the characteristic value $c$. What is the restriction operator $T_{\boldsymbol{w}}$ ?

4. Let

$$
A=\left[\begin{array}{rrr}
0 & 1 & 0 \\
2 & -2 & 2 \\
2 & -3 & 2
\end{array}\right]
$$

Is $A$ similar over the field of real numbers to a triangular matrix? If so, find such a triangular matrix.

5. Every matrix $A$ such that $A^{2}=A$ is similar to a diagonal matrix.

6. Let $T$ be a diagonalizable linear operator on the $n$-dimensional vector space $V$, and let $W$ be a subspace which is invariant under $T$. Prove that the restriction operator $T_{W}$ is diagonalizable.

7. Let $T$ be a linear operator on a finite-dimensional vector space over the field of complex numbers. Prove that $T$ is diagonalizable if and only if $T$ is annihilated by some polynomial over $C$ which has distinct roots.

8. Let $T$ be a linear operator on $V$. If every subspace of $V$ is invariant under $T$, then $T$ is a scalar multiple of the identity operator.

9. Let $T$ be the indefinite integral operator

$$
(T f)(x)=\int_{0}^{x} f(t) d t
$$

on the space of continuous functions on the interval $[0,1]$. Is the space of polynomial functions invariant under $T$ ? The space of differentiable functions? The space of functions which vanish at $x=\frac{1}{2}$ ?

10. Let $A$ be a $3 \times 3$ matrix with real entries. Prove that, if $A$ is not similar over $R$ to a triangular matrix, then $A$ is similar over $C$ to a diagonal matrix.

11. True or false? If the triangular matrix $A$ is similar to a diagonal matrix, then $A$ is already diagonal.

12. Let $T$ be a linear operator on a finite-dimensional vector space over an algebraically closed field $F$. Let $f$ be a polynomial over $F$. Prove that $c$ is a characteristic value of $f(T)$ if and only if $c=f(t)$, where $t$ is a characteristic value of $T$.

13. Let $V$ be the space of $n \times n$ matrices over $F$. Let $A$ be a fixed $n \times n$ matrix over $F$. Let $T$ and $U$ be the linear operators on $V$ defined by

$$
\begin{aligned}
& T(B)=A B \\
& U(B)=A B-B A
\end{aligned}
$$

(a) True or false? If $A$ is diagonalizable (over $F$ ), then $T$ is diagonalizable.

(b) True or false? If $A$ is diagonalizable, then $U$ is diagonalizable.

\subsection{Simultaneous Triangulation; \\ Simultaneous Diagonalization}

Let $V$ be a finite-dimensional space and let $F$ be a family of linear operators on $V$. We ask when we can simultaneously triangulate or diagonalize the operators in $\mathcal{F}$, i.e., find one basis $B$ such that all of the matrices $[T](\mathbf{k}, T$ in $F$, are triangular (or diagonal). In the case of diagonalization, it is necessary that $F$ be a commuting family of operators: $U T=T U$ for all $T, U$ in $F$. That follows from the fact that all diagonal matrices commute. Of course, it is also necessary that each operator in $\mathfrak{F}$ be a diagonalizable operator. In order to simultaneously triangulate, each operator in $\mathfrak{F}$ must be triangulable. It is not necessary that $\mathfrak{F}$ be a commuting family; however, that condition is sufficient for simultaneous triangulation (if each $T$ can be individually triangulated). These results follow from minor variations of the proofs of Theorems 5 and 6.

The subspace $W$ is invariant under (the family of operators) $F$ if $W$ is invariant under each operator in $\mathscr{F}$.

Lemma. Let F be a commuting family of triangulable linear operators on V. Let $\mathrm{W}$ be a proper subspace of $\mathrm{V}$ which is invariant under $\mathfrak{F}$. There exists a vector $\alpha$ in $\mathrm{V}$ such that

(a) $\alpha$ is not in $\mathrm{W}$;

(b) for each $\mathrm{T}$ in $\mathfrak{F}$, the vector $\mathrm{T} \alpha$ is in the subspace spanned by $\alpha$ and $\mathrm{W}$.

Proof. It is no loss of generality to assume that $F$ contains only a finite number of operators, because of this observation. Let $\left\{T_{1}, \ldots, T_{r}\right\}$ be a maximal linearly independent subset of $\mathcal{F}$, i.e., a basis for the subspace spanned by $\mathcal{F}$. If $\alpha$ is a vector such that (b) holds for each $T_{i}$, then (b) will hold for every operator which is a linear combination of $T_{1}, \ldots, T_{r}$.

By the lemma before Theorem 5 (this lemma for a single operator), we can find a vector $\beta_{1}$ (not in $W^{\prime}$ ) and a scalar $c_{1}$ such that $\left(T_{1}-c_{1} I\right) \beta_{1}$ is in $W$. Let $V_{1}$ be the collection of all vectors $\beta$ in $V$ such that $\left(T_{1}-c_{1} I\right) \beta$ is in $W$. Then $V_{1}$ is a subspace of $V$ which is properly larger than $W$. Furthermore, $V_{1}$ is invariant under $\mathcal{F}$, for this reason. If $T$ commutes with $T_{1}$, then

$$
\left(T_{1}-c_{1} I\right)(T \beta)=T\left(T_{1}-c_{1} I\right) \beta .
$$

If $\beta$ is in $V_{1}$, then $\left(T_{1}-c_{1} I\right) \beta$ is in $W$. Since $W$ is invariant under each $T$ in $\mathcal{F}$, we have $T\left(T_{1}-c_{1} I\right) \beta$ in $W$, i.e., $T \beta$ in $V_{1}$, for all $\beta$ in $V_{1}$ and all $T$ in $\mathcal{F}$.

Now $W$ is a proper subspace of $V_{1}$. Let $U_{2}$ be the linear operator on $V_{1}$ obtained by restricting $T_{2}$ to the subspace $V_{1}$. The minimal polynomial for $U_{2}$ divides the minimal polynomial for $T_{2}$. Therefore, we may apply the lemma before Theorem 5 to that operator and the invariant subspace $W$. We obtain a vector $\beta_{2}$ in $V_{1}$ (not in $W$ ) and a scalar $c_{2}$ such that $\left(T_{2}-c_{2} I\right) \beta_{2}$ is in $W$. Note that

(a) $\beta_{2}$ is not in $W$;

(b) $\left(T_{1}-c_{1} I\right) \beta_{2}$ is in $W$;

(c) $\left(T_{2}-c_{2} I\right) \beta_{2}$ is in $W$.

Let $V_{2}$ be the set of all vectors $\beta$ in $V_{1}$ such that $\left(T_{2}-c_{2} I\right) \beta$ is in $W$. Then $V_{2}$ is invariant under $F$. Apply the lemma before Theorem 5 to $U_{3}$, the restriction of $T_{3}$ to $V_{2}$. If we continue in this way, we shall reach a vector $\alpha=\beta_{r}($ not in $W)$ such that $\left(T_{j}-c_{j} I\right) \alpha$ is in $W, j=1, \ldots, r$.

Theorem 7. Let $\mathrm{V}$ be a finite-dimensional vector space over the field $\mathrm{F}$. Let $\mathfrak{F}$ be a commuting family of triangulable linear operators on $\mathrm{V}$. There exists an ordered basis for $\mathrm{V}$ such that every operator in $\mathfrak{F}$ is represented by a triangular matrix in that basis.

Proof. Given the lemma which we just proved, this theorem has the same proof as does Theorem 5, if one replaces $T$ by $\mathscr{F}$.

Corollary. Let $\mathfrak{F}$ be a commuting family of $\mathrm{n} \times \mathrm{n}$ matrices over an algebraically closed field $\mathrm{F}$. There exists a non-singular $\mathrm{n} \times \mathrm{n}$ matrix $\mathrm{P}$ with entries in $\mathrm{F}$ such that $\mathrm{P}^{-1} \mathrm{Al}$ ' is upper-triangular, for every matrix $\mathrm{A}$ in $\mathscr{F}$.

Theorem 8. Let $\mathcal{F}$ be a commuting family of diagonalizable linear operators on the finite-dimensional vector space $\mathrm{V}$. There exists an ordered basis for $\mathrm{V}$ such that every operator in $\mathfrak{F}$ is represented in that basis by a diagonal matrix.

Proof. We could prove this theorem by adapting the lemma before Theorem 7 to the diagonalizable case, just as we adapted the lemma before Theorem 5 to the diagonalizable case in order to prove Theorem 6 . However, at this point it is easier to proceed by induction on the dimension of $V$.

If $\operatorname{dim} V=1$, there is nothing to prove. Assume the theorem for vector spaces of dimension less than $n$, and let $V$ be an $n$-dimensional space. Choose any $T$ in $\mathscr{F}$ which is not a scalar multiple of the identity. Let $c_{1}, \ldots, c_{k}$ be the distinct characteristic values of $T$, and (for each $i$ ) let $W_{i}$ be the null space of $T-c_{i} I$. Fix an index $i$. Then $W_{i}$ is invariant under every operator which commutes with $T$. Let $\mathscr{F}_{i}$ be the family of linear operators on $W_{i}$ obtained by restricting the operators in $\mathcal{F}$ to the (invariant) subspace $W_{i}$. Each operator in $\mathscr{F}_{i}$ is diagonalizable, because its minimal polynomial divides the minimal polynomial for the corresponding operator in $\mathscr{F}$. Since $\operatorname{dim} W_{i}<\operatorname{dim} V$, the operators in $\mathscr{F}_{i}$ can be simultaneously diagonalized. In other words, $W_{i}$ has a basis $\boldsymbol{\beta}_{i}$ which consists of vectors which are simultaneously characteristic vectors for every operator in $\mathscr{F}_{i}$.

Since $T$ is diagonalizable, the lemma before Theorem 2 tells us that $\boldsymbol{B}=\left(\mathbb{B}_{1}, \ldots, \mathbb{B}_{k}\right)$ is a basis for $V$. That is the basis we seek.

\section{Exercises}

1. Find an invertible real matrix $P$ such that $P^{-1} A P$ and $P^{-1} B P$ are both diagonal, where $A$ and $B$ are the real matrices

$$
\begin{aligned}
A & =\left[\begin{array}{ll}
1 & 2 \\
0 & 2
\end{array}\right], & B & =\left[\begin{array}{ll}
3 & -8 \\
0 & -1
\end{array}\right] \\
A & =\left[\begin{array}{ll}
1 & 1 \\
1 & 1
\end{array}\right], & B & =\left[\begin{array}{ll}
1 & a \\
a & 1
\end{array}\right]
\end{aligned}
$$

2. Let $\mathscr{F}$ be a commuting family of $3 \times 3$ complex matrices. How many linearly independent matrices can $\mathcal{F}$ contain? What about the $n \times n$ case?

3. Let $T$ be a linear operator on an $n$-dimensional space, and suppose that $T$ has $n$ distinct characteristic values. Prove that any linear operator which commutes with $T$ is a polynomial in $T$.

4. Let $A, B, C$, and $D$ be $n \times n$ complex matrices which commute. Let $E$ be the $2 n \times 2 n$ matrix

$$
E=\left[\begin{array}{ll}
A & B \\
C & \boldsymbol{D}
\end{array}\right] .
$$

Prove that $\operatorname{det} E=\operatorname{det}(A D-B C)$.

5. Let $F$ be a field, $n$ a positive integer, and let $V$ be the space of $n \times n$ matrices over $F$. If $A$ is a fixed $n \times n$ matrix over $F$, let $T_{A}$ be the linear operator on $V$ defined by $T_{A}(B)=A B-B A$. Consider the family of linear operators $T_{A}$ obtained by letting $A$ vary over all diagonal matrices. Prove that the operators in that family are simultaneously diagonc iizable. 

\subsection{Direct-Sum Decompositions}

As we continue with our analysis of a single linear operator, we shall formulate our ideas in a slightly more sophisticated way-less in terms of matrices and more in terms of subspaces. When we began this chapter, we described our goal this way: To find an ordered basis in which the matrix of $T$ assumes an especially simple form. Now, we shall describe our goal as follows: To decompose the underlying space $V$ into a sum of invariant subspaces for $T$ such that the restriction operators on those subspaces are simple.

Definition. Let $\mathrm{W}_{1}, \ldots, \mathrm{W}_{\mathrm{k}}$ be subspaces of the vector space $\mathrm{V}$. We say that $\mathrm{W}_{1}, \ldots, \mathrm{W}_{\mathrm{k}}$ are independent if

$$
\alpha_{1}+\cdots+\alpha_{\mathrm{k}}=0, \quad \alpha_{\mathrm{i}} \dot{n} \mathrm{~W}_{\mathrm{i}}
$$

implies that each $\alpha_{\mathrm{i}}$ is 0 .

For $k=2$, the meaning of independence is $\{0\}$ intersection, i.e., $W_{1}$ and $W_{2}$ are independent if and only if $W_{1} \cap W_{2}=\{0\}$. If $k>2$, the independence of $W_{1}, \ldots, W_{k}$ says much more than $W_{1} \cap \cdots \cap W_{k}=$ $\{0\}$. It says that each $W_{j}$ intersects the sum of the other subspaces $W_{i}$ only in the zero vector.

The significance of independence is this. Let $W=W_{1}+\cdots+W_{k}$ be the subspace spanned by $W_{1}, \ldots, W_{k}$. Each vector $\alpha$ in $W$ can be expressed as a sum

$$
\alpha=\alpha_{1}+\cdots+\alpha_{k}, \quad \alpha_{i} \text { in } W_{i} .
$$

If $W_{1}, \ldots, W_{k}$ are independent, then that expression for $\alpha$ is unique; for if

$$
\alpha=\beta_{1}+\cdots+\beta_{k}, \quad \beta_{i} \text { in } W_{i}
$$

then $0=\left(\alpha_{1}-\beta_{1}\right)+\cdots+\left(\alpha_{k}-\beta_{k}\right)$, hence $\alpha_{i}-\beta_{i}=0, i=1, \ldots, k$. Thus, when $W_{1}, \ldots, W_{k}$ are independent, we can operate with the vectors in $W$ as $k$-tuples $\left(\alpha_{1}, \ldots, \alpha_{k}\right), \alpha_{i}$ in $W_{i}$, in the same way as we operate with vectors in $R^{k}$ as $k$-tuples of numbers.

Lemma. Let $\mathrm{V}$ be a finite-dimensional vector space. Let $\mathrm{W}_{1}, \ldots, \mathrm{W}_{\mathrm{k}}$ be subspaces of $\mathrm{V}$ and let $\mathrm{W}=\mathrm{W}_{1}+\cdots+\mathrm{W}_{\mathrm{k}}$. The following are equivalent.

(a) $\mathrm{W}_{\mathrm{l}}, \ldots, \mathrm{W}_{\mathrm{k}}$ are independent.

(b) For each $\mathrm{j}, 2 \leq \mathrm{j} \leq \mathrm{k}$, we have

$$
\mathrm{W}_{\mathrm{j}} \cap\left(\mathrm{W}_{1}+\cdots+\mathrm{W}_{\mathrm{j}-1}\right)=\{0\} .
$$

(c) If $\mathrm{B}_{\mathrm{i}}$ is an ordered basis for $\mathrm{W}_{\mathrm{i}}, 1 \leq \mathrm{i} \leq \mathrm{k}$, then the sequence $B=$ $\left(B_{1}, \ldots, B_{\mathbf{k}}\right)$ is an ordered basis for $\mathrm{W}$. Proof. Assume (a). Let $\alpha$ be a vector in the intersection $W_{j} \cap$ $\left(W_{1}+\cdots+W_{j-1}\right)$. Then there are vectors $\alpha_{1}, \ldots, \alpha_{j-1}$ with $\alpha_{i}$ in $W_{i}$ such that $\alpha=\alpha_{1}+\cdots+\alpha_{j-1}$. Since

$$
\alpha_{1}+\cdots+\alpha_{j-1}+(-\alpha)+0+\cdots+0=0
$$

and since $W_{1}, \ldots, W_{k}$ are independent, it must be that $\alpha_{1}=\alpha_{2}=\cdots=$ $\alpha_{j-1}=\alpha=0$.

Now, let us observe that (b) implies (a). Suppose

$$
0=\alpha_{1}+\cdots+\alpha_{\mathrm{k}}, \quad \alpha_{i} \text { in } W_{i} .
$$

Let $j$ be the largest integer $i$ such that $\alpha_{i} \neq 0$. Then

$$
0=\alpha_{1}+\cdots+\alpha_{j}, \quad \alpha_{j} \neq 0 .
$$

Thus $\alpha_{j}=-\alpha_{1}-\cdots-\alpha_{j-1}$ is a non-zero vector in $W_{j} \cap\left(W_{1}+\cdots+\right.$ $\left.W_{j-1}\right)$.

Now that we know (a) and (b) are the same, let us see why (a) is equivalent to (c). Assume (a). Let $\mathbb{B}_{i}$ be a basis for $W_{i}, 1 \leq i \leq k$, and let $B=\left(Q_{1}, \ldots, B_{k}\right)$. Any linear relation between the vectors in $B$ will have the form

$$
\beta_{1}+\cdots+\beta_{k}=0
$$

where $\beta_{i}$ is some linear combination of the vectors in $B_{i}$. Since $W_{1}, \ldots, W_{k}$ are independent, each $\beta_{i}$ is 0 . Since each $\Theta_{i}$ is independent, the relation we have between the vectors in $B$ is the trivial relation.

We relegate the proof that (c) implies (a) to the exercises (Exercise 2).

If any (and hence all) of the conditions of the last lemma hold, we say that the sum $W=W_{1}+\cdots+W_{k}$ is direct or that $W$ is the direct sum of $W_{1}, \ldots, W_{k}$ and we write

$$
W=W_{1} \oplus \cdots \oplus W_{k} .
$$

In the literature, the reader may find this direct sum referred to as an independent sum or the interior direct sum of $W_{1}, \ldots, W_{k}$.

Example 11. Let $V$ be a finite-dimensional vector space over the field $F$ and let $\left\{\alpha_{1}, \ldots, \alpha_{n}\right\}$ be any basis for $V$. If $W_{i}$ is the one-dimensional subspace spanned by $\alpha_{i}$, then $V=W_{1} \oplus \cdots \oplus W_{n}$.

Example 12 . Let $n$ be a positive integer and $F$ a subfield of the complex numbers, and let $V$ be the space of all $n \times n$ matrices over $F$. Let $W_{1}$ be the subspace of all symmetric matrices, i.e., matrices $A$ such that $A^{t}=A$. Let $W_{2}$ be the subspace of all skew-symmetric matrices, i.e., matrices $A$ such that $A^{t}=-A$. Then $V=W_{1} \oplus W_{2}$. If $A$ is any matrix in $V$, the unique expression for $A$ as a sum of matrices, one in $W_{1}$ and the other in $W_{2}$, is 

$$
\begin{aligned}
A & =A_{1}+A_{2} \\
A_{1} & =\frac{1}{2}\left(A+A^{t}\right) \\
A_{2} & =\frac{1}{2}\left(A-A^{t}\right) .
\end{aligned}
$$

Example 13. Let $T$ be any linear operator on a finite-dimensional space $V$. Let $c_{1}, \ldots, c_{k}$ be the distinct characteristic values of $T$, and let $W_{i}$ be the space of characteristic vectors associated with the characteristic value $c_{i}$. Then $W_{1}, \ldots, W_{k}$ are independent. See the lemma before Theorem 2. In particular, if $T$ is diagonalizable, then $V=W_{1} \oplus \cdots \oplus W_{k}$.

Definition. If $\mathrm{V}$ is a vector space, a projection of $\mathrm{V}$ is a linear operator $\mathrm{E}$ on $\mathrm{V}$ such that $\mathrm{E}^{2}=\mathrm{E}$.

Suppose that $E$ is a projection. Let $R$ be the range of $E$ and let $N$ be the null space of $E$.

1. The vector $\beta$ is in the range $R$ if and only if $E \beta=\beta$. If $\beta=E \alpha$, then $E \beta=E^{2} \alpha=E \alpha=\beta$. Conversely, if $\beta=E \beta$, then (of course) $\beta$ is in the range of $E$.

2. $V=R \oplus N$.

3. The unique expression for $\alpha$ as a sum of vectors in $R$ and $N$ is $\alpha=E \alpha+(\alpha-E \alpha)$.

From (1), (2), (3) it is easy to see the following. If $R$ and $N$ are subspaces of $V$ such that $V=R \oplus N$, there is one and only one projection operator $E$ which has range $R$ and null space $N$. That operator is called the projection on $R$ along $N$.

Any projection $E$ is (trivially) diagonalizable. If $\left\{\alpha_{1}, \ldots, \alpha_{r}\right\}$ is a basis for $R$ and $\left\{\alpha_{r+1}, \ldots, \alpha_{n}\right\}$ a basis for $N$, then the basis $B=\left\{\alpha_{1}, \ldots\right.$, $\left.\alpha_{n}\right\}$ diagonalizes $E$ :

$$
[E]_{\mathscr{Q}}=\left[\begin{array}{ll}
I & 0 \\
0 & 0
\end{array}\right]
$$

where $I$ is the $r \times r$ identity matrix. That should help explain some of the terminology connected with projections. The reader should look at various cases in the plane $R^{2}$ (or 3 -space, $R^{3}$ ), to convince himself that the projection on $R$ along $N$ sends each vector into $R$ by projecting it parallel to $N$.

Projections can be used to describe direct-sum decompositions of the space $V$. For, suppose $V=W_{1} \oplus \cdots \oplus W_{k}$. For each $j$ we shall define an operator $E_{j}$ on $V$. Let $\alpha$ be in $V$, say $\alpha=\alpha_{1}+\cdots+\alpha_{k}$ with $\alpha_{i}$ in $W_{i}$. Define $E_{j} \alpha=\alpha_{j}$. Then $E_{j}$ is a well-defined rule. It is easy to see that $E_{j}$ is linear, that the range of $E_{j}$ is $W_{j}$, and that $E_{j}^{2}=E_{j}$. The null space of $E_{j}$ is the subspace

$$
\left(W_{1}+\cdots+W_{j-1}+W_{j+1}+\cdots+W_{k}\right)
$$

for, the statement that $\boldsymbol{E}_{j} \alpha=0$ simply means $\alpha_{j}=0$, i.e., that $\alpha$ is actually a sum of vectors from the spaces $W_{i}$ with $i \neq j$. In terms of the projections $E_{j}$ we have

$$
\alpha=E_{1} \alpha+\cdots+E_{k} \alpha
$$

for each $\alpha$ in $V$. What (6-13) says is that

$$
I=E_{1}+\cdots+E_{k} .
$$

Note also that if $i \neq j$, then $E_{i} E_{j}=0$, because the range of $E_{j}$ is the subspace $W_{j}$ which is contained in the null space of $E_{i}$. We shall now summarize our findings and state and prove a converse.

Theorem 9. If $\mathrm{V}=\mathrm{W}_{1} \oplus \cdots \oplus \mathrm{W}_{\mathrm{k}}$, then there exist $\mathrm{k}$ linear operators $\mathrm{E}_{1}, \ldots, \mathrm{E}_{\mathrm{k}}$ on $\mathrm{V}$ such that

(i) each $\mathrm{E}_{\mathrm{i}}$ is a projection $\left(\mathrm{E}_{1}^{2}=\mathrm{E}_{\mathrm{i}}\right)$;

(ii) $\mathrm{E}_{\mathrm{i}} \mathrm{E}_{\mathrm{j}}=0$, if $\mathrm{i} \neq \mathrm{j}$;

(iii) $\mathrm{I}=\mathrm{E}_{1}+\cdots+\mathrm{E}_{\mathrm{k}}$

(iv) the range of $\mathrm{E}_{\mathrm{i}}$ is $\mathrm{W}_{\mathrm{i}}$.

Conversely, if $\mathrm{E}_{1}, \ldots, \mathrm{E}_{\mathrm{k}}$ are $\mathrm{k}$ linear operators on $\mathrm{V}$ which satisfy conditions (i), (ii), and (iii), and if we let $\mathrm{W}_{\mathrm{i}}$ be the range of $\mathrm{E}_{\mathrm{i}}$, then $\mathrm{V}=\mathrm{W}_{\mathrm{i}} \oplus \cdots \oplus$ $\mathrm{W}_{\mathrm{k}}$.

Proof. We have only to prove the converse statement. Suppose $E_{1}, \ldots, E_{k}$ are linear operators on $V$ which satisfy the first three conditions, and let $W_{i}$ be the range of $E_{i}$. Then certainly

$$
V=W_{1}+\cdots+W_{k}
$$

for, by condition (iii) we have

$$
\alpha=E_{1} \alpha+\cdots+E_{k} \alpha
$$

for each $\alpha$ in $V$, and $E_{i} \alpha$ is in $W_{i}$. This expression for $\alpha$ is unique, because if

$$
\alpha=\alpha_{1}+\cdots+\alpha_{k}
$$

with $\alpha_{i}$ in $W_{i}$, say $\alpha_{i}=E_{i} \beta_{i}$, then using (i) and (ii) we have

$$
\begin{aligned}
E_{j} \alpha & =\sum_{i=1}^{k} E_{j} \alpha_{i} \\
& =\sum_{i=1}^{k} E_{j} E_{i} \beta_{i} \\
& =E_{j}^{2} \beta_{j} \\
& =E_{j} \beta_{j} \\
& =\alpha_{j} .
\end{aligned}
$$

This shows that $V$ is the direct sum of the $W_{i}$. 

\section{Exercises}

1. Let $V$ be a finite-dimensional vector space and let $W_{1}$ be any subspace of $V$. Prove that there is a subspace $W_{2}$ of $V$ such that $V=W_{1} \oplus W_{2}$.

2. Let $V$ be a finite-dimensional vector space and let $W_{1}, \ldots, W_{k}$ be subspaces of $V$ such that

$$
V=W_{1}+\cdots+W_{k} \text { and } \operatorname{dim} V=\operatorname{dim} W_{1}+\cdots+\operatorname{dim} W_{k} \text {. }
$$

Prove that $V=W_{1} \oplus \cdots \oplus W_{k}$.

3. Find a projection $E$ which projects $R^{2}$ onto the subspace spanned by $(1,-1)$ along the subspace spanned by $(1,2)$.

4. If $E_{1}$ and $E_{2}$ are projections onto independent subspaces, then $E_{1}+E_{2}$ is a projection. True or false?

5. If $E$ is a projection and $f$ is a polynomial, then $f(E)=a I+b E$. What are $a$ and $b$ in terms of the coefficients of $f$ ?

6. True or false? If a diagonalizable operator has only the characteristic values 0 and 1 , it is a projection.

7. Prove that if $E$ is the projection on $R$ along $N$, then $(I-E)$ is the projection on $N$ along $R$.

8. Let $E_{1}, \ldots, E_{k}$ be linear operators on the space $V$ such that $E_{1}+\cdots+E_{k}=I$.

(a) Prove that if $E_{i} E_{j}=0$ for $i \neq j$, then $E_{i}^{2}=E_{i}$ for each $i$.

(b) In the case $k=2$, prove the converse of (a). That is, if $E_{1}+E_{2}=I$ and $E_{1}^{2}=E_{1}, E_{2}^{2}=E_{2}$, then $E_{1} E_{2}=0$.

9. Let $V$ be a real vector space and $E$ an idempotent linear operator on $V$, i.e., a projection. Prove that $(I+E)$ is invertible. Find $(I+E)^{-1}$.

10. Let $F$ be a subfield of the complex numbers (or, a field of characteristic zero). Let $V$ be a finite-dimensional vector space over $F$. Suppose that $E_{1}, \ldots, E_{k}$ are projections of $V$ and that $E_{1}+\cdots+E_{k}=I$. Prove that $E_{i} E_{j}=0$ for $i \neq j$ (Hint: Use the trace function and ask yourself what the trace of a projection is.)

11. Let $V$ be a vector space, let $W_{1}, \ldots, W_{k}$ be subspaces of $V$, and let

$$
V_{j}=W_{1}+\cdots+W_{j-1}+W_{i+1}+\cdots+W_{k} \text {. }
$$

Suppose that $V=W_{1} \oplus \cdots \oplus W_{k}$. Prove that the dual space $V^{*}$ has the directsum decomposition $V^{*}=V_{1}^{0} \oplus \cdots \oplus V_{k}^{0}$.

\subsection{Invariant Direct Sums}

We are primarily interested in direct-sum decompositions $V=$ $W_{1} \oplus \ldots \bigoplus W_{k}$, where each of the subspaces $W_{i}$ is invariant under some given linear operator $T$. Given such a decomposition of $V, T$ induces a linear operator $T_{i}$ on each $W_{i}$ by restriction. The action of $T$ is then this. If $\alpha$ is a vector in $V$, we have unique vectors $\alpha_{1}, \ldots, \alpha_{k}$ with $\alpha_{i}$ in $W_{i}$ such that

and then

$$
\alpha=\alpha_{1}+\cdots+\alpha_{k}
$$

$$
T \alpha=T_{1} \alpha_{1}+\cdots+T_{k} \alpha_{k} .
$$

We shall describe this situation by saying that $T$ is the direct sum of the operators $T_{1}, \ldots, T_{k}$. It must be remembered in using this terminology that the $T_{i}$ are not linear operators on the space $V$ but on the various subspaces $W_{i}$. The fact that $V=W_{1} \oplus \cdots \oplus W_{k}$ enables us to associate with each $\alpha$ in $V$ a unique $k$-tuple $\left(\alpha_{1}, \ldots, \alpha_{k}\right.$ ) of vectors $\alpha_{i}$ in $W_{i}$ (by $\alpha=$ $\left.\alpha_{1}+\cdots+\alpha_{k}\right)$ in such a way that we can carry out the linear operations in $V$ by working in the individual subspaces $W_{i}$. The fact that each $W_{i}$ is invariant under $T$ enables us to view the action of $T$ as the independent action of the operators $T_{i}$ on the subspaces $W_{i}$. Our purpose is to study $T$ by finding invariant direct-sum decompositions in which the $T_{i}$ are operators of an elementary nature.

Before looking at an example, let us note the matrix analogue of this situation. Suppose we select an ordered basis $\boldsymbol{\Theta}_{i}$ for each $W_{i}$, and let $\beta$ be the ordered basis for $V$ consisting of the union of the $B_{i}$ arranged in the order $\Theta_{1}, \ldots, \Theta_{k}$, so that $\Omega$ is a basis for $V$. From our discussion concerning the matrix analogue for a single invariant subspace, it is easy to see that if $A=[T]_{\mathscr{G}}$ and $A_{i}=\left[T_{i}\right]_{\Omega_{i}}$, then $A$ has the block form

$$
A=\left[\begin{array}{llll}
A_{1} & 0 & \cdots & 0 \\
0 & A_{2} & \cdots & 0 \\
\vdots & \vdots & & \vdots \\
0 & 0 & \cdots & A_{k}
\end{array}\right] .
$$

In (6-14), $A_{i}$ is a $d_{i} \times d_{i}$ matrix $\left(d_{i}=\operatorname{dim} W_{i}\right)$, and the 0 's are symbols for rectangular blocks of scalar 0's of various sizes. It also seems appropriate to describe (6-14) by saying that $A$ is the direct sum of the matrices $A_{1}, \ldots, A_{k}$.

Most often, we shall describe the subspace $W_{i}$ by means of the associated projections $E_{i}$ (Theorem 9). Therefore, we need to be able to phrase the invariance of the subspaces $W_{i}$ in terms of the $E_{i}$.

Theorem 10. Let $\mathrm{T}$ be a linear operator on the space $\mathrm{V}$, and let $\mathrm{W}_{1}, \ldots, \mathrm{W}_{\mathrm{k}}$ and $\mathrm{E}_{1}, \ldots, \mathrm{E}_{\mathrm{k}}$ be as in Theorem 9. Then a necessary and sufficient condition that each subspace $\mathrm{W}_{\mathrm{i}}$ be invariant under $\mathrm{T}$ is that $\mathrm{T}$ commute with each of ihe projections $\mathrm{E}_{\mathrm{i}}$, i.e.,

$$
\mathrm{TE}_{\mathrm{i}}=\mathrm{E}_{\mathrm{i}} \mathrm{T}, \quad \mathrm{i}=1, \ldots, \mathrm{k} .
$$

Proof. Suppose $T$ commutes with each $E_{i}$. Let $\alpha$ be in $W_{j}$. Then $E_{j} \alpha=\alpha$, and

$$
\begin{aligned}
T \alpha & =T\left(E_{j} \alpha\right) \\
& =E_{j}(T \alpha)
\end{aligned}
$$

which shows that $T \alpha$ is in the range of $E_{j}$, i.e., that $W_{j}$ is invariant under $T$.

Assume now that each $W_{i}$ is invariant under $T$. We shall show that $T E_{j}=E_{j} T$. Let $\alpha$ be any vector in $V$. Then

$$
\begin{aligned}
\alpha & =E_{1} \alpha+\cdots+E_{k} \alpha \\
T \alpha & =T E_{1} \alpha+\cdots+T E_{k} \alpha .
\end{aligned}
$$

Since $E_{i} \alpha$ is in $W_{i}$, which is invariant under $T$, we must have $T\left(E_{i} \alpha\right)=$ $E_{i} \boldsymbol{\beta}_{i}$ for some vector $\boldsymbol{\beta}_{i}$. Then

$$
\begin{aligned}
E_{j} T E_{i} \alpha & =E_{j} E_{i} \beta_{i} \\
& = \begin{cases}0, & \text { if } \quad i \neq j \\
E_{j} \beta_{j}, & \text { if } \quad i=j .\end{cases}
\end{aligned}
$$

Thus

$$
\begin{aligned}
E_{j}^{\prime} T \alpha & =E_{j} T E_{1} \boldsymbol{\alpha}+\cdots+E_{j} T E_{k} \alpha \\
& =E_{j} \beta_{j} \\
& =T E_{j} \alpha .
\end{aligned}
$$

This holds for each $\alpha$ in $V$, so $E_{j}^{\prime} T=T E_{j}^{\prime}$.

We shall now describe a diagonalizable operator $T$ in the language of invariant direct sum decompositions (projections which commute with $T$ ). This will be a great help to us in understanding some deeper decomposition theorems later. The reader may feel that the description which we are about to give is rather complicated, in comparison to the matrix formulation or to the simple statement that the characteristic vectors of $T$ span the underlying space. But, he should bear in mind that this is our first glimpse at a very effective method, by means of which various problems concerned with subspaces, bases, matrices, and the like can be reduced to algebraic calculations with linear operators. With a little experience, the efficiency and elegance of this method of reasoning should become apparent.

Theorem 11. Let $\mathrm{T}$ be a linear operator on a finite-dimensional space $\mathrm{V}$.

If $\mathrm{T}$ is diagonalizable and if $\mathrm{c}_{1}, \ldots, \mathrm{c}_{\mathrm{k}}$ are the distinct characteristic values of $\mathrm{T}$, then there exist linear operators $\mathrm{E}_{1}, \ldots, \mathrm{E}_{\mathrm{k}}$ on $\mathrm{V}$ such that

(i) $\mathrm{T}=\mathrm{c}_{1} \mathrm{E}_{1}+\cdots+\mathrm{c}_{\mathrm{k}} \mathrm{E}_{\mathrm{k}}$;

(ii) $\mathrm{I}=\mathrm{E}_{1}+\cdots+\mathrm{E}_{\mathrm{k}}$;

(iii) $\mathrm{E}_{\mathrm{i}} \mathrm{E}_{\mathrm{j}}=0, \mathrm{i} \neq \mathrm{j}$;

(iv) $\mathrm{E}_{\mathrm{i}}^{2}=\mathrm{E}_{\mathrm{i}}\left(\mathrm{E}_{\mathrm{i}}\right.$ is a projection);

(v) the range of $\mathrm{E}_{\mathrm{i}}$ is the characteristic space for $\mathrm{T}$ associated with $\mathrm{c}_{\mathrm{i}}$.

Conversely, if there exist $\mathrm{k}$ distinct scalars $\mathrm{c}_{1}, \ldots, \mathrm{c}_{\mathrm{k}}$ and $\mathrm{k}$ non-zero linear operators $\mathrm{E}_{1}, \ldots, \mathrm{E}_{\mathrm{k}}$ which satisfy conditions (i), (ii), and (iii), then $\mathrm{T}$ is diagonalizable, $\mathrm{c}_{1}, \ldots, \mathrm{c}_{\mathrm{k}}$ are the distinct characteristic values of $\mathrm{T}$, and conditions (iv) and (v) are satisfied also.

Proof. Suppose that $T$ is diagonalizable, with distinct charac- teristic values $c_{1}, \ldots, c_{k}$. Let $W_{i}$ be the space of characteristic vectors associated with the characteristic value $c_{i}$. As we have seen,

$$
V=W_{1} \oplus \cdots \oplus W_{k} .
$$

Let $E_{1}, \ldots, E_{k}$ be the projections associated with this decomposition, as in Theorem 9. Then (ii), (iii), (iv) and (v) are satisfied. To verify (i), proceed as follows. For each $\alpha$ in $V$,

$$
\alpha=E_{1} \alpha+\cdots+E_{k} \alpha
$$

and so

$$
\begin{aligned}
T \alpha & =T E_{1} \alpha+\cdots+T E_{k} \alpha \\
& =c_{1} E_{1} \alpha+\cdots+c_{k} E_{k} \alpha .
\end{aligned}
$$

In other words, $T=c_{1} E_{1}+\cdots+c_{k} E_{k}$.

Now suppose that we are given a linear operator $T$ along with distinct scalars $c_{i}$ and non-zero operators $E_{i}$ which satisfy (i), (ii) and (iii). Since $E_{i} E_{i}=0$ when $i \neq j$, we multiply both sides of $I=E_{1}+\cdots+E_{k}$ by $E_{i}$ and obtain immediately $E_{i}^{2}=E_{i}$. Multiplying $T=c_{1} E_{1}+\cdots+c_{k} E_{k}$ by $E_{i}$, we then have $T E_{i}=c_{i} E_{i}$, which shows that any vector in the range of $E_{i}$ is in the null space of $\left(T-c_{i} I\right)$. Since we have assumed that $E_{i} \neq 0$, this proves that there is a non-zero vector in the null space of $\left(T-c_{i} I\right)$, i.e., that $c_{i}$ is a characteristic value of $T$. Furthermore, the $c_{i}$ are all of the characteristic values of $T$; for, if $c$ is any scalar, then

$$
T-c I=\left(c_{1}-c\right) E_{1}+\cdots+\left(c_{k}-c\right) E_{k}
$$

so if $(T-c I) \alpha=0$, we must have $\left(c_{i}-c\right) E_{i} \alpha=0$. If $\alpha$ is not the zero vector, then $E_{i} \alpha \neq 0$ for some $i$, so that for this $i$ we have $c_{i}-c=0$.

Certainly $T$ is diagonalizable, since we have shown that every nonzero vector in the range of $E_{i}$ is a characteristic vector of $T$, and the fact that $I=E_{1}+\cdots+E_{k}$ shows that these characteristic vectors span $V$. All that remains to be demonstrated is that the null space of $\left(T-c_{i} I\right)$ is exactly the range of $E_{i}$. But this is clear, because if $T \alpha=c_{i} \alpha$, then

$$
\sum_{j=1}^{k}\left(c_{j}-c_{i}\right) E_{j} \alpha=0
$$

hence

and then

$$
\left(c_{j}-c_{i}\right) E_{j} \alpha=0 \quad \text { for each } j
$$

$$
E_{j} \alpha=0, \quad j \neq i .
$$

Since $\alpha=E_{1} \alpha+\cdots+E_{k} \alpha$, and $E_{j} \alpha=0$ for $j \neq i$, we have $\alpha=E_{i} \alpha$, which proves that $\alpha$ is in the range of $E_{i}$.

One part of Theorem 9 says that for a diagonalizable operator $T$, the scalars $c_{1}, \ldots, c_{k}$ and the operators $E_{1}, \ldots, E_{k}$ are uniquely determined by conditions (i), (ii), (iii), the fact that the $c_{i}$ are distinct, and the fact that the $E_{i}$ are non-zero. One of the pleasant features of the decomposition $T=c_{1} E_{1}+\cdots+c_{k} E_{k}$ is that if $g$ is any polynomial over the field $F$, then

$$
g(T)=g\left(c_{1}\right) E_{1}+\cdots+g\left(c_{k}\right) E_{k} .
$$

We leave the details of the proof to the reader. To see how it is proved one need only compute $T^{r}$ for each positive integer $r$. For example,

$$
\begin{aligned}
T^{2} & =\sum_{i=1}^{k} c_{i} E_{i} \sum_{j=1}^{k} c_{j} E_{j} \\
& =\sum_{i=1}^{k} \sum_{j=1}^{k} c_{i} c_{j} E_{i} E_{j} \\
& =\sum_{i=1}^{k} c_{i}^{2} E_{i}^{2} \\
& =\sum_{i=1}^{k} c_{i}^{2} E_{i} .
\end{aligned}
$$

The reader should compare this with $g(A)$ where $A$ is a diagonal matrix; for then $g(A)$ is simply the diagonal matrix with diagonal entries $g\left(A_{11}\right)$, $\ldots, g\left(A_{n n}\right)$.

We should like in particular to note what happens when one applies the Lagrange polynomials corresponding to the scalars $c_{1}, \ldots, c_{k}$ :

$$
p_{j}=\prod_{i \neq j} \frac{\left(x-c_{i}\right)}{\left(c_{j}-c_{i}\right)} .
$$

We have $p_{j}\left(c_{i}\right)=\delta_{i j}$, which means that

$$
\begin{aligned}
p_{j}(T) & =\sum_{i=1}^{k} \delta_{i j} E_{i} \\
& =E_{j} .
\end{aligned}
$$

Thus the projections $E_{j}$ not only commute with $T$ but are polynomials in $T$.

Such calculations with polynomials in $T$ can be used to give an alternative proof of Theorem 6 , which characterized diagonalizable operators in terms of their minimal polynomials. The proof is entirely independent of our earlier proof.

If $T$ is diagonalizable, $T=c_{1} E_{1}+\cdots+c_{k} E_{k}$, then

$$
g(T)=g\left(c_{1}\right) E_{1}+\cdots+g\left(c_{k}\right) E_{k}
$$

for every polynomial $g$. Thus $g(T)=0$ if and only if $g\left(c_{i}\right)=0$ for each $i$. In particular, the minimal polynomial for $T$ is

$$
p=\left(x-c_{1}\right) \cdots\left(x-c_{k}\right) .
$$

Now suppose $T$ is a linear operator with minimal polynomial $p=$ $\left(x-c_{1}\right) \cdots\left(x-c_{k}\right)$, where $c_{1}, \ldots, c_{k}$ are distinct elements of the scalar field. We form the Lagrange polynomials 

$$
p_{j}=\prod_{i \neq j} \frac{\left(x-c_{i}\right)}{\left(c_{j}-c_{i}\right)} .
$$

We recall from Chapter 4 that $p_{j}\left(c_{i}\right)=\delta_{i j}$ and for any polynomial $g$ of degree less than or equal to $(k-1)$ we have

$$
g=g\left(c_{1}\right) p_{1}+\cdots+g\left(c_{k}\right) p_{k}
$$

Taking $g$ to be the scalar polynomial 1 and then the polynomial $x$, we have

$$
\begin{aligned}
1 & =p_{1}+\cdots+p_{k} \\
x & =c_{1} p_{1}+\cdots+c_{k} p_{k}
\end{aligned}
$$

(The astute reader will note that the application to $x$ may not be valid because $k$ may be 1 . But if $k=1, T$ is a scalar multiple of the identity and hence diagonalizable.) Now let $E_{j}=p_{j}(T)$. From (6-15) we have

$$
\begin{aligned}
I & =E_{1}+\cdots+E_{k} \\
T & =c_{1} E_{1}+\cdots+c_{k} E_{k}
\end{aligned}
$$

Observe that if $i \neq j$, then $p_{i} p_{j}$ is divisible by the minimal polynomial $p$, because $p_{i} p_{j}$ contains every $\left(x-c_{r}\right)$ as a factor. Thus

$$
E_{i} E_{j}=0, \quad i \neq j .
$$

We must note one further thing, namely, that $E_{i} \neq 0$ for each $i$. This is because $p$ is the minimal polynomial for $T$ and so we cannot have $p_{i}(T)=0$ since $p_{i}$ has degree less than the degree of $p$. This last comment, together with (6-16), (6-17), and the fact that the $c_{i}$ are distinct enables us to apply Theorem 11 to conclude that $T$ is diagonalizable.

\section{Exercises}

1. Let $E$ be a projection of $V$ and let $T$ be a linear operator on $V$. Prove that the range of $E$ is invariant under $T$ if and only if $E T E=T E$. Prove that both the range and null space of $E$ are invariant under $T$ if and only if $E T=T E$.

2. Let $T$ be the linear operator on $R^{2}$, the matrix of which in the standard ordered basis is

$$
\left[\begin{array}{ll}
2 & 1 \\
0 & 2
\end{array}\right] \text {. }
$$

Let $W_{1}$ be the subspace of $R^{2}$ spanned by the vector $\epsilon_{1}=(1,0)$.

(a) Prove that $W_{1}$ is invariant under $T$.

(b) Prove that there is no subspace $W_{2}$ which is invariant under $T$ and which is complementary to $W_{1}$ :

$$
R^{2}=W_{1} \oplus W_{2}
$$

(Compare with Exercise 1 of Section 6.5.)

3. Let $T$ be a linear operator on a finite-dimensional vector space $V$. Let $R$ be the range of $T$ and let $N$ be the null space of $T$. Prove that $R$ and $N$ are independent if and only if $V=R \oplus N$. 4. Let $T$ be a linear operator on $V$. Suppose $V=W_{1} \oplus \cdots \oplus W_{k}$, where each $W_{i}$ is invariant under $T$. Let $T_{i}$ be the induced (restriction) operator on $W_{i}$.

(a) Prove that $\operatorname{det}(T)=\operatorname{clet}\left(T_{1}\right) \cdots \operatorname{det}\left(T_{k}\right)$.

(b) Prove that the characteristic polynomial for $f$ is the product of the characteristic polynomials for $f_{1}, \ldots, f_{k}$.

(c) Prove that the minimal polynomial for $T$ is the least common multiple of the minimal polynomials for: $T_{1}, \ldots, T_{k}$. (Hint: Prove and then use the corresponding facts about direct sums of matrices.)

5. Let $T$ be the diagonalizable linear operator on $R^{3}$ which we discussed in Example 3 of Section 6.2. Use the Lagrange polynomials to write the representing matrix $A$ in the form $A=E_{1}+2 E_{2}, E_{1}+E_{2}=I, E_{1} E_{2}=0$.

6. Let $A$ be the $4 \times 4$ matrix in Example 6 of Section 6.3. Find matrices $E_{1}, E_{2}, E_{3}$ such that $A=c_{1} E_{1}+c_{2} E_{2}+c_{3} E_{3}, E_{1}+E_{2}+E_{3}=I$, and $E_{i} E_{j}=0, i \neq j$.

7. In Exercises 5 and 6, notice that (for each $i$ ) the space of characteristic vectors associated with the characteristic value $c_{i}$ is spanned by the column vectors of the various matrices $E_{j}$ with $j \neq i$. Is that a coincidence?

8. Let $T$ be a linear operator on $V$ which commutes with every projection operator on $V$. What can you say about $T$ ?

9. Let $V$ be the vector space of continuous real-valued functions on the interval $[-1,1]$ of the real line. Let $W_{e}$ be the subspace of even functions, $f(-x)=f(x)$, and let $W_{o}$ be the subspace of odd functions, $f(-x)=-f(x)$.

(a) Show that $V=W_{e} \oplus W_{o}$.

(b) If $T$ is the indefinite integral operator

$$
(T f)(x)=\int_{0}^{x} f(t) d t
$$

are $W_{e}$ and $W_{o}$ invariant under $T$ ?

\subsection{The Primary Decomposition Theorem}

We are trying to study a linear operator $T$ on the finite-dimensional space $V$, by decomposing $T$ into a direct sum of operators which are in some sense elementary. We can do this through the characteristic values and vectors of $T$ in certain special cases, i.e., when the minimal polynomial for $T$ factors over the scalar field $F$ into a product of distinct monic polynomials of degree 1 . What can we do with the general $T$ ? If we try to study $T$ using characteristic values, we are confronted with two problems. First, $T$ may not have a single characteristic value; this is really a deficiency in the scalar field, namely, that it is not algebraically closed. Second, even if the characteristic polynomial factors completely over $F$ into a product of polynomials of degree 1 , there may not be enough characteristic vectors for $T$ to span the space $V$; this is clearly a deficiency in $T$. The second situation is illustrated by the operator $T$ on $F^{3}$ ( $F$ any field) represented in the standard basis by

$$
A=\left[\begin{array}{rrr}
2 & 0 & 0 \\
1 & 2 & 0 \\
0 & 0 & -1
\end{array}\right] \text {. }
$$

The characteristic polynomial for $A$ is $(x-2)^{2}(x+1)$ and this is plainly also the minimal polynomial for $A$ (or for $T$ ). Thus $T$ is not diagonalizable. One sees that this happens because the null space of $(T-2 I)$ has dimension 1 only. On the other hand, the null space of $(T+I)$ and the null space of $(T-2 I)^{2}$ together span $V$, the former being the subspace spanned by $\epsilon_{3}$ and the latter the subspace spanned by $\epsilon_{1}$ and $\epsilon_{2}$.

This will be more or less our general method for the second problem. If (remember this is an assumption) the minimal polynomial for $T$ decomposes

$$
p=\left(x-c_{1}\right)^{r_{1}} \cdots\left(x-c_{k}\right)^{r_{k}}
$$

where $c_{1}, \ldots, c_{k}$ are distinct elements of $F$, then we shall show that the space $V$ is the direct sum of the null spaces of $\left(T-c_{i} I\right)^{r_{i}}, i=1, \ldots, k$. The hypothesis about $p$ is equivalent to the fact that $T$ is triangulable (Theorem 5); however, that knowledge will not help us.

The theorem which we prove is more general than what we have described, since it works with the primary decomposition of the minimal polynomial, whether or not the primes which enter are all of first degree. The reader will find it helpful to think of the special case when the primes are of degree 1, and even more particularly, to think of the projection-type proof of Theorem 6, a special case of this theorem.

Theorem 12 (Primary Decomposition Theorem). Let $\mathrm{T}$ be a linear operator on the finite-dimensional vector space $\mathrm{V}$ over the field $\mathrm{F}$. Let $\mathrm{p}$ be the minimal polynomial for $\mathrm{T}$,

$$
\mathrm{p}=\mathrm{p}_{1}^{\mathrm{r}_{1}} \cdots \mathrm{p}_{k}^{r_{k}}
$$

where the $\mathrm{p}_{\mathrm{i}}$ are distinct irreducible monic polynomials over $\mathrm{F}$ and the $\mathrm{r}_{\mathrm{i}}$ are positive integers. Let $\mathrm{W}_{\mathrm{i}}$ be the null space of $\mathrm{p}_{\mathrm{i}}(\mathrm{T})^{\mathbf{r}_{i}}, \mathrm{i}=1, \ldots, \mathrm{k}$. Then

(i) $\mathrm{V}=\mathrm{W}_{1} \oplus \cdots \oplus \mathrm{W}_{\mathrm{k}}$;

(ii) each $\mathrm{W}_{\mathrm{i}}$ is invariant under $\mathrm{T}$;

(iii) if $\mathrm{T}_{\mathrm{i}}$ is the operator induced on $\mathrm{W}_{\mathrm{i}}$ by $\mathrm{T}$, then the minimal polynomial for $\mathrm{T}_{\mathrm{i}}$ is $\mathrm{p}_{\mathrm{i}}^{\mathrm{r}}$.

Proof. The idea of the proof is this. If the direct-sum decomposition (i) is valid, how can we get hold of the projections $E_{1}, \ldots, E_{k}$ associated with the decomposition? The projection $E_{i}$ will be the identity on $W_{i}$ and zero on the other $W_{j}$. We shall find a polynomial $h_{i}$ such that $h_{i}(T)$ is the identity on $W_{i}$ and is zero on the other $W_{j}$, and so that $h_{1}(T)+\cdots+$ $h_{k}(T)=I$, etc. For each $i$, let

$$
f_{i}=\frac{p}{p_{i}^{r_{i}}}=\prod_{j \neq i} p_{j}^{r_{i}} .
$$

Since $p_{1}, \ldots, p_{k}$ are distinct prime polynomials, the polynomials $f_{1}, \ldots, f_{k}$ are relatively prime (Theorem 10, Chapter 4 ). Thus there are polynomials $g_{1}, \ldots, g_{k}$ such that

$$
\sum_{i=1}^{n} f_{i} g_{i}=1 .
$$

Note also that if $i \neq j$, then $f_{i} f_{j}$ is divisible by the polynomial $p$, because $f_{i} f_{j}$ contains each $p_{m}^{\gamma_{m}}$ as a factor. We shall show that the polynomials $h_{i}=f_{i} g_{i}$ behave in the manner described in the first paragraph of the proof.

Let $E_{i}=h_{i}(T)=f_{i}(T) g_{i}(T)$. Since $h_{1}+\cdots+h_{k}=1$ and $p$ divides $f_{i} f_{j}$ for $i \neq j$, we have

$$
\begin{array}{r}
E_{1}+\cdots+E_{k}=I \\
E_{i} E_{j}=0, \quad \text { if } i \neq j .
\end{array}
$$

Thus the $E_{i}$ are projections which correspond to some direct-sum decomposition of the space $V$. We wish to show that the range of $E_{i}$ is exactly the subspace $W_{i}$. It is clear that each vector in the range of $E_{i}$ is in $W_{i}$, for if $\alpha$ is in the range of $E_{i}$, then $\alpha=E_{i} \alpha$ and so

$$
\begin{aligned}
\left.p_{i}(T)\right)_{i} \alpha & =p_{i}(T)^{r_{i}} E_{i} \alpha \\
& =p_{i}(T)^{r_{i}} f_{i}(T) g_{i}(T) \alpha \\
& =0
\end{aligned}
$$

because $p^{r_{i} f_{i}} g_{i}$ is divisible by the minimal polynomial $p$. Conversely, suppose that $\alpha$ is in the null space of $p_{i}(T)^{r_{i}}$. If $j \neq i$, then $f_{j} g_{j}$ is divisible by $p_{i}^{r_{i}}$ and so $f_{j}(T) g_{j}(T) \alpha=0$, i.e., $E_{j} \alpha=0$ for $j \neq i$. But then it is immediate that $E_{i} \alpha=\alpha$, i.e., that $\alpha$ is in the range of $E_{i}$. This completes the proof of statement (i).

It is certainly clear that the subspaces $W_{i}$ are invariant under $T$. If $T_{i}$ is the operator induced on $W_{i}$ by $T$, then evidently $p_{i}\left(T_{i}\right)^{r_{i}}=0$, because by definition $p_{i}(T)^{r_{i}}$ is 0 on the subspace $W_{i}$. This shows that the minimal polynomial for $T_{i}$ clivides $p_{i}^{r_{i}}$. Conversely, let $g$ be any polynomial such that $g\left(T_{i}\right)=0$. Then $g(T) f_{i}(\boldsymbol{T})=0$. Thus $g f_{i}$ is divisible by the minimal polynomial $p$ of $T_{i}$ i.e., $p_{i}^{r_{i}} f_{i}$ divides $g f_{i}$. It is easily seen that $p_{i}^{r_{i}}$ divides $g$. Hence the minimal polynomial for $T_{i}$ is $p_{i}^{r_{i}}$.

Corollary. If $\mathrm{E}_{1}, \ldots, \mathrm{E}_{\mathrm{k}}$ are the projections associated with the primary decomposition of $\mathrm{T}$, then each $\mathrm{E}_{\mathrm{i}}$ is a polynomial in $\mathrm{T}$, and accordingly if a linear operator $\mathrm{U}$ commutes with $\mathrm{T}$ then $\mathrm{U}$ commutes with each of the $\mathrm{E}_{\mathrm{i}}$, i.e., each subspace $\mathrm{W}_{\mathrm{i}}$ is invariarit under $\mathrm{U}$.

In the notation of the proof of Theorem 12, let us take a look at the special case in which the minimal polynomial for $T$ is a product of first- degree polynomials, i.e., the case in which each $p_{i}$ is of the form $p_{i}=x-c_{i}$. Now the range of $E_{i}$ is the null space $W_{i}$ of $\left(T-c_{i} I\right)^{r_{i}}$. Let us put $D=c_{1} E_{1}+\cdots+c_{k} E_{k}$. By Theorem $11, D$ is a diagonalizable operator which we shall call the diagonalizable part of $T$. Let us look at the operator $N=T-D$. Now

$$
\begin{aligned}
& T=T E_{1}+\cdots+T E_{k} \\
& D=c_{1} E_{1}+\cdots+c_{k} E_{k}
\end{aligned}
$$

so

$$
N=\left(T-c_{1} I\right) E_{1}+\cdots+\left(T-c_{k} I\right) E_{k} .
$$

The reader should be familiar enough with projections by now so that he sees that

$$
N^{2}=\left(T-c_{1} I\right)^{2} E_{1}+\cdots+\left(T-c_{k} I\right)^{2} E_{k}
$$

and in general that

$$
N^{r}=\left(T-c_{1} I\right)^{r} E_{1}+\cdots+\left(T-c_{k} I\right)^{r} E_{k} .
$$

When $r \geq r_{i}$ for each $i$, we shall have $N^{r}=0$, because the operator $\left(T-c_{i} I\right)^{r}$ will then be 0 on the range of $E_{i}$.

Definition. Let $\mathrm{N}$ be a linear operator on the vector space $\mathrm{V}$. We say that $\mathrm{N}$ is nilpotent if there is some positive integer $\mathrm{r}$ such that $\mathrm{N}^{\mathrm{r}}=0$.

Theorem 13. Let T be a linear operator on the finite-dimensional vector space $\mathrm{V}$ over the field $\mathrm{F}$. Suppose that the minimal polynomial for $\mathrm{T}$ decomposes over $\mathrm{F}$ into a product of linear polynomials. Then there is a diagonalizable operator $\mathrm{D}$ on $\mathrm{V}$ and a nilpotent operator $\mathrm{N}$ on $\mathrm{V}$ such that

(i) $\mathrm{T}=\mathrm{D}+\mathrm{N}$,

(ii) $\mathrm{DN}=\mathrm{ND}$.

The diagonalizable operator $\mathrm{D}$ and the nilpotent operator $\mathrm{N}$ are uniquely determined by (i) and (ii) and each of them is a polynomial in $\mathrm{T}$.

Proof. We have just observed that we can write $T=D+N$ where $D$ is diagonalizable and $N$ is nilpotent, and where $D$ and $N$ not only commute but are polynomials in $T$. Now suppose that we also have $T=$ $D^{\prime}+N^{\prime}$ where $D^{\prime}$ is diagonalizable, $N^{\prime}$ is nilpotent, and $D^{\prime} N^{\prime}=N^{\prime} D^{\prime}$. We shall prove that $D=D^{\prime}$ and $N=N^{\prime}$.

Since $D^{\prime}$ and $N^{\prime}$ commute with one another and $T=D^{\prime}+N^{\prime}$, we see that $D^{\prime}$ and $N^{\prime}$ commute with $T$. Thus $D^{\prime}$ and $N^{\prime}$ commute with any polynomial in $T$; hence they commute with $D$ and with $N$. Now we have

$$
D+N=D^{\prime}+N^{\prime}
$$

or

$$
D-D^{\prime}=N^{\prime}-N
$$

and all four of these operators commute with one another. Since $D$ and $D^{\prime}$ are both diagonalizable and they commute, they are simultaneously diagonalizable, and $D-D^{\prime}$ is diagonalizable. Since $N$ and $N^{\prime}$ are both nilpotent and they commute, the operator $\left(N^{\prime}-N\right)$ is nilpotent; for, using the fact that $N$ and $N^{\prime}$ commute

$$
\left(N^{\prime}-N\right)^{r}=\sum_{j=0}^{r}\left(\begin{array}{l}
r \\
j
\end{array}\right)\left(N^{\prime}\right)^{r-j}(-N)^{j}
$$

and so when $r$ is sufficiently large every term in this expression for

![](https://cdn.mathpix.com/cropped/2023_01_16_0f6afdf132d84aaf6bdeg-232.jpg?height=47&width=1122&top_left_y=514&top_left_x=69)
space must have its $n$th power 0 ; if we take $r=2 n$ above, that will be large enough. It then follows that $r=n$ is large enough, but this is not obvious from the above expression.) Now $D-D^{\prime}$ is a diagonalizable operator which is also nilpotent. Such an operator is obviously the zero operator; for since it is nilpotent, the minimal polynomial for this operator is of the form $x^{r}$ for some $r \leq m$; but then since the operator is diagonalizable, the minimal polynomial cannot have a repeated root; hence $r=1$ and the minimal polynomial is simply $x$, which says the operator is 0 . Thus we see that $D=D^{\prime}$ and $N=N^{\prime}$.

Corollary. Let $\mathrm{V}$ be a finite-dimensional vector space over an algebraically closed field F, e.g., the field of complex numbers. Then every linear operator $\mathrm{T}$ on $\mathrm{V}$ can be written as the sum of a diagonalizable operator $\mathrm{D}$ and a nilpotent operator $\mathrm{N}$ which commute. These operators $\mathrm{D}$ and $\mathrm{N}$ are unique and each is a polynomial in $\mathrm{T}$.

From these results, one sees that the study of linear operators on vector spaces over an algebraically closed field is essentially reduced to the study of nilpotent operators. For vector spaces over non-algebraically closed fields, we still need to find some substitute for characteristic values and vectors. It is a very interesting fact that these two problems can be handled simultaneously and this is what we shall do in the next chapter.

In concluding this section, we should like to give an example which illustrates some of the ideas of the primary decomposition theorem. We have chosen to give it at the end of the section since it deals with differential equations and thus is not purely linear algebra.

Example 14. In the primary decomposition theorem, it is not necessary that the vector space $V$ be finite dimensional, nor is it necessary for parts (i) and (ii) that $p$ be the minimal polynomial for $T$. If $T$ is a linear operator on an arbitrary vector space and if there is a monic polynomial $p$ such that $p(T)=0$, then parts (i) and (ii) of Theorem 12 are valid for $T$ with the proof which we gave.

Let $n$ be a positive integer and let $V$ be the space of all $n$ times continuously differentiable functions $f$ on the real line which satisfy the differential equation 

$$
\frac{d^{n} f}{d t^{n}}+a_{n-1} \frac{d^{n-1} f}{d t^{n-1}}+\cdots+a_{1} \frac{d f}{d t}+a_{0} f=0
$$

where $a_{\mathbf{0}}, \ldots, a_{n-1}$ are some fixed constants. If $C_{n}$ denotes the space of $n$ times continuously differentiable functions, then the space $V$ of solutions of this differential equation is a subspace of $C_{n}$. If $D$ denotes the differentiation operator and $p$ is the polynomial

$$
p=x^{n}+a_{n-1} x^{n-1}+\cdots+a_{1} x+a_{0}
$$

then $V$ is the null space of the operator $p(D)$, because (6-18) simply says $p(D) f=0$. Therefore, $V$ is invariant under $D$. Let us now regard $D$ as a linear operator on the subspace $V$. Then $p(D)=0$.

If we are discussing differentiable complex-valued functions, then $C_{n}$ and $V$ are complex vector spaces, and $a_{0}, \ldots, a_{n-1}$ may be any complex numbers. We now write

$$
p=\left(x-c_{1}\right)^{\tau_{1}} \cdots\left(x-c_{k}\right)^{r_{k}}
$$

where $c_{1}, \ldots, c_{k}$ are distinct complex numbers. If $W_{j}$ is the null space of $\left(D-c_{j} I\right)^{r_{i}}$, then Theorem 12 says that

$$
V=W_{1} \oplus \cdots \oplus W_{k} .
$$

In other words, if $f$ satisfies the differential equation (6-18), then $f$ is uniquely expressible in the form

$$
f=f_{1}+\cdots+f_{k}
$$

where $f_{j}$ satisfies the differential equation $\left(D-c_{j} I\right)^{r_{i}} f_{j}=0$. Thus, the study of the solutions to the equation (6-18) is reduced to the study of the space of solutions of a differential equation of the form

$$
(D-c I)^{r} f=0 .
$$

This reduction has been accomplished by the general methods of linear algebra, i.e., by the primary decomposition theorem.

To describe the space of solutions to (6-19), one must know something about differential equations, that is, one must know something about $D$ other than the fact that it is a linear operator. However, one does not need to know very much. It is very easy to establish by induction on $r$ that if $f$ is in $C_{r}$ then

that is,

$$
(D-c I)^{r} f=e^{c t} D^{r}\left(e^{-c t f}\right)
$$

$$
\frac{d f}{d t}-c f(t)=e^{c t} \frac{d}{d t}\left(e^{-c t f}\right), \text { etc. }
$$

Thus $(D-c I)^{r} f=0$ if and only if $D^{r}\left(e^{-c t} f\right)=0$. A function $g$ such that $D^{r} g=0$, i.e., $d^{r} g / d t^{r}=0$, must be a polynomial function of degree $(r-1)$ or less:

$$
g(t)=b_{0}+b_{1} t+\cdots+b_{r-1} t^{r-1} .
$$

Thus $f$ satisfies (6-19) if and only if $f$ has the form

$$
f(t)=e^{c t}\left(b_{0}+b_{1} t+\cdots+b_{r-1} t^{r-1}\right) .
$$

Accordingly, the 'functions' eft $t e^{c t}, \ldots, t^{r-1} e^{c t}$ span the space of solutions of (6-19). Since $1, t, \ldots, t^{r-1}$ are linearly independent functions and the exponential function has no zeros, these $r$ functions $t^{j} e^{c t}, 0 \leq j \leq r-1$, form a basis for the space of solutions.

Returning to the differential equation (6-18), which is

$$
\begin{gathered}
p(\boldsymbol{D}) f=0 \\
p=\left(x-c_{1}\right)^{r_{1}} \cdots\left(x-c_{k}\right)^{r_{k}}
\end{gathered}
$$

we see that the $n$ functions $t^{m} e^{c_{i} t}, 0 \leq m \leq r_{j}-1,1 \leq j \leq k$, form a basis for the space of solutions to (6-18). In particular, the space of solutions is finite-dimensional and has dimension equal to the degree of the polynomial $p$.

\section{Exercises}

1. Let $T$ be a linear operator on $R^{3}$ which is represented in the standard ordered basis by the matrix

$$
\left[\begin{array}{rrr}
6 & -3 & -2 \\
4 & -1 & -2 \\
10 & -5 & -3
\end{array}\right] \text {. }
$$

Express the minimal polynomial $p$ for $T$ in the form $\boldsymbol{p}=p_{1} p_{2}$, where $p_{1}$ and $p_{2}$ are monic and irreducible over the field of real numbers. Let $W_{i}$ be the null space of $p_{i}(T)$. Find bases $B_{i}$ for the spaces $W_{1}$ and $W_{2}$. If $T_{i}$ is the operator induced on $W_{i}$ by $T$, find the matrix of $T_{i}$ in the basis $B_{i}$ (above).

2. Let $T$ be the linear operatcr on $R^{3}$ which is represented by the matrix

$$
\left[\begin{array}{rrr}
3 & 1 & -1 \\
2 & 2 & -1 \\
2 & 2 & 0
\end{array}\right]
$$

in the standard ordered basis. Show that there is a diagonalizable operator $D$ on $R^{3}$ and a nilpotent operator $N$ on $R^{3}$ such that $T=D+N$ and $D N=N D$. Find the matrices of $D$ and $N$ in the standard basis. (Just repeat the proof of Theorem 12 for this special case.)

3. If $V$ is the space of all polynomials of degree less than or equal to $n$ over a field $F$, prove that the differentiation operator on $V$ is nilpotent.

4. Let $T$ be a linear operator on the finite-dimensional space $V$ with characteristic polynomial

$$
f=\left(x-c_{1}\right)^{d_{1}} \cdots\left(x-c_{k}\right)^{d_{k}}
$$

and minimal polynomial

$$
p=\left(x-c_{1}\right)^{r_{1}} \cdots\left(x-c_{k}\right)^{r_{k}} .
$$

Let $W_{i}$ be the null space of $\left(T-c_{i} I\right)^{r_{i}}$. (a) Prove that $W_{i}$ is the set of all vectors $\alpha$ in $V$ such that $\left(T-c_{i} I\right)^{m} \alpha=0$ for some positive integer $m$ (which may depend upon $\alpha$ ).

(b) Prove that the dimension of $W_{i}$ is $d_{i}$. (Hint: If $T_{i}$ is the operator induced on $W_{i}$ by $T$, then $T_{i}-c_{i} I$ is nilpotent; thus the characteristic polynomial for $T_{i}-c_{i} I$ must be $x^{e_{i}}$ where $e_{i}$ is the dimension of $W_{i}$ (proof?); thus the characteristic polynomial of $T_{i}$ is $\left(x-c_{i}\right)^{e_{i}}$; now use the fact that the characteristic polynomial for $T$ is the product of the characteristic polynomials of the $T_{i}$ to show that $e_{i}=d_{i}$.)

5. Let $V$ be a finite-dimensional vector space over the field of complex numbers. Let $T$ be a linear operator on $V$ and let $D$ be the diagonalizable part of $T$. Prove that if $g$ is any polynomial with complex coefficients, then the diagonalizable part of $g(T)$ is $g(D)$

6. Let $V$ be a finite-dimensional vector space over the field $F$, and let $T$ be a linear operator on $V$ such that rank $(T)=1$. Prove that either $T$ is diagonalizable or $T$ is nilpotent, not both.

7. Let $V$ be a finite-dimensional vector space over $F$, and let $T$ be a linear operator on $V$. Suppose that $T$ commutes with every diagonalizable linear operator on $V$. Prove that $T$ is a scalar multiple of the identity operator.

8. Let $V$ be the space of $n \times n$ matrices over a field $F$, and let $A$ be a fixed $n \times n$ matrix over $F$. Define a linear operator $T$ on $V$ by $T(B)=A B-B A$. Prove that if $A$ is a nilpotent matrix, then $T$ is a nilpotent operator.

9. Give an example of two $4 \times 4$ nilpotent matrices which have the same minimal polynomial (they necessarily have the same characteristic polynomial) but which are not similar.

10. Let $T$ be a linear operator on the finite-dimensional space $V$, let $p=p_{1}^{\tau_{1}} \cdots p_{k}^{\tau_{k}}$ be the minimal polynomial for $T$, and let $V=W_{1} \oplus \cdots \oplus W_{k}$ be the primary decomposition for $T$, i.e., $W_{j}$ is the null space of $p_{j}(T)^{r_{i}}$. Let $W$ be any subspace of $V$ which is invariant under $T$. Prove that

$$
W=\left(W \bigcap W_{1}\right) \oplus\left(W \cap W_{2}\right) \oplus \cdots \oplus\left(W \cap W_{k}\right) .
$$

11. What's wrong with the following proof of Theorem 13? Suppose that the minimal polynomial for $T$ is a product of linear factors. Then, by Theorem 5, $T$ is triangulable. Let $\beta$ be an ordered basis such that $A=[T]_{B}$ is upper-triangular. Let $D$ be the diagonal matrix with diagonal entries $a_{11}, \ldots, a_{n n}$. Then $A=D+N$, where $N$ is strictly upper-triangular. Evidently $N$ is nilpotent.

12. If you thought about Exercise 11, think about it again, after you observe what Theorem 7 tells you about the diagonalizable and nilpotent parts of $T$.

13. Let $T$ be a linear operator on $V$ with minimal polynomial of the form $p^{n}$, where $p$ is irreducible over the scalar field. Show that there is a vector $\alpha$ in $V$ such that the $T$-annihilator of $\alpha$ is $p^{n}$.

14. Use the primary decomposition theorem and the result of Exercise 13 to prove the following. If $T$ is any linear operator on a finite-dimensional vector space $V$, then there is a vector $\alpha$ in $V$ with $T$-annihilator equal to the minimal polynomial for $T$.

15. If $N$ is a nilpotent linear operator on an $n$-dimensional vector space $V$, then the characteristic polynomial for $N$ is $x^{n}$. 

\section{The Rational \\ and Jordan Forms}

\subsection{Cyclic Subspaces and Annihilators}

Once again $V$ is a finite-dimensional vector space over the field $F$ and $T$ is a fixed (but arbitrary) linear operator on $V$. If $\alpha$ is any vector in $V$, there is a smallest subspace of $V$ which is invariant under $T$ and contains $\alpha$. This subspace can be defined as the intersection of all $T$ invariant subspaces which contain $\alpha$; however, it is more profitable at the moment for us to look at things this way. If $W$ is any subspace of $V$ which is invariant under $T$ and contains $\alpha$, then $W$ must also contain the vector $T \alpha$; hence $W$ must contain $T(T \alpha)=T^{2} \alpha, T\left(T^{2} \alpha\right)=T^{3} \alpha$, etc. In other words $W$ must contain $g(T) \alpha$ for every polynomial $g$ over $F$. The set of all vectors of the form $g(T) \alpha$, with $g$ in $F[x]$, is clearly invariant under $T$, and is thus the smallest $T$-invariant subspace which contains $\alpha$.

Definition. If $\alpha$ is any vector in $\mathrm{V}$, the T-cyclic subspace generated by $\alpha$ is the subspace $\mathrm{Z}(\alpha ; \mathrm{T})$ of all vectors of the form $\mathrm{g}(\mathrm{T}) \alpha$, $\mathrm{g}$ in $\mathrm{F}[\mathrm{x}]$. If $\mathrm{Z}(\alpha ; \mathrm{T})=\mathrm{V}$, then $\alpha$ is called $a$ cyclic vector for $\mathrm{T}$.

Another way of describing the subspace $Z(\alpha ; T)$ is that $Z(\alpha ; T)$ is the subspace spanned by the vectors $T^{k} \alpha, k \geq 0$, and thus $\alpha$ is a cyclic vector for $T$ if and only if these vectors span $V$. We caution the reader that the general operator $T$ has no cyclic vectors.

Example 1. For any $T$, the $T$-cyclic subspace generated by the zero vector is the zero subspace. The space $Z(\alpha ; T)$ is one-dimensional if and only if $\alpha$ is a characteristic vector for $T$. For the identity operator, every non-zero vector generates a one-dimensional cyclic subspace; thus, if $\operatorname{dim} V>1$, the identity operator has no cyclic vector. An example of an operator which has a cyclic vector is the linear operator $T$ on $F^{2}$ which is represented in the standard ordered basis by the matrix

$$
\left[\begin{array}{ll}
0 & 0 \\
1 & 0
\end{array}\right] \text {. }
$$

Here the cyclic vector (a cyclic vector) is $\epsilon_{1}$; for, if $\beta=(a, b)$, then with $g=a+b x$ we have $\beta=g(T) \epsilon_{1}$. For this same operator $T$, the cyclic subspace generated by $\epsilon_{2}$ is the one-dimensional space spanned by $\epsilon_{2}$, because $\epsilon_{2}$ is a characteristic vector of $T$.

For any $T$ and $\alpha$, we shall be interested in linear relations

$$
c_{0} \alpha+c_{1} T \alpha+\cdots+c_{k} T^{k} \alpha=0
$$

between the vectors $T^{j} \alpha$, that is, we shall be interested in the polynomials $g=c_{0}+c_{1} x+\cdots+c_{k} x^{k}$ which have the property that $g(T) \alpha=0$. The set of all $g$ in $F[x]$ such that $g(T) \alpha=0$ is clearly an ideal in $F[x]$. It is also a non-zero ideal, because it contains the minimal polynomial $p$ of the operator $T(p(T) \alpha=0$ for every $\alpha$ in $V)$.

Definition. If $\alpha$ is any vector in $\mathrm{V}$, the T-annihilator of $\alpha$ is the ideal $\mathrm{M}(\alpha ; \mathrm{T})$ in $\mathrm{F}[\mathrm{x}]$ consisting of all polynomials $\mathrm{g}$ over $\mathrm{F}$ such that $\mathrm{g}(\mathrm{T}) \alpha=0$. The unique monic polynomial $\mathrm{p}_{\alpha}$ which generates this ideal will also be called the T-annihilator of $\alpha$.

As we pointed out above, the $T$-annihilator $p_{\alpha}$ divides the minimal polynomial of the operator $T$. The reader should also note that $\operatorname{deg}\left(p_{\alpha}\right)>0$ unless $\alpha$ is the zero vector.

Theorem 1. Let $\alpha$ be any non-zero vector in $\mathrm{V}$ and let $\mathrm{p}_{\alpha}$ be the T-annihilator of $\alpha$.

(i) The degree of $\mathrm{p}_{\alpha}$ is equal to the dimension of the cyclic subspace $\mathrm{Z}(\alpha ; \mathrm{T})$.

(ii) If the degree of $\mathrm{p}_{\alpha}$ is $\mathrm{k}$, then the vectors $\alpha, \mathrm{T} \alpha, \mathrm{T}^{2} \alpha, \ldots, \mathrm{T}^{\mathrm{k}-1} \alpha$ form a basis for $\mathrm{Z}(\alpha ; \mathrm{T})$.

(iii) If $\mathrm{U}$ is the linear operator on $\mathrm{Z}(\alpha ; \mathrm{T})$ induced by $\mathrm{T}$, then the minimal polynomial for $\mathrm{U}$ is $\mathrm{p}_{\alpha}$.

Proof. Let $g$ be any polynomial over the field $F$. Write

$$
g=p_{\alpha} q+r
$$

where either $r=0$ or $\operatorname{deg}(r)<\operatorname{deg}\left(p_{\alpha}\right)=k$. The polynomial $p_{\alpha} q$ is in the $T$-annihilator of $\alpha$, and so

$$
g(T) \alpha=r(T) \alpha .
$$

Since $r=0$ or $\operatorname{deg}(r)<k$, the vector $r(T) \alpha$ is a linear combination of the vectors $\alpha, T \alpha, \ldots, T^{k-1} \alpha$, and since $g(T) \alpha$ is a typical vector in $Z(\alpha ; T)$, this shows that these $k$ vectors span $Z(\alpha ; T)$. These vectors are certainly linearly independent, because any non-trivial linear relation between them would give us a non-zero polynomial $g$ such that $g(T) \alpha=0$ and $\operatorname{deg}(g)<\operatorname{deg}\left(p_{\alpha}\right)$, which is absurd. This proves (i) and (ii).

Let $U$ be the linear operator on $Z(\alpha ; T)$ obtained by restricting $T$ to that subspace. If $g$ is any polynomial over $F$, then

$$
\begin{aligned}
p_{\alpha}(U) g(T) \alpha & =p_{\alpha}(T) g(T) \alpha \\
& =g(T) p_{\alpha}(T) \alpha \\
& =g(T) 0 \\
& =0 .
\end{aligned}
$$

Thus the operator $p_{\alpha}(U)$ sends every vector in $Z(\alpha ; T)$ into 0 and is the zero operator on $Z(\alpha ; T)$. Furthermore, if $h$ is a polynomial of degree less than $k$, we cannot have $h(U)=0$, for then $h(U) \alpha=h(T) \alpha=0$, contradicting the definition. of $p_{\alpha}$. This shows that $p_{\alpha}$ is the minimal polynomial for $U$.

A particular consequence of this theorem is the following: If $\alpha$ happens to be a cyclic vector for $T$, then the minimal polynomial for $T$ must have degree equal to the dimension of the space $V$; hence, the Cayley-Hamilton theorem tells us that the minimal polynomial for $T$ is the characteristic polynomial for $T$. We shall prove later that for any $T$ there is a vector $\alpha$ in $V$ which has the minimal polynomial of $T$ for its annihilator. It will then follow that $T$ has a cyclic vector if and only if the minimal and characteristic polynomials for $T$ are identical. But it will take a little work for us to see this.

Our plan is to study the general $T$ by using operators which have a cyclic vector. So, let us take a look at a linear operator $U$ on a space $W$ of dimension $k$ which has a cyclic vector $\alpha$. By Theorem 1, the vectors $\alpha, \ldots, U^{k-1} \alpha$ form a basis for the space $W$, and the annihilator $p_{\alpha}$ of $\alpha$ is the minimal polynomial for $U$ (and hence also the characteristic polynomial for $U$ ). If we let $\alpha_{i}=U^{i-1} \alpha, i=1, \ldots, k$, then the action of $U$ on the ordered basis $B=\left\{\alpha_{1}, \ldots, \alpha_{k}\right\}$ is

$$
\begin{aligned}
& U \alpha_{i}=\alpha_{i+1}, \quad i=1, \ldots, k-1 \\
& U \alpha_{k}=-c_{0} \alpha_{1}-c_{1} \alpha_{2}-\cdots-c_{k-1} \alpha_{k}
\end{aligned}
$$

where $p_{\alpha}=c_{0}+c_{1} x+\cdots+c_{k-1} x^{k-1}+x^{k}$. The expression for $U \alpha_{k}$ follows from the fact that $p_{\alpha}(U) \alpha=0$, i.e.,

$$
U^{k} \alpha+c_{k-1} U^{k-1} \alpha+\cdots+c_{1} U \alpha+c_{0} \alpha=0 \text {. }
$$

This says that the matrix of $U$ in the ordered basis $B$ is

$$
\left[\begin{array}{cccccc}
0 & 0 & 0 & \cdots & 0 & -c_{0} \\
1 & 0 & 0 & \cdots & 0 & -c_{1} \\
0 & 1 & 0 & \cdots & 0 & -c_{2} \\
\vdots & \vdots & \vdots & & \vdots & \vdots \\
0 & 0 & 0 & \cdots & 1 & -c_{k-1}
\end{array}\right] .
$$

The matrix $(7-2)$ is called the companion matrix of the monic polynomial $p_{\alpha}$.

Theorem 2. If $\mathrm{U}$ is a linear operator on the finite-dimensional space $\mathrm{W}$, then $\mathrm{U}$ has a cyclic vector if and only if there is some ordered basis for $\mathrm{W}$ in which $\mathrm{U}$ is represented by the companion matrix of the minimal polynomial for U.

Proof. We have just observed that if $U$ has a cyclic vector, then there is such an ordered basis for $W$. Conversely, if we have some ordered basis $\left\{\alpha_{1}, \ldots, \alpha_{k}\right\}$ for $W$ in which $U$ is represented by the companion matrix of its minimal polynomial, it is obvious that $\alpha_{1}$ is a cyclic vector for $U$.

Corollary. If $\mathrm{A}$ is the companion matrix of a monic polynomial p, then $\mathrm{p}$ is both the minimal and the characteristic polynomial of $\mathrm{A}$.

Proof. One way to see this is to let $U$ be the linear operator on $F^{k}$ which is represented by $A$ in the standard ordered basis, and to apply Theorem 1 together with the Cayley-Hamilton theorem. Another method is to use Theorem 1 to see that $p$ is the minimal polynomial for $A$ and to verify by a direct calculation that $p$ is the characteristic polynomial for $A$.

One last comment if $T$ is any linear operator on the space $V$ and $\alpha$ is any vector in $V$, then the operator $U$ which $T$ induces on the cyclic subspace $Z(\alpha ; T)$ has a cyclic vector, namely, $\alpha$. Thus $Z(\alpha ; T)$ has an ordered basis in which $U$ is represented by the companion matrix of $p_{\alpha}$, the $T$-annihilator of $\alpha$.

\section{Exercises}

1. Let $T$ be a linear operator on $F^{2}$. Prove that any non-zero vector which is not a characteristic vector for $T$ is a cyclic vector for $T$. Hence, prove that either $T$ has a cyclic vector or $T$ is a scalar multiple of the identity operator.

2. Let $T$ be the linear operator on $R^{3}$ which is represented in the standard ordered basis by the matrix

$$
\left[\begin{array}{rrr}
2 & 0 & 0 \\
0 & 2 & 0 \\
0 & 0 & -1
\end{array}\right] \text {. }
$$

Prove that $T$ has n o cyclic vector. What is the $T$-cyclic subspace generated by the vector $(1,-1,3)$ ?

3. Let $T$ be the linear operator on $C^{3}$ which is represented in the standard ordered basis by the matrix

$$
\left[\begin{array}{rrr}
1 & i & 0 \\
-1 & 2 & -i \\
0 & 1 & 1
\end{array}\right]
$$

Find the $T$-annihilator of the vector $(1,0,0)$. Find the $T$-annihilator of $(1,0, i)$.

4. Prove that if $T^{2}$ has a cyclic vector, then $T$ has a cyclic vector. Is the converse true?

5. Let $V$ be an $n$-dimensional vector space over the field $F$, and let $N$ be a nilpotent linear operator on $V$. Suppose $N^{n-1} \neq 0$, and let $\alpha$ be any vector in $V$ such that $N^{n-1} \alpha \neq 0$. Prove that $\alpha$ is a cyclic vector for $N$. What exactly is the matrix of $N$ in the ordered basis $\left\{\alpha, N \alpha, \ldots, N^{n-1} \alpha\right\}$ ?

6. Give a direct proof that if $A$ is the companion matrix of the monic polynomial $p$, then $p$ is the characteristic polynomial for $A$.

7. Let $V$ be an $n$-dimensional vector space, and let $T$ be a linear operator on $V$. Suppose that $T$ is diagonalizable.

(a) If $T$ has a cyclic vector, show that $T$ has $n$ distinct characteristic values.

(b) If $T$ has $n$ distinct characteristic values, and if $\left\{\alpha_{1}, \ldots, \alpha_{n}\right\}$ is a basis of characteristic vectors for $T$, show that $\alpha=\alpha_{1}+\cdots+\alpha_{n}$ is a cyclic vector for $T$.

8. Let $T$ be a linear operator on the finite-dimensional vector space $V$. Suppose $T$ has a cyclic vector. Prove that if $U$ is any linear operator which commutes with $T$, then $U$ is a polynomial in $T$.

\subsection{Cyclic Decompositions and the Rational Form}

The primary purpose of this section is to prove that if $T$ is any linear operator on a finite-dimensional space $V$, then there exist vectors $\alpha_{1}, \ldots, \alpha_{r}$ in $V$ such that

$$
V=Z\left(\alpha_{1} ; T\right) \oplus \cdots\left(\nexists\left(\alpha_{r} ; T\right)\right.
$$

In other words, we wish to prove that $V$ is a direct sum of $T$-cyclic subspaces. This will show that $T$ is the direct sum of a finite number of linear operators, each of which has a cyclic vector. The effect of this will be to reduce many questions about the general linear operator to similar questions about an operator which has a cyclic vector. The theorem which we prove (Theorem 3) is one of the deepest results in linear algebra and has many interesting corollaries.

The cyclic decomposition theorem is closely related to the following question. Which $T$-invariant subspaces $W$ have the property that there exists a $T$-invariant subspace $W^{\prime}$ such that $V=W \oplus W^{\prime}$ ? If $W$ is any subspace of a finite-dimensional space $V$, then there exists a subspace $W^{\prime}$ such that $V=W \oplus W^{\prime}$. Usually there are many such subspaces $W^{\prime}$ and each of these is called complementary to $W$. We are asking when a $T$ invariant subspace has a complementary subspace which is also invariant under $T$.

Let us suppose that $V=W \oplus W^{\prime}$ where both $W$ and $W^{\prime}$ are invariant under $T$ and then see what we can discover about the subspace $W$. Each vector $\beta$ in $V$ is of the form $\beta=\gamma+\gamma^{\prime}$ where $\gamma$ is in $W$ and $\gamma^{\prime}$ is in $W^{\prime}$. If $f$ is any polynomial over the scalar field, then

$$
f(T) \beta=f(T) \gamma+f(T) \gamma^{\prime} .
$$

Since $W$ and $W^{\prime}$ are invariant under $T$, the vector $f(T) \gamma$ is in $W$ and $f(T) \gamma^{\prime}$ is in $W^{\prime}$. Therefore $f(T) \beta$ is in $W$ if and only if $f(T) \gamma^{\prime}=0$. What interests us is the seemingly innocent fact that, if $f(T) \beta$ is in $W$, then $f(T) \beta=f(T) \gamma$.

Definition. Let $\mathrm{T}$ be a linear operator on a vector space $\mathrm{V}$ and let $\mathrm{W}$ be a subspace of $\mathrm{V}$. We say that $\mathrm{W}$ is T-admissible if

(i) $\mathrm{W}$ is invariant under $\mathrm{T}$;

(ii) if $\mathrm{f}(\mathrm{T}) \beta$ is in $\mathrm{W}$, there exists a vector $\gamma$ in $\mathrm{W}$ such that $\mathrm{f}(\mathrm{T}) \beta=\mathrm{f}(\mathrm{T}) \gamma$.

As we just showed, if $W$ is invariant and has a complementary invariant subspace, then $W$ is admissible. One of the consequences of Theorem 3 will be the converse, so that admissibility characterizes those invariant subspaces which have complementary invariant subspaces.

Let us indicate how the admissibility property is involved in the attempt to obtain a decomposition

$$
V=Z\left(\alpha_{1} ; T\right) \oplus \cdots \oplus Z\left(\alpha_{r} ; T\right) .
$$

Our basic method for arriving at such a decomposition will be to inductively select the vectors $\alpha_{1}, \ldots, \alpha_{r}$. Suppose that by some process or another we have selected $\alpha_{1}, \ldots, \alpha_{j}$ and the subspace

$$
W_{j}=Z\left(\alpha_{1} ; T\right)+\cdots+Z\left(\alpha_{j} ; T\right)
$$

is proper. We would like to find a non-zero vector $\alpha_{j+1}$ such that

$$
W_{j} \cap Z\left(\alpha_{j+1} ; T\right)=\{0\}
$$

because the subspace $W_{j+1}=W_{j} \oplus Z\left(\alpha_{j+1} ; T\right)$ would then come at least one dimension nearer to exhausting $V$. But, why should any such $\alpha_{j+1}$ exist? If $\alpha_{1}, \ldots, \alpha_{j}$ have been chosen so that $W_{j}$ is a $T$-admissible subspace, then it is rather easy to see that we can find a suitable $\alpha_{j+1}$. This is what will make our proof of Theorem 3 work, even if that is not how we phrase the argument.

Let $W$ be a proper $T$-invariant subspace. Let us try to find a non-zero vector $\alpha$ such that

$$
W \cap Z(\alpha ; T)=\{0\} .
$$

We can choose some vector $\beta$ which is not in $W$. Consider the $T$-conductor $S(\beta ; W)$, which consists of all polynomials $g$ such that $g(T) \beta$ is in $W$. Recall that the monic polynomial $f=s(\beta ; W)$ which generates the ideal $S(\beta ; W)$ is also called the $T$-conductor of $\beta$ into $W$. The vector $f(T) \beta$ is in $W$. Now, if $W$ is $T$-admissible, there is a $\gamma$ in $W$ with $f(T) \beta=f(T) \gamma$. Let $\alpha=\beta-\gamma$ and let $g$ be any polynomial. Since $\beta-\alpha$ is in $W, g(T) \beta$ will be in $W$ if and only if $g(T) \alpha$ is in $W$; in other words, $S(\alpha ; W)=S(\beta ; W)$. Thus the polynomial $f$ is also the $T$-conductor of $\alpha$ into $W$. But $f(T) \alpha=0$. That tells us that $g(T) \alpha$ is in $W$ if and only if $g(T) \alpha=0$, i.e., the subspaces $Z(\alpha ; T)$ and $W$ are independent (7-3) and $f$ is the $T$-annihilator of $\alpha$.

Theorem 3 (Cyclic Decomposition Theorem). Let T be a linear operator on a finite-dimensional vector space $\mathrm{V}$ and let $\mathrm{W}_{0}$ be a proper $\mathrm{T}$ admissible subspace of $\mathrm{V}$. There exist non-zero vectors $\alpha_{1}, \ldots, \alpha_{r}$ in $\mathrm{V}$ with respective T-annihilators $\mathrm{p}_{1}, \ldots, \mathrm{p}_{\mathrm{r}}$ such that

(i) $\mathrm{V}=\mathrm{W}_{0} \oplus \mathrm{Z}\left(\alpha_{1} ; \mathrm{T}\right) \oplus \cdots \oplus \mathrm{Z}\left(\alpha_{r} ; \mathrm{T}\right)$;

(ii) $\mathrm{p}_{\mathrm{k}}$ divides $\mathrm{p}_{\mathrm{k}-1}, \mathrm{k}=2, \ldots, \mathrm{r}$.

Furthermore, the integer $\mathrm{r}$ and the annihilators $\mathrm{p}_{1}, \ldots, \mathrm{p}_{\mathrm{r}}$ are uniquely determined by (i), (ii), and the fact that no $\alpha_{\mathrm{k}}$ is 0 .

Proof. The proof is rather long; hence, we shall divide it into four steps. For the first reading it may seem easier to take $W_{0}=\{0\}$, although it does not produce any substantial simplification. Throughout the proof, we shall abbreviate $f(T) \beta$ to $f \beta$.

Step 1. There exist non-zero vectors $\beta_{1}, \ldots, \beta_{\mathrm{r}}$ in $\mathrm{V}$ such that

(a) $\mathrm{V}=\mathrm{W}_{0}+\mathrm{Z}\left(\beta_{1} ; \mathrm{T}\right)+\cdots+\mathrm{Z}\left(\beta_{r} ; \mathrm{T}\right)$;

(b) if $1 \leq \mathrm{k} \leq \mathrm{r}$ and

$$
\mathrm{W}_{\mathrm{k}}=\mathrm{W}_{0}+\mathrm{Z}\left(\beta_{1} ; \mathrm{T}\right)+\cdots+\mathrm{Z}\left(\beta_{\mathrm{k}} ; \mathrm{T}\right)
$$

then the conductor $\mathrm{p}_{\mathrm{k}}=\mathrm{s}\left(\beta_{\mathrm{k}} ; \mathrm{W}_{\mathrm{k} \sim 1}\right)$ has maximum degree among all $\mathrm{T}$ conductors into the subspace $\mathrm{W}_{\mathrm{k}-1}$, i.e., for every $\mathrm{k}$

$$
\operatorname{deg} \mathrm{p}_{\mathrm{k}}=\max _{\alpha \text { in } V} \operatorname{deg} \mathrm{s}\left(\alpha ; \mathrm{W}_{\mathrm{k}-1}\right) .
$$

This step depends only upon the fact that $W_{0}$ is an invariant subspace. If $W$ is a proper $T$-invariant subspace, then

$$
0<\max _{\alpha} \operatorname{deg} s(\alpha ; W) \leq \operatorname{dim} V
$$

and we can choose a vector $\beta$ so that $\operatorname{deg} s(\beta ; W)$ attains that maximum. The subspace $W+Z(\beta ; T)$ is then $T$-invariant and has dimension larger than $\operatorname{dim} W$. Apply this process to $W=W_{0}$ to obtain $\beta_{1}$. If $W_{1}=W_{0}+$ $Z\left(\beta_{1} ; T\right)$ is still proper, then apply the process to $W_{1}$ to obtain $\beta_{2}$. Continue in that manner. Since $\operatorname{dim} W_{k}>\operatorname{dim} W_{k-1}$, we must reach $W_{r}=V$ in not more than $\operatorname{dim} V$ steps.

Step 2. Let $\beta_{1}, \ldots, \beta_{\mathrm{r}}$ be non-zero vectors which satisfy conditions (a) and (b) of Step 1. Fix k, $1 \leq \mathrm{k} \leq \mathrm{r}$. Let $\beta$ be any vector in $\mathrm{V}$ and let $\mathrm{f}=\mathrm{s}\left(\beta ; \mathrm{W}_{\mathrm{k}-1}\right)$. If

$$
\mathrm{f} \beta=\beta_{0}+\sum_{1 \leq i<k} \mathrm{~g}_{\mathrm{i}} \beta_{\mathrm{i}}, \quad \beta_{\mathbf{i}} i n \mathrm{~W}_{\mathbf{i}}
$$

then $\mathrm{f}$ divides each polynomial $\mathrm{g}_{\mathrm{i}}$ and $\beta_{0}=\mathrm{f} \gamma_{0}$, where $\gamma_{0}$ is in $\mathrm{W}_{0}$. If $k=1$, this is just the statement that $W_{0}$ is $T$-admissible. In order to prove the assertion for $k>1$, apply the division algorithm:

$$
g_{i}=f h_{i}+r_{i}, \quad r_{i}=0 \quad \text { or } \quad \operatorname{deg} r_{i}<\operatorname{deg} f .
$$

We wish to show that $r_{i}=0$ for each $i$. Let

$$
\gamma=\beta-\sum_{1}^{k-1} h_{i} \beta_{i} .
$$

Since $\gamma-\beta$ is in $W_{k-1}$,

Furthermore

$$
s\left(\gamma ; W_{k-1}\right)=s\left(\beta ; W_{k-1}\right)=f .
$$

$$
f \gamma=\beta_{0}+\sum_{1}^{k-1} r_{i} \beta_{i} .
$$

Suppose that some $r_{i}$ is different from 0 . We shall deduce a contradiction. Let $j$ be the largest index $i$ for which $r_{i} \neq 0$. Then

$$
f \gamma=\beta_{0}+\sum_{1}^{j} r_{i} \beta_{i}, \quad r_{j} \neq 0 \quad \text { and } \quad \operatorname{deg} r_{j}<\operatorname{deg} f .
$$

Let $p=s\left(\gamma ; W_{j-1}\right)$. Since $W_{k-1}$ contains $W_{j-1}$, the conductor $f=s\left(\gamma ; W_{k-1}\right)$ must divide $p$ :

$$
p=f g .
$$

Apply $g(T)$ to both sides of (7-7):

$$
p \gamma=g f \gamma=g r_{j} \beta_{j}+g \beta_{0}+\underset{1 \leq i<j}{\Sigma} g r_{i} \beta_{i} .
$$

By definition, $p \gamma$ is in $W_{j-1}$, and the last two terms on the right side of (7-8) are in $W_{j-1}$. Therefore, $g r_{j} \beta_{j}$ is in $W_{j-1}$. Now we use condition (b) of Step 1:

$$
\begin{aligned}
\operatorname{deg}\left(g r_{j}\right) & \geq \operatorname{deg} s\left(\beta_{j} ; W_{j-1}\right) \\
& =\operatorname{deg} p_{j} \\
& \geq \operatorname{deg} s\left(\gamma ; W_{j-1}\right) \\
& =\operatorname{deg} p \\
& =\operatorname{deg}(f g) .
\end{aligned}
$$

Thus $\operatorname{deg} r_{j} \geq \operatorname{deg} f$, and that contradicts the choice of $j$. We now know that $f$ divides each $g_{i}$ and hence that $\beta_{0}=f \gamma$. Since $W_{0}$ is $T$-admissible, $\beta_{0}=f \gamma_{0}$ where $\gamma_{0}$ is in $W_{0}$. We remark in passing that Step 2 is a strengthened form of the assertion that each of the subspaces $W_{1}, W_{2}, \ldots, W_{r}$ is T-admissible.

Step 3. There exist non-zero vectors $\alpha_{1}, \ldots, \alpha_{\mathrm{r}}$ in $\mathrm{V}$ which satisfy conditions (i) and (ii) of Theorem 3.

Start with vectors $\beta_{1}, \ldots, \beta_{r}$ as in Step 1 . Fix $k, 1 \leq k \leq r$. We apply Step 2 to the vector $\beta=\beta_{k}$ and the $T$-conductor $f=p_{k}$. We obtain

$$
p_{k} \beta_{k}=p_{k} \gamma_{0}+\sum_{1 \leq i<k} p_{k} h_{i} \beta_{i}
$$

where $\gamma_{0}$ is in $W_{0}$ and $h_{1}, \ldots, h_{k-1}$ are polynomials. Let

$$
\alpha_{k}=\beta_{k}-\gamma_{0}-\sum_{1 \leq i<k} h_{i} \beta_{i} .
$$

Since $\beta_{k}-\alpha_{k}$ is in $W_{k-1}$,

$$
s\left(\alpha_{k} ; W_{k-1}\right)=s\left(\beta_{k} ; W_{k-1}\right)=p_{k}
$$

and since $p_{k} \alpha_{k}=0$, we have

$$
W_{k-1} \cap Z\left(\alpha_{k} ; T\right)=\{0\} .
$$

Because each $\alpha_{k}$ satisfies (7-11) and (7-12), it follows that

$$
W_{k}=W_{0} \oplus Z\left(\alpha_{1} ; T\right) \oplus \cdots \oplus Z\left(\alpha_{k} ; T\right)
$$

and that $p_{k}$ is the $T$-annihilator of $\alpha_{k}$. In other words, the vectors $\alpha_{1}, \ldots, \alpha_{r}$ define the same sequence of subspaces $W_{1}, W_{2}, \ldots$ as do the vectors $\beta_{1}, \ldots, \beta_{r}$ and the $T$-conductors $p_{k}=s\left(\alpha_{k}, W_{k-1}\right)$ have the same maximality properties (condition (b) of Step 1). The vectors $\alpha_{1}, \ldots, \alpha_{r}$ have the additional property that the subspaces $W_{0}, Z\left(\alpha_{1} ; T\right), Z\left(\alpha_{2} ; T\right), \ldots$ are independent. It is therefore easy to verify condition (ii) in Theorem 3. Since $p_{i} \alpha_{i}=0$ for each $i$, we have the trivial relation

$$
p_{k} \alpha_{k}=0+p_{1} \alpha_{1}+\cdots+p_{k-1} \alpha_{k-1} .
$$

Apply Step 2 with $\beta_{1}, \ldots, \beta_{k}$ replaced by $\alpha_{1}, \ldots, \alpha_{k}$ and with $\beta=\alpha_{k}$. Conclusion: $p_{k}$ divides each $p_{i}$ with $i<k$.

Step 4. The number $\mathrm{r}$ and the polynomials $\mathrm{p}_{1}, \ldots, \mathrm{p}_{\mathrm{r}}$ are uniquely determined by the conditions of Theorem 3 .

Suppose that in addition to the vectors $\alpha_{1}, \ldots, \alpha_{r}$ in Theorem 3 we have non-zero vectors $\gamma_{1}, \ldots, \gamma_{s}$ with respective $T$-annihilators $g_{1}, \ldots, g_{s}$ such that

$$
\begin{aligned}
& V=W_{0} \oplus Z\left(\gamma_{1} ; T\right) \oplus \cdots \oplus Z\left(\gamma_{s} ; T\right) \\
& g_{k} \text { divides } g_{k-1}, \quad k=2, \ldots, s .
\end{aligned}
$$

We shall show that $r=s$ and $p_{i}=g_{i}$ for each $i$.

It is very easy to see that $p_{1}=g_{1}$. The polynomial $g_{1}$ is determined from (7-13) as the $T$-conductor of $V$ into $W_{0}$. Let $S\left(V ; W_{0}\right)$ be the collection of polynomials $f$ such that $f \beta$ is in $W_{0}$ for every $\beta$ in $V$, i.e., polynomials $f$ such that the range of $f(T)$ is contained in $W_{0}$. Then $S\left(V ; W_{0}\right)$ is a non-zero ideal in the polynomial algebra. The polynomial $g_{1}$ is the monic generator of that ideal, for this reason. $\operatorname{Each} \beta$ in $V$ has the form

and so

$$
\beta=\beta_{0}+f_{1} \gamma_{1}+\cdots+f_{8} \gamma_{8}
$$

$$
g_{1} \beta=g_{1} \beta_{0}+\sum_{1}^{8} g_{1} f_{i} \gamma_{i} .
$$

Since each $g_{i}$ divides $g_{1}$, we have $g_{1} \gamma_{i}=0$ for all $i$ and $g_{1} \beta=g_{1} \beta_{0}$ is in $W_{0}$. Thus $g_{1}$ is in $S\left(V ; W_{0}\right)$. Since $g_{1}$ is the monic polynomial of least degree which sends $\gamma_{1}$ into $W_{0}$, we see that $g_{1}$ is the monic polynomial of least degree in the ideal $S\left(V ; W_{0}\right)$. By the same argument, $p_{1}$ is the generator of that ideal, so $p_{1}=\boldsymbol{g}_{1}$.

If $f$ is a polynomial and $W$ is a subspace of $V$, we shall employ the shorthand $f W$ for the set of all vectors $f \alpha$ with $\alpha$ in $W$. We have left to the exercises the proofs of the following three facts.

1. $f Z(\alpha ; T)=Z(f \alpha ; T)$.

2. If $V=V_{1} \oplus \cdots \oplus V_{k}$, where each $V_{\boldsymbol{i}}$ is invariant under $T$, then $f V=f V_{1} \oplus \cdots \oplus f V_{k}$.

3. If $\alpha$ and $\gamma$ have the same $T$-annihilator, then $f \alpha$ and $f \gamma$ have the same $T$-annihilator and (therefore)

$$
\operatorname{dim} Z(f \alpha ; T)=\operatorname{dim} Z(f \gamma ; T) .
$$

Now, we proceed by induction to show that $r=s$ and $p_{i}=g_{i}$ for $i=2, \ldots, r$. The argument consists of counting dimensions in the right way. We shall give the proof that if $r \geq 2$ then $p_{2}=g_{2}$, and from that the induction should be clear. Suppose that $r \geq 2$. Then

$$
\operatorname{dim} W_{0}+\operatorname{dim} Z\left(\alpha_{1} ; T\right)<\operatorname{dim} V .
$$

Since we know that $p_{1}=g_{1}$, we know that $Z\left(\alpha_{1} ; T\right)$ and $Z\left(\gamma_{1} ; T\right)$ have the same dimension. Therefore,

$$
\operatorname{dim} W_{0}+\operatorname{dim} Z\left(\gamma_{1} ; T\right)<\operatorname{dim} V
$$

which shows that $s \geq 2$. Now it makes sense to ask whether or not $p_{2}=g_{2}$. From the two decompositions of $V$, we obtain two decompositions of the subspace $p_{2} V$ :

$$
\begin{aligned}
& p_{2} V=p_{2} W_{0} \oplus Z\left(p_{2} \alpha_{1} ; T\right) \\
& p_{2} V=p_{2} W_{0} \oplus Z\left(p_{2} \gamma_{1} ; T\right) \oplus \cdots \oplus Z\left(p_{2} \gamma_{s} ; T\right)
\end{aligned}
$$

We have made use of facts (1) and (2) above and we have used the fact that $p_{2} \alpha_{i}=0, i \geq 2$. Since we know that $p_{1}=g_{1}$, fact (3) above tells us that $Z\left(p_{2} \alpha_{1} ; T\right)$ and $Z\left(p_{2} \gamma_{1} ; T\right)$ have the same dimension. Hence, it is apparent from (7-14) that

$$
\operatorname{dim} Z\left(p_{2} \gamma_{i} ; T\right)=0, \quad i \geq 2 .
$$

We conclude that $p_{2} \gamma_{2}=0$ and $g_{2}$ divides $p_{2}$. The argument can be reversed to show that $p_{2}$ divides $g_{2}$. Therefore $p_{2}=g_{2}$.

Corollary. If $\mathrm{T}$ is a linear operator on a finite-dimensional vector space, then every T-admissible subspace has a complementary subspace which is also invariant under $\mathrm{T}$.

Proof. Let $W_{0}$ be an admissible subspace of $V$. If $W_{0}=V$, the complement we seek is $\{0\}$. If $W_{0}$ is proper, apply Theorem 3 and let

$$
W_{\bullet}^{\prime}=Z\left(\alpha_{1} ; T\right) \oplus \cdots \oplus Z\left(\alpha_{r} ; T\right) .
$$

Then $W_{0}^{\prime}$ is invariant under $T$ and $V=W_{0} \oplus W^{\prime}$. Corollary. Let $\mathrm{T}$ be a linear operator on a finite-dimensional vector space $\mathrm{V}$.

(a) There exists a vector $\alpha$ in $\mathrm{V}$ such that the T-annihilator of $\alpha$ is the minimal polynomial for $\mathrm{T}$.

(b) $\mathrm{T}$ has a cyclic vector if and only if the characteristic and minimal polynomials for $\mathrm{T}$ are identical.

Proof. If $V=\{0\}$, the results are trivially true. If $V \neq\{0\}$, let where the $T$-annihilators $p_{1}, \ldots, p_{r}$ are such that $p_{k+1}$ divides $p_{k}, 1 \leq k \leq$ $r-1$. As we noted in the proof of Theorem 3 , it follows easily that $p_{1}$ is the minimal polynomial for $T$, i.e., the $T$-conductor of $V$ into $\{0\}$. We have proved (a).

We saw in Section $7.1$ that, if $T$ has a cyclic vector, the minimal polynomial for $T$ coincides with the characteristic polynomial. The contents of (b) is in the converse. Choose any $\alpha$ as in (a). If the degree of the minimal polynomial is $\operatorname{dim} V$, then $V=Z(\alpha ; T)$.

Theorem 4 (Generalized Cayley-Hamilton Theorem). Let T be a linear operator on a finite-dimensional vector space $\mathrm{V}$. Let $\mathrm{p}$ and $\mathrm{f}$ be the minimal and characteristic polynomials for $\mathrm{T}$, respectively.

(i) $\mathrm{p}$ divides $\mathrm{f}$.

(ii) $\mathrm{p}$ and $\mathrm{f}$ have the same prime factors, except for multiplicities.

(iii) If

$$
\mathrm{p}=\mathrm{f}_{\mathrm{1}}^{\mathrm{r}_{1} \cdots} \cdots \mathrm{f}_{\mathrm{k}}^{\mathrm{f}_{\mathrm{k}}}
$$

is the prime factorization of $\mathrm{p}$, then

$$
\mathrm{f}=\mathrm{f}_{1}^{\mathrm{d}_{1}} \cdots \mathrm{f}_{\mathrm{k}}^{\mathrm{d}_{\mathrm{k}}}
$$

where $\mathrm{d}_{\mathrm{i}}$ is the nullity of $\mathrm{f}_{\mathrm{i}}(\mathrm{T})^{\mathrm{r}}$ divided by the degree of $\mathrm{f}_{\mathrm{i}}$.

Proof. We disregard the trivial case $V=\{0\}$. To prove (i) and (ii), consider a cyclic decomposition (7-15) of $V$ obtained from Theorem 3. As we noted in the proof of the second corollary, $p_{1}=p$. Let $U_{i}$ be the restriction of $T$ to $Z\left(\alpha_{i} ; T\right)$. Then $U_{i}$ has a cyclic vector and so $p_{i}$ is both the minimal polynomial and the characteristic polynomial for $U_{i}$. Therefore, the characteristic polynomial $f$ is the product $f=p_{1} \cdots p_{r}$. That is evident from the block form (6-14) which the matrix of $T$ assumes in a suitable basis. Clearly $p_{1}=p$ divides $f$, and this proves (i). Obviously any prime divisor of $p$ is a prime divisor of $f$. Conversely, a prime divisor of $f=p_{1} \cdots p_{r}$ must divide one of the factors $p_{i}$, which in turn divides $p_{1}$. Let (7-16) be the prime factorization of $p$. We employ the primary decomposition theorem (Theorem 12 of Chapter 6). It tells us that, if $V_{i}$ is the null space of $f_{i}(T)^{r_{i}}$, then 

$$
V=V_{1} \oplus \cdots \oplus V_{k}
$$

and $f_{i}^{f_{i}}$ is the minimal polynomial of the operator $T_{i}$, obtained by restricting $T$ to the (invariant) subspace $V_{i}$. Apply part (ii) of the present theorem to the operator $T_{i}$. Since its minimal polynomial is a power of the prime $f_{i}$, the characteristic polynomial for $T_{i}$ has the form $f_{i}^{t_{i}}$, where $d_{i} \geq r_{i}$. Obviously

$$
d_{i}=\frac{\operatorname{dim} V_{i}}{\operatorname{deg} f_{i}}
$$

and (almost by definition) $\operatorname{dim} V_{i}=$ nullity $f_{i}(T)^{r_{i}}$. Since $T$ is the direct sum of the operators $T_{1}, \ldots, T_{k}$, the characteristic polynomial $f$ is the product

$$
f=f_{1}^{d_{1}} \cdots f_{k}^{d_{k}}
$$

Corollary. If $\mathrm{T}$ is a nilpotent linear operator on a vector space of dimension $\mathrm{n}$, then the characteristic polynomial for $\mathrm{T}$ is $\mathrm{x}^{\mathrm{n}}$.

Now let us look at the matrix analogue of the cyclic decomposition theorem. If we have the operator $T$ and the direct-sum decomposition of Theorem 3 , let $B_{i}$ be the 'cyclic ordered basis'

$$
\left\{\alpha_{i}, T \alpha_{i}, \ldots, T^{k_{i}-1} \alpha_{i}\right\}
$$

for $Z\left(\alpha_{i} ; T\right)$. Here $k_{i}$ denotes the dimension of $Z\left(\alpha_{i} ; T\right)$, that is, the degree of the annihilator $p_{i}$. The matrix of the induced operator $T_{i}$ in the ordered basis $\Theta_{i}$ is the companion matrix of the polynomial $p_{i}$. Thus, if we let $B$ be the ordered basis for $V$ which is the union of the $B_{i}$ arranged in the order $Q_{1}, \ldots, Q_{r}$, then the matrix of $T$ in the ordered basis $Q$ will be

$$
A=\left[\begin{array}{llll}
A_{1} & 0 & \cdots & 0 \\
0 & A_{2} & \cdots & 0 \\
\vdots & \vdots & & \vdots \\
0 & 0 & \cdots & A_{r}
\end{array}\right]
$$

where $A_{i}$ is the $k_{i} \times k_{i}$ companion matrix of $p_{i}$. An $n \times n$ matrix $A$, which is the direct sum (7-19) of companion matrices of non-scalar monic polynomials $p_{1}, \ldots, p_{r}$ such that $p_{i+1}$ divides $p_{i}$ for $i=1, \ldots, r-1$, will be said to be in rational form. The cyclic decomposition theorem tells us the following concerning matrices.

Theorem 5. Let $\mathrm{F}$ be a field and let $\mathrm{B}$ be an $\mathrm{n} \times \mathrm{n}$ matrix over $\mathrm{F}$. Then $\mathrm{B}$ is similar over the field $\mathrm{F}$ to one and only one matrix which is in rational form.

Proof. Let $T$ ' be the linear operator on $F^{n}$ which is represented by $B$ in the standard ordered basis. As we have just observed, there is some ordered basis for $F^{n}$ in which $T$ is represented by a matrix $A$ in rational form. Then $B$ is similar to this matrix $A$. Suppose $B$ is similar over $F$ to another matrix $C$ which is in rational form. This means simply that there is some ordered basis for $F^{n}$ in which the operator $T$ is represented by the matrix $C$. If $C$ is the direct sum of companion matrices $C_{i}$ of monic polynomials $g_{1}, \ldots, g_{s}$ such that $g_{i+1}$ divides $g_{i}$ for $i=1, \ldots, s-1$, then it is apparent that we shall have non-zero vectors $\beta_{1}, \ldots, \beta_{s}$ in $V$ with $T$ annihilators $g_{1}, \ldots, g_{s}$ such that

$$
V=Z\left(\beta_{1} ; T\right) \oplus \cdots \oplus Z\left(\boldsymbol{\beta}_{s} ; T\right) .
$$

But then by the uniqueness statement in the cyclic decomposition theorem, the polynomials $g_{i}$ are identical with the polynomials $p_{i}$ which define the matrix $A$. Thus $C=A$.

The polynomials $p_{1}, \ldots, p_{r}$ are called the invariant factors for the matrix B. In Section 7.4, we shall describe an algorithm for calculating the invariant factors of a given matrix $B$. The fact that it is possible to compute these polynomials by means of a finite number of rational operations on the entries of $B$ is what gives the rational form its name.

Example 2. Suppose that $V$ is a two-dimensional vector space over the field $F$ and $T$ is a linear operator on $V$. The possibilities for the cyclic subspace decomposition for $T$ are very limited. For, if the minimal polynomial for $T$ has degree 2 , it is equal to the characteristic polynomial for $T$ ' and $T$ has a cyclic vector. Thus there is some ordered basis for $V$ in which $T$ is represented by the companion matrix of its characteristic polynomial. If, on the other hand, the minimal polynomial for $T$ has degree 1 , then $T$ is a scalar multiple of the identity operator. If $T=c I$, then for any two linear independent vectors $\alpha_{1}$ and $\alpha_{2}$ in $V$ we have

$$
\begin{aligned}
V & =Z\left(\alpha_{1} ; T\right) \oplus Z\left(\alpha_{2} ; T\right) \\
p_{1} & =p_{2}=x-c .
\end{aligned}
$$

For matrices, this analysis says that every $2 \times 2$ matrix over the field $F$ is similar over $F$ to exactly one matrix of the types

$$
\left[\begin{array}{ll}
c & 0 \\
0 & c
\end{array}\right], \quad\left[\begin{array}{ll}
0 & -c_{0} \\
1 & -c_{1}
\end{array}\right] .
$$

Example 3 . Let $T$ be the linear operator on $R^{3}$ which is represented by the matrix

$$
A=\left[\begin{array}{rrr}
5 & -6 & -6 \\
-1 & 4 & 2 \\
3 & -6 & -4
\end{array}\right]
$$

in the standard ordered basis. We have computed earlier that the characteristic polynomial for $T$ is $f=(x-1)(x-2)^{2}$ and the minimal polynomial for $T$ is $p=(x-1)(x-2)$. Thus we know that in the cyclic decomposition for $T$ the first vector $\alpha_{1}$ will have $p$ as its $T$-annihilator. Since we are operating in a three-dimensional space, there can be only one further vector, $\alpha_{2}$. It must generate a cyclic subspace of dimension 1, i.e., it must be a characteristic vector for $T$. Its $T$-annihilator $p_{2}$ must be $(x-2)$, because we must have $p p_{2}=f$. Notice that this tells us immediately that the matrix $A$ is similar to the matrix

$$
B=\left[\begin{array}{rrr}
0 & -2 & 0 \\
1 & 3 & 0 \\
0 & 0 & 2
\end{array}\right]
$$

that is, that $T$ is represented by $B$ in some ordered basis. How can we find suitable vectors $\alpha_{1}$ and $\alpha_{2}$ ? Well, we know that any vector which generates a $T$-cyclic subspace of dimension 2 is a suitable $\boldsymbol{\alpha}_{1}$. So let's just try $\epsilon_{1}$. We have

$$
T \epsilon_{1}=(5,-1,3)
$$

which is not a scalar multiple of $\epsilon_{1}$; hence $Z\left(\epsilon_{1} ; T\right)$ has dimension 2. This space consists of all vectors $a_{\epsilon_{1}}+b\left(T \epsilon_{1}\right)$ :

$$
a(1,0,0)+b(5,-1,3)=(a+5 b,-b, 3 b)
$$

or, all vectors $\left(x_{1}, x_{2}, x_{3}\right)$ satisfying $x_{3}=-3 x_{2}$. Now what we want is a vector $\alpha_{2}$ such that $T^{\prime} \alpha_{2}=2 \alpha_{2}$ and $Z\left(\alpha_{2} ; T\right)$ is disjoint from $Z\left(\epsilon_{1} ; T\right)$. Since $\alpha_{2}$ is to be a characteristic vector for $T$, the space $Z\left(\alpha_{2} ; T\right)$ will simply be the one-dimensional space spanned by $\alpha_{2}$, and so what we require is that $\alpha_{2}$ not be in $Z\left(\epsilon_{1} ; T\right)$. If $\alpha=\left(x_{1}, x_{2}, x_{3}\right)$, one can easily compute that $T \alpha=2 \alpha$ if and only if $x_{1}=2 x_{2}+2 x_{3}$. Thus $\alpha_{2}=(2,1,0)$ satisfies $T \alpha_{2}=$ $2 \alpha_{2}$ and generates a $T$-cyclic subspace disjoint from $Z\left(\epsilon_{1} ; T\right)$. The reader should verify directly that the matrix of $T$ in the ordered basis

$$
\{(1,0,0),(5,-1,3),(2,1,0)\}
$$

is the matrix $B$ above.

Example 4. Suppose that $T$ is a diagonalizable linear operator on $V$. It is interesting to relate a cyclic decomposition for $T$ to a basis which diagonalizes the matrix of $T$. Let $c_{1}, \ldots, c_{k}$ be the distinct characteristic values of $T$ and let $V_{i}$ be the space of characteristic vectors associated with the characteristic value $c_{i}$. Then

$$
V=V_{1} \oplus \cdots \oplus V_{k}
$$

and if $d_{i}=\operatorname{dim} V_{i}$ then

$$
f=\left(x-c_{1}\right)^{d_{1}} \cdots\left(x-c_{k}\right)^{d_{k}}
$$

is the characteristic polynomial for $T$. If $\alpha$ is a vector in $V$, it is easy to relate the cyclic subspace $Z(\alpha ; T)$ to the subspaces $V_{1}, \ldots, V_{k}$. There are unique vectors $\beta_{1}, \ldots, \beta_{k}$ such that $\beta_{i}$ is in $V_{i}$ and

$$
\alpha=\beta_{1}+\cdots+\beta_{k} .
$$

Since $T \beta_{i}=c_{i} \beta_{i}$, we have

$$
f(T) \alpha=f\left(c_{1}\right) \beta_{1}+\cdots+f\left(c_{k}\right) \beta_{k}
$$

for every polynomial $f$. Given any scalars $t_{1}, \ldots, t_{k}$ there exists a polynomial $f$ such that $f\left(c_{i}\right)=t_{i}, 1 \leq i \leq k$. Therefore, $Z(\alpha ; T)$ is just the subspace spanned by the vectors $\beta_{1}, \ldots, \beta_{k}$. What is the annihilator of $\alpha$ ? According to (7-20), we have $f(T) \alpha=0$ if and only if $f\left(c_{i}\right) \beta_{i}=0$ for each $i$. In other words, $f(T) \alpha=0$ provided $f\left(c_{i}\right)=0$ for each $i$ such that $\beta_{i} \neq 0$. Accordingly, the annihilator of $\alpha$ is the product

$$
\operatorname{II}_{\beta_{i} \neq 0}\left(x-c_{i}\right) \text {. }
$$

Now, let $B_{i}=\left\{\beta_{1}^{t}, \ldots, \beta_{d_{i}}^{i}\right\}$ be an ordered basis for $V_{i}$. Let

$$
r=\max _{i} d_{i} .
$$

We define vectors $\alpha_{1}, \ldots, \alpha_{r}$ by

$$
\alpha_{j}=\sum_{d_{i} \geq j} \beta_{j}^{i}, \quad 1 \leq j \leq r .
$$

The cyclic subspace $Z\left(\alpha_{j} ; T\right)$ is the subspace spanned by the vectors $\beta_{j}^{i}$, as $i$ runs over those indices for which $d_{i} \geq j$. The $T$-annihilator of $\alpha_{j}$ is

We have

$$
p_{j}=\prod_{d_{i} \geq j}\left(x-c_{i}\right) \text {. }
$$

$$
V=Z\left(\alpha_{1} ; T\right) \oplus \cdots \oplus Z\left(\alpha_{r} ; T\right)
$$

because each $\beta_{j}^{i}$ belongs to one and only one of the subspaces $Z\left(\alpha_{1} ; T\right), \ldots$, $Z\left(\alpha_{r} ; T\right)$ and $B=\left(\mathcal{B}_{1}, \ldots, B_{k}\right)$ is a basis for $V$. By $(7-23), p_{j+1}$ divides $p_{j}$.

\section{Exercises}

1. Let $T$ be the linear operator on $F^{2}$ which is represented in the standard ordered basis by the matrix

$$
\left[\begin{array}{ll}
0 & 0 \\
1 & 0
\end{array}\right] \text {. }
$$

Let $\alpha_{1}=(0,1)$. Show that $F^{2} \neq Z\left(\alpha_{1} ; T\right)$ and that there is no non-zero vector $\alpha_{2}$ in $F^{2}$ with $Z\left(\alpha_{2} ; T\right)$ disjoint from $Z\left(\alpha_{1} ; T\right)$.

2. Let $T$ be a linear operator on the finite-dimensional space $V$, and let $R$ be the range of $T$.

(a) Prove that $R$ has a complementary $T$-invariant subspace if and only if $R$ is independent of the null space $N$ of $T$.

(b) If $R$ and $N$ are independent, prove that $N$ is the unique $T$-invariant subspace complementary to $R$.

3. Let $T$ be the linear operator on $R^{3}$ which is represented in the standard ordered basis by the matrix

$$
\left[\begin{array}{lll}
2 & 0 & 0 \\
1 & 2 & 0 \\
0 & 0 & 3
\end{array}\right] \text {. }
$$

Let $W$ be the null space of $T-2 I$. Prove that $W$ has no complementary $T$-invariant subspace. (Hint: Let $\beta=\epsilon_{1}$ and observe that $(T-2 I) \boldsymbol{\beta}$ is in $W$. Prove there is no $\alpha$ in $W$ with $(T-2 I) \beta=(T-2 I) \alpha$.)

4. Let $T$ be the linear operator on $F^{4}$ which is represented in the standard ordered basis by the matrix

$$
\left[\begin{array}{llll}
c & 0 & 0 & 0 \\
1 & c & 0 & 0 \\
0 & 1 & c & 0 \\
0 & 0 & 1 & c
\end{array}\right] .
$$

Let $W$ be the null space of $T-c I$.

(a) Prove that $W$ is the subspace spanned by $\epsilon_{4}$.

(b) Find the monic generators of the ideals $S\left(\epsilon_{4} ; W\right), S\left(\epsilon_{3} ; W\right), S\left(\epsilon_{2} ; W\right)$, $S\left(\epsilon_{1} ; W\right)$

5. Let $T$ be a linear operator on the vector space $V$ over the field $F$. If $f$ is a polynomial over $F$ and $\alpha$ is in $V$, let $f \alpha=f(T) \alpha$. If $V_{1}, \ldots, V_{k}$ are $T$-invariant subspaces and $V=V_{1} \oplus \cdots \oplus V_{k}$, show that

$$
f V=f V_{1} \oplus \cdots \oplus f V_{k} .
$$

6. Let $T, V$, and $F$ be as in Exercise 5. Suppose $\alpha$ and $\beta$ are vectors in $V$ which have the same $T$-annihilator. Prove that, for any polynomial $f$, the vectors $f \alpha$ and $f \beta$ have the same $T$-annihilator.

7. Find the minimal polynomials and the rational forms of each of the following real matrices.

$$
\left[\begin{array}{rrr}
0 & -1 & -1 \\
1 & 0 & 0 \\
-1 & 0 & 0
\end{array}\right], \quad\left[\begin{array}{rrr}
c & 0 & -1 \\
0 & c & 1 \\
-1 & 1 & c
\end{array}\right], \quad\left[\begin{array}{rr}
\cos \theta & \sin \theta \\
-\sin \theta & \cos \theta
\end{array}\right] .
$$

8. Let $T$ be the linear operator on $R^{3}$ which is represented in the standard ordered basis by

$$
\left[\begin{array}{rrr}
3 & -4 & -4 \\
-1 & 3 & 2 \\
2 & -4 & -3
\end{array}\right] \text {. }
$$

Find non-zero vectors $\alpha_{1}, \ldots, \alpha_{r}$ satisfying the conditions of Theorem 3 .

9. Let $A$ be the real matrix

$$
A=\left[\begin{array}{rrr}
1 & 3 & 3 \\
3 & 1 & 3 \\
-3 & -3 & -5
\end{array}\right]
$$

Find an invertible $3 \times 3$ real matrix $P$ such that $P^{-1} A P$ is in rational form.

10. Let $F$ be a subfield of the complex numbers and let $T$ be the linear operator on $F^{4}$ which is represented in the standard ordered basis by the matrix

$$
\left[\begin{array}{llll}
2 & 0 & 0 & 0 \\
1 & 2 & 0 & 0 \\
0 & a & 2 & 0 \\
0 & 0 & b & 2
\end{array}\right] .
$$

Find the characteristic polynomial for $T$. Consider the cases $a=b=1 ; a=b=0$; $a=0, b=1$. In each of these cases, find the minimal polynomial for $T$ and nonzero vectors $\alpha_{1}, \ldots, \alpha_{r}$ which satisfy the conditions of Theorem 3.

11. Prove that if $A$ and $B$ are $3 \times 3$ matrices over the field $F$, a necessary and sufficient condition that $A$ and $B$ be similar over $F$ is that they have the same characteristic polynomial and the same minimal polynomial. Give an example which shows that this is false for $4 \times 4$ matrices.

12. Let $F$ be a subfield of the field of complex numbers, and let $A$ and $B$ be $n \times n$ matrices over $F$. Prove that if $A$ and $B$ are similar over the field of complex numbers, then they are similar over $F$. (Hint: Prove that the rational form of $A$ is the same whether $A$ is viewed as a matrix over $F$ or a matrix over $C$; likewise for $B$.)

13. Let $A$ be an $n \times n$ matrix with complex entries. Prove that if every characteristic value of $A$ is real, then $A$ is similar to a matrix with real entries.

14. Let $T$ be a linear operator on the finite-dimensional space $V$. Prove that there exists a vector $\alpha$ in $V$ with this property. If $f$ is a polynomial and $f(T) \alpha=0$, then $f(T)=0$. (Such a vector $\alpha$ is called a separating vector for the algebra of polynomials in $T$.) When $T$ has a cyclic vector, give a direct proof that any cyclic vector is a separating vector for the algebra of polynomials in $T$.

15. Let $F$ be a subfield of the field of complex numbers, and let $A$ be an $n \times n$ matrix over $F$. Let $p$ be the minimal polynomial for $A$. If we regard $A$ as a matrix over $C$, then $A$ has a minimal polynomial $f$ as an $n \times n$ matrix over $C$. Use a theorem on linear equations to prove $\boldsymbol{p}=f$. Can you also see how this follows from the cyclic decomposition theorem?

16. Let $A$ be an $n \times n$ matrix with real entries such that $A^{2}+I=0$. Prove that $n$ is even, and if $n=2 k$, then $A$ is similar over the field of real numbers to a matrix of the block form

$$
\left[\begin{array}{rr}
0 & -I \\
I & 0
\end{array}\right]
$$

where $I$ is the $k \times k$ identity matrix.

17. Let $T$ be a linear operator on a finite-dimensional vector space $V$. Suppose that

(a) the minimal polynomial for $T$ is a power of an irreducible polynomial;

(b) the minimal polynomial is equal to the characteristic polynomial.

Show that no non-trivial $T$-invariant subspace has a complementary $T$-invariant subspace.

18. If $T$ is a diagonalizable linear operator, then every $T$-invariant subspace has a complementary $T$-invariant subspace.

19. Let $T$ be a linear operator on the finite-dimensional space $V$. Prove that $T$ has a cyclic vector if and only if the following is true: Every linear operator $U$ which commutes with $T$ is a polynomial in $T$.

20. Let $V$ be a finite-dimensional vector space over the field $F$, and let $T$ be a linear operator on $V$. We ask when it is true that every non-zero vector in $V$ is a cyclic vector for $T$. Prove that this is the case if and only if the characteristic polynomial for $T$ is irreducible over $F$. 21. Let $A$ be an $n \times n$ matrix with real entries. Let $T$ be the linear operator on $R^{n}$ which is represented by $A$ in the standard ordered basis, and let $U$ be the linear operator on $C^{n}$ which is represented by $A$ in the standard ordered basis. Use the result of Exercise 20 to prove the following: If the only subspaces invariant under $T$ are $R^{n}$ and the zero subspace, then $U$ is diagonalizable.

\subsection{The Jordan Form}

Suppose that $N$ is a nilpotent linear operator on the finite-dimensional space $V$. Let us look at the cyclic decomposition for $N$ which we obtain from Theorem 3. We have a positive integer $r$ and $r$ non-zero vectors $\alpha_{1}, \ldots, \alpha_{r}$ in $V$ with $N$-annihilators $p_{1}, \ldots, p_{r}$, such that

$$
V=Z\left(\alpha_{1} ; N\right) \oplus \cdots \oplus Z\left(\alpha_{r} ; N\right)
$$

and $p_{i+1}$ divides $p_{i}$ for $i=1, \ldots, r-1$. Since $N$ is nilpotent, the minimal polynomial is $x^{k}$ for some $k \leq n$. Thus each $p_{i}$ is of the form $p_{i}=x^{k_{i}}$, and the divisibility condition simply says that

$$
k_{1} \geq k_{2} \geq \cdots \geq k_{r}
$$

Of course, $k_{1}=k$ and $k_{r} \geq 1$. The companion matrix of $x^{k_{i}}$ is the $k_{i} \times k_{i}$ matrix

$$
A_{i}=\left[\begin{array}{ccccc}
0 & 0 & \cdots & 0 & 0 \\
1 & 0 & \cdots & 0 & 0 \\
0 & 1 & \cdots & 0 & 0 \\
\vdots & \vdots & & \vdots & \vdots \\
0 & 0 & \cdots & 1 & 0
\end{array}\right] .
$$

Thus Theorem 3 gives us an ordered basis for $V$ in which the matrix of $N$ is the direct sum of the elementary nilpotent matrices (7-24), the sizes of which decrease as $i$ increases. One sees from this that associated with a nilpotent $n \times n$ matrix is a positive integer $r$ and $r$ positive integers $k_{1}, \ldots, k_{r}$ such that $k_{1}+\cdots+k_{r}=n$ and $k_{i} \geq k_{i+1}$, and these positive integers determine the rational form of the matrix, i.e., determine the matrix up to similarity.

Here is one thing we should like to point out about the nilpotent operator $N$ above. The positive integer $r$ is precisely the nullity of $N$; in fact, the null space has as a basis the $r$ vectors

$$
N^{k_{i}-1} \alpha_{i} \text {. }
$$

For, let $\alpha$ be in the null space of $N$. We write $\alpha$ in the form

$$
\alpha=f_{1} \alpha_{1}+\cdots+f_{r} \alpha_{r}
$$

where $f_{i}$ is a polynomial, the degree of which we may assume is less than $k_{i}$. Since $N \alpha=0$, for each $i$ we have 

$$
\begin{aligned}
0 & =N\left(f_{i} \alpha_{i}\right) \\
& =N f_{i}(N) \alpha_{i} \\
& =\left(x f_{i}\right) \alpha_{i} .
\end{aligned}
$$

Thus $x f_{i}$ is divisible by $x^{k_{i}}$, and since $\operatorname{deg}\left(f_{i}\right)>k_{i}$ this means that

$$
f_{i}=c_{i} x^{k_{i}-1}
$$

where $c_{i}$ is some scalar. But then

$$
\alpha=c_{1}\left(x^{k_{1}-1} \alpha_{1}\right)+\cdots+c_{r}\left(x^{k_{r}-1} \alpha_{r}\right)
$$

which shows us that the vectors (7-25) form a basis for the null space of $N$. The reader should note that this fact is also quite clear from the matrix point of view.

Now what we wish to do is to combine our findings about nilpotent operators or matrices with the primary decomposition theorem of Chapter 6. The situation is this: Suppose that $T$ is a linear operator on $V$ and that, the characteristic polynomial for $T$ factors over $F$ as follows:

$$
f=\left(x-c_{1}\right)^{d_{1}} \cdots\left(x-c_{k}\right)^{d_{k}}
$$

where $c_{1}, \ldots, c_{k}$ are distinct elements of $F$ and $d_{i} \geq 1$. Then the minimal polynomial for $T$ will be

$$
p=\left(x-c_{1}\right)^{r_{1}} \cdots\left(x-c_{k}\right)^{r_{k}}
$$

where $1 \leq r_{i} \leq d_{i}$. If $W_{i}$ is the null space of $\left(T-c_{i} I\right)^{r_{i}}$, then the primary decomposition theorem tells us that

$$
V=W_{1} \oplus \cdots \oplus W_{k}
$$

and that the operator $T_{i}$ induced on $W_{i}$ by $T$ has minimal polynomial $\left(x-c_{i}\right)^{r_{i}}$. Let $N_{i}$ be the linear operator on $W_{i}$ defined by $N_{i}=T_{i}-c_{i} I$. Then $N_{i}$ is nilpotent and has minimal polynomial $x^{r_{i}}$. On $W_{i}, T$ acts like $N_{i}$ plus the scalar $c_{i}$ times the identity operator. Suppose we choose a basis for the subspace $W_{i}$ corresponding to the cyclic decomposition for the nilpotent operator $N_{i}$. Then the matrix of $T_{i}$ in this ordered basis will be the direct sum of matrices

$$
\left[\begin{array}{ccccc}
c & 0 & \cdots & 0 & 0 \\
1 & c & \cdots & 0 & 0 \\
\vdots & \vdots & & \vdots & \vdots \\
& & & c & \\
0 & 0 & \cdots & 1 & c
\end{array}\right]
$$

each with $c=c_{i}$. Furthermore, the sizes of these matrices will decrease as one reads from left to right. A matrix of the form (7-26) is called an elementary Jordan matrix with characteristic value $c$. Now if we put all the bases for the $W_{i}$ together, we obtain an ordered basis for $V$. Let us describe the matrix $A$ of $T$ in this ordered basis. The matrix $A$ is the direct sum

$$
A=\left[\begin{array}{llll}
A_{1} & 0 & \cdots & 0 \\
0 & A_{2} & \cdots & 0 \\
\vdots & \vdots & & \vdots \\
0 & 0 & \cdots & A_{k}
\end{array}\right]
$$

of matrices $A_{1}, \ldots, A_{k}$. Each $A_{i}$ is of the form

$$
A_{i}=\left[\begin{array}{llll}
J_{1}^{(i)} & 0 & \cdots & 0 \\
0 & J_{2}^{(i)} & \cdots & 0 \\
\vdots & \vdots & & \vdots \\
0 & 0 & \cdots & J_{n_{i}}^{(i)}
\end{array}\right]
$$

where each $J_{j}^{(i)}$ is an elementary Jordan matrix with characteristic value $c_{i}$. Also, within each $A_{i}$, the sizes of the matrices $J_{j}^{(i)}$ decrease as $j$ increases. An $n \times n$ matrix $A$ which satisfies all the conditions described so far in this paragraph (for some distinct scalars $c_{1}, \ldots, c_{k}$ ) will be said to be in Jordan form.

We have just pointed out that if $T$ is a linear operator for which the characteristic polynomial factors completely over the scalar field, then there is an ordered basis for $V$ in which $T$ is represented by a matrix which is in Jordan form. We should like to show now that this matrix is something uniquely associated with $T$, up to the order in which the characteristic values of $T$ are written down. In other words, if two matrices are in Jordan form and they are similar, then they can differ only in that the order of the scalars $c_{i}$ is different.

The uniqueness we see as follows. Suppose there is some ordered basis for $V$ in which $T$ is represented by the Jordan matrix $A$ described in the previous paragraph. If $A_{i}$ is a $d_{i} \times d_{i}$ matrix, then $d_{i}$ is clearly the multiplicity of $c_{i}$ as a root of the characteristic polynomial for $A$, or for $T$. In other words, the characteristic polynomial for $T$ is

$$
f=\left(x-c_{1}\right)^{d_{1}} \cdots\left(x-c_{k}\right)^{d_{k}} .
$$

This shows that $c_{1}, \ldots, c_{k}$ and $d_{1}, \ldots, d_{k}$ are unique, up to the order in which we write them. The fact that $A$ is the direct sum of the matrices $\mathrm{A}_{i}$ gives us a direct sum decomposition $V=W_{1} \oplus \cdots \oplus W_{k}$ invariant under $T$. Now note that $W_{i}$ must be the null space of $\left(T-c_{i} I\right)^{n}$, where $n=\operatorname{dim} V$; for, $A_{i}-c_{i} I$ is clearly nilpotent and $A_{j}-c_{i} I$ is non-singular for $j \neq i$. So we see that the subspaces $W_{i}$ are unique. If $T_{i}$ is the operator induced on $W_{i}$ by $T$, then the matrix $A_{i}$ is uniquely determined as the rational form for $\left(T_{i}-c_{i} I\right)$.

Now we wish to make some further observations about the operator $T$ and the Jordan matrix $A$ which represents $T$ in some ordered basis. We shall list a string of observations:

(1) Every entry of $A$ not on or immediately below the main diagonal is 0 . On the diagonal of $A$ occur the $k$ distinct characteristic values $c_{1}, \ldots, c_{k}$ of $T$. Also, $c_{i}$ is repeated $d_{i}$ times, where $d_{i}$ is the multiplicity of $c_{i}$ as a root of the characteristic polynomial, i.e., $d_{i}=\operatorname{dim} W_{i}$.

(2) For each $i$, the matrix $A_{i}$ is the direct sum of $n_{i}$ elementary Jordan matrices $J_{j}^{(i)}$ with characteristic value $c_{i}$. The number $n_{i}$ is precisely the dimension of the space of characteristic vectors associated with the characteristic value $c_{i}$. For, $n_{i}$ is the number of elementary nilpotent blocks in the rational form for $\left(T_{i}-c_{i} I\right)$, and is thus equal to the dimension of the null space of $\left(T-c_{i} I\right)$. In particular notice that $T$ is diagonalizable if and only if $n_{i}=d_{i}$ for each $i$.

(3) For each $i$, the first block $J_{1}^{(i)}$ in the matrix $A_{i}$ is an $r_{i} \times r_{i}$ matrix, where $r_{i}$ is the multiplicity of $c_{i}$ as a root of the minimal polynomial for $T$. This follows from the fact that the minimal polynomial for the nilpotent operator $\left(T_{i}-c_{i} I\right)$ is $x^{r_{i}}$.

Of course we have as usual the straight matrix result. If $B$ is an $n \times n$ matrix over the field $F$ and if the characteristic polynomial for $B$ factors completely over $F$, then $B$ is similar over $F$ to an $n \times n$ matrix $A$ in Jordan form, and $A$ is unique up to a rearrangement of the order of its characteristic values. We call $A$ the Jordan form of $B$.

Also, note that if $F$ is an algebraically closed field, then the above remarks apply to every linear operator on a finite-dimensional space over $F$, or to every $n \times n$ matrix over $F$. Thus, for example, every $n \times n$ matrix over the field of complex numbers is similar to an essentially unique matrix in Jordan form.

Example 5. Suppose $T$ is a linear operator on $C^{2}$. The characteristic polynomial for $T$ is either $\left(x-c_{1}\right)\left(x-c_{2}\right)$ where $c_{1}$ and $c_{2}$ are distinct complex numbers, or is $(x-c)^{2}$. In the former case, $T$ is diagonalizable and is represented in some ordered basis by

$$
\left[\begin{array}{cc}
c_{1} & 0 \\
0 & c_{2}
\end{array}\right] \text {. }
$$

In the latter case, the minimal polynomial for $T$ may be $(x-c)$, in which case $T=c I$, or may be $(x-c)^{2}$, in which case $T$ is represented in some ordered basis by the matrix

$$
\left[\begin{array}{ll}
c & 0 \\
1 & c
\end{array}\right] \text {. }
$$

Thus every $2 \times 2$ matrix over the field of complex numbers is similar to a matrix of one of the two types displayed above, possibly with $c_{1}=c_{2}$.

Example 6 . Let $A$ be the complex $3 \times 3$ matrix

$$
A=\left[\begin{array}{rrr}
2 & 0 & 0 \\
a & 2 & 0 \\
b & c & -1
\end{array}\right] .
$$

The characteristic polynomial for $A$ is obviously $(x-2)^{2}(x+1)$. Either this is the minimal polynomial, in which case $A$ is similar to

$$
\left[\begin{array}{rrr}
2 & 0 & 0 \\
1 & 2 & 0 \\
0 & 0 & -1
\end{array}\right]
$$

or the minimal polynomial is $(x-2)(x+1)$, in which case $A$ is similar to

$$
\left[\begin{array}{rrr}
2 & 0 & 0 \\
0 & 2 & 0 \\
0 & 0 & -1
\end{array}\right] \text {. }
$$

Now

$$
(A-2 I)(A+I)=\left[\begin{array}{rrr}
0 & 0 & 0 \\
3 a & 0 & 0 \\
a c & 0 & 0
\end{array}\right]
$$

and thus $A$ is similar to a diagonal matrix if and only if $a=0$.

EXAMple 7. Let

$$
A=\left[\begin{array}{llll}
2 & 0 & 0 & 0 \\
1 & 2 & 0 & 0 \\
0 & 0 & 2 & 0 \\
0 & 0 & a & 2
\end{array}\right] .
$$

The characteristic polynomial for $A$ is $(x-2)^{4}$. Since $A$ is the direct sum of two $2 \times 2$ matrices, it is clear that the minimal polynomial for $A$ is $(x-2)^{2}$. Now if $a=0$ or if $a=1$, then the matrix $A$ is in Jordan form. Notice that the two matrices we obtain for $a=0$ and $a=1$ have the same characteristic polynomial and the same minimal polynomial, but are not similar. They are not similar because for the first matrix the solution space of $(A-2 I)$ has dimension 3, while for the second matrix it has dimension 2 .

Example 8. Linear differential equations with constant coefficients (Example 14, Chapter 6) provide a nice illustration of the Jordan form. Let $a_{0}, \ldots, a_{n-1}$ be complex numbers and let $V$ be the space of all $n$ times differentiable functions $f$ on an interval of the real line which satisfy the differential equation

$$
\frac{d^{n} f}{d x^{n}}+a_{n-1} \frac{d^{n-1} f}{d x^{n-1}}+\cdots+a_{1} \frac{d f}{d x}+a_{0} f=0 .
$$

Let $D$ be the differentiation operator. Then $V$ is invariant under $D$, because $V$ is the null space of $p(D)$, where

$$
p=x^{n}+\cdots+a_{1} x+a_{0} .
$$

What is the Jordan form for the differentiation operator on $V$ ? Let $c_{1}, \ldots, c_{k}$ be the distinct complex roots of $p$ :

$$
p=\left(x-c_{1}\right)^{r_{1}} \cdots\left(x-c_{k}\right)^{r_{k}} .
$$

Let $V_{i}$ be the null space of $\left(D-c_{i} I\right)^{r_{i}}$, that is, the set of solutions to the differential equation

$$
\left(D-c_{i} I\right)^{r_{i}} f=0 \text {. }
$$

Then as we noted in Example 15, Chapter 6 the primary decomposition theorem tells us that

$$
V=V_{1} \oplus \cdots \oplus V_{k} .
$$

Let $N_{i}$ be the restriction of $D-c_{i} I$ to $V_{i}$. The Jordan form for the operator $D$ (on $V$ ) is then determined by the rational forms for the nilpotent operators $N_{1}, \ldots, N_{k}$ on the spaces $V_{1}, \ldots, V_{k}$.

So, what we must know (for various values of $c$ ) is the rational form for the operator $N=(D-c I)$ on the space $V_{c}$, which consists of the solutions of the equation

$$
(D-c I)^{r} f=0 .
$$

How many elementary nilpotent blocks will there be in the rational form for $N$ ? The number will be the nullity of $N$, i.e., the dimension of the characteristic space associated with the characteristic value c. That dimension is 1 , because any function which satisfies the differential equation

$$
D f=c f
$$

is a scalar multiple of the exponential function $h(x)=e^{c x}$. Therefore, the operator $N$ (on the space $V_{c}$ ) has a cyclic vector. A good choice for a cyclic vector is $g=x^{r-1} h$ :

\section{This gives}

$$
g(x)=x^{r-1} e^{c x}
$$

$$
\begin{gathered}
N g=(r-1) x^{r-2} h \\
\vdots \\
N^{r-1} g=(r-1) ! h
\end{gathered}
$$

The preceding paragraph shows us that the Jordan form for $D$ (on the space $V$ ) is the direct sum of $k$ elementary Jordan matrices, one for each $\operatorname{root} c_{i}$.

\section{Exercises}

1. Let $N_{1}$ and $N_{2}$ be $3 \times 3$ nilpotent matrices over the field $F$. Prove that $N_{1}$ and $N_{2}$ are similar if and only if they have the same minimal polynomial.

2. Use the result of Exercise 1 and the Jordan form to prove the following: Let $A$ and $B$ be $n \times n$ matrices over the field $F$ which have the same characteristic polynomial

$$
f=\left(x-c_{1}\right)^{d_{1}} \cdots\left(x-c_{k}\right)^{d_{k}}
$$

and the same minimal polynomial. If no $d_{i}$ is greater than 3 , then $A$ and $B$ are similar.

3. If $A$ is a complex $5 \times 5$ matrix with characteristic polynomial

$$
f=(x-2)^{3}(x+7)^{2}
$$

and minimal polynomial $p=(x-2)^{2}(x+7)$, what is the Jordan form for $A$ ?

4. How many possible Jordan forms are there for a $6 \times 6$ complex matrix with characteristic polynomial $(x+2)^{4}(x-1)^{2} ?$

5. The differentiation operator on the space of polynomials of degree less than or equal to 3 is represented in the 'natural' ordered basis by the matrix

$$
\left[\begin{array}{llll}
0 & 1 & 0 & 0 \\
0 & 0 & 2 & 0 \\
0 & 0 & 0 & 3 \\
0 & 0 & 0 & 0
\end{array}\right] \text {. }
$$

What is the Jordan form of this matrix? ( $F$ a subfield of the complex numbers.)

6. Let $A$ be the complex matrix

$$
\left[\begin{array}{rrrrrr}
2 & 0 & 0 & 0 & 0 & 0 \\
1 & 2 & 0 & 0 & 0 & 0 \\
-1 & 0 & 2 & 0 & 0 & 0 \\
0 & 1 & 0 & 2 & 0 & 0 \\
1 & 1 & 1 & 1 & 2 & 0 \\
0 & 0 & 0 & 0 & 1 & -1
\end{array}\right]
$$

Find the Jordan form for $A$.

7. If $A$ is an $n \times n$ matrix over the field $F$ with characteristic polynomial

$$
f=\left(x-c_{1}\right)^{d_{1}} \cdots\left(x-c_{k}\right)^{d_{k}}
$$

what is the trace of $A$ ?

8. Classify up to similarity all $3 \times 3$ complex matrices $A$ such that $A^{3}=I$.

9. Classify up to similarity all $n \times n$ complex matrices $A$ such that $A^{n}=I$.

10. Let $n$ be a positive integer, $n \geq 2$, and let $N$ be an $n \times n$ matrix over the field $F$ such that $N^{n}=0$ but $N^{n-1} \neq 0$. Prove that $N$ has no square root, i.e., that there is no $n \times n$ matrix $A$ such that $A^{2}=N$.

11. Let $N_{1}$ and $N_{2}$ be $6 \times 6$ nilpotent matrices over the field $F$. Suppose that $N_{1}$ and $N_{2}$ have the same minimal polynomial and the same nullity. Prove that $N_{1}$ and $N_{2}$ are similar. Show that this is not true for $7 \times 7$ nilpotent matrices.

12. Use the result of Exercise 11 and the Jordan form to prove the following: Let $A$ and $B$ be $n \times n$ matrices over the field $F$ which have the same characteristic polynomial

$$
f=\left(x-c_{1}\right)^{d_{1}} \cdots\left(x-c_{k}\right)^{d_{k}}
$$

and the same minimal polynomial. Suppose also that for each $i$ the solution spaces of $\left(A-c_{i} I\right)$ and $\left(B-c_{i} I\right)$ have the same dimension. If no $d_{i}$ is greater than 6 , then $A$ and $B$ are similar.

13. If $N$ is a $k \times k$ elementary nilpotent matrix, i.e., $N^{k}=0$ but $N^{k-1} \neq 0$, show that $N^{t}$ is similar to $N$. Now use the Jordan form to prove that every complex $n \times n$ matrix is similar to its transpose.

14. What's wrong with the following proof? If $A$ is a complex $n \times n$ matrix such that $A^{t}=-A$, then $A$ is 0 . (Proof: Let $J$ be the Jordan form of $A$. Since $A^{t}=-A, J^{t}=-J$. But $J$ is triangular so that $J^{t}=-J$ implies that every entry of $J$ is zero. Since $J=0$ and $A$ is similar to $J$, we see that $A=0$.) (Give an example of a non-zero $A$ such that $A^{t}=-A$.)

15. If $N$ is a nilpotent $3 \times 3$ matrix over $C$, prove that $A=I+\frac{1}{2} N-\frac{1}{8} N^{2}$ satisfies $A^{2}=I+N$, i.e., $A$ is a square root of $I+N$. Use the binomial series for $(1+t)^{1 / 2}$ to obtain a similar formula for a square root of $I+N$, where $N$ is any nilpotent $n \times n$ matrix over $C$.

16. Use the result of Exercise 15 to prove that if $c$ is a non-zero complex number and $N$ is a nilpotent complex matrix, then $(c I+N)$ has a square root. Now use the Jordan form to prove that every non-singular complex $n \times n$ matrix has a square root.

\subsection{Computation of Invariant Factors}

Suppose that $A$ is an $n \times n$ matrix with entries in the field $F$. We wish to find a method for computing the invariant factors $p_{1}, \ldots, p_{r}$ which define the rational form for $A$. Let us begin with the very simple case in which $A$ is the companion matrix (7.2) of a monic polynomial

$$
p=x^{n}+c_{n-1} x^{n-1}+\cdots+c_{1} x+c_{0} .
$$

In Section $7.1$ we saw that $p$ is both the minimal and the characteristic polynomial for the companion matrix $A$. Now, we want to give a direct calculation which shows that $p$ is the characteristic polynomial for $A$. In this case,

$$
x I-A=\left[\begin{array}{rrcccc}
x & 0 & 0 & \cdots & 0 & c_{0} \\
-1 & x & 0 & \cdots & 0 & c_{1} \\
0 & -1 & x & \cdots & 0 & c_{2} \\
\vdots & \vdots & \vdots & & \vdots & \vdots \\
0 & 0 & 0 & \cdots & x & c_{n-2} \\
0 & 0 & 0 & \cdots & -1 & x+c_{n-1}
\end{array}\right] .
$$

Add $x$ times row $n$ to row $(n-1)$. This will remove the $x$ in the $(n-1$, $n-1$ ) place and it will not change the determinant. Then, add $x$ times the new row $(n-1)$ to row $(n-2)$. Continue successively until all of the $x$ 's on the main diagonal have been removed by that process. The result is the matrix 

$$
\left[\begin{array}{rrrrrc}
0 & 0 & 0 & \cdots & 0 & x^{n}+\cdots+c_{1} x+c_{0} \\
-1 & 0 & 0 & \cdots & 0 & x^{n-1}+\cdots+c_{2} x+c_{1} \\
0 & -1 & 0 & \cdots & 0 & x^{n-2}+\cdots+c_{3} x+c_{2} \\
\vdots & \vdots & \vdots & & \vdots & \vdots \\
0 & 0 & 0 & \cdots & 0 & x^{2}+c_{n-1} x+c_{n-2} \\
0 & 0 & 0 & \cdots & -1 & x+c_{n-1}
\end{array}\right]
$$

which has the same determinant as $x I-A$. The upper right-hand entry of this matrix is the polynomial $p$. We clean up the last column by adding to it appropriate multiples of the other columns:

$$
\left[\begin{array}{rrclrc}
0 & 0 & 0 & \cdots & 0 & p \\
-1 & 0 & 0 & \cdots & 0 & 0 \\
0 & -1 & 0 & \cdots & 0 & 0 \\
\vdots & \vdots & \vdots & & \vdots & \vdots \\
0 & 0 & 0 & \cdots & 0 & 0 \\
0 & 0 & 0 & \cdots & -1 & 0
\end{array}\right]
$$

Multiply each of the first $(n-1)$ columns by $-1$ and then perform $(n-1)$ interchanges of adjacent columns to bring the present column $n$ to the first position. The total effect of the $2 n-2$ sign changes is to leave the determinant unaltered. We obtain the matrix

$$
\left[\begin{array}{ccccc}
p & 0 & 0 & \cdots & 0 \\
0 & 1 & 0 & \cdots & 0 \\
0 & 0 & 1 & \cdots & 0 \\
\vdots & \vdots & \vdots & & \vdots \\
0 & 0 & 0 & \cdots & 1
\end{array}\right] .
$$

It is then clear that $p=\operatorname{det}(x I-A)$.

We are going to show that, for any $n \times n$ matrix $A$, there is a succession of row and column operations which will transform $x I-A$ into a matrix much like (7-28), in which the invariant factors of $A$ appear down the main diagonal. Let us be completely clear about the operations we shall use.

We shall be concerned with $F[x]^{m \times n}$, the collection of $m \times n$ matrices with entries which are polynomials over the field $F$. If $M$ is such a matrix, an elementary row operation on $M$ is one of the following

1. multiplication of one row of $M$ by a non-zero scalar in $F$;

2. replacement of the $r$ th row of $M$ by row $r$ plus $f$ times row $s$, where $f$ is any polynomial over $F$ and $r \neq s$;

3. interchange of two rows of $M$.

The inverse operation of an elementary row operation is an elementary row operation of the same type. Notice that we could not make such an assertion if we allowed non-scalar polynomials in (1). An $m \times m$ ele- mentary matrix, that is, an elementary matrix in $F[x]^{m \times m}$, is one which can be obtained from the $m \times m$ identity matrix by means of a single elementary row operation. Clearly each elementary row operation on $M$ can be effected by multiplying $M$ on the left by a suitable $m \times m$ elementary matrix; in fact, if $e$ is the operation, then

$$
e(M)=e(I) M .
$$

Let $M, N$ be matrices in $F[x]^{m \times n}$. We say that $N$ is row-equivalent to $M$ if $N$ can be obtained from $M$ by a finite succession of elementary row operations:

$$
M=M_{0} \rightarrow M_{1} \rightarrow \cdots \rightarrow M_{k}=N .
$$

Evidently $N$ is row-equivalent to $M$ if and only if $M$ is row-equivalent to $N$, so that we may use the terminology ' $M$ and $N$ are row-equivalent.' If $N$ is row-equivalent to $M$, then

$$
N=P M
$$

where the $m \times m$ matrix $P$ is a product of elementary matrices:

$$
P=E_{1} \cdots E_{k} \text {. }
$$

In particular, $P$ is an invertible matrix with inverse

$$
P^{-1}=E_{k}^{-1} \cdots E_{1}^{-1} \text {. }
$$

Of course, the inverse of $E_{j}$ comes from the inverse elementary row operation.

All of this is just as it is in the case of matrices with entries in $F$. It parallels the elementary results in Chapter 1. Thus, the next problem which suggests itself is to introduce a row-reduced echelon form for polynomial matrices. Here, we meet a new obstacle. How do we row-reduce a matrix? The first step is to single out the leading non-zero entry of row 1 and to divide every entry of row 1 by that entry. We cannot (necessarily) do that when the matrix has polynomial entries. As we shall see in the next theorem, we can circumvent this difficulty in certain cases; however, there is not any entirely suitable row-reduced form for the general matrix in $F[x]^{m \times n}$. If we introduce column operations as well and study the type of equivalence which results from allowing the use of both types of operations, we can obtain a very useful standard form for each matrix. The basic tool is the following.

Lemma. Let $\mathrm{M}$ be a matrix in $\mathrm{F}[\mathrm{x}]^{m \times n}$ which has some non-zero entry in its first column, and let $\mathrm{p}$ be the greatest common divisor of the entries in column 1 of $\mathrm{M}$. Then $\mathrm{M}$ is row-equivalent to a matrix $\mathrm{N}$ which has

as its first column.

$$
\left[\begin{array}{c}
\mathrm{p} \\
0 \\
\vdots \\
0
\end{array}\right]
$$

Proof. We shall prove something more than we have stated. We shall show that there is an algorithm for finding $N$, i.e., a prescription which a machine could use to calculate $N$ in a finite number of steps. First, we need some notation.

Let $M$ be any $m \times n$ matrix with entries in $F[x]$ which has a nonzero first column

Define

$$
M_{1}=\left[\begin{array}{c}
f_{1} \\
\vdots \\
f_{m}
\end{array}\right]
$$

$$
\begin{aligned}
l\left(M_{1}\right) & =\min _{f_{i} \neq 0} \operatorname{deg} f_{i} \\
p\left(M_{1}\right) & =\text { g.c.d. }\left(f_{1}, \ldots, f_{m}\right) .
\end{aligned}
$$

Let $j$ be some index such that $\operatorname{deg} f_{j}=l\left(M_{1}\right)$. To be specific, let $j$ be the smallest index $i$ for which $\operatorname{deg} f_{i}=l\left(M_{1}\right)$. Attempt to divide each $f_{i}$ by $f_{j}$ :

$$
f_{i}=f_{j} g_{i}+r_{i}, \quad r_{i}=0 \quad \text { or } \quad \operatorname{deg} r_{i}<\operatorname{deg} f_{j} .
$$

For each $i$ different from $j$, replace row $i$ of $M$ by row $i$ minus $g_{i}$ times row $j$. Multiply row $j$ by the reciprocal of the leading coefficient of $f_{j}$ and then interchange rows $j$ and 1 . The result of all these operations is a matrix $M^{\prime}$ which has for its first column

$$
M_{1}^{\prime}=\left[\begin{array}{l}
\tilde{f}_{j} \\
r_{2} \\
\vdots \\
r_{j-1} \\
r_{1} \\
r_{j+1} \\
\vdots \\
r_{m}
\end{array}\right] .
$$

where $\tilde{f}_{j}$ is the monic polynomial obtained by normalizing $f_{j}$ to have leading coefficient 1. We have given a well-defined procedure for associating with each $M$ a matrix $M^{\prime}$ with these properties.

(a) $M^{\prime}$ is row-equivalent to $M$.

(b) $p\left(M_{1}^{\prime}\right)=p\left(M_{1}\right)$.

(c) Either $l\left(M_{1}^{\prime}\right)<l\left(M_{1}\right)$ or

$$
M_{1}^{\prime}=\left[\begin{array}{c}
p\left(M_{1}\right) \\
0 \\
\vdots \\
0
\end{array}\right] .
$$

It is easy to verify (b) and (c) from (7-30) and (7-31). Property (c) is just another way of stating that either there is some $i$ such that $r_{i} \neq 0$ and $\operatorname{deg} r_{i}<\operatorname{deg} f_{j}$ or else $r_{i}=0$ for all $i$ and $\tilde{f}_{j}$ is (therefore) the greatest common divisor of $f_{1}, \ldots, f_{m}$.

The proof of the lemma is now quite simple. We start with the matrix $M$ and apply the above procedure to obtain $M^{\prime}$. Property (c) tells us that either $M^{\prime}$ will serve as the matrix $N$ in the lemma or $l\left(M_{1}^{\prime}\right)<l\left(M_{1}\right)$. In the latter case, we apply the procedure to $M^{\prime}$ to obtain the matrix $M^{(2)}=$ $\left(M^{\prime}\right)^{\prime}$. If $M^{(2)}$ is not a suitable $N$, we form $M^{(3)}=\left(M^{(2)}\right)^{\prime}$, and so on. The point is that the strict inequalities

$$
l\left(M_{1}\right)>l\left(M_{1}^{\prime}\right)>l\left(M_{1}^{(2)}\right)>\cdots
$$

cannot continue for very long. After not more than $l\left(M_{1}\right)$ iterations of our procedure, we must arrive at a matrix $M^{(k)}$ which has the properties we seek.

Theorem 6. Let $\mathrm{P}$ be an $\mathrm{m} \times \mathrm{m}$ matrix with entries in the polynomial algebra $\mathrm{F}[\mathrm{x}]$. The following are equivalent.

(i) $\mathrm{P}$ is invertible.

(ii) The determinant of $\mathrm{P}$ is a non-zero scalar polynomial.

(iii) $\mathrm{P}$ is row-equivalent to the $\mathrm{m} \times \mathrm{m}$ identity matrix.

(iv) $\mathrm{P}$ is a product of elementary matrices.

Proof. Certainly (i) implies (ii) because the determinant function is multiplicative and the only polynomials invertible in $F[x]$ are the non-zero scalar ones. As a matter of fact, in Chapter 5 we used the classical adjoint to show that (i) and (ii) are equivalent. Our argument here provides a different proof that (i) follows from (ii). We shall complete the merry-go-round

![](https://cdn.mathpix.com/cropped/2023_01_16_0f6afdf132d84aaf6bdeg-264.jpg?height=142&width=192&top_left_y=1359&top_left_x=544)

The only implication which is not obvious is that (iii) follows from (ii).

Assume (ii) and consider the first column of $P$. It contains certain polynomials $p_{\mathbf{1}}, \ldots, p_{m}$, and

$$
\text { g.c.d. }\left(p_{1}, \ldots, p_{m}\right)=1
$$

because any common divisor of $p_{1}, \ldots, p_{n}$ must divide (the scalar) $\operatorname{det} P$. Apply the previous lemma to $P$ to obtain a matrix

$$
Q=\left[\begin{array}{cccc}
1 & a_{2} & \cdots & a_{m} \\
0 & & & \\
\vdots & & B & \\
0 & & &
\end{array}\right]
$$

which is row-equivalent to $P$. An elementary row operation changes the determinant of a matrix by (at most) a non-zero scalar factor. Thus $\operatorname{det} Q$ is a non-zero scalar polynomial. Evidently the $(m-1) \times(m-1)$ matrix $B$ in (7-32) has the same determinant as does $Q$. Therefore, we may apply the last lemma to $B$. If we continue this way for $m$ steps, we obtain an upper-triangular matrix

$$
R=\left[\begin{array}{cccc}
1 & a_{2} & \cdots & a_{m} \\
0 & 1 & \cdots & b_{m} \\
\vdots & \vdots & & \vdots \\
0 & 0 & \cdots & 1
\end{array}\right]
$$

which is row-equivalent to $R$. Obviously $R$ is row-equivalent to the $m \times m$ identity matrix.

Corollary. Let $\mathrm{M}$ and $\mathrm{N}$ be $\mathrm{m} \times \mathrm{n}$ matrices with entries in the polynomial algebra $\mathrm{F}[\mathrm{x}]$. Then $\mathrm{N}$ is row-equivalent to $\mathrm{M}$ if and only if

$$
\mathrm{N}=\mathrm{PM}
$$

where $\mathrm{P}$ is an invertible $\mathrm{m} \times \mathrm{m}$ matrix with entries in $\mathrm{F}[\mathrm{x}]$.

We now define elementary column operations and columnequivalence in a manner analogous to row operations and row-equivalence. We do not need a new concept of elementary matrix because the class of matrices which can be obtained by performing one elementary column operation on the identity matrix is the same as the class obtained by using a single elementary row operation.

Definition. The matrix $\mathrm{N}$ is equivalent to the matrix $\mathrm{M}$ if we can pass from $\mathrm{M}$ to $\mathrm{N}$ by means of a sequence of operations

$$
\mathrm{M}=\mathrm{M}_{0} \rightarrow \mathrm{M}_{1} \rightarrow \cdots \rightarrow \mathrm{M}_{\mathrm{k}}=\mathrm{N}
$$

each of which is an elementary row operation or an elementary column operation.

Theorem 7. Let $\mathrm{M}$ and $\mathrm{N}$ be $\mathrm{m} \times \mathrm{n}$ matrices with entries in the polynomial algebra $\mathrm{F}[\mathrm{x}]$. Then $\mathrm{N}$ is equivalent to $\mathrm{M}$ if and only if

$$
\mathrm{N}=\mathrm{PMQ}
$$

where $\mathrm{P}$ is an invertible matrix in $\mathrm{F}[\mathrm{x}]^{m \times m}$ and $\mathrm{Q}$ is an invertible matrix in $\mathrm{F}\lceil\mathrm{x}\rceil^{n \times_{n}}$.

Theorem 8. Let $\mathrm{A}$ be an $\mathrm{n} \times \mathrm{n}$ matrix with entries in the field $\mathrm{F}$, and let $\mathrm{p}_{1}, \ldots, \mathrm{p}_{\mathrm{r}}$ be the invariant factors for $\mathrm{A}$. The matrix $\mathrm{xI}-\mathrm{A}$ is equivalent to the $\mathrm{n} \times \mathrm{n}$ diagonal matrix with diagonal entries $\mathrm{p}_{\mathbf{1}}, \ldots, \mathrm{p}_{\mathrm{r}}$, $1,1, \ldots, 1$.

Proof. There exists an invertible $n \times n$ matrix $P$, with entries in $F$, such that $P A P^{-1}$ is in rational form, that is, has the block form 

$$
P A P^{-1}=\left[\begin{array}{cccc}
A_{1} & 0 & \cdots & 0 \\
0 & A_{2} & \cdots & 0 \\
\vdots & \vdots & \ddots & \vdots \\
0 & 0 & \cdots & A_{r}
\end{array}\right]
$$

where $A_{i}$ is the companion matrix of the polynomial $p_{i}$. According to Theorem 7 , the matrix

$$
P(x I-A) P^{-1}=x I-P A P^{-1}
$$

is equivalent to $x I-A$. Now

$$
x I-P A P^{-1}=\left[\begin{array}{cccc}
x I-A_{1} & 0 & \cdots & 0 \\
0 & x I-A_{2} & \cdots & 0 \\
\vdots & \vdots & & \vdots \\
0 & 0 & \cdots & x I-A_{r}
\end{array}\right]
$$

where the various $I$ 's we have used are identity matrices of appropriate sizes. At the beginning of this section, we showed that $x I-A_{i}$ is equivalent to the matrix

$$
\left[\begin{array}{cccc}
p_{i} & 0 & \cdots & 0 \\
0 & 1 & \cdots & 0 \\
\vdots & \vdots & & \vdots \\
0 & 0 & \cdots & 1
\end{array}\right] .
$$

From (7-33) and (7-34) it is then clear that $x I-A$ is equivalent to a diagonal matrix which has the polynomials $p_{i}$ and $(n-r) 1^{\prime}$ 's on its main diagonal. By a succession of row and column interchanges, we can arrange those diagonal entries in any order we choose, for example: $p_{1}, \ldots, p_{r}$, $1, \ldots, 1$.

Theorem 8 does not give us an effective way of calculating the elementary divisors $p_{1}, \ldots, p_{r}$ because our proof depends upon the cyclic decomposition theorem. We shall now give an explicit algorithm for reducing a polynomial matrix to diagonal form. Theorem 8 suggests that we may also arrange that successive elements on the main diagonal divide one another.

Definition. Let $\mathrm{N}$ be a matrix in $\mathrm{F}[\mathrm{x}]^{\mathrm{m} \times \mathrm{n}}$. We say that $\mathrm{N}$ is in (Smith) normal form if

(a) every entry off the main diagonal of $\mathrm{N}$ is 0 ;

(b) on the main diagonal of $\mathrm{N}$ there appear (in order) polynomials $\mathrm{f}_{1}, \ldots, \mathrm{f}_{l}$ such that $\mathrm{f}_{\mathrm{k}}$ divides $\mathrm{f}_{\mathrm{k}+1}, 1 \leq \mathrm{k} \leq l-1$.

In the definition, the number $l$ is $l=\min (m, n)$. The main diagonal entries are $f_{k}=N_{k k}, k=1, \ldots, l$.

Theorem 9. Let $\mathrm{M}$ be an $\mathrm{m} \times \mathrm{n}$ matrix with entries in the polynomial algebra $\mathrm{F}[\mathrm{x}]$. Then $\mathrm{M}$ is equivalent to a matrix $\mathrm{N}$ which is in normal form. Proof. If $M=0$, there is nothing to prove. If $M \neq 0$, we shall give an algorithm for finding a matrix $M^{\prime}$ which is equivalent to $M$ and which has the form

$$
M^{\prime}=\left[\begin{array}{cccc}
f_{1} & 0 & \cdots & 0 \\
0 & & & \\
\vdots & & R & \\
0 & & &
\end{array}\right]
$$

where $R$ is an $(m-1) \times(n-1)$ matrix and $f_{1}$ divides every entry of $R$. We shall then be finished, because we can apply the same procedure to $R$ and obtain $f_{2}$, etc.

Let $l(M)$ be the minimum of the degrees of the non-zero entries of $M$. Find the first column which contains an entry with degree $l(M)$ and interchange that column with column 1 . Call the resulting matrix $M^{(0)}$. We describe a procedure for finding a matrix of the form

$$
\left[\begin{array}{cccc}
g & 0 & \cdots & 0 \\
0 & & & \\
\vdots & & S & \\
0 & & &
\end{array}\right]
$$

which is equivalent to $M^{(0)}$. We begin by applying to the matrix $M^{(0)}$ the procedure of the lemma before Theorem 6 , a procedure which we shall call PL6. There results a matrix

$$
M^{(1)}=\left[\begin{array}{cccc}
p & a & \cdots & b \\
0 & c & \cdots & d \\
\vdots & \vdots & & \vdots \\
0 & e & \cdots & f
\end{array}\right] .
$$

If the entries $a, \ldots, b$ are all 0 , fine. If not, we use the analogue of PL6 for the first row, a procedure which we might call PL6'. The result is a matrix

$$
M^{(2)}=\left[\begin{array}{cccc}
q & 0 & \cdots & 0 \\
a^{\prime} & c^{\prime} & \cdots & e^{\prime} \\
\vdots & \vdots & & \vdots \\
b^{\prime} & d^{\prime} & \cdots & f^{\prime}
\end{array}\right]
$$

where $q$ is the greatest common divisor of $p, a, \ldots, b$. In producing $M^{(2)}$, we may or may not have disturbed the nice form of column 1 . If we did, we can apply PL6 once again. Here is the point. In not more than $l(M)$ steps:

$$
M^{(0)} \stackrel{\mathrm{PL} 6}{\longrightarrow} M^{(1)} \stackrel{\mathrm{PL} 6^{\prime}}{\longrightarrow} M^{(2)} \stackrel{\mathrm{PL} 6}{\longrightarrow} \cdots \rightarrow M^{(t)}
$$

we must arrive at a matrix $M^{(t)}$ which has the form $(7-36)$, because at each successive step we have $l\left(M^{(k+1)}\right)<l\left(M^{(k)}\right)$. We name the process which we have just defined P7-36:

$$
M^{(0)} \stackrel{\mathrm{P7}-36}{\longrightarrow} M^{(t)} .
$$

In (7-36), the polynomial $g$ may or may not divide every entry of $S$. If it does not, find the first column which has an entry not divisible by $g$ and add that column to column 1. The new first column contains both $g$ and an entry $g h+r$ where $r \neq 0$ and $\operatorname{deg} r<\operatorname{deg} g$. Apply process P7-36 and the result will be another matrix of the form (7-36), where the degree of the corresponding $g$ has decreased.

It should now be obvious that in a finite number of steps we will obtain (7-35), i.e., we will reach a matrix of the form (7-36) where the degree of $g$ cannot be further reduced.

We want to show that the normal form associated with a matrix $M$ is unique. Two things we have seen provide clues as to how the polynomials $f_{1}, \ldots, f_{1}$ in Theorem 9 are uniquely determined by $M$. First, elementary row and column operations do not change the determinant of a square matrix by more than a non-zero scalar factor. Second, elementary row and column operations do not change the greatest common divisor of the entries of a matrix.

Definition. Let $\mathrm{M}$ be an $\mathrm{m} \times \mathrm{n}$ matrix with entries in $\mathrm{F}[\mathrm{x}]$. If $1 \leq \mathrm{k} \leq \min (\mathrm{m}, \mathrm{n})$, we define $\delta_{\mathrm{k}}(\mathrm{M})$ to be the greatest common divisor of the determinants of all $\mathrm{k} \times \mathrm{k}$ submatrices of $\mathrm{M}$.

Recall that a $k \times k$ submatrix of $M$ is one obtained by deleting some $m-k$ rows and some $n-k$ columns of $M$. In other words, we select certain $k$-tuples

$$
\begin{array}{ll}
I=\left(i_{1}, \ldots, i_{k}\right), & 1 \leq i_{1}<\cdots<i_{k} \leq m \\
J=\left(j_{1}, \ldots, j_{k}\right), & 1 \leq j_{1}<\cdots<j_{k} \leq n
\end{array}
$$

and look at the matrix formed using those rows and columns of $M$. We are interested in the determinants

$$
D_{I, J}(M)=\operatorname{det}\left[\begin{array}{ccc}
M_{i j_{1}} & \cdots & M_{i, j_{k}} \\
\vdots & & \vdots \\
M_{i_{k} j_{1}} & \cdots & M_{i_{k} j_{k}}
\end{array}\right] .
$$

The polynomial $\delta_{k}(M)$ is the greatest common divisor of the polynomials $\mathrm{D}_{I, J}(M)$, as $I$ and $J$ range over the possible $k$-tuples.

Theorem 10. If $\mathrm{M}$ and $\mathrm{N}$ are equivalent $\mathrm{m} \times \mathrm{n}$ matrices with entries in $\mathrm{F}[\mathrm{x}]$, then

$$
\delta_{\mathrm{k}}(\mathrm{M})=\delta_{\mathrm{k}}(\mathrm{N}), \quad 1 \leq \mathrm{k} \leq \min (\mathrm{m}, \mathrm{n}) .
$$

Proof. It will suffice to show that a single elementary row operation $e$ does not change $\delta_{k}$. Since the inverse of $e$ is also an elementary row operation, it will suffice to show this: If a polynomial $f$ divides every $D_{I, J}(M)$, then $f$ divides $D_{I, J}(e(M))$ for all $k$-tuples $I$ and $J$. Since we are considering a row operation, let $\alpha_{1}, \ldots, \alpha_{m}$ be the rows of $M$ and let us employ the notation

$$
D_{J}\left(\alpha_{i_{1}}, \ldots, \alpha_{i_{k}}\right)=D_{I, J}(M) .
$$

Given $I$ and $J$, what is the relation between $D_{I, J}(M)$ and $D_{I, J}(e(M))$ ? Consider the three types of operations $e$ :

(a) multiplication of row $r$ by a non-zero scalar $c$;

(b) replacement of row $r$ by row $r$ plus $g$ times row $s, r \neq s$;

(c) interchange of rows $r$ and $s, r \neq s$.

Forget about type (c) operations for the moment, and concentrate on types (a) and (b), which change only row $r$. If $r$ is not one of the indices $i_{1}, \ldots, i_{k}$, then

$$
D_{I, J}(e(M))=D_{I, J}(M) .
$$

If $r$ is among the indices $i_{1}, \ldots, i_{k}$, then in the two cases we have

(a) $D_{I, J}(e(M))=D_{J}\left(\alpha_{i_{i}}, \ldots, c \alpha_{r}, \ldots, \alpha_{i_{k}}\right)$

$$
=c D_{J}\left(\alpha_{i_{1}}, \ldots, \alpha_{r}, \ldots, \alpha_{i_{k}}\right)
$$

$$
\begin{aligned}
& =c D_{I, J}(M)
\end{aligned}
$$

(b) $D_{I, J}(e(M))=D_{J}\left(\alpha_{i_{1}}, \ldots, \alpha_{r}+g \alpha_{s}, \ldots, \alpha_{i_{k}}\right)$

$=D_{I, J}(M)+g D_{J}\left(\alpha_{i_{1}}, \ldots, \alpha_{s}, \ldots, \alpha_{i_{k}}\right)$.

For type (a) operations, it is clear that any $f$ which divides $D_{I, J}(M)$ also divides $D_{I, J}(e(M))$. For the case of a type (c) operation, notice that

$$
\begin{gathered}
D_{J}\left(\alpha_{i_{1}}, \ldots, \alpha_{s}, \ldots, \alpha_{i_{k}}\right)=0, \quad \text { if } s=i_{j} \text { for some } j \\
D_{\boldsymbol{J}}\left(\alpha_{i_{1}}, \ldots, \alpha_{s}, \ldots, \alpha_{i_{k}}\right)=\pm D_{I^{\prime}, J}^{\prime}(M), \quad \text { if } s \neq i_{j} \text { for all } j
\end{gathered}
$$

The $I^{\prime}$ in the last equation is the $k$-tuple $\left(i_{1}, \ldots, s, \ldots, i_{k}\right)$ arranged in increasing order. It should now be apparent that, if $f$ dividesevery $D_{I, J}(M)$, then $f$ divides every $D_{I, J}(e(M))$.

Operations of type (c) can be taken care of by roughly the same argument or by using the fact that such an operation can be effected by a sequence of operations of types (a) and (b).

Corollary. Each matrix $\mathrm{M}$ in $\mathrm{F}[\mathrm{x}]^{\mathrm{m} \times \mathrm{n}}$ is equivalent to precisely one matrix $\mathrm{N}$ which is in normal form. The polynomials $\mathrm{f}_{1}, \ldots, \mathrm{f}_{l}$ which occur on the main diagonal of $\mathrm{N}$ are

$$
\mathrm{f}_{\mathrm{k}}=\frac{\delta_{\mathrm{k}}(\mathrm{M})}{\delta_{\mathrm{k}-1}(\mathrm{M})}, \quad 1 \leq \mathrm{k} \leq \min (\mathrm{m}, \mathrm{n})
$$

where, for convenience, we define $\delta_{0}(\mathbf{M})=1$.

Proof. If $N$ is in normal form with diagonal entries $f_{1}, \ldots, f_{l}$, it is quite easy to see that

$$
\delta_{k}(N)=f_{1} f_{2} \cdots f_{k} .
$$

Of course, we call the matrix $N$ in the last corollary the normal form of $M$. The polynomials $f_{1}, \ldots, f_{l}$ are of ten called the invariant factors of $M$.

Suppose that $A$ is an $n \times n$ matrix with entries in $F$, and let $p_{1}, \ldots, p_{r}$ be the invariant factors for $A$. We now see that the normal form of the matrix $x I-A$ has diagonal entries $1,1, \ldots, 1, p_{r}, \ldots, p_{1}$. The last corollary tells us what $p_{1}, \ldots, p_{r}$ are, in terms of submatrices of $x I-A$. The number $n-r$ is the largest $k$ such that $\delta_{k}(x I-A)=1$. The minimal polynomial $p_{1}$ is the characteristic polynomial for $A$ divided by the greatest common divisor of the determinants of all $(n-1) \times(n-1)$ submatrices of $x I-A$, etc.

\section{Exercises}

1. True or false? Every matrix in $F[x]^{n \times n}$ is row-equivalent to an upper-triangular matrix.

2. Let $T$ be a linear operator on a finite-dimensional vector space and let $A$ be the matrix of $T$ in some ordered basis. Then $T$ has a cyclic vector if and only if the determinants of the $(n-1) \times(n-1)$ submatrices of $x I-A$ are relatively prime.

3. Let $A$ be an $n \times n$ matrix with entries in the field $F$ and let $f_{1}, \ldots, f_{n}$ be the diagonal entries of the normal form of $x I-A$. For which matrices $A$ is $f_{1} \neq 1$ ?

4. Construct a linear operator $T$ with minimal polynomial $x^{2}(x-1)^{2}$ and characteristic polynomial $x^{3}(x-1)^{4}$. Describe the primary decomposition of the vector space under $T$ and find the projections on the primary components. Find a basis in which the matrix of $T$ is in Jordan form. Also find an explicit direct sum decomposition of the space into $T$-cyclic subspaces as in Theorem 3 and give the invariant factors.

5. Let $T$ be the linear operator on $R^{8}$ which is represented in the standard basis by the matrix

$$
A=\left[\begin{array}{rrrrrrrr}
1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 \\
0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 \\
0 & 0 & 0 & 0 & 0 & 0 & 0 & -1 \\
0 & 1 & 1 & 0 & 0 & 0 & 0 & 1 \\
0 & 0 & 0 & 1 & 1 & 0 & 0 & 0 \\
0 & 1 & 1 & 1 & 1 & 1 & 0 & 1 \\
0 & -1 & -1 & -1 & -1 & 0 & 1 & -1 \\
0 & 0 & 0 & 0 & 0 & 0 & 0 & 0
\end{array}\right] .
$$

(a) Find the characteristic polynomial and the invariant factors.

(b) Find the primary decomposition of $R^{8}$ under $T$ and the projections on the primary components. Find cyclic decompositions of each primary component as in Theorem 3. (c) Find the Jordan form of $A$.

(d) Find a direct-sum decomposition of $R^{8}$ into $T$-cyclic subspaces as in Theorem 3. (Hint: One way to do this is to use the results in (b) and an appropriate generalization of the ideas discussed in Example 4.)

\subsection{Summary; Semi-Simple Operators}

In the last two chapters, we have been dealing with a single linear operator $T$ on a finite-dimensional vector space $V$. The program has been to decompose $T$ into a direct sum of linear operators of an elementary nature, for the purpose of gaining detailed information about how $T$ 'operates' on the space $V$. Let us review briefly where we stand.

We began to study $T$ by means of characteristic values and characteristic vectors. We introduced diagonalizable operators, the operators which can be completely described in terms of characteristic values and vectors. We then observed that $T$ might not have a single characteristic vector. Even in the case of an algebraically closed scalar field, when every linear operator does have at least one characteristic vector, we noted that the characteristic vectors of $T$ need not span the space.

We then proved the cyclic decomposition theorem, expressing any linear operator as the direct sum of operators with a cyclic vector, with no assumption about the scalar field. If $U$ is a linear operator with a cyclic vector, there is a basis $\left\{\alpha_{1}, \ldots, \alpha_{n}\right\}$ with

$$
\begin{aligned}
& U \alpha_{j}=\alpha_{j+1}, \quad j=1, \ldots, n-1 \\
& U \alpha_{n}=-c_{0} \alpha_{1}-c_{1} \alpha_{2}-\cdots-c_{n-1} \alpha_{n}
\end{aligned}
$$

The action of $U$ on this basis is then to shift each $\alpha_{j}$ to the next vector $\alpha_{j+1}$, except that $U \alpha_{n}$ is some prescribed linear combination of the vectors in the basis. Since the general linear operator $T$ is the direct sum of a finite number of such operators $U$, we obtained an explicit and reasonably elementary description of the action of $T$.

We next applied the cyclic decomposition theorem to nilpotent operators. For the case of an algebraically closed scalar field, we combined this with the primary decomposition theorem to obtain the Jordan form. The Jordan form gives a basis $\left\{\alpha_{1}, \ldots, \alpha_{n}\right\}$ for the space $V$ such that, for each $j$, either $T \alpha_{j}$ is a scalar multiple of $\alpha_{j}$ or $T \alpha_{j}=c \alpha_{j}+\alpha_{j+1}$. Such a basis certainly describes the action of $T$ in an explicit and elementary manner.

The importance of the rational form (or the Jordan form) derives from the fact that it exists, rather than from the fact that it can be computed in specific cases. Of course, if one is given a specific linear operator $T$ and can compute its cyclic or Jordan form, that is the thing to do; for, having such a form, one can reel off vast amounts of information about $T$. Two different types of difficulties arise in the computation of such standard forms. One difficulty is, of course, the length of the computations. The other difficulty is that there may not be any method for doing the computations, even if one has the necessary time and patience. The second difficulty arises in, say, trying to find the Jordan form of a complex matrix. There simply is no well-defined method for factoring the characteristic polynomial, and thus one is stopped at the outset. The rational form does not suffer from this difficulty. As we showed in Section 7.4, there is a well-defined method for finding the rational form of a given $n \times n$ matrix; however, such computations are usually extremely lengthy.

In our summary of the results of these last two chapters, we have not yet mentioned one of the theorems which we proved. This is the theorem which states that if $T$ is a linear operator on a finite-dimensional vector space over an algebraically closed field, then $T$ is uniquely expressible as the sum of a diagonalizable operator and a nilpotent operator which commute. This was proved from the primary decomposition theorem and certain information about diagonalizable operators. It is not as deep a theorem as the cyclic decomposition theorem or the existence of the Jordan form, but it does have important and useful applications in certain parts of mathematics. In concluding this chapter, we shall prove an analogous theorem, without assuming that the scalar field is algebraically closed. We begin by defining the operators which will play the role of the diagonalizable operators.

Definition. Let $\mathrm{V}$ be a finite-dimensional vector space over the field $\mathrm{F}$, and let $\mathrm{T}$ be a linear operator on $\mathrm{V}$. We say that $\mathrm{T}$ is semi-simple if every T-invariant subspace has a complementary T-invariant subspace.

What we are about to prove is that, with some restriction on the field $F$, every linear operator $T$ is uniquely expressible in the form $T=$ $S+N$, where $S$ is semi-simple, $N$ is nilpotent, and $S N=N S$. First, we are going to characterize semi-simple operators by means of their minimal polynomials, and this characterization will show us that, when $F$ is algebraically closed, an operator is semi-simple if and only if it is diagonalizable.

Lemma. Let $\mathrm{T}$ be a linear operator on the finite-dimensional vector space $\mathrm{V}$, and let $\mathrm{V}=\mathrm{W}_{1} \oplus \cdots \oplus \mathrm{W}_{\mathrm{k}}$ be the primary decomposition for $\mathrm{T}$. In other words, if $\mathrm{p}$ is the minimal polynomial for $\mathrm{T}$ and $\mathrm{p}=\mathrm{p}_{1}^{\mathrm{r}_{1}} \cdots \mathrm{p}_{\mathrm{k}}^{\mathrm{r}_{\mathrm{k}}}$ is the prime factorization of $\mathrm{p}$, then $\mathrm{W}_{\mathrm{j}}$ is the null space of $\mathrm{p}_{\mathrm{j}}(\mathrm{T})^{\mathrm{r}_{\mathrm{i}}}$. Let $\mathrm{W}$ be any subspace of $\mathrm{V}$ which is invariant under $\mathrm{T}$. Then

$$
\mathrm{W}=\left(\mathrm{W} \cap \mathrm{W}_{1}\right) \oplus \cdots \oplus\left(\mathrm{W} \cap \mathrm{W}_{\mathrm{k}}\right)
$$

Proof. For the proof we need to recall a corollary to our proof of the primary decomposition theorem in Section 6.8. If $E_{1}, \ldots, E_{k}$ are the projections associated with the decomposition $V=W_{1} \oplus \cdots \oplus W_{k}$, then each $E_{j}$ is a polynomial in $T$. That is, there are polynomials $h_{1}, \ldots, h_{k}$ such that $E_{j}=h_{j}(T)$.

Now let $W$ be a subspace which is invariant under $T$. If $\alpha$ is any vector in $W$, then $\alpha=\alpha_{1}+\cdots+\alpha_{k}$, where $\alpha_{j}$ is in $W_{j}$. Now $\alpha_{j}=E_{j} \alpha=$ $h_{j}(T) \alpha$, and since $W$ is invariant under $T$, each $\alpha_{j}$ is also in $W$. Thus each vector $\alpha$ in $W$ is of the form $\alpha=\alpha_{1}+\cdots+\alpha_{k}$, where $\alpha_{j}$ is in the intersection $W \cap W_{j}$. This expression is unique, since $V=W_{1} \oplus \cdots \oplus W_{k}$. Therefore

$$
W=\left(W \cap W_{1}\right) \oplus \cdots \oplus\left(W \cap W_{k}\right)
$$

Lemma. Let $\mathrm{T}$ be a linear operator on $\mathrm{V}$, and suppose that the minimal polynomial for $\mathrm{T}$ is irreducible over the scalar field $\mathrm{F}$. Then $\mathrm{T}$ is semi-simple.

Proof. Let $W$ be a subspace of $V$ which is invariant under $T$. We must prove that $W$ has a complementary $T$-invariant subspace. According to a corollary of Theorem 3, it will suffice to prove that if $f$ is a polynomial and $\beta$ is a vector in $V$ such that $f(T) \beta$ is in $W$, then there is a vector $\alpha$ in $W$ with $f(T) \beta=f(T) \alpha$. So suppose $\beta$ is in $V$ and $f$ is a polynomial such that $f(T) \beta$ is in $W$. If $f(T) \beta=0$, we let $\alpha=0$ and then $\alpha$ is a vector in $W$ with $f(T) \beta=f(T) \alpha$. If $f(T) \beta \neq 0$, the polynomial $f$ is not divisible by the minimal polynomial $p$ of the operator $T$. Since $p$ is prime, this means that $f$ and $p$ are relatively prime, and there exist polynomials $g$ and $h$ such that $f g+p h=1$. Because $p(T)=0$, we then have $f(T) g(T)=I$. From this it follows that the vector $\beta$ must itself be in the subspace $W$; for

$$
\begin{aligned}
\beta & =g(T) f(T) \beta \\
& =g(T)(f(T) \beta)
\end{aligned}
$$

while $f(T) \beta$ is in $W$ and $W$ is invariant under $T$. Take $\alpha=\beta$.

Theorem 11. Let $\mathrm{T}$ be a linear operator on the finite-dimensional vector space V. A necessary and sufficient condition that $\mathrm{T}$ be semi $i$-simple is that the minimal polynomial $\mathrm{p}$ for $\mathrm{T}$ be of the form $\mathrm{p}=\mathrm{p}_{\mathbf{1}} \cdots \mathrm{p}_{\mathrm{k}}$, where $\mathrm{p}_{\mathbf{1}}, \ldots, \mathrm{p}_{\mathrm{k}}$ are distinct irreducible polynomials over the scalar field $\mathrm{F}$.

Proof. Suppose $T$ is semi-simple. We shall show that no irreducible polynomial is repeated in the prime factorization of the minimal polynomial $p$. Suppose the contrary. Then there is some non-scalar monic polynomial $g$ such that $g^{2}$ divides $p$. Let $W$ be the null space of the operator $g(T)$. Then $W$ is invariant under $T$. Now $p=g^{2} h$ for some polynomial $h$. Since $g$ is not a scalar polynomial, the operator $g(T) h(T)$ is not the zero operator, and there is some vector $\beta$ in $V$ such that $g(T) h(T) \beta \neq 0$, i.e., $(g h) \beta \neq 0$. Now $(g h) \beta$ is in the subspace $W$, since $g(g h \beta)=g^{2} h \beta=$ $p \beta=0$. But there is no vector $\alpha$ in $W$ such that $g h \beta=g h \alpha$; for, if $\alpha$ is in $W$

$$
(g h) \alpha=(h g) \alpha=h(g \alpha)=h(0)=0 .
$$

Thus, $W$ cannot have a complementary $T$-invariant subspace, contradicting the hypothesis that $T$ is semi-simple.

Now suppose the prime factorization of $p$ is $p=p_{1} \cdots p_{k}$, where $p_{1}, \ldots, p_{k}$ are distinct irreducible (non-scalar) monic polynomials. Let $W$ be a subspace of $V$ which is invariant under $T$. We shall prove that $W^{\circ}$ has a complementary $T$-invariant subspace. Let $V=W_{1} \oplus \cdots \oplus W_{k}$ be the primary decomposition for $T$, i.e., let $W_{j}$ be the null space of $p_{j}(T)$. Let $T_{j}$ be the linear operator induced on $W_{j}$ by $T$, so that the minimal polynomial for $T_{j}$ is the prime $p_{j}$. Now $W \cap W_{j}$ is a subspace of $W_{j}$ which is invariant under $T_{j}$ (or under $T$ ). By the last lemma, there is a subspace $V_{j}$ of $W_{j}$ such that $W_{j}=\left(W \cap W_{j}\right) \oplus V_{j}$ and $V_{j}$ is invariant under $T_{j}$ (and hence under $T$ ). Then we have

$$
\begin{aligned}
V & =W_{1} \oplus \cdots \oplus W_{k} \\
& =\left(W \cap W_{1}\right) \oplus V_{1} \oplus \cdots \oplus\left(W \cap W_{k}\right) \oplus V_{k} \\
& =\left(W \cap W_{1}\right)+\cdots+\left(W \cap W_{k}\right) \oplus V_{1} \oplus \cdots \oplus V_{k} .
\end{aligned}
$$

By the first lemma above, $W=\left(W \cap W_{1}\right) \oplus \cdots \oplus\left(W \cap W_{k}\right)$, so that if $W^{\prime}=V_{1} \oplus \cdots \oplus V_{k}$, then $V=W \oplus W^{\prime}$ and $W^{\prime}$ is invariant under $T$.

Corollary. If $\mathrm{T}$ is a linear operator on a finite-dimensional vector space over an algebraically closed field, then $\mathrm{T}$ is semi-simple if and only if $\mathrm{T}$ is diagonalizable.

Proof. If the scalar field $F$ is algebraically closed, the monic primes over $F$ are the polynomials $x-c$. In this case, $T$ is semi-simple if and only if the minimal polynomial for $T$ is $p=\left(x-c_{1}\right) \cdots\left(x-c_{k}\right)$, where $c_{1}, \ldots, c_{k}$ are distinct elements of $F$. This is precisely the criterion for $T$ to be diagonalizable, which we established in Chapter 6.

We should point out that $T$ is semi-simple if and only if there is some polynomial $f$, which is a product of distinct primes, such that $f(T)=0$. This is only superficially different from the condition that the minimal polynomial be a product of distinct primes.

We turn now to expressing a linear operator as the sum of a semisimple operator and a nilpotent operator which commute. In this, we shall restrict the scalar field to a subfield of the complex numbers. The informed reader will see that what is important is that the field $F$ be a field of characteristic zero, that is, that for each positive integer $n$ the sum $1+\cdots+1$ ( $n$ times) in $F$ should not be 0 . For a polynomial $f$ over $F$, we denote by $f^{(k)}$ the $k$ th formal derivative of $f$. In other words, $f^{(k)}=D^{k} f$, where $D$ is the differentiation operator on the space of polynomials. If $g$ is another polynomial, $f(g)$ denotes the result of substituting $g$ in $f$, i.e., the polynomial obtained by applying $f$ to the element $g$ in the linear algebra $F[x]$. Lemma (Taylor's Formula). Let F be a field of characteristic zero and let $\mathrm{g}$ and $\mathrm{h}$ be polynomials over $\mathrm{F}$. If $\mathrm{f}$ is any polynomial over $\mathrm{F}$ with $\operatorname{deg} \mathrm{f} \leq \mathrm{n}$, then

$\mathrm{f}(\mathrm{g})=\mathrm{f}(\mathrm{h})+\mathrm{f}^{(1)}(\mathrm{h})(\mathrm{g}-\mathrm{h})+\frac{\mathrm{f}^{(2)}(\mathrm{h})}{2 !}(\mathrm{g}-\mathrm{h})^{2}+\cdots+\frac{\mathrm{f}^{(\mathrm{n})}(\mathrm{h})}{\mathrm{n} !}(\mathrm{g}-\mathrm{h})^{\mathrm{n}}$.

Proof. What we are proving is a generalized Taylor formula. The reader is probably used to seeing the special case in which $h=c$, a scalar polynomial, and $g=x$. Then the formula says

$f=f(x)=f(c)+f^{(1)}(c)(x-c)$

$$
+\frac{f^{(2)}(c)}{2 !}(x-c)^{2}+\cdots+\frac{f^{(n)}(c)}{n !}(x-c)^{n} .
$$

The proof of the general formula is just an application of the binomial theorem

$$
(a+b)^{k}=a^{k}+k a^{k-1} b+\frac{k(k-1)}{2 !} a^{k-2} b^{2}+\cdots+b^{k} .
$$

For the reader should see that, since substitution and differentiation are linear processes, one need only prove the formula when $f=x^{k}$. The formula for $f=\sum_{k=0}^{n} c_{k} x^{k}$ follows by a linear combination. In the case $f=x^{k}$ with $k \leq n$, the formula says

$$
g^{k}=h^{k}+k h^{k-1}(g-h)+\frac{k(k-1)}{2 !} h^{k-2}(g-h)^{2}+\cdots+(g-h)^{k}
$$

which is just the binomial expansion of

$$
g^{k}=[h+(g-h)]^{k} .
$$

Lemma. Let $\mathrm{F}$ be a subfield of the complex numbers, let $\mathrm{f}$ be a polynomial over $\mathrm{F}$, and let $\mathrm{f}^{\prime}$ be the derivative of $\mathrm{f}$. The following are equivalent:

(a) $\mathrm{f}$ is the product of distinct polynomials irreducible over $\mathrm{F}$.

(b) $\mathrm{f}$ and $\mathrm{f}^{\prime}$ are relatively prime.

(c) As a polynomial with complex coefficients, f has no repeated root.

Proof. Let us first prove that (a) and (b) are equivalent statements about $f$. Suppose in the prime factorization of $f$ over the field $F$ that some (non-scalar) prime polynomial $p$ is repeated. Then $f=p^{2} h$ for some $h$ in $F[x]$. Then

$$
f^{\prime}=p^{2} h^{\prime}+2 p p^{\prime} h
$$

and $p$ is also a divisor of $f^{\prime}$. Hence $f$ and $f^{\prime}$ are not relatively prime. We conclude that (b) implies (a).

Now suppose $f=p_{1} \cdots p_{k}$, where $p_{1}, \ldots, p_{k}$ are distinct non-scalar irreducible polynomials over $F$. Let $f_{j}=f / p_{j}$. Then

$$
f^{\prime}=p_{1}^{\prime} f_{1}+p_{2}^{\prime} f_{2}+\cdots+p_{k}^{\prime} f_{k} .
$$

Let $p$ be a prime polynomial which divides both $f$ and $f^{\prime}$. Then $p=p_{i}$ for some $i$. Now $p_{i}$ divides $f_{j}$ for $j \neq i$, and since $p_{i}$ also divides

$$
f^{\prime}=\sum_{j=1}^{k} p_{j}^{\prime} f_{j}
$$

we see that $p_{i}$ must divide $p_{i}^{\prime} f_{i}$. Therefore $p_{i}$ divides either $f_{i}$ or $p_{i}^{\prime}$. But $p_{i}$ does not divide $f_{i}$ since $p_{1}, \ldots, p_{k}$ are distinct. So $p_{i}$ divides $p_{i}^{\prime}$. This is not possible, since $p_{i}^{\prime}$ has degree one less than the degree of $p_{i}$. We conclude that no prime divides both $f$ and $f^{\prime}$, or that $\left(f, f^{\prime}\right)=1$.

To see that statement (c) is equivalent to (a) and (b), we need only observe the following: Suppose $f$ and $g$ are polynomials over $F$, a subfield of the complex numbers. We may also regard $f$ and $g$ as polynomials with complex coefficients. The statement that $f$ and $g$ are relatively prime as polynomials over $F$ is equivalent to the statement that $f$ and $g$ are relatively prime as polynomials over the field of complex numbers. We leave the proof of this as an exercise. We use this fact with $g=f^{\prime}$. Note that (c) is just (a) when $f$ is regarded as a polynomial over the field of complex numbers. Thus (b) and (c) are equivalent, by the same argument that we used above.

We can now prove a theorem which makes the relation between semisimple operators and diagonalizable operators even more apparent.

Theorem 12. Let $\mathrm{F}$ be a subfield of the field of complex numbers, let $\mathrm{V}$ be a finite-dimensional vector space over $\mathrm{F}$, and let $\mathrm{T}$ be a linear operator on $\mathrm{V}$. Let $\mathrm{B}$ be an ordered basis for $\mathrm{V}$ and let $\mathrm{A}$ be the matrix of $\mathrm{T}$ in the ordered basis $ß$. Then $\mathrm{T}$ is semi-simple if and only if the matrix $\mathrm{A}$ is similar over the field of complex numbers to a diagonal matrix.

Proof. Let $p$ be the minimal polynomial for $T$. According to Theorem 11, T is semi-simple if and only if $p=p_{1} \cdots p_{k}$ where $p_{1}, \ldots, p_{k}$ are distinct irreducible polynomials over $F$. By the last lemma, we see that $T$ is semi-simple if and only if $p$ has no repeated complex root.

Now $p$ is also the minimal polynomial for the matrix $A$. We know that $A$ is similar over the field of complex numbers to a diagonal matrix if and only if its minimal polynomial has no repeated complex root. This proves the theorem.

Theorem 13. Let $\mathrm{F}$ be a subfield of the field of complex numbers, let $\mathrm{V}$ be a finite-dimensional vector space over $\mathrm{F}$, and let $\mathrm{T}$ be a linear operator on $\mathrm{V}$. There is a semi-simple operator $\mathrm{S}$ on $\mathrm{V}$ and a nilpotent operator $\mathrm{N}$ on $\mathrm{V}$ such that

(i) $\mathrm{T}=\mathrm{S}+\mathrm{N}$;

(ii) $\mathrm{SN}=\mathrm{NS}$. Furthermore, the semi $i$-simple $\mathrm{S}$ and nilpotent $\mathrm{N}$ satisfying (i) and (ii) are unique, and each is a polynomial in $\mathrm{T}$.

Proof. Let $p_{1}^{r_{1}} \cdots p_{k}^{r_{k}}$ be the prime factorization of the minimal polynomial for $T$, and let $f=p_{1} \cdots p_{k}$. Let $r$ be the greatest of the positive integers $r_{1}, \ldots, r_{k}$. Then the polynomial $f$ is a product of distinct primes, $f^{r}$ is divisible by the minimal polynomial for $T$, and so

$$
f(T)^{r}=0 .
$$

We are going to construct a sequence of polynomials: $g_{0}, g_{1}, g_{2}, \ldots$ such that

$$
f\left(x-\sum_{j=0}^{n} g_{j} f^{i}\right)
$$

is divisible by $f^{n+1}, n=0,1,2, \ldots$ We take $g_{0}=0$ and then $f\left(x-g_{0} f^{0}\right)=$ $f(x)=f$ is divisible by $f$. Suppose we have chosen $g_{0}, \ldots, g_{n-\mathbf{1}}$. Let

$$
h=x-\sum_{j=0}^{n-1} g_{j} f^{j}
$$

so that, by assumption, $f(h)$ is divisible by $f^{n}$. We want to choose $g_{n}$ so that

$$
f\left(h-g_{n} f^{n}\right)
$$

is divisible by $f^{n+1}$. We apply the general Taylor formula and obtain

$$
f\left(h-g_{n} f^{n}\right)=f(h)-g_{n} f^{n} f^{\prime}(h)+f^{n+1} b
$$

where $b$ is some polynomial. By assumption $f(h)=q f^{n}$. Thus, we see that to have $f\left(h-g_{n} f^{n}\right)$ divisible by $f^{n+1}$ we need only choose $g_{n}$ in such a way that $\left(q-g_{n} f^{\prime}\right)$ is divisible by $f$. This can be done, because $f$ has no repeated prime factors and so $f$ and $f^{\prime}$ are relatively prime. If $a$ and $e$ are polynomials such that $a f+e f^{\prime}=1$, and if we let $g_{n}=e q$, then $q-g_{n} f^{\prime}$ is divisible by $f$.

Now we have a sequence $g_{0}, g_{1}, \ldots$ such that $f^{n+1}$ divides $f\left(x-\sum_{j=0}^{n} g_{j} f^{i}\right)$. Let us take $n=r-1$ and then since $f(T)^{r}=0$

Let

$$
f\left(T-\sum_{j=0}^{r-1} g_{j}(T) f(T)^{j}\right)=0 .
$$

$$
N=\sum_{j=1}^{r-1} g_{j}(T) f(T)^{j}=\sum_{j=0}^{r-1} g_{j}(T) f(T)^{j}
$$

Since $\sum_{j=1}^{n} g_{j} f^{j}$ is divisible by $f$, we see that $N^{r}=0$ and $N$ is nilpotent. Let $S=T-N$. Then $f(S)=f(T-N)=0$. Since $f$ has distinct prime factors, $S$ is semi-simple.

Now we have $T=S+N$ where $S$ is semi-simple, $N$ is nilpotent, and each is a polynomial in $T$. To prove the uniqueness statement, we shall pass from the scalar field $F$ to the field of complex numbers. Let $\&$ be some ordered basis for the space $V$. Then we have

![](https://cdn.mathpix.com/cropped/2023_01_16_0f6afdf132d84aaf6bdeg-278.jpg?height=54&width=333&top_left_y=320&top_left_x=532)

while $[S]_{\mathbb{B}}$ is diagonalizable over the complex numbers and $[N]_{\mathbb{\Omega}}$ is nilpotent. This diagonalizable matrix and nilpotent matrix which commute are uniquely determined, as we have shown in Chapter 6.

\section{Exercises}

1. If $N$ is a nilpotent linear operator on $V$, show that for any polynomial $f$ the semi-simple part of $f(N)$ is a scalar multiple of the identity operator ( $F$ a subfield of $C$ ).

2. Let $F$ be a subfield of the complex numbers, $V$ a finite-dimensional vector space over $F$, and $T$ a semi-simple linear operator on $V$. If $f$ is any polynomial over $F$, prove that $f(T)$ is semi-simple.

3. Let $T$ be a linear operator on a finite-dimensional space over a subfield of $C$. Prove that $T$ is semi-simple if and only if the following is true: If $f$ is a polynomial and $f(T)$ is nilpotent, then $f(T)=0$. 

\section{Inner Product}

\section{Spaces}

\subsection{Inner Products}

Throughout this chapter we consider only real or complex vector spaces, that is, vector spaces over the field of real numbers or the field of complex numbers. Our main object is to study vector spaces in which it makes sense to speak of the 'length' of a vector and of the 'angle' between two vectors. We shall do this by studying a certain type of scalar-valued function on pairs of vectors, known as an inner product. One example of an inner product is the scalar or dot product of vectors in $R^{3}$. The scalar product of the vectors

$$
\alpha=\left(x_{1}, x_{2}, x_{3}\right) \quad \text { and } \quad \beta=\left(y_{1}, y_{2}, y_{3}\right)
$$

in $R^{3}$ is the real number

$$
(\alpha \mid \beta)=x_{1} y_{1}+x_{2} y_{2}+x_{3} y_{3} .
$$

Geometrically, this dot product is the product of the length of $\alpha$, the length of $\beta$, and the cosine of the angle between $\alpha$ and $\beta$. It is therefore possible to define the geometric concepts of 'length' and 'angle' in $R^{3}$ by means of the algebraically defined scalar product.

An inner product on a vector space is a function with properties similar to the dot product in $R^{3}$, and in terms of such an inner product one can also define 'length' and 'angle.' Our comments about the general notion of angle will be restricted to the concept of perpendicularity (or orthogonality) of vectors. In this first section we shall say what an inner product is, consider some particular examples, and establish a few basic properties of inner products. Then we turn to the task of discussing length and orthogonality.

Definition. Let $\mathrm{F}$ be the field of real numbers or the field of complex numbers, and $\mathrm{V}$ a vector space over $\mathrm{F}$. An inner product on $\mathrm{V}$ is a function which assigns to each ordered pair of vectors $\alpha, \beta$ in $\mathrm{V}$ a scalar $(\alpha \mid \beta)$ in $\mathrm{F}$ in such a way that for all $\alpha, \beta, \gamma$ in $\mathrm{V}$ and all scalars $\mathrm{c}$

(a) $(\alpha+\beta \mid \gamma)=(\alpha \mid \gamma)+(\beta \mid \gamma)$

(b) $(\mathrm{c} \alpha \mid \beta)=\mathrm{c}(\alpha \mid \beta)$;

(c) $(\beta \mid \alpha)=(\bar{\alpha} \bar{\beta})$, the bar denoting complex conjugation;

(d) $(\alpha \mid \alpha)>0$ if $\alpha \neq 0$.

It should be observed that conditions (a), (b), and (c) imply that

$$
(\alpha \mid c \beta+\gamma)=\bar{c}(\alpha \mid \beta)+(\alpha \mid \gamma) .
$$

One other point should be made. When $F$ is the field $R$ of real numbers, the complex conjugates appearing in (c) and (e) are superfluous; however, in the complex case they are necessary for the consistency of the conditions. Without these complex conjugates, we would have the contradiction:

$$
(\alpha \mid \alpha)>0 \text { and } \quad(i \alpha \mid i \alpha)=-1(\alpha \mid \alpha)>0 .
$$

In the examples that follow and throughout the chapter, $F$ is either the field of real numbers or the field of complex numbers.

Example 1. On $F^{n}$ there is an inner product which we call the standard inner product. It is defined on $\alpha=\left(x_{1}, \ldots, x_{n}\right)$ and $\beta=$ $\left(y_{1}, \ldots, y_{n}\right)$ by

$$
(\alpha \mid \beta)=\sum_{j} x_{j} \bar{y}_{j} .
$$

When $F=R$, this may also be written

$$
(\alpha \mid \beta)=\sum_{j} x_{j} y_{j} .
$$

In the real case, the standard inner product is of called the dot or scalar product and denoted by $\alpha \cdot \beta$.

EXAmple 2. For $\alpha=\left(x_{1}, x_{2}\right)$ and $\beta=\left(y_{1}, y_{2}\right)$ in $R^{2}$, let

$$
(\alpha \mid \beta)=x_{1} y_{1}-x_{2} y_{1}-x_{1} y_{2}+4 x_{2} y_{2} .
$$

Since $(\alpha \mid \alpha)=\left(x_{1}-x_{2}\right)^{2}+3 x_{2}^{2}$, it follows that $(\alpha \mid \alpha)>0$ if $\alpha \neq 0$. Conditions (a), (b), and (c) of the definition are easily verified.

Example 3. Let $V$ be $F^{n \times n}$, the space of all $n \times n$ matrices over $F$. Then $V$ is isomorphic to $F^{n^{2}}$ in a natural way. It therefore follows from Example 1 that the equation

$$
(A \mid B)=\sum_{j, k} A_{j k} \bar{B}_{j k}
$$

defines an inner product on $V$. Furthermore, if we introduce the con jugate transpose matrix $B^{*}$, where $B_{k j}^{*}=\bar{B}_{j k}$, we may express this inner product on $F^{n \times n}$ in terms of the trace function:

For

$$
(A \mid B)=\operatorname{tr}\left(A B^{*}\right)=\operatorname{tr}\left(B^{*} A\right) .
$$

$$
\begin{aligned}
\operatorname{tr}\left(A B^{*}\right) & =\sum_{j}\left(A B^{*}\right)_{j j} \\
& =\sum_{j} \sum_{k} A_{j k} B_{k j}^{*} \\
& =\sum_{j} \sum_{k} A_{j k} \bar{B}_{j k .}
\end{aligned}
$$

EXample 4. Let $F^{n \times 1}$ be the space of $n \times 1$ (column) matrices over $F$, and let $Q$ be an $n \times n$ invertible matrix over $F$. For $X, Y$ in $F^{n \times 1}$ set

$$
(X \mid Y)=Y^{*} Q^{*} Q X .
$$

We are identifying the $1 \times 1$ matrix on the right with its single entry. When $Q$ is the identity matrix, this inner product is essentially the same as that in Example 1 ; we call it the standard inner product on $F_{n \times 1}$. The reader should note that the terminology 'standard inner product' is used in two special contexts. For a general finite-dimensional vector space over $F$, there is no obvious inner product that one may call standard.

Example 5. Let $V$ be the vector space of all continuous complexvalued functions on the unit interval, $0 \leq t \leq 1$. Let

$$
(f \mid g)=\int_{0}^{1} f(t) \overline{g(t)} d t .
$$

The reader is probably more familiar with the space of real-valued continuous functions on the unit interval, and for this space the complex conjugate on $g$ may be omitted.

Example 6. This is really a whole class of examples. One may construct new inner products from a given one by the following method. Let $V$ and $W$ be vector spaces over $F$ and suppose ( $\mid$ ) is an inner product on $W$. If $T$ is a non-singular linear transformation from $V$ into $W$, then the equation

$$
p_{T}(\alpha, \beta)=(T \alpha \mid T \beta)
$$

defines an inner product $p_{T}$ on $V$. The inner product in Example 4 is a special case of this situation. The following are also special cases.

(a) Let $V$ be a finite-dimensional vector space, and let

$$
B=\left\{\alpha_{1}, \ldots, \alpha_{n}\right\}
$$

be an ordered basis for $V$. Let $\epsilon_{1}, \ldots, \epsilon_{n}$ be the standard basis vectors in $F^{n}$, and let $T$ be the linear transformation from $V$ into $F^{n}$ such that $T \alpha_{j}=$ $\epsilon_{j}, j=1, \ldots, n$. In other words, let $T$ be the 'natural' isomorphism of $V$ onto $F^{n}$ that is determined by $B$. If we take the standard inner product on $F^{n}$, then

$$
p_{T}\left(\sum_{j} x_{j} \alpha_{j}, \sum_{k} y_{k} \alpha_{k}\right)=\sum_{j=1}^{n} x_{j} \bar{y}_{j} .
$$

Thus, for any basis for $V$ there is an inner product on $V$ with the property $\left(\alpha_{j} \mid \alpha_{k}\right)=\delta_{j k}$; in fact, it is easy to show that there is exactly one such inner product. Later we shall show that every inner product on $V$ is determined by some basis $B$ in the above manner.

(b) We look again at Example 5 and take $V=W$, the space of continuous functions on the unit interval. Let $T$ be the linear operator 'multiplication by $t$,' that is, $(T f)(t)=t f(t), 0 \leq t \leq 1$. It is easy to see that $T$ is linear. Also $T$ is non-singular; for suppose $T f=0$. Then $t f(t)=0$ for $0 \leq t \leq 1$; hence $f(t)=0$ for $t>0$. Since $f$ is continuous, we have $f(0)=0$ as well, or $f=0$. Now using the inner product of Example 5, we construct a new inner product on $V$ by setting

$$
\begin{aligned}
p_{T}(f, g) & =\int_{0}^{1}(T f)(t) \overline{(T g)(t)} d t \\
& =\int_{0}^{1} f(t) \overline{g(t)} t^{2} d t .
\end{aligned}
$$

We turn now to some general observations about inner products. Suppose $V$ is a complex vector space with an inner product. Then for all $\alpha, \beta$ in $V$

$$
(\alpha \mid \beta)=\operatorname{Re}(\alpha \mid \beta)+i \operatorname{Im}(\alpha \mid \beta)
$$

where $\operatorname{Re}(\alpha \mid \beta)$ and $\operatorname{Im}(\alpha \mid \beta)$ are the real and imaginary parts of the complex number $(\alpha \mid \beta)$. If $z$ is a complex number, then $\operatorname{Im}(z)=\operatorname{Re}(-i z)$. It follows that

$$
\operatorname{Im}(\alpha \mid \beta)=\operatorname{Re}[-i(\alpha \mid \beta)]=\operatorname{Re}(\alpha \mid i \beta) .
$$

Thus the inner product is completely determined by its 'real part' in accordance with

$$
(\alpha \mid \beta)=\operatorname{Re}(\alpha \mid \beta)+i \operatorname{Re}(\alpha \mid i \beta) .
$$

Occasionally it is very useful to know that an inner product on a real or complex vector space is determined by another function, the so-called quadratic form determined by the inner product. To define it, we first denote the positive square root of $(\alpha \mid \alpha)$ by $\|\alpha\| ;\|\alpha\|$ is called the norm of $\alpha$ with respect to the inner product. By looking at the standard inner products in $R^{1}, C^{1}, R^{2}$, and $R^{3}$, the reader should be able to convince himself that it is appropriate to think of the norm of $\alpha$ as the 'length' or 'magnitude' of $\alpha$. The quadratic form determined by the inner product is the function that assigns to each vector $\alpha$ the scalar $\|\alpha\|^{2}$. It follows from the properties of the inner product that

$$
\|\alpha \pm \beta\|^{2}=\|\alpha\|^{2} \pm 2 \operatorname{Re}(\alpha \mid \beta)+\|\beta\|^{2}
$$

for all vectors $\alpha$ and $\beta$. Thus in the real case

$$
(\alpha \mid \beta)=\frac{1}{4}\|\alpha+\beta\|^{2}-\frac{1}{4}\|\alpha-\beta\|^{2} .
$$

In the complex case we use (8-2) to obtain the more complicated expression

$$
(\alpha \mid \beta)=\frac{1}{4}\|\alpha+\beta\|^{2}-\frac{1}{4}\|\alpha-\beta\|^{2}+\frac{i}{4}\|\alpha+i \beta\|^{2}-\frac{i}{4}\|\alpha-i \beta\|^{2} .
$$

Equations (8-3) and (8-4) are called the polarization identities. Note that (8-4) may also be written as follows:

$$
(\alpha \mid \beta)=\frac{1}{4} \sum_{n=1}^{4} i^{n}\left\|\alpha+i^{n} \beta\right\|^{2} .
$$

The properties obtained above hold for any inner product on a real or complex vector space $V$, regardless of its dimension. We turn now to the case in which $V$ is finite-dimensional. As one might guess, an inner product on a finite-dimensional space may always be described in terms of an ordered basis by means of a matrix.

Suppose that $V$ is finite-dimensional, that

$$
B=\left\{\alpha_{1}, \ldots, \alpha_{n}\right\}
$$

is an ordered basis for $V$, and that we are given a particular inner product on $V$; we shall show that the inner product is completely determined by the values

$$
G_{i k}=\left(\alpha_{k} \mid \alpha_{j}\right)
$$

it assumes on pairs of vectors in $B$. If $\alpha=\sum_{k} x_{k} \alpha_{k}$ and $\beta=\sum_{j} y_{j} \alpha_{j}$, then

$$
\begin{aligned}
(\alpha \mid \beta) & =\left(\sum_{k} x_{n} \alpha_{k} \mid \beta\right) \\
& =\sum_{k} x_{k}\left(\alpha_{k} \mid \beta\right) \\
& =\sum_{k} x_{k} \sum_{j} \bar{y}_{j}\left(\alpha_{k} \mid \alpha_{j}\right) \\
& =\sum_{j, k} \bar{y}_{j} G_{j k} x_{k} \\
& =Y^{*} G X
\end{aligned}
$$

where $X, Y$ are the coordinate matrices of $\alpha, \beta$ in the ordered basis $B$, and $G$ is the matrix with entries $G_{j k}=\left(\alpha_{k} \mid \alpha_{j}\right)$. We call $G$ the matrix of the inner product in the ordered basis Q. It follows from (8-5) that $G$ is hermitian, i.e., that $G=G^{*}$; however, $G$ is a rather special kind of hermitian matrix. For $G$ must satisfy the additional condition

$$
X^{*} G X>0, \quad X \neq 0 .
$$

In particular, $G$ must be invertible. For otherwise there exists an $X \neq 0$ such that $G X=0$, and for any such $X$, (8-6) is impossible. More explicitly, (8-6) says that for any scalars $x_{1}, \ldots, x_{n}$ not all of which are 0

$$
\sum_{j, k} \bar{x}_{j} G_{j k} x_{k}>0 .
$$

From this we see immediately that each diagonal entry of $G$ must be positive; however, this condition on the diagonal entries is by no means sufficient to insure the validity of (8-6). Sufficient conditions for the validity of (8-6) will be given later.

The above process is reversible; that is, if $G$ is any $n \times n$ matrix over $F$ which satisfies (8-6) and the condition $G=G^{*}$, then $G$ is the matrix in the ordered basis $B$ of an inner product on $V$. This inner product is given by the equation

$$
(\alpha \mid \beta)=Y^{*} G X
$$

where $X$ and $Y$ are the coordinate matrices of $\alpha$ and $\beta$ in the ordered basis $\Theta$.

\section{Exercises}

1. Let $V$ be a vector space and $(\mid)$ an inner product on $V$.

(a) Show that $(0 \mid \beta)=0$ for all $\beta$ in $V$.

(b) Show that if $(\alpha \mid \beta)=0$ for all $\beta$ in $V$, then $\alpha=0$.

2. Let $V$ be a vector space over $F$. Show that the sum of two inner products on $V$ is an inner product on $V$. Is the difference of two inner products an inner product? Show that a positive multiple of an inner product is an inner product.

3. Describe explicitly all inner products on $R^{1}$ and on $C^{1}$.

4. Verify that the standard inner product on $F^{n}$ is an inner product.

5. Let $(\mid)$ be the standard inner product on $R^{2}$.

(a) Let $\alpha=(1,2), \beta=(-1,1)$. If $\gamma$ is a vector such that $(\alpha \mid \gamma)=-1$ and $(\beta \mid \gamma)=3$, find $\gamma$.

(b) Show that for any $\alpha$ in $R^{2}$ we have $\alpha=\left(\alpha \mid \epsilon_{1}\right) \epsilon_{1}+\left(\alpha \mid \epsilon_{2}\right) \epsilon_{2}$.

6. Let ( 1 ) be the standard inner product on $R^{2}$, and let $T$ be the linear operator $T\left(x_{1}, x_{2}\right)=\left(-x_{2}, x_{1}\right)$. Now $T$ is 'rotation through $90^{\circ}$ ' and has the property that $(\alpha \mid T \alpha)=0$ for all $\alpha$ in $R^{2}$. Find all inner products [ | ] on $R^{2}$ such that $[\alpha \mid T \alpha]=0$ for each $\alpha$.

7. Let $(\mid)$ be the standard inner product on $C^{2}$. Prove that there is no nonzero linear operator on $C^{2}$ such that $(\alpha \mid T \alpha)=0$ for every $\alpha$ in $C^{2}$. Generalize. 8. Let $A$ be a $2 \times 2$ matrix with real entries. For $X, Y$ in $R^{2 \times 1}$ let

$$
f_{A}(X, Y)=Y^{t} A X
$$

Show that $f_{A}$ is an inner product on $R^{2 \times 1}$ if and only if $A=A^{t}, A_{11}>0, A_{22}>0$, and $\operatorname{det} A>0$.

9. Let $V$ be a real or complex vector space with an inner product. Show that the quadratic form determined by the inner product satisfies the parallelogram law

$$
\|\alpha+\beta\|^{2}+\|\alpha-\beta\|^{2}=2\|\alpha\|^{2}+2\|\beta\|^{2} .
$$

10. Let $(\mid)$ be the inner product on $R^{2}$ defined in Example 2, and let $B$ be the standard ordered basis for $R^{2}$. Find the matrix of this inner product relative to $B$.

11. Show that the formula

$$
\left(\sum_{j} a_{j} x^{i} \mid \sum_{k} b_{k} x^{k}\right)=\sum_{j, k} \frac{a_{j} b_{k}}{j+k+1}
$$

defines an inner product on the space $R[x]$ of polynomials over the field $R$. Let $W$ be the subspace of polynomials of degree less than or equal to $n$. Restrict the above inner product to $W$, and find the matrix of this inner product on $W$, relative to the ordered basis $\left\{1, x, x^{2}, \ldots, x^{n}\right\}$. (Hint: To show that the formula defines an inner product, observe that

$$
(f \mid g)=\int_{0}^{1} f(t) g(t) d t
$$

and work with the integral.)

12. Let $V$ be a finite-dimensional vector space and let $B=\left\{\alpha_{1}, \ldots, \alpha_{n}\right\}$ be a basis for $V$. Let $\left(\mid\right.$ ) be an inner product on $V$. If $c_{1}, \ldots, c_{n}$ are any $n$ scalars, show that there is exactly one vector $\alpha$ in $V$ such that $\left(\alpha \mid \alpha_{j}\right)=c_{j}, j=1, \ldots, n$. 13. Let $V$ be a complex vector space. A function $J$ from $V$ into $V$ is called a conjugation if $J(\alpha+\beta)=J(\alpha)+J(\beta), J(c \alpha)=\bar{c} J(\alpha)$, and $J(J(\alpha))=\alpha$, for all scalars $c$ and all $\alpha, \beta$ in $V$. If $J$ is a conjugation show that:

(a) The set $W$ of all $\alpha$ in $V$ such that $J \alpha=\alpha$ is a vector space over $R$ with respect to the operations defined in $V$.

(b) For each $\alpha$ in $V$ there exist unique vectors $\beta, \gamma$ in $W$ such that $\alpha=\beta+i \gamma$.

14. Let $V$ be a complex vector space and $W$ a subset of $V$ with the following properties:

(a) $W$ is a real vector space with respect to the operations defined in $V$.

(b) For each $\alpha$ in $V$ there exist unique vectors $\beta, \gamma$ in $W$ such that $\alpha=\beta+i \gamma$.

Show that the equation $J \alpha=\beta-i \gamma$ defines a conjugation on $V$ such that $J \alpha=\alpha$ if and only if $\alpha$ belongs to $W$, and show also that $J$ is the only conjugation on $V$ with this property.

15. Find all conjugations on $C^{1}$ and $C^{2}$.

16. Let $W$ be a finite-dimensional real subspace of a complex vector space $V$. Show that $W$ satisfies condition (b) of Exercise 14 if and only if every basis of $W$ is also a basis of $V$. 17. Let $V$ be a complex vector space, $J$ a conjugation on $V, W$ the set of $\alpha$ in $V$ such that $J \alpha=\alpha$, and $f$ an inner product on $W$. Show that:

(a) There is a unique inner product $g$ on $V$ such that $g(\alpha, \beta)=f(\alpha, \beta)$ for all $\alpha, \beta$ in $W$,

(b) $g(J \alpha, J \beta)=g(\beta, \alpha)$ for all $\alpha, \beta$ in $V$.

What does part (a) say about the relation between the standard inner products on $R^{1}$ and $C^{1}$, or on $R^{n}$ and $C^{n}$ ?

\subsection{Inner Product Spaces}

Now that we have some idea of what an inner product is, we shall turn our attention to what can be said about the combination of a vector space and some particular inner product on it. Specifically, we shall establish the basic properties of the concepts of 'length' and 'orthogonality' which are imposed on the space by the inner product.

Definition. An inner product space is a real or complex vector space, together with a specified inner product on that space.

A finite-dimensional real inner product space is often called a Euclidean space. A complex inner product space is of ten referred to as a unitary space.

Theorem 1. If $\mathrm{V}$ is an inner product space, then for any vectors $\alpha, \beta$ in $\mathrm{V}$ and any scalar c

(i) $\|\mathrm{c} \alpha\|=|\mathrm{c}|\|\alpha\|$;

(ii) $\|\alpha\|>0$ for $\alpha \neq 0$;

(iii) $|(\alpha \mid \beta)| \leq\|\alpha\|\|\beta\|$;

(iv) $\|\alpha+\beta\| \leq\|\alpha\|+\|\beta\|$.

Proof. Statements (i) and (ii) follow almost immediately from the various definitions involved. The inequality in (iii) is clearly valid when $\alpha=0$. If $\alpha \neq 0$, put

$$
\gamma=\beta-\frac{(\beta \mid \alpha)}{\|\alpha\|^{2}} \alpha
$$

Then $(\gamma \mid \alpha)=0$ and

$$
\begin{aligned}
0 \leq\|\gamma\|^{2} & =\left(\beta-\frac{(\beta \mid \alpha)}{\|\alpha\|^{2}} \alpha \mid \beta-\frac{(\beta \mid \alpha)}{\|\alpha\|^{2}} \alpha\right) \\
& =(\beta \mid \beta)-\frac{(\beta \mid \alpha)(\alpha \mid \beta)}{\|\alpha\|^{2}} \\
& =\|\beta\|^{2}-\frac{|(\alpha \mid \beta)|^{2}}{\|\alpha\|^{2}} .
\end{aligned}
$$

Hence $|(\alpha \mid \beta)|^{2} \leq\left.\|\alpha\|\right|^{2}|| \beta \|^{2}$. Now using (c) we find that

$$
\begin{aligned}
\|\alpha+\beta\|^{2} & =\|\alpha\|^{2}+(\alpha \mid \beta)+(\beta \mid \alpha)+\|\beta\|^{2} \\
& =\|\alpha\|^{2}+2 \operatorname{Re}(\alpha \mid \beta)+\|\beta\|^{2} \\
& \leq\|\alpha\|^{2}+2\|\alpha\|\|\beta\|+\|\beta\|^{2} \\
& =(\|\alpha\|+\|\beta\|)^{2} .
\end{aligned}
$$

Thus, $\|\alpha+\beta\| \leq\|\alpha\|+\|\beta\|$.

The inequality in (iii) is called the Cauchy-Schwarz inequality. It has a wide variety of applications. The proof shows that if (for example) $\alpha$ is non-zero, then $|(\alpha \mid \beta)|<\|\alpha\|\|\beta\|$ unless

$$
\beta=\frac{(\beta \mid \alpha)}{\|\alpha\|^{2}} \alpha .
$$

Thus, equality occurs in (iii) if and only if $\alpha$ and $\beta$ are linearly dependent.

Example 7. If we apply the Cauchy-Schwarz inequality to the inner products given in Examples 1, 2, 3, and 5, we obtain the following:

(a) $\quad\left|\Sigma x_{k} \bar{y}_{k}\right| \leq\left(\Sigma\left|x_{k}\right|^{2}\right)^{1 / 2}\left(\Sigma\left|y_{k}\right|^{2}\right)^{1 / 2}$

(b) $\quad\left|x_{1} y_{1}-x_{2} y_{1}-x_{1} y_{2}+4 x_{2} y_{2}\right|$ $\leq\left(\left(x_{1}-x_{2}\right)^{2}+3 x_{2}^{2}\right)^{1 / 2}\left(\left(y_{1}-y_{2}\right)^{2}+3 y_{2}^{2}\right)^{1 / 2}$

(c) $\quad\left|\operatorname{tr}\left(A B^{*}\right)\right| \leq\left(\operatorname{tr}\left(A A^{*}\right)\right)^{1 / 2}\left(\operatorname{tr}\left(B B^{*}\right)\right)^{1 / 2}$

(d) $\quad\left|\int_{0}^{1} f(x) \overline{g(x)} d x\right| \leq\left(\int_{0}^{1}|f(x)|^{2} d x\right)^{1 / 2}\left(\int_{0}^{1}|g(x)|^{2} d x\right)^{1 / 2}$.

Definitions. Let $\alpha$ and $\beta$ be vectors in an inner product space V. Then $\alpha$ is orthogonal to $\beta$ if $(\alpha \mid \beta)=0$; since this implies $\beta$ is orthogonal to $\alpha$, we often simply say that $\alpha$ and $\beta$ are orthogonal. If $\mathrm{S}$ is a set of vectors in $\mathrm{V}$, $\mathrm{S}$ is called an orthogonal set provided all pairs of distinct vectors in $\mathrm{S}$ are orthogonal. An orthonormal set is an orthogonal set $\mathrm{S}$ with the additional property that $\|\alpha\|=1$ for every $\alpha$ in $\mathrm{S}$.

The zero vector is orthogonal to every vector in $V$ and is the only vector with this property. It is appropriate to think of an orthonormal set as a set of mutually perpendicular vectors, each having length 1.

Example 8. The standard basis of either $R^{n}$ or $C^{n}$ is an orthonormal set with respect to the standard inner product.

Example 9 . The vector $(x, y)$ in $R^{2}$ is orthogonal to $(-y, x)$ with respect to the standard inner product, for

$$
((x, y) \mid(-y, x))=-x y+y x=0 .
$$

However, if $R^{2}$ is equipped with the inner product of Example 2, then $(x, y)$ and $(-y, x)$ are orthogonal if and only if 

$$
y=\frac{1}{2}(-3 \pm \sqrt{13}) x .
$$

Example 10. Let $V$ be $C^{n \times n}$, the space of complex $n \times n$ matrices, and let $E^{p q}$ be the matrix whose only non-zero entry is a 1 in row $p$ and column $q$. Then the set of all such matrices $E^{q q}$ is orthonormal with respect to the inner product given in Example 3. For

$$
\left(E^{p q} \mid E^{r s}\right)=\operatorname{tr}\left(E^{p q} E^{\mathrm{ar}}\right)=\delta_{q s} \operatorname{tr}\left(E^{p r}\right)=\delta_{q s} \delta_{p r} .
$$

Example 11. Let $V$ be the space of continuous complex-valued (or real-valued) functions on the interval $0 \leq x \leq 1$ with the inner product

$$
(f \mid g)=\int_{0}^{1} f(x) \overline{g(x)} d x .
$$

Suppose $f_{n}(x)=\sqrt{2} \cos 2 \pi n x$ and that $g_{n}(x)=\sqrt{2} \sin 2 \pi n x$. Then $\left\{1, f_{1}, g_{1}, f_{2}, g_{2}, \ldots\right\}$ is an infinite orthonormal set. In the complex case, we may also form the linear combinations

$$
\frac{1}{\sqrt{2}}\left(f_{n}+i g_{n}\right), \quad n=1,2, \ldots
$$

In this way we get a new orthonormal set $S$ which consists of all functions of the form

$$
h_{n}(x)=e^{2 \pi i n x}, \quad n=\pm 1, \pm 2, \ldots
$$

The set $S^{\prime}$ obtained from $S$ by adjoining the constant function 1 is also orthonormal. We assume here that the reader is familiar with the calculation of the integrals in question.

The orthonormal sets given in the examples above are all linearly independent. We show now that this is necessarily the case.

Theorem 2. An orthogonal set of non-zero vectors is linearly independent.

Proof. Let $S$ be a finite or infinite orthogonal set of non-zero vectors in a given inner product space. Suppose $\alpha_{1}, \alpha_{2}, \ldots, \alpha_{m}$ are distinct vectors in $S$ and that

Then

$$
\beta=c_{1} \alpha_{1}+c_{2} \alpha_{2}+\cdots+c_{m} \alpha_{m} .
$$

$$
\begin{aligned}
\left(\beta \mid \alpha_{k}\right) & =\left(\sum_{j} c_{j} \alpha_{j} \mid \alpha_{k}\right) \\
& =\sum_{j} c_{j}\left(\alpha_{j} \mid \alpha_{k}\right) \\
& =c_{k}\left(\alpha_{k} \mid \alpha_{k}\right) .
\end{aligned}
$$

Since $\left(\alpha_{k} \mid \alpha_{k}\right) \neq 0$, it follows that 

$$
c_{k}=\frac{\left(\beta \mid \alpha_{k}\right)}{\left\|\alpha_{k}\right\|^{2}}, \quad 1 \leq k \leq m
$$

Thus when $\beta=0$, each $c_{k}=0 ;$ so $S$ is an independent set.

Corollary. If a vector $\beta$ is a linear combination of an orthogonal sequence of non-zero vectors $\alpha_{1}, \ldots, \alpha_{m}$, then $\beta$ is the particular linear combination

$$
\beta=\sum_{k=1}^{m} \frac{\left(\beta \mid \alpha_{k}\right)}{\left\|\alpha_{k}\right\|^{2}} \alpha_{k}
$$

This corollary follows from the proof of the theorem. There is another corollary which although obvious, should be mentioned. If $\left\{\alpha_{1}, \ldots, \alpha_{m}\right\}$ is an orthogonal set of non-zero vectors in a finite-dimensional inner product space $V$, then $m \leq \operatorname{dim} V$. This says that the number of mutually orthogonal directions in $V$ cannot exceed the algebraically defined dimension of $V$. The maximum number of mutually orthogonal directions in $V$ is what one would intuitively regard as the geometric dimension of $V$, and we have just seen that this is not greater than the algebraic dimension. The fact that these two dimensions are equal is a particular corollary of the next result.

Theorem 3. Let $\mathrm{V}$ be an inner product space and let $\beta_{1}, \ldots, \beta_{n}$ be any independent vectors in $\mathrm{V}$. Then one may construct orthogonal vectors $\alpha_{1}, \ldots, \alpha_{\mathrm{n}}$ in $\mathrm{V}$ such that for each $\mathrm{k}=1,2, \ldots, \mathrm{n}$ the set

$$
\left\{\alpha_{1}, \ldots, \alpha_{\mathrm{k}}\right\}
$$

is a basis for the subspace spanned by $\beta_{1}, \ldots, \beta_{\mathrm{k}}$.

Proof. The vectors $\alpha_{1}, \ldots, \alpha_{n}$ will be obtained by means of a construction known as the Gram-Schmidl orthogonalization process. First let $\alpha_{1}=\beta_{1}$. The other vectors are then given inductively as follows: Suppose $\alpha_{1}, \ldots, \alpha_{m}(1 \leq m<n)$ have been chosen so that for every $k$

$$
\left\{\alpha_{1}, \ldots, \alpha_{k}\right\}, \quad 1 \leq k \leq m
$$

is an orthogonal basis for the subspace of $V$ that is spanned by $\beta_{1}, \ldots, \beta_{k}$. To construct the next vector $\alpha_{m+1}$, let

$$
\alpha_{m+1}=\beta_{m+1}-\sum_{k=1}^{m} \frac{\left(\beta_{m+1} \mid \alpha_{k}\right)}{\left\|\alpha_{k}\right\|^{2}} \alpha_{k} .
$$

Then $\alpha_{m+1} \neq 0$. For otherwise $\beta_{m+1}$ is a linear combination of $\alpha_{1}, \ldots, \alpha_{m}$ and hence a linear combination of $\beta_{1}, \ldots, \beta_{m}$. Furthermore, if $1 \leq j \leq m$, then

$$
\begin{aligned}
\left(\alpha_{m+1} \mid \alpha_{j}\right) & =\left(\beta_{m+1} \mid \alpha_{j}\right)-\sum_{k=1}^{m} \frac{\left(\beta_{m+1} \mid \alpha_{k}\right)}{\left\|\alpha_{k}\right\|^{2}}\left(\alpha_{k} \mid \alpha_{j}\right) \\
& =\left(\beta_{m+1} \mid \alpha_{j}\right)-\left(\beta_{m+1} \mid \alpha_{j}\right) \\
& =0 .
\end{aligned}
$$

Therefore $\left\{\alpha_{1}, \ldots, \alpha_{m+1}\right\}$ is an orthogonal set consisting of $m+1$ nonzero vectors in the subspace spanned by $\beta_{1}, \ldots, \beta_{m+1}$. By Theorem 2 , it is a basis for this subspace. Thus the vectors $\alpha_{1}, \ldots, \alpha_{n}$ may be constructed one after the other in accordance with (8-9). In particular, when $n=4$, we have

$$
\begin{aligned}
& \alpha_{1}=\beta_{1} \\
& \alpha_{2}=\beta_{2}-\frac{\left(\beta_{2} \mid \alpha_{1}\right)}{\left\|\alpha_{1}\right\|^{2}} \alpha_{1} \\
& \alpha_{3}=\beta_{3}-\frac{\left(\beta_{3} \mid \alpha_{1}\right)}{\left\|\alpha_{1}\right\|^{2}} \alpha_{1}-\frac{\left(\beta_{3} \mid \alpha_{2}\right)}{\left\|\alpha_{2}\right\|^{2}} \alpha_{2} \\
& \alpha_{4}=\beta_{4}-\frac{\left(\beta_{4} \mid \alpha_{1}\right)}{\left\|\alpha_{1}\right\|^{2}} \alpha_{1}-\frac{\left(\beta_{4} \mid \alpha_{2}\right)}{\left\|\alpha_{2}\right\|^{2}} \alpha_{2}-\frac{\left(\beta_{4} \mid \alpha_{3}\right)}{\left\|\alpha_{3}\right\|^{2}} \alpha_{3} .
\end{aligned}
$$

Corollary. Every finite-dimensional inner product space has an orthonormal basis.

Proof. Let $V$ be a finite-dimensional inner product space and $\left\{\beta_{1}, \ldots, \beta_{n}\right\}$ a basis for $V$. Apply the Gram-Schmidt process to construct an orthogonal basis $\left\{\alpha_{1}, \ldots, \alpha_{n}\right\}$. Then to obtain an orthonormal basis, simply replace each vector $\alpha_{k}$ by $\alpha_{k} /\left\|\alpha_{k}\right\|$.

One of the main advantages which orthonormal bases have over arbitrary bases is that computations involving coordinates are simpler. To indicate in general terms why this is true, suppose that $V$ is a finitedimensional inner product space. Then, as in the last section, we may use Equation (8-5) to associate a matrix $G$ with every ordered basis $B=$ $\left\{\alpha_{1}, \ldots, \alpha_{n}\right\}$ of $V$. Using this matrix

$$
G_{j k}=\left(\alpha_{k} \mid \alpha_{j}\right),
$$

we may compute inner products in terms of coordinates. If $B$ is an orthonormal basis, then $G$ is the identity matrix, and for any scalars $x_{j}$ and $y_{k}$

$$
\left(\sum_{j} x_{j} \alpha_{j} \mid \sum_{k} y_{k} \alpha_{k}\right)=\sum_{j} x_{j} \bar{y}_{j} .
$$

Thus in terms of an orthonormal basis, the inner product in $V$ looks like the standard inner product in $F^{n}$.

Although it is of limited practical use for computations, it is interesting to note that the Gram-Schmidt process may also be used to test for linear dependence. For suppose $\beta_{1}, \ldots, \beta_{n}$ are linearly dependent vectors in an inner product space $V$. To exclude a trivial case, assume that $\beta_{1} \neq 0$. Let $m$ be the largest integer for which $\beta_{1}, \ldots, \beta_{m}$ are independent. Then $1 \leq m<n$. Let $\alpha_{1}, \ldots, \alpha_{m}$ be the vectors obtained by applying the orthogonalization process to $\beta_{1}, \ldots, \beta_{m}$. Then the vector $\alpha_{m+1}$ given by (8-9) is necessarily 0 . For $\alpha_{m+1}$ is in the subspace spanned by $\alpha_{1}, \ldots, \alpha_{m}$ and orthogonal to each of these vectors; hence it is 0 by (8-8). Conversely, if $\alpha_{1}, \ldots, \alpha_{m}$ are different from 0 and $\alpha_{m+1}=0$, then $\beta_{1}, \ldots, \beta_{m+1}$ are linearly dependent.

Example 12. Consider the vectors

$$
\begin{aligned}
& \beta_{1}=(3,0,4) \\
& \beta_{2}=(-1,0,7) \\
& \beta_{3}=(2,9,11)
\end{aligned}
$$

in $R^{3}$ equipped with the standard inner product. Applying the GramSchmidt process to $\beta_{1}, \beta_{2}, \beta_{3}$, we obtain the following vectors.

$$
\begin{aligned}
& \alpha_{1}=(3,0,4) \\
& \alpha_{2}=(-1,0,7)-\frac{((-1,0,7) \mid(3,0,4))}{25}(3,0,4) \\
&=(-1,0,7)-(3,0,4) \\
&=(-4,0,3) \\
& \alpha_{3}=(2,9,11)-\frac{((2,9,11) \mid(3,0,4))}{25}(3,0,4) \\
& \quad-\frac{((2,9,11) \mid(-4,0,3))}{25}(-4,0,3) \\
&=(2,9,11)-2(3,0,4)--4,0,3) \\
&=(0,9,0) .
\end{aligned}
$$

These vectors are evidently non-zero and mutually orthogonal. Hence $\left\{\alpha_{1}, \alpha_{2}, \alpha_{3}\right\}$ is an orthogonal basis for $R^{3}$. To express an arbitrary vector $\left(x_{1}, x_{2}, x_{3}\right)$ in $R^{3}$ as a linear combination of $\alpha_{1}, \alpha_{2}, \alpha_{3}$ it is not necessary to solve any linear equations. For it suffices to use (8-8). Thus

$$
\left(x_{1}, x_{2}, x_{3}\right)=\frac{3 x_{1}+4 x_{3}}{25} \alpha_{1}+\frac{-4 x_{1}+3 x_{3}}{25} \alpha_{2}+\frac{x_{2}}{9} \alpha_{3}
$$

as is readily verified. In particular,

$$
(1,2,3)=\frac{3}{5}(3,0,4)+\frac{1}{5}(-4,0,3)+\frac{2}{3}(0,9,0) .
$$

To put this point in another way, what we have shown is the following: The basis $\left\{f_{1}, f_{2}, f_{3}\right\}$ of $\left(R^{3}\right)^{*}$ which is dual to the basis $\left\{\alpha_{1}, \alpha_{2}, \alpha_{3}\right\}$ is defined explicitly by the equations

$$
\begin{aligned}
& f_{1}\left(x_{1}, x_{2}, x_{3}\right)=\frac{3 x_{1}+4 x_{3}}{25} \\
& f_{2}\left(x_{1}, x_{2}, x_{3}\right)=\frac{-4 x_{1}+3 x_{3}}{25} \\
& f_{3}\left(x_{1}, x_{2}, x_{3}\right)=\frac{x_{2}}{9}
\end{aligned}
$$

and these equations may be written more generally in the form

$$
f_{j}\left(x_{1}, x_{2}, x_{3}\right)=\frac{\left(\left(x_{1}, x_{2}, x_{3}\right) \mid \alpha_{j}\right)}{\left\|\alpha_{j}\right\|^{2}} .
$$

Finally, note that from $\alpha_{1}, \alpha_{2}, \alpha_{3}$ we get the orthonormal basis

$$
\frac{1}{5}(3,0,4), \quad \frac{1}{5}(-4,0,3), \quad(0,1,0) \text {. }
$$

EXAMPLE 13. Let $A=\left[\begin{array}{ll}a & b \\ c & d\end{array}\right]$ where $a, b, c$, and $d$ are complex numbers. Set $\beta_{1}=(a, b), \beta_{2}=(c, d)$, and suppose that $\beta_{1} \neq 0$. If we apply the orthogonalization process to $\beta_{1}, \beta_{2}$, using the standard inner product in $C^{2}$, we obtain the following vectors:

$$
\begin{aligned}
\alpha_{1} & =(a, b) \\
\alpha_{2} & =(c, d)-\frac{((c, d) \mid(a, b))}{|a|^{2}+|b|^{2}}(a, b) \\
& =(c, d)-\frac{(c \bar{a}+d \bar{b})}{|a|^{2}+|b|^{2}}(a, b) \\
& =\left(\frac{c b \bar{b}-d \bar{b} a}{|a|^{2}+\left.|\bar{b}|\right|^{2}}, \frac{d \bar{a} a-c \bar{a} b}{|a|^{2}+|b|^{2}}\right) \\
& =\frac{\operatorname{det} A}{|a|^{2}+|b|^{2}}(-\bar{b}, \bar{a}) .
\end{aligned}
$$

Now the general theory tells us that $\alpha_{2} \neq 0$ if and only if $\beta_{1}, \beta_{2}$ are linearly independent. On the other hand, the formula for $\alpha_{2}$ shows that this is the case if and only if $\operatorname{det} A \neq 0$.

In essence, the Gram-Schmidt process consists of repeated applications of a basic geometric operation called orthogonal projection, and it is best understood from this point of view. The method of orthogonal projection also arises naturally in the solution of an important approximation problem.

Suppose $W$ is a subspace of an inner product space $V$, and let $\beta$ be an arbitrary vector in $V$. The problem is to find a best possible approximation to $\beta$ by vectors in $W$. This means we want to find a vector $\alpha$ for which $\|\beta-\alpha\|$ is as small as possible subject to the restriction that $\alpha$ should belong to $W$. Let us make our language precise.

A best approximation to $\beta$ by vectorsin $W$ is a vector $\alpha$ in $W$ such that

$$
\|\beta-\alpha\| \leq\|\beta-\gamma\|
$$

for every vector $\gamma$ in $W$.

By looking at this problem in $R^{2}$ or in $R^{3}$, one sees intuitively that a best approximation to $\beta$ by vectors in $W$ ought to be a vector $\alpha$ in $W$ such that $\beta-\alpha$ is perpendicular (orthogonal) to $W$ and that there ought to be exactly one such $\alpha$. These intuitive ideas are correct for finite-dimensional subspaces and for some, but not all, infinite-dimensional subspaces. Since the precise situation is too complicated to treat here, we shall prove only the following result.

Theorem 4. Let $\mathrm{W}$ be a subspace of an inner product space $\mathrm{V}$ and let $\beta$ be a vector in $\mathrm{V}$.

(i) The vector $\alpha$ in $\mathrm{W}$ is a best approximation to $\beta$ by vectors in $\mathrm{W}$ if and only if $\beta-\alpha$ is orthogonal to every vector in $\mathrm{W}$.

(ii) If a best approximation to $\beta$ by vectors in $\mathrm{W}$ exists, it is unique.

(iii) If $\mathrm{W}$ is finite-dimensional and $\left\{\alpha_{1}, \ldots, \alpha_{\mathrm{n}}\right\}$ is any orthonormal basis for $\mathrm{W}$, then the vector

$$
\alpha=\sum_{k} \frac{\left(\beta \mid \alpha_{\mathbf{k}}\right)}{\left\|\alpha_{\mathrm{k}}\right\|^{2}} \alpha_{\mathbf{k}}
$$

is the (unique) best approximation to $\beta$ by vectors in $\mathrm{W}$.

Proof. First note that if $\gamma$ is any vector in $V$, then $\beta-\gamma=$ $(\beta-\alpha)+(\alpha-\gamma)$, and

$$
\|\beta-\gamma\|^{2}=\|\beta-\alpha\|^{2}+2 \operatorname{Re}(\beta-\alpha \mid \alpha-\gamma)+\|\alpha-\gamma\|^{2} .
$$

Now suppose $\beta-\alpha$ is orthogonal to every vector in $W$, that $\gamma$ is in $W$ and that $\gamma \neq \alpha$. Then, since $\alpha-\gamma$ is in $W$, it follows that

$$
\begin{aligned}
\|\beta-\gamma\|^{2} & =\|\beta-\alpha\|^{2}+\|\alpha-\gamma\|^{2} \\
& >\|\beta-\alpha\|^{2} .
\end{aligned}
$$

Conversely, suppose that $\|\beta-\gamma\| \geq\|\beta-\alpha\|$ for every $\gamma$ in $W$. Then from the first equation above it follows that

$$
2 \operatorname{Re}(\beta-\alpha \mid \alpha-\gamma)+\|\alpha-\gamma\|^{2} \geq 0
$$

for all $\gamma$ in $W$. Since every vector in $W$ may be expressed in the form $\alpha-\gamma$ with $\gamma$ in $W$, we see that

$$
2 \operatorname{Re}(\beta-\alpha \mid \boldsymbol{\tau})+\|\boldsymbol{\tau}\|^{2} \geq 0
$$

for every $\tau$ in $W$. In particular, if $\gamma$ is in $W$ and $\gamma \neq \alpha$, we may take

$$
\tau=-\frac{(\beta-\alpha \mid \alpha-\gamma)}{\|\alpha-\gamma\|^{2}}(\alpha-\gamma) .
$$

Then the inequality reduces to the statement

$$
-2 \frac{|(\beta-\alpha \mid \alpha-\gamma)|^{2}}{\|\alpha-\gamma\|^{2}}+\frac{|(\beta-\alpha \mid \alpha-\gamma)|^{2}}{\|\alpha-\gamma\|^{2}} \geq 0 .
$$

This holds if and only if $(\beta-\alpha \mid \alpha-\gamma)=0$. Therefore, $\beta-\alpha$ is orthogonal to every vector in $W$. This completes the proof of the equivalence of the two conditions on $\alpha$ given in (i). The orthogonality condition is evidently satisfied by at most one vector in $W$, which proves (ii). Now suppose that $W$ is a finite-dimensional subspace of $V$. Then we know, as a corollary of Theorem 3, that $W$ has an orthogonal basis. Let $\left\{\alpha_{1}, \ldots, \alpha_{n}\right\}$ be any orthogonal basis for $W$ and define $\alpha$ by (8-11). Then, by the computation in the proof of Theorem $3, \beta-\alpha$ is orthogonal to each of the vectors $\alpha_{k}$ ( $\beta-\alpha$ is the vector obtained at the last stage when the orthogonalization process is applied to $\left.\alpha_{1}, \ldots, \alpha_{n}, \beta\right)$. Thus $\beta-\alpha$ is orthogonal to every linear combination of $\alpha_{1}, \ldots, \alpha_{n}$, i.e., to every vector in $W$. If $\gamma$ is in $W$ and $\gamma \neq \alpha$, it follows that $\|\beta-\gamma\|>\|\beta-\alpha\|$. Therefore, $\alpha$ is the best approximation to $\beta$ that lies in $W$.

Definition. Let $\mathrm{V}$ be an inner product space and $\mathrm{S}$ any set of vectors in $\mathrm{V}$. The orthogonal complement of $\mathrm{S}$ is the set $\mathrm{S}^{\perp}$ of all vectors in $\mathrm{V}$ which are orthogonal to every vector in $\mathbf{S}$.

The orthogonal complement of $V$ is the zero subspace, and conversely $\{0\}^{\perp}=V$. If $S$ is any subset of $V$, its orthogonal complement $S^{\perp}(S$ perp) is always a subspace of $V$. For $S$ is non-empty, since it contains 0 ; and whenever $\alpha$ and $\beta$ are in $S^{\perp}$ and $c$ is any scalar,

$$
\begin{aligned}
(c \alpha+\beta \mid \gamma) & =c(\alpha \mid \gamma)+(\beta \mid \gamma) \\
& =c 0+0 \\
& =0
\end{aligned}
$$

for every $\gamma$ in $S$, thus $c \alpha+\beta$ also lies in $S$. In Theorem 4 the characteristic property of the vector $\alpha$ is that it is the only vector in $W$ such that $\beta-\alpha$ belongs to $W^{\perp}$.

Definition. Whenever the vector $\alpha$ in Theorem 4 exists it is called the orthogonal projection of $\beta$ on $\mathrm{W}$. If every vector in $\mathrm{V}$ has an orthogonal projection on $\mathrm{W}$, the mapping that assigns to each vector in $\mathrm{V}$ its orthogonal projection on $\mathrm{W}$ is called the orthogonal projection of $\mathrm{V}$ on $\mathrm{W}$.

B y Theorem 4, the orthogonal projection of an inner product space on a finite-dimensional subspace always exists. But Theorem 4 also implies the following result.

Corollary. Let $\mathrm{V}$ be an inner product space, $\mathrm{W}$ a finite-dimensional subspace, and $\mathrm{E}$ the orthogonal projection of $\mathrm{V}$ on $\mathrm{W}$. Then the mapping

$$
\beta \rightarrow \beta-E \beta
$$

is the orthogonal projection of $\mathrm{V}$ on $\mathrm{W}^{\perp}$.

Proof. Let $\beta$ be an arbitrary vector in $V$. Then $\beta-E \beta$ is in $W^{\perp}$, and for any $\gamma$ in $W^{\perp}, \beta-\gamma=E \beta+(\beta-E \beta-\gamma)$. Since $E \beta$ is in $W$ and $\beta-E \beta-\gamma$ is in $W^{\perp}$, it follows that 

$$
\begin{aligned}
\|\beta-\gamma\|^{2} & =\|E \beta\|^{2}+\|\beta-E \beta-\gamma\|^{2} \\
& \geq\|\beta-(\beta-E \beta)\|^{2}
\end{aligned}
$$

with strict inequality when $\gamma \neq \beta-E \beta$. Therefore, $\beta-E \beta$ is the best approximation to $\beta$ by vectors in $W^{\perp}$.

Example 14. Give $R^{3}$ the standard inner product. Then the orthogonal projection of $(-10,2,8)$ on the subspace $W$ that is spanned by $(3,12,-1)$ is the vector

$$
\begin{aligned}
\alpha & =\frac{((-10,2,8) \mid(3,12,-1))}{9+144+1}(3,12,-1) \\
& =\frac{-14}{154}(3,12,-1) .
\end{aligned}
$$

The orthogonal projection of $R^{3}$ on $W$ is the linear transformation $E$ defined by

$$
\left(x_{1}, x_{2}, x_{3}\right) \rightarrow\left(\frac{3 x_{1}+12 x_{2}-x_{3}}{154}\right)(3,12,-1) .
$$

The rank of $E$ is clearly 1 ; hence its nullity is 2 . On the other hand,

$$
E\left(x_{1}, x_{2}, x_{3}\right)=(0,0,0)
$$

if and only if $3 x_{1}+12 x_{2}-x_{3}=0$. This is the case if and only if $\left(x_{1}, x_{2}, x_{3}\right)$ is in $W^{\perp}$. Therefore, $W^{\perp \perp}$ is the null space of $E$, and $\operatorname{dim}\left(W^{\perp}\right)=2$. Computing

$$
\left(x_{1}, x_{2}, x_{3}\right)-\left(\frac{3 x_{1}+12 x_{2}-x_{3}}{154}\right)(3,12,-1)
$$

we see that the orthogonal projection of $R^{3}$ on $W^{. \perp}$ is the linear transformation $I-E$ that maps the vector $\left(x_{1}, x_{2}, x_{3}\right)$ onto the vector

$$
\frac{1}{154}\left(145 x_{1}-36 x_{2}+3 x_{3},-36 x_{1}+10 x_{2}+12 x_{3}, 3 x_{1}+12 x_{2}+153 x_{3}\right) .
$$

The observations made in Example 14 generalize in the following fashion.

Theorem 5. Let $\mathrm{W}$ be a finite-dimensional subspace of an inner product space $\mathrm{V}$ and let $\mathrm{E}$ be the orthogonal projection of $\mathrm{V}$ on $\mathrm{W}$. Then $\mathrm{E}$ is an idempotent linear transformation of $\mathrm{V}$ onto $\mathrm{W}, \mathrm{W}^{\perp}$ is the null space of $\mathrm{E}$, and

$$
\mathrm{V}=\mathrm{W} \oplus \mathrm{W}^{-\mathrm{L}} \text {. }
$$

Proof. Let $\beta$ be an arbitrary vector in $V$. Then $E \beta$ is the best approximation to $\beta$ that lies in $W$. In particular, $E \beta=\beta$ when $\beta$ is in $W$. Therefore, $E(E \boldsymbol{\beta})=E \beta$ for every $\beta$ in $V$; that is, $E$ is idempotent: $E^{2}=E$. To prove that $E$ is a linear transformation, let $\alpha$ and $\beta$ be any vectors in $V$ and $c$ an arbitrary scalar. Then, by Theorem $4, \alpha-E \alpha$ and $\beta-E \beta$ are each orthogonal to every vector in $W$. Hence the vector

$$
c(\alpha-E \alpha)+(\beta-E \beta)=(c \alpha+\beta)-(c E \alpha+E \beta)
$$

also belongs to $W^{\perp}$. Since $c E \alpha+E \beta$ is a vector in $W$, it follows from Theorem 4 that

$$
E(c \alpha+\beta)=c E \alpha+E \beta .
$$

Of course, one may also prove the linearity of $E$ by using (8-11). Again let $\beta$ be any vector in $V$. Then $E \beta$ is the unique vector in $W$ such that $\beta-E \beta$ is in $W^{\perp}$. Thus $E \boldsymbol{\beta}=0$ when $\beta$ is in $W^{\perp}$. Conversely, $\beta$ is in $W^{\perp}$ when $E \beta=0$. Thus $W^{\perp}$ is the null space of $E$. The equation

$$
\beta=E \beta+\beta-E \beta
$$

shows that $V=W+W^{\perp}$; moreover, $W \cap W^{\perp}=\{0\}$. For if $\alpha$ is a vector in $W \cap W^{\perp}$, then $(\alpha \mid \alpha)=0$. Therefore, $\alpha=0$, and $V$ is the direct sum of $W$ and $W^{\lrcorner}$.

Corollary. Under the conditions of the theorem, I - E is the orthogonal projection of $\mathrm{V}$ on $\mathrm{W}^{\circ}$. It is an idempotent linear transformation of $\mathrm{V}$ onto $\mathrm{W}^{\perp}$ with null space $\mathrm{W}$.

Proof. We have already seen that the mapping $\beta \rightarrow \beta-E \beta$ is the orthogonal projection of $V$ on $W$ ㄴ. Since $E$ is a linear transformation, this projection on $W^{.-}$is the linear transformation $I-E$. From its geometric properties one sees that $I-E$ is an idempotent transformation of $V$ onto $W$. This also follows from the computation

$$
\begin{aligned}
(I-E)(I-E) & =I-E-E+E^{2} \\
& =I-E .
\end{aligned}
$$

Moreover, $(I-E) \beta=0$ if and only if $\beta=E \beta$, and this is the case if and only if $\beta$ is in $W$. Therefore $W$ is the null space of $I-E$.

The Gram-Schmidt process may now be described geometrically in the following way. Given an inner product space $V$ and vectors $\beta_{1}, \ldots, \boldsymbol{\beta}_{n}$ in $V$, let $P_{k}(k>1)$ be the orthogonal projection of $V$ on the orthogonal complement of the subspace spanned by $\beta_{1}, \ldots, \beta_{k-1}$, and set $P_{1}=I$. Then the vectors one obtains by applying the orthogonalization process to $\beta_{1}, \ldots, \beta_{n}$ are defined by the equations

$$
\alpha_{k}=P_{k} \beta_{k}, \quad 1 \leq k \leq n .
$$

Theorem 5 implies another result known as Bessel's inequality.

Corollary. Let $\left\{\alpha_{1}, \ldots, \alpha_{n}\right\}$ be an orthogonal set of non-zero vectors in an inner product space $\mathrm{V}$. If $\beta$ is any vector in $\mathrm{V}$, then

$$
\sum_{\mathbf{k}} \frac{\left|\left(\beta \mid \alpha_{\mathbf{k}}\right)\right|^{2}}{\left\|\alpha_{\mathbf{k}}\right\|^{2}} \leq\|\beta\|^{2}
$$

and equality holds if and only if

$$
\beta=\sum_{\mathbf{k}} \frac{\left(\beta \mid \alpha_{\mathbf{k}}\right)}{\left\|\alpha_{\mathrm{k}}\right\|^{2}} \alpha_{\mathrm{k}} .
$$

Proof. Let $\gamma=\sum_{k}\left[\left(\beta \mid \alpha_{k}\right) /\left\|\alpha_{k}\right\|^{2}\right] \alpha_{k}$. Then $\beta=\gamma+\delta$ where $(\gamma \mid \delta)=0$. Hence

$$
\|\beta\|^{2}=\|\gamma\|^{2}+\|\delta\|^{2} .
$$

It now suffices to prove that

$$
\|\gamma\|^{2}=\sum_{k} \frac{\left|\left(\beta \mid \alpha_{k}\right)\right|^{2}}{\left\|\alpha_{k}\right\|^{2}} .
$$

This is straightforward computation in which one uses the fact that $\left(\alpha_{j} \mid \alpha_{k}\right)=0$ for $j \neq k$.

In the special case in which $\left\{\alpha_{1}, \ldots, \alpha_{n}\right\}$ is an orthonormal set, Bessel's inequality says that

$$
\sum_{k}\left|\left(\beta \mid \alpha_{k}\right)\right|^{2} \leq\|\beta\|^{2} .
$$

The corollary also tells us in this case that $\beta$ is in the subspace spanned by $\alpha_{1}, \ldots, \alpha_{n}$ if and only if

$$
\beta=\sum_{k}\left(\beta \mid \alpha_{k}\right) \alpha_{k}
$$

or if and only if Bessel's inequality is actually an equality. Of course, in the event that $V$ is finite dimensional and $\left\{\alpha_{1}, \ldots, \alpha_{n}\right\}$ is an orthogonal basis for $V$, the above formula holds for every vector $\beta$ in $V$. In other words, if $\left\{\alpha_{1}, \ldots, \alpha_{n}\right\}$ is an orthonormal basis for $V$, the $k$ th coordinate of $\beta$ in the ordered basis $\left\{\alpha_{1}, \ldots, \alpha_{n}\right\}$ is $\left(\beta \mid \alpha_{k}\right)$.

Example 15. We shall apply the last corollary to the orthogonal sets described in Example 11. We find that

(c)

$$
\begin{gathered}
\sum_{k=-n}^{n}\left|\int_{0}^{1} f(t) e^{-2 \pi i k t} d t\right|^{2} \leq \int_{0}^{1}|f(t)|^{2} d t \\
\int_{0}^{1}\left|\sum_{k=-n}^{n} c_{k} e^{2 \pi i k t}\right|^{2} d t=\sum_{k=-n}^{n}\left|c_{k}\right|^{2} \\
\int_{0}^{1}(\sqrt{2} \cos 2 \pi t+\sqrt{2} \sin 4 \pi t)^{2} d t=1+1=2 .
\end{gathered}
$$

\section{Exercises}

1. Consider $R^{4}$ with the standard inner product. Let $W$ be the subspace of $R^{4}$ consisting of all vectors which are orthogonal to both $\alpha=(1,0,-1,1)$ and $\beta=(2,3,-1,2)$. Find a basis for $W$. 2. Apply the Gram-Schmidt process to the vectors $\beta_{1}=(1,0,1), \beta_{2}=(1,0,-1)$, $\beta_{3}=(0,3,4)$, to obtain an orthonormal basis for $R^{3}$ with the standard inner product.

3. Consider $C^{3}$, with the standard inner product. Find an orthonormal basis for the subspace spanned by $\beta_{1}=(1,0, i)$ and $\beta_{2}=(2,1,1+i)$.

4. Let $V$ be an inner product space. The distance between two vectors $\alpha$ and $\beta$ in $V$ is defined by

$$
d(\alpha, \beta)=\|\alpha-\beta\|
$$

Show that

(a) $d(\alpha, \beta) \geq 0$

(b) $d(\alpha, \beta)=0$ if and only if $\alpha=\beta$;

(c) $d(\alpha, \beta)=d(\beta, \alpha)$

(d) $d(\alpha, \beta) \leq d(\alpha, \gamma)+d(\gamma, \beta)$

5. Let $V$ be an inner product space, and let $\alpha, \beta$ be vectors in $V$. Show that $\alpha=\beta$ if and only if $(\alpha \mid \gamma)=(\beta \mid \gamma)$ for every $\gamma$ in $V$.

6. Let $W$ be the subspace of $R^{2}$ spanned by the vector $(3,4)$. Using the standard inner product, let $E$ be the orthogonal projection of $R^{2}$ onto $W$. Find

(a) a formula for $E\left(x_{1}, x_{2}\right)$;

(b) the matrix of $E$ in the standard ordered basis;

(c) $W^{\perp}$

(d) an orthonormal basis in which $E$ is represented by the matrix

$$
\left[\begin{array}{ll}
1 & 0 \\
0 & 0
\end{array}\right] \text {. }
$$

7. Let $V$ be the inner product space consisting of $R^{2}$ and the inner product whose quadratic form is defined by

$$
\|\left(x_{1}, x_{2} \|^{2}=\left(x_{1}-x_{2}\right)^{2}+3 x_{2}^{2}\right. \text {. }
$$

Let $E$ be the orthogonal projection of $V$ onto the subspace $W$ spanned by the vector $(3,4)$. Now answer the four questions of Exercise 6 .

8. Find an inner product on $R^{2}$ such that $\left(\epsilon_{1}, \epsilon_{2}\right)=2$.

9. Let $V$ be the subspace of $R[x]$ of polynomials of degree at most 3. Equip $V$ with the inner product

$$
(f \mid g)=\int_{0}^{1} f(t) g(t) d t .
$$

(a) Find the orthogonal complement of the subspace of scalar polynomials.

(b) Apply the Gram-Schmidt process to the basis $\left\{1, x, x^{2}, x^{3}\right\}$.

10. Let $V$ be the vector space of all $n \times n$ matrices over $C$, with the inner product $(A \mid B)=\operatorname{tr}\left(A B^{*}\right)$. Find the orthogonal complement of the subspace of diagonal matrices.

11. Let $V$ be a finite-dimensional inner product space, and let $\left\{\alpha_{1}, \ldots, \alpha_{n}\right\}$ be an orthonormal basis for $V$. Show that for any vectors $\alpha, \beta$ in $V$

$$
\left.(\alpha \mid \beta)=\sum_{k=1}^{n}\left(\alpha \mid \alpha_{k}\right) \overline{\left(\beta \mid \alpha_{k}\right.}\right) .
$$

12. Let $W$ be a finite-dimensional subspace of an inner product space $V$, and let $E$ be the orthogonal projection of $V$ on $W$. Prove that $(E \alpha \mid \beta)=(\alpha \mid E \beta)$ for all $\alpha, \beta$ in $V$.

13. Let $S$ be a subset of an inner product space $V$. Show that $\left(S^{\perp}\right)^{\perp}$ contains the subspace spanned by $S$. When $V$ is finite-dimensional, show that $\left(S^{\perp}\right)^{\perp}$ is the subspace spanned by $S$.

14. Let $V$ be a finite-dimensional inner product space, and let $B=\left\{\alpha_{1}, \ldots, \alpha_{n}\right\}$ be an orthonormal basis for $V$. Let $T$ be a linear operator on $V$ and $A$ the matrix of $T$ in the ordered basis Q. Prove that

$$
A_{i j}=\left(T \alpha_{j} \mid \alpha_{i}\right)
$$

15. Suppose $V=W_{1} \oplus W_{2}$ and that $f_{1}$ and $f_{2}$ are inner products on $W_{1}$ and $W_{2}$, respectively. Show that there is a unique inner product $f$ on $V$ such that

(a) $W_{2}=W_{1}^{\perp}$

(b) $f(\alpha, \beta)=f_{k}(\alpha, \beta)$, when $\alpha, \beta$ are in $W_{k}, k=1,2$.

16. Let $V$ be an inner product space and $W$ a finite-dimensional subspace of $V$. There are (in general) many projections which have $W$ as their range. One of these, the orthogonal projection on $W$, has the property that $\|E \alpha\| \leq\|\alpha\|$ for every $\alpha$ in $V$. Prove that if $E$ is a projection with range $W$, such that $\|E \alpha\| \leq\|\alpha\|$ for all $\alpha$ in $V$, then $E$ is the orthogonal projection on $W$.

17. Let $V$ be the real inner product space consisting of the space of real-valued continuous functions on the interval, $-1 \leq t \leq 1$, with the inner product

$$
(f \mid g)=\int_{-1}^{1} f(t) g(t) d t .
$$

Let $W$ be the subspace of odd functions, i.e., functions satisf ying $f(-t)=-f(t)$. Find the orthogonal complement of $W$.

\subsection{Linear Functionals and Adjoints}

The first portion of this section treats linear functionals on an inner product space and their relation to the inner product. The basic result is that any linear functional $f$ on a finite-dimensional inner product space is 'inner product with a fixed vector in the space,' i.e., that such an $f$ has the form $f(\alpha)=(\alpha \mid \beta)$ for some fixed $\beta$ in $V$. We use this result to prove the existence of the 'adjoint' of a linear operator $T$ on $V$, this being a linear operator $T^{*}$ such that $(T \alpha \mid \beta)=\left(\alpha \mid T^{*} \boldsymbol{\beta}\right)$ for all $\alpha$ and $\boldsymbol{\beta}$ in $V$. Through the use of an orthonormal basis, this adjoint operation on linear operators (passing from $T$ to $T^{*}$ ) is identified with the operation of forming the conjugate transpose of a matrix. We explore slightly the analogy between the adjoint operation and conjugation on complex numbers.

Let $V$ be any inner product space, and let $\beta$ be some fixed vector in $V$. We define a function $f_{\beta}$ from $V$ into the scalar field by 

$$
f_{\beta}(\alpha)=(\alpha \mid \beta) .
$$

This function $f_{\beta}$ is a linear functional on $V$, because, by its very definition, $(\alpha \mid \beta)$ is linear as a function of $\alpha$. If $V$ is finite-dimensional, every linear functional on $V$ arises in this way from some $\beta$.

Theorem 6. Let $\mathrm{V}$ be a finite-dimensional inner product space, and $\mathrm{f}$ a linear functional on $\mathrm{V}$. Then there exists a unique vector $\beta$ in $\mathrm{V}$ such that $\mathrm{f}(\alpha)=(\alpha \mid \beta)$ for all $\alpha$ in $\mathrm{V}$.

Proof. Let $\left\{\alpha_{1}, \alpha_{2}, \ldots, \alpha_{n}\right\}$ be an orthonormal basis for $V$. Put

$$
\beta=\sum_{j=1}^{n} \overline{f\left(\alpha_{j}\right)} \alpha_{j}
$$

and let $f_{\beta}$ be the linear functional defined by

Then

$$
f_{\beta}(\alpha)=(\alpha \mid \beta) .
$$

$$
f_{\beta}\left(\alpha_{k}\right)=\left(\alpha_{k} \mid \sum_{j} \overline{f\left(\alpha_{j}\right)} \alpha_{j}\right)=f\left(\alpha_{k}\right) .
$$

Since this is true for each $\alpha_{k}$, it follows that $f=f_{\beta}$. Now suppose $\gamma$ is a vector in $V$ such that $(\alpha \mid \beta)=(\alpha \mid \gamma)$ for all $\alpha$. Then $(\beta-\gamma \mid \beta-\gamma)=0$ and $\beta=\gamma$. Thus there is exactly one vector $\beta$ determining the linear functional $f$ in the stated manner.

The proof of this theorem can be reworded slightly, in terms of the representation of linear functionals in a basis. If we choose an orthonormal basis $\left\{\alpha_{1}, \ldots, \alpha_{n}\right\}$ for $V$, the inner product of $\alpha=x_{1} \alpha_{1}+\cdots+$ $x_{n} \alpha_{n}$ and $\beta=y_{1} \alpha_{1}+\cdots+y_{n} \alpha_{n}$ will be

$$
(\alpha \mid \beta)=x_{1} \bar{y}_{1}+\cdots+x_{n} \bar{y}_{n} .
$$

If $f$ is any linear functional on $V$, then $f$ has the form

$$
f(\alpha)=c_{1} x_{1}+\cdots+c_{n} x_{n}
$$

for some fixed scalars $c_{1}, \ldots, c_{n}$ determined by the basis. Of course $c_{j}=f\left(\alpha_{j}\right)$. If we wish to find a vector $\beta$ in $V$ such that $(\alpha \mid \beta)=f(\alpha)$ for all $\alpha$, then clearly the coordinates $y_{j}$ of $\beta$ must satisfy $\bar{y}_{j}=c_{j}$ or $y_{j}=\overline{f\left(\alpha_{j}\right)}$. Accordingly,

$$
\beta=\overline{f\left(\alpha_{1}\right)} \alpha_{1}+\cdots+\overline{f\left(\alpha_{n}\right)} \alpha_{n}
$$

is the desired vector.

Some further comments are in order. The proof of Theorem 6 that we have given is admirably brief, but it fails to emphasize the essential geometric fact that $\beta$ lies in the orthogonal complement of the null space of $f$. Let $W$ be the null space of $f$. Then $V=W+W^{\perp}$, and $f$ is completely determined by its values on $W^{\lrcorner}$. In fact, if $P$ is the orthogonal projection of $V$ on $W^{\lrcorner}$, then 

$$
f(\alpha)=f(P \alpha)
$$

for all $\alpha$ in $V$. Suppose $f \neq 0$. Then $f$ is of rank 1 and $\operatorname{dim}\left(W^{\perp}\right)=1$. If $\gamma$ is any non-zero vector in $W^{\perp}$, it follows that

for all $\alpha$ in $V$. Thus

$$
P \alpha=\frac{(\alpha \mid \gamma)}{\|\gamma\|^{2}} \gamma
$$

$$
f(\alpha)=(\alpha \mid \gamma) \cdot \frac{f(\gamma)}{\|\gamma\|^{2}}
$$

for all $\alpha$, and $\beta=\left[\overline{f(\gamma)} /\|\gamma\|^{2}\right] \gamma$.

Example 16. We should give one example showing that Theorem 6 is not true without the assumption that $V$ is finite dimensional. Let $V$ be the vector space of polynomials over the field of complex numbers, with the inner product

$$
(f \mid g)=\int_{0}^{1} f(t) \overline{g(t)} d t .
$$

This inner product can also be defined algebraically. If $f=\Sigma a_{k} x^{k}$ and $g=\Sigma b_{k} x^{k}$, then

$$
(f \mid g)=\sum_{j, k} \frac{1}{j+k+1} a_{j} \bar{b}_{k} .
$$

Let $z$ be a fixed complex number, and let $L$ be the linear functional 'evaluation at $z$ ':

$$
L(f)=f(z) .
$$

Is there a polynomial $g$ such that $(f \mid g)=L(f)$ for every $f$ ? The answer is no; for suppose we have

$$
f(z)=\int_{0}^{1} f(t) \overline{g(t)} d t
$$

for every $f$. Let $h=x-z$, so that for any $f$ we have $(h f)(z)=0$. Then

$$
0=\int_{0}^{1} h(t) f(t) \overline{g(t)} d t
$$

for all $f$. In particular this holds when $f=\bar{h} g$ so that

$$
\int_{0}^{1}|h(t)|^{2}|g(t)|^{2} d t=0
$$

and so $h g=0$. Since $h \neq 0$, it must be that $g=0$. But $L$ is not the zero functional; hence, no such $g$ exists.

One can generalize the example somewhat, to the case where $L$ is a linear combination of point evaluations. Suppose we select fixed complex numbers $z_{1}, \ldots, z_{n}$ and scalars $c_{1}, \ldots, c_{n}$ and let

$$
L(f)=c_{1} f\left(z_{1}\right)+\cdots+c_{n} f\left(z_{n}\right) .
$$

Then $L$ is a linear functional on $V$, but there is no $g$ with $L(f)=(f \mid g)$, unless $c_{1}=c_{2}=\cdots=c_{n}=0$. Just repeat the above argument with $h=\left(x-z_{1}\right) \cdots\left(x-z_{n}\right)$.

We turn now to the concept of the adjoint of a linear opcrator.

Theorem 7. For any linear operator $\mathrm{T}$ on a finite-dimensional inner product space $\mathrm{V}$, there exists a unique linear operator $\mathrm{T}^{*}$ on $\mathrm{V}$ such that

$$
(\mathrm{T} \alpha \mid \beta)=\left(\alpha \mid \mathrm{T}^{*} \beta\right)
$$

for all $\alpha, \beta$ in $\mathrm{V}$.

Proof. Let $\beta$ be any vector in $V$. Then $\alpha \rightarrow(T \alpha \mid \beta)$ is a linear functional on $V$. By Theorem 6 there is a unique vector $\beta^{\prime}$ in $V$ such that $(T \alpha \mid \beta)=\left(\alpha \mid \beta^{\prime}\right)$ for every $\alpha$ in $V$. Let $T^{*}$ denote the mapping $\beta \rightarrow \beta^{\prime}$ :

$$
\beta^{\prime}=T^{*} \beta \text {. }
$$

We have (8-14), but we must verify that $T^{*}$ is a linear operator. Let $\beta, \gamma$ be in $V$ and let $c$ be a scalar. Then for any $\alpha$,

$$
\begin{aligned}
\left(\alpha \mid T^{*}(c \beta+\gamma)\right) & =(T \alpha \mid c \beta+\gamma) \\
& =(T \alpha \mid c \beta)+(T \alpha \mid \gamma) \\
& =\bar{c}(T \alpha \mid \beta)+(T \alpha \mid \gamma) \\
& =\bar{c}\left(\alpha \mid T^{*} \beta\right)+\left(\alpha \mid T^{*} \gamma\right) \\
& =\left(\alpha \mid c T^{*} \beta\right)+\left(\alpha \mid T^{*} \gamma\right) \\
& =\left(\alpha \mid c T^{*} \beta+T^{*} \gamma\right) .
\end{aligned}
$$

Thus $T^{*}(c \beta+\gamma)=c T^{*} \beta+T^{*} \gamma$ and $T^{*}$ is linear.

The uniqueness of $T^{*}$ is clear. For any $\beta$ in $V$, the vector $T^{*} \beta$ is uniquely determined as the vector $\beta^{\prime}$ such that $(T \alpha \mid \beta)=\left(\alpha \mid \boldsymbol{\beta}^{\prime}\right)$ for every $\alpha$.

Theorem 8. Let $\mathrm{V}$ be a finite-dimensional inner product space and let $\mathbb{Q}=\left\{\alpha_{1}, \ldots, \alpha_{n}\right\}$ be an (ordered) orthonormal basis for $\mathrm{V}$. Let $\mathrm{T}$ be a linear operator on $\mathrm{V}$ and let $\mathrm{A}$ be the matrix of $\mathrm{T}$ in the ordered basis $\mathrm{B}$. Then $\mathrm{A}_{k j}=\left(T \alpha_{j} \mid \alpha_{k}\right)$.

Proof. Since $B$ is an orthonormal basis, we have

$$
\alpha=\sum_{k=1}^{n}\left(\alpha \mid \alpha_{k}\right) \alpha_{k} .
$$

The matrix $A$ is defined by

and since

$$
T \alpha_{j}=\sum_{k=1}^{n} A_{k j} \alpha_{k}
$$

$$
T \alpha_{j}=\sum_{k=1}^{n}\left(T \alpha_{j} \mid \alpha_{k}\right) \alpha_{k}
$$

we have $A_{k j}=\left(T \alpha_{j} \mid \alpha_{k}\right)$. Corollary. Let $\mathrm{V}$ be a finite-dimensional inner product space, and let $\mathrm{T}$ be a linear operator on $\mathrm{V}$. In any orthonormal basis for $\mathrm{V}$, the matrix of $\mathrm{T}^{*}$ is the conjugate transpose of the matrix of $\mathrm{T}$.

Proof. Let $B=\left\{\alpha_{1}, \ldots, \alpha_{n}\right\}$ be an orthonormal basis for $V$, let $A=[T]_{\mathbb{G}}$ and $B=\left[T^{*}\right]_{\mathbb{G}}$. According to Theorem 8 ,

$$
\begin{aligned}
A_{k j} & =\left(T \alpha_{j} \mid \alpha_{k}\right) \\
B_{k j} & =\left(T^{*} \alpha_{j} \mid \alpha_{k}\right) .
\end{aligned}
$$

By the definition of $T^{*}$ we then have

$$
\begin{aligned}
B_{k j} & =\left(T^{*} \alpha_{j} \mid \alpha_{k}\right) \\
& =\overline{\left(\alpha_{k} \mid T^{*} \alpha_{j}\right)} \\
& =\overline{\left(T \alpha_{k} \mid \alpha_{j}\right)} \\
& =\overline{A_{j k}} .
\end{aligned}
$$

EXAMple 17. Let $V$ be a finite-dimensional inner product space and $E$ the orthogonal projection of $V$ on a subspace $W$. Then for any vectors $\alpha$ and $\beta$ in $V$.

$$
\begin{aligned}
(E \alpha \mid \beta) & =(E \alpha \mid E \beta+(1-E) \beta) \\
& =(E \alpha \mid E \beta) \\
& =(E \alpha+(1-E) \alpha \mid E \beta) \\
& =(\alpha \mid E \beta) .
\end{aligned}
$$

From the uniqueness of the operator $E^{*}$ it follows that $E^{*}=E$. Now consider the projection $E$ described in Example 14. Then

$$
A=\frac{1}{154}\left[\begin{array}{rrr}
9 & 36 & -3 \\
36 & 144 & -12 \\
-3 & -12 & 1
\end{array}\right]
$$

is the matrix of $E$ in the standard orthonormal basis. Since $E=E^{*}, A$ is also the matrix of $E^{*}$, and because $A=A^{*}$, this does not contradict the preceding corollary. On the other hand, suppose

$$
\begin{aligned}
& \alpha_{1}=(154,0,0) \\
& \alpha_{2}=(145,-36,3) \\
& \alpha_{3}=(-36,10,12) .
\end{aligned}
$$

Then $\left\{\alpha_{1}, \alpha_{2}, \alpha_{3}\right\}$ is a basis, and

$$
\begin{aligned}
& E \alpha_{1}=(9,36,-3) \\
& E \alpha_{2}=(0,0,0) \\
& E \alpha_{3}=(0,0,0) .
\end{aligned}
$$

Since $(9,36,-3)=-(154,0,0)-(145,-36,3)$, the matrix $B$ of $E$ in the basis $\left\{\alpha_{1}, \alpha_{2}, \alpha_{3}\right\}$ is defined by the equation 

$$
B=\left[\begin{array}{rrr}
-1 & 0 & 0 \\
-1 & 0 & 0 \\
0 & 0 & 0
\end{array}\right] \text {. }
$$

In this case $B \neq B^{*}$, and $B^{*}$ is not the matrix of $E^{*}=E$ in the basis $\left\{\alpha_{1}, \alpha_{2}, \alpha_{3}\right\}$. Applying the corollary, we conclude that $\left\{\alpha_{1}, \alpha_{2}, \alpha_{3}\right\}$ is not an orthonormal basis. Of course this is quite obvious anyway.

Definition. Let $\mathrm{T}$ be a linear operator on an inner product space $\mathrm{V}$. Then we say that $\mathrm{T}$ has an adjoint on $\mathrm{V}$ if there exists a linear operator $\mathrm{T}^{*}$ on $\mathrm{V}$ such that $(\mathrm{T} \alpha \mid \beta)=\left(\alpha \mid \mathrm{T}^{*} \beta\right)$ for all $\alpha$ and $\beta$ in $\mathrm{V}$.

By Theorem 7 every linear operator on a finite-dimensional inner product space $V$ has an adjoint on $V$. In the infinite-dimensional case this is not always true. But in any case there is at most one such operator $T^{*}$; when it exists, we call it the adjoint of $T$.

Two comments should be made about the finite-dimensional case.

1. The adjoint of $T$ depends not only on $T$ but on the inner product as well.

2. As shown by Example 17, in an arbitrary ordered basis $Q$, the relation between $[T]_{\Theta}$ and $\left[T^{*}\right]_{\Theta}$ is more complicated than that given in the corollary above.

Example 18. Let $V$ be $C^{n \times 1}$, the space of complex $n \times 1$ matrices, with inner product $(X \mid Y)=Y^{*} X$. If $A$ is an $n \times n$ matrix with complex entries, the adjoint of the linear operator $X \rightarrow A X$ is the operator $X \rightarrow A^{*} X$. For

$$
(A X \mid Y)=Y^{*} A X=\left(A^{*} Y\right)^{*} X=\left(X \mid A^{*} Y\right) .
$$

The reader should convince himself that this is really a special case of the last corollary.

Example 19. This is similar to Example 18. Let $V$ be $C^{n \times n}$ with the inner product $(A \mid B)=\operatorname{tr}\left(B^{*} A\right)$. Let $M$ be a fixed $n \times n$ matrix over $C$. The adjoint of left multiplication by $M$ is left multiplication by $M^{*}$. Of course, 'left multiplication by $M$ ' is the linear operator $L_{M}$ defined by $L_{M}(A)=M A$.

$$
\begin{aligned}
\left(L_{M}(A) \mid B\right) & =\operatorname{tr}\left(B^{*}(M A)\right) \\
& =\operatorname{tr}\left(M A B^{*}\right) \\
& =\operatorname{tr}\left(A B^{*} M\right) \\
& =\operatorname{tr}\left(A\left(M^{*} B\right)^{*}\right) \\
& =\left(A \mid L_{M}^{*}(B)\right) .
\end{aligned}
$$

Thus $\left(L_{M}\right)^{*}=L_{M *}$. In the computation above, we twice used the characteristic property of the trace function: $\operatorname{tr}(A B)=\operatorname{tr}(B A)$.

Example 20. Let $V$ be the space of polynomials over the field of complex numbers, with the inner product

$$
(f \mid g)=\int_{0}^{1} f(t) \overline{g(t)} d t .
$$

If $f$ is a polynomial, $f=\Sigma a_{k} x^{k}$, we let $\bar{f}=\Sigma \bar{a}_{k} x^{k}$. That is, $\bar{f}$ is the polynomial whose associated polynomial function is the complex conjugate of that for $f$ :

$$
\bar{f}(t)=\overline{f(t)}, \quad t \text { real }
$$

Consider the operator 'multiplication by $f$,' that is, the linear operator $M_{f}$ defined by $M_{f}(g)=f g$. Then this operator has an adjoint, namely, multiplication by $\bar{f}$. For

$$
\begin{aligned}
\left(M_{f}(g) \mid h\right) & =(f g \mid h) \\
& =\int_{0}^{1} f(t) g(t) \overline{h(t)} d t \\
& =\int_{0}^{1} g(t)[\overline{\overline{f(t)} h(t)}] d t \\
& =(g \mid \bar{f} h) \\
& =\left(g \mid M_{\bar{f}}(h)\right)
\end{aligned}
$$

and so $\left(M_{\bar{j}}\right)^{*}=M_{j}$.

Example 21. In Example 20, we saw that some linear operators on an infinite-dimensional inner product space do have an adjoint. As we commented earlier, some do not. Let $V$ be the inner product space of Example 21, and let $D$ be the differentiation operator on $C[x]$. Integration by parts shows that

$$
(D f \mid g)=f(1) g(1)-f(0) g(0)-(f \mid D g) .
$$

Let us fix $g$ and inquire when there is a polynomial $D^{*} g$ such that $(D f \mid g)=\left(f \mid D^{*} g\right)$ for all $f$. If such a $D^{*} g$ exists, we shall have

$$
\left(f \mid D^{*} g\right)=f(1) g(1)-f(0) g(0)-(f \mid D g)
$$

or

$$
\left(f \mid D^{*} g+D g\right)=f(1) g(1)-f(0) g(0) .
$$

With $g$ fixed, $L(f)=f(1) g(1)-f(0) g(0)$ is a linear functional of the type considered in Example 16 and cannot be of the form $L(f)=(f \mid h)$ unless $L=0$. If $D^{*} g$ exists, then with $h=D^{*} g+D g$ we do have $L(f)=(f \mid h)$, and so $g(0)=g(1)=0$. The existence of a suitable polynomial $D^{*} g$ implies $g(0)=g(1)=0$. Conversely, if $g(0)=g(1)=0$, the polynomial $D^{*} g=$ $-D g$ satisfies $(D f \mid g)=\left(f \mid D^{*} g\right)$ for all $f$. If we choose any $g$ for which $g(0) \neq 0$ or $g(1) \neq 0$, we cannot suitably define $D^{*} g$, and so we conclude that $D$ has no adjoint. We hope that these examples enhance the reader's understanding of the adjoint of a linear operator. We see that the adjoint operation, passing from $T$ to $T^{*}$, behaves somewhat like conjugation on complex numbers. The following theorem strengthens the analogy.

Theorem 9. Let $\mathrm{V}$ be a finite-dimensional inner product space. If $\mathrm{T}$ and $\mathrm{U}$ are linear operators on $\mathrm{V}$ and $\mathrm{c}$ is a scalar,

(i) $(\mathrm{T}+\mathrm{U})^{*}=\mathrm{T}^{*}+\mathrm{U}^{*}$;

(ii) $(\mathrm{cT})^{*}=\overline{\mathrm{c}} \mathrm{T}^{*}$;

(iii) $(\mathrm{TU})^{*}=\mathrm{U}^{*} \mathrm{~T}^{*}$;

(iv) $\left(\mathrm{T}^{*}\right)^{*}=\mathrm{T}$.

Then

Proof. To prove (i), let $\alpha$ and $\beta$ be any vectors in $V$.

$$
\begin{aligned}
((T+U) \alpha \mid \beta) & =(T \alpha+U \alpha \mid \beta) \\
& =(T \alpha \mid \beta)+(U \alpha \mid \beta) \\
& =\left(\alpha \mid T^{*} \beta\right)+\left(\alpha \mid U^{*} \beta\right) \\
& =\left(\alpha \mid T^{*} \beta+U^{*} \beta\right) \\
& =\left(\alpha \mid\left(T^{*}+U^{*}\right) \beta\right) .
\end{aligned}
$$

From the uniqueness of the adjoint we have $(T+U)^{*}=T^{*}+U^{*}$. We leave the proof of (ii) to the reader. We obtain (iii) and (iv) from the relations

$$
\begin{array}{r}
(T U \alpha \mid \beta)=\left(U \alpha \mid T^{*} \beta\right)=\left(\alpha \mid U^{*} T^{*} \beta\right) \\
\left(T^{*} \alpha \mid \beta\right)=\left(\overline{\beta \mid T^{*} \alpha}\right)=(\overline{T \beta} \mid \alpha)=(\alpha \mid T \beta) .
\end{array}
$$

Theorem 9 is often phrased as follows: The mapping $T \rightarrow T^{*}$ is a conjugate-linear anti-isomorphism of period 2. The analogy with complex conjugation which we mentioned above is, of course, based upon the observation that complex conjugation has the properties $\left(\overline{z_{1}+z_{2}}\right)=$ $\bar{z}_{1}+\bar{z}_{2},\left(\overline{z_{1} z_{2}}\right)=\bar{z}_{1} \bar{z}_{2}, \overline{\bar{z}}=z$. One must be careful to observe the reversal of order in a product, which the adjoint operation imposes: $(U T)^{*}=$ $T^{*} U^{*}$. We shall mention extensions of this analogy as we continue our study of linear operators on an inner product space. We might mention something along these lines now. A complex number $z$ is real if and only if $z=\bar{z}$. One might expect that the linear operators $T$ such that $T=T^{*}$ behave in some way like the real numbers. This is in fact the case. For example, if $T$ is a linear operator on a finite-dimensional complex inner product space, then

$$
T=U_{1}+i U_{2}
$$

where $U_{1}=U_{1}^{*}$ and $U_{2}=U_{2}^{*}$. Thus, in some sense, $T$ has a 'real part' and an 'imaginary part.' The operators $U_{1}$ and $U_{2}$ satisfying $U_{1}=U_{1}^{*}$, and $U_{2}=U_{2}^{*}$, and (8-15) are unique, and are given by 

$$
\begin{aligned}
& U_{1}=\frac{1}{2}\left(T+T^{*}\right) \\
& U_{2}=\frac{1}{2 i}\left(T-T^{*}\right) .
\end{aligned}
$$

A linear operator $T$ such that $T=T^{*}$ is called self-adjoint (or Hermitian). If $B$ is an orthonormal basis for $V$, then

$$
\left[T^{*}\right]_{\mathscr{B}}=[T]_{\mathscr{B}}^{*}
$$

and so $T$ is self-adjoint if and only if its matrix in every orthonormal basis is a self-adjoint matrix. Self-adjoint operators are important, not simply because they provide us with some sort of real and imaginary part for the general linear operator, but for the following reasons: (1) Self-adjoint operators have many special properties. For example, for such an operator there is an orthonormal basis of characteristic vectors. (2) Many operators which arise in practice are self-adjoint. We shall consider the special properties of self-adjoint operators later.

\section{Exercises}

1. Let $V$ be the space $C^{2}$, with the standard inner product. Let $T$ be the linear operator defined by $T \epsilon_{1}=(1,-2), T \epsilon_{2}=(i,-1)$. If $\alpha=\left(x_{1}, x_{2}\right)$, find $T^{*} \alpha$.

2. Let $T$ be the linear operator on $C^{2}$ defined by $T \epsilon_{1}=(1+i, 2), T \epsilon_{2}=(i, i)$. Using the standard inner product, find the matrix of $T^{*}$ in the standard ordered basis. Does $T$ commute with $T^{*}$ ?

3. Let $V$ be $C^{3}$ with the standard inner product. Let $T$ be the linear operator on $V$ whose matrix in the standard ordered basis is defined by

$$
A_{j k}=i^{i+k}, \quad\left(i^{2}=-1\right) .
$$

Find a basis for the null space of $T^{*}$.

4. Let $V$ be a finite-dimensional inner product space and $T$ a linear operator on $V$. Show that the range of $T^{*}$ is the orthogonal complement of the null space of $T$.

5. Let $V$ be a finite-dimensional inner product space and $T$ a linear operator on $V$. If $T$ is invertible, show that $T^{*}$ is invertible and $\left(T^{*}\right)^{-1}=\left(T^{-1}\right)^{*}$.

6. Let $V$ be an inner product space and $\beta, \gamma$ fixed vectors in $V$. Show that $T \alpha=(\alpha \mid \beta) \gamma$ defines a linear operator on $V$. Show that $T$ has an adjoint, and describe $T^{*}$ explicitly.

Now suppose $V$ is $C^{n}$ with the standard inner product, $\beta=\left(y_{1}, \ldots, y_{n}\right)$, and $\gamma=\left(x_{1}, \ldots, x_{n}\right)$. What is the $j, k$ entry of the matrix of $T$ in the standard ordered basis? What is the rank of this matrix?

7. Show that the product of two self-adjoint operators is self-adjoint if and only if the two operators commute. 8. Let $V$ be the vector space of the polynomials over $R$ of degree less than or equal to 3 , with the inner product

$$
(f \mid g)=\int_{0}^{1} f(t) g(t) d t
$$

If $t$ is a real number, find the polynomial $g_{t}$ in $V$ such that $\left(f \mid g_{t}\right)=f(t)$ for all $f$ in $V$.

9. Let $V$ be the inner product space of Exercise 8, and let $D$ be the differentiation operator on $V$. Find $D^{*}$.

10. Let $V$ be the space of $n \times n$ matrices over the complex numbers, with the inner product $(A, B)=\operatorname{tr}\left(A B^{*}\right)$. Let $P$ be a fixed invertible matrix in $V$, and let $T_{P}$ be the linear operator on $V$ defined by $T_{P}(A)=P^{-1} A P$. Find the adjoint of $T_{P}$.

11. Let $V$ be a finite-dimensional inner product space, and let $E$ be an idempotent linear operator on $V$, i.e., $E^{2}=E$. Prove that $E$ is self-adjoint if and only if $E E^{*}=E^{*} E$.

12. Let $V$ be a finite-dimensional complex inner product space, and let $T$ be a linear operator on $V$. Prove that $T$ is self-adjoint if and only if $(T \alpha \mid \alpha)$ is real for every $\alpha$ in $V$.

\subsection{Unitary Operators}

In this section, we consider the concept of an isomorphism between two inner product spaces. If $V$ and $W$ are vector spaces, an isomorphism of $V$ onto $W$ is a one-one linear transformation from $V$ onto $W$, i.e., a one-one correspondence between the elements of $V$ and those of $W$, which 'preserves' the vector space operations. Now an inner product space consists of a vector space and a specified inner product on that space. Thus, when $V$ and $W$ are inner product spaces, we shall require an isomorphism from $V$ onto $W$ not only to preserve the linear operations, but also to preserve inner products. An isomorphism of an inner product space onto itself is called a 'unitary operator' on that space. We shall consider various examples of unitary operators and establish their basic properties.

Definition. Let $\mathrm{V}$ and $\mathrm{W}$ be inner product spaces over the same field, and let $\mathrm{T}$ be a linear transformation from $\mathrm{V}$ into $\mathrm{W}$. We say that $\mathrm{T}$ preserves inner products if $(\mathrm{T} \alpha \mid \mathrm{T} \beta)=(\alpha \mid \beta)$ for all $\alpha, \beta$ in $\mathrm{V}$. An isomorphism of $\mathrm{V}$ onto $\mathrm{W}$ is a vector space isomorphism $\mathrm{T}$ of $\mathrm{V}$ onto $\mathrm{W}$ which also preserves inner products.

If $T$ preserves inner products, then $\|T \alpha\|=\|\alpha\|$ and so $T$ is necessarily non-singular. Thus an isomorphism from $V$ onto $W$ can also be defined as a linear transformation from $V$ onto $W$ which preserves inner products. If $T$ is an isomorphism of $V$ onto $W$, then $T^{-1}$ is an isomorphism of $W$ onto $V$; hence, when such a $T$ exists, we shall simply say $V$ and $W$ are isomorphic. Of course, isomorphism of inner product spaces is an equivalence relation.

Theorem 10. Let $\mathrm{V}$ and $\mathrm{W}$ be finite-dimensional inner product spaces over the same field, having the same dimension. If $\mathrm{T}$ is a linear transformation from $\mathrm{V}$ into $\mathrm{W}$, the following are equivalent.

(i) $\mathrm{T}$ preserves inner products.

(ii) $\mathrm{T}$ is an (inner product space) isomorphism.

(iii) $\mathrm{T}$ carries every orthonormal basis for $\mathrm{V}$ onto an orthonormal basis for W.

(iv) $\mathrm{T}$ carries some orthonormal basis for $\mathrm{V}$ onto an orthonormal basis for W.

Proof. (i) $\rightarrow$ (ii) If $T$ preserves inner products, then $\|T \alpha\|=\|\alpha\|$ for all $\alpha$ in $V$. Thus $T$ is non-singular, and since $\operatorname{dim} V=\operatorname{dim} W$, we know that $T$ is a vector space isomorphism.

(ii) $\rightarrow$ (iii) Suppose $T$ is an isomorphism. Let $\left\{\alpha_{1}, \ldots, \alpha_{n}\right\}$ be an orthonormal basis for $V$. Since $T$ is a vector space isomorphism and $\operatorname{dim} W=\operatorname{dim} V$, it follows that $\left\{T \alpha_{1}, \ldots, T \alpha_{n}\right\}$ is a basis for $W$. Since $T$ also preserves inner products, $\left(T \alpha_{j} \mid T \alpha_{k}\right)=\left(\alpha_{j} \mid \alpha_{k}\right)=\delta_{j k}$.

(iii) $\rightarrow$ (iv) This requires no comment.

(iv) $\rightarrow$ (i) Let $\left\{\alpha_{1}, \ldots, \alpha_{n}\right\}$ be an orthonormal basis for $V$ such that $\left\{T \alpha_{1}, \ldots, T \alpha_{n}\right\}$ is an orthonormal basis for $W$. Then

$$
\left(T \alpha_{j} \mid T \alpha_{k}\right)=\left(\alpha_{j} \mid \alpha_{k}\right)=\delta_{j k} .
$$

For any $\alpha=x_{1} \alpha_{1}+\cdots+x_{n} \alpha_{n}$ and $\beta=y_{1} \alpha_{1}+\cdots+y_{n} \alpha_{n}$ in $V$, we have

$$
\begin{aligned}
(\alpha \mid \beta) & =\sum_{j=1}^{n} x_{j} \bar{y}_{j} \\
(T \alpha \mid T \beta) & =\left(\sum_{j} x_{j} T \alpha_{j} \mid \sum_{k} y_{k} T \alpha_{k}\right) \\
& =\sum_{j} \sum_{k} x_{j} \bar{y}_{k}\left(T \alpha_{j} \mid T \alpha_{k}\right) \\
& =\sum_{j=1}^{n} x_{j} \bar{y}_{j}
\end{aligned}
$$

and so $T$ preserves inner products.

Corollary. Let $\mathrm{V}$ and $\mathrm{W}$ be finite-dimensional inner product spaces over the same field. Then $\mathrm{V}$ and $\mathrm{W}$ are isomorphic if and only if they have the same dimension.

Proof. If $\left\{\alpha_{1}, \ldots, \alpha_{n}\right\}$ is an orthonormal basis for $V$ and $\left\{\beta_{1}, \ldots, \beta_{n}\right\}$ is an orthonormal basis for $W$, let $T$ be the linear transformation from $V$ into $W$ defined by $T \alpha_{j}=\beta_{j}$. Then $T$ is an isomorphism of $V$ onto $W$. EXAMPLE 22. If $V$ is an $n$-dimensional inner product space, then each ordered orthonormal basis $B=\left\{\alpha_{1}, \ldots, \alpha_{n}\right\}$ determines an isomorphism of $V$ onto $F^{n}$ with the standard inner product. The isomorphism is simply

$$
T\left(x_{1} \alpha_{1}+\cdots+x_{n} \alpha_{n}\right)=\left(x_{1}, \ldots, x_{n}\right) .
$$

There is the superficially different isomorphism which \& determines of $V$ onto the space $F^{n \times 1}$ with $(X \mid Y)=Y^{*} X$ as inner product. The isomorphism is

$$
\alpha \rightarrow[\alpha]_{\omega}
$$

i.e., the transformation sending $\alpha$ into its coordinate matrix in the ordered basis $B$. For any ordered basis $B$, this is a vector space isomorphism; however, it is an isomorphism of the two inner product spaces if and only if $B$ is orthonormal.

Example 23. Here is a slightly less superficial isomorphism. Let $W$ be the space of all $3 \times 3$ matrices $A$ over $R$ which are skew-symmetric, i.e., $A^{t}=-A$. We equip $W$ with the inner product $(A \mid B)=\frac{1}{2} \operatorname{tr}\left(A B^{t}\right)$, the $\frac{1}{2}$ being put in as a matter of convenience. Let $V$ be the space $R^{3}$ with the standard inner product. Let $T$ be the linear transformation from $V$ into $W$ defined by

$$
T\left(x_{1}, x_{2}, x_{3}\right)=\left[\begin{array}{rrr}
0 & -x_{3} & x_{2} \\
x_{3} & 0 & -x_{1} \\
-x_{2} & x_{1} & 0
\end{array}\right] .
$$

Then $T$ maps $V$ onto $W$, and putting

$$
A=\left[\begin{array}{ccc}
0 & -x_{3} & x_{2} \\
x_{3} & 0 & -x_{1} \\
-x_{2} & x_{1} & 0
\end{array}\right], \quad B=\left[\begin{array}{ccc}
0 & -y_{3} & y_{2} \\
y_{3} & 0 & -y_{1} \\
-y_{2} & y_{1} & 0
\end{array}\right]
$$

we have

$$
\begin{aligned}
\operatorname{tr}\left(A B^{t}\right) & =x_{3} y_{3}+x_{2} y_{2}+x_{3} y_{3}+x_{2} y_{2}+x_{1} y_{1} \\
& =2\left(x_{1} y_{1}+x_{2} y_{2}+x_{3} y_{3}\right) .
\end{aligned}
$$

Thus $(\alpha \mid \beta)=(T \alpha \mid T \beta)$ and $T$ is a vector space isomorphism. Note that $T$ carries the standard basis $\left\{\epsilon_{1}, \epsilon_{2}, \epsilon_{3}\right\}$ onto the orthonormal basis consisting of the three matrices

$$
\left[\begin{array}{rrr}
0 & 0 & 0 \\
0 & 0 & -1 \\
0 & 1 & 0
\end{array}\right], \quad\left[\begin{array}{rrr}
0 & 0 & 1 \\
0 & 0 & 0 \\
-1 & 0 & 0
\end{array}\right], \quad\left[\begin{array}{rrr}
0 & -1 & 0 \\
1 & 0 & 0 \\
0 & 0 & 0
\end{array}\right] \text {. }
$$

Example 24. It is not always particularly convenient to describe an isomorphism in terms of orthonormal bases. For example, suppose $G=P^{*} P$ where $P$ is an invertible $n \times n$ matrix with complex entries. Let $V$ be the space of complex $n \times 1$ matrices, with the inner product $[X \mid Y]=Y^{*} G X$. Let $W$ be the same vector space, with the standard inner product $(X \mid Y)=$ $Y^{*} X$. We know that $V$ and $W$ are isomorphic inner product spaces. It would seem that the most convenient way to describe an isomorphism betwcen $V$ and $W$ is the following: Let $T$ be the linear transformation from $V$ into $W$ defined by $T(X)=P X$. Then

$$
\begin{aligned}
(T X \mid T Y) & =(P X \mid P Y) \\
& =(P Y)^{*}(P X) \\
& =Y^{*} P^{*} P X \\
& =Y^{*} G X \\
& =[X \mid Y] .
\end{aligned}
$$

Hence $T$ is an isomorphism.

EXAMple 25. Let $V$ be the space of all continuous real-valued functions on the unit interval, $0 \leq t \leq 1$, with the inner product

$$
[f \mid g]=\int_{0}^{1} f(t) g(t) t^{2} d t .
$$

Let $W$ be the same vector space with the inner product

$$
(f \mid \boldsymbol{g})=\int_{0}^{1} f(t) g(t) d t .
$$

Let $T$ be the linear transformation from $V$ into $W$ given by

$$
(T f)(t)=t f(t) .
$$

Then $(T f \mid T g)=[f \mid g]$, and so $T$ preserves inner products; however, $T$ is not an isomorphism of $V$ onto $W$, because the range of $T$ is not all of $W$. Of course, this happens because the underlying vector space is not finitedimensional.

Theorem 11. Let $\mathrm{V}$ and $\mathrm{W}$ be inner product spaces over the same field, and let $\mathrm{T}$ be a linear transformation from $\mathrm{V}$ into $\mathrm{W}$. Then $\mathrm{T}$ preserves inner products if and only if $\|\mathrm{T} \alpha\|=\|\alpha\|$ for every $\alpha$ in $\mathrm{V}$.

Proof. If $T$ preserves inner products, $T$ 'preserves norms.' Suppose $\|T \alpha\|=\|\alpha\|$ for every $\alpha$ in $V$. Then $\|T \alpha\|^{2}=\|\alpha\|^{2}$. Now using the appropriate polarization identity, (8-3) or (8-4), and the fact that $T$ is linear, one easily obtains $(\alpha \mid \beta)=(T \alpha \mid T \beta)$ for all $\alpha, \beta$ in $V$.

Definition. A unitary operator on an inner product space is an isomorphism of the space omto itself.

The product of two unitary operators is unitary. For, if $U_{1}$ and $U_{2}$ are unitary, then $U_{2} U_{1}$ is invertible and $\left\|U_{2} U_{1} \alpha\right\|=\left\|U_{1} \alpha\right\|=\|\alpha\|$ for each $\alpha$. Also, the inverse of a unitary operator is unitary, since $\left\|U_{\alpha}\right\|=$ $\|\alpha\|$ says $\left\|U^{-1} \beta\right\|=\|\beta\|$, where $\beta=U \alpha$. Since the identity operator is clearly unitary, we see that the set of all unitary operators on an inner product space is a group, under the operation of composition.

If $V$ is a finite-dimensional inner product space and $U$ is a linear operator on $V$, Theorem 10 tells us that $U$ is unitary if and only if $(U \alpha \mid U \beta)=(\alpha \mid \beta)$ for each $\alpha, \beta$ in $V$; or, if and only if for some (every) orthonormal basis $\left\{\alpha_{1}, \ldots, \alpha_{n}\right\}$ it is true that $\left\{U \alpha_{1}, \ldots, U \alpha_{n}\right\}$ is an orthonormal basis.

Theorem 12. Let $\mathrm{U}$ be a linear operator on an inner prod:unt space $\mathrm{V}$. Then $\mathrm{U}$ is unitary if and only if the adjoint $\mathrm{U}^{*}$ of $\mathrm{U}$ exists and $\mathrm{UL}^{*}=$ $\mathrm{U}^{*} \mathrm{U}=\mathrm{I}$.

Proof. Suppose $U$ is unitary. Then $U$ is invertible and

$$
(U \alpha \mid \beta)=\left(U \alpha \mid U U^{-1} \beta\right)=\left(\alpha \mid U^{-1} \beta\right)
$$

for all $\alpha, \beta$. Hence $U^{-1}$ is the adjoint of $U$.

Conversely, suppose $U^{*}$ exists and $U U^{*}=U^{*} U=I$. Then $U$ is invertible, with $U^{-1}=U^{*}$. So, we need only show that $U$ preserves inner products. We have

$$
\begin{aligned}
(U \alpha \mid U \beta) & =\left(\alpha \mid U^{*} U \beta\right) \\
& =(\alpha \mid I \beta) \\
& =(\alpha \mid \beta)
\end{aligned}
$$

for all $\alpha, \beta$.

EXAMple 26. Consider $C^{n \times 1}$ with the inner product $(X \mid Y)=Y^{*} X$. Let $A$ be an $n \times n$ matrix over $C$, and let $U$ be the linear operator defined by $U(X)=A X$. Then

$$
(U X \mid U Y)=(A X \mid A Y)=Y^{*} A^{*} A X
$$

for all $X, Y$. Hence, $U$ is unitary if and only if $A^{*} A=I$.

Definition. A complex $\mathrm{n} \times \mathrm{n}$ matrix $\mathrm{A}$ is called unitary, if $\mathrm{A} * \mathrm{~A}=\mathrm{I}$.

Theorem 13. Let V be a finite-dimensional inner product space and let $\mathrm{U}$ be a linear operator on $\mathrm{V}$. Then $\mathrm{U}$ is unitary if and only if the matrix of $\mathrm{U}$ in some (or every) ordered orthonormal basis is a unitary matrix.

Proof. At this point, this is not much of a theorem, and we state it largely for emphasis. If $B=\left\{\alpha_{1}, \ldots, \alpha_{n}\right\}$ is an ordered orthonormal basis for $V$ and $A$ is the matrix of $U$ relative to $\otimes$, then $A^{*} A=I$ if and only if $U^{*} U=I$. The result now follows from Theorem 12 .

Let $A$ be an $n \times n$ matrix. The statement that $A$ is unitary simply means

$$
\left(A^{*} A\right)_{j k}=\delta_{j k}
$$

or

$$
\sum_{r=1}^{n} \overline{A_{r j}} A_{r k}=\delta_{j k} .
$$

In other words, it means that the columns of $A$ form an orthonormal set of column matrices, with respect to the standard inner product $(X \mid Y)=$ $Y^{*} X$. Since $A^{*} A=I$ if and only if $A A^{*}=I$, we see that $A$ is unitary exactly when the rows of $A$ comprise an orthonormal set of $n$-tuples in $C_{n}$ (with the standard inner product). So, using standard inner products, $A$ is unitary if and only if the rows and columns of $A$ are orthonormal sets. One sees here an example of the power of the theorem which states that a one-sided inverse for a matrix is a two-sided inverse. Applying this theorem as we did above, say to real matrices, we have the following: Suppose we have a square array of real numbers such that the sum of the squares of the entries in each row is 1 and distinct rows are orthogonal. Then the sum of the squares of the entries in each column is 1 and distinct columns are orthogonal. Write down the proof of this for a $3 \times 3$ array, without using any knowledge of matrices, and you should be reasonably impressed.

Definition. A real or complex $\mathrm{n} \times \mathrm{n}$ matrix $\mathrm{A}$ is said to be orthogonal, if $\mathrm{A}^{\mathrm{t}} \mathrm{A}=\mathrm{I}$.

A real orthogonal matrix is unitary; and, a unitary matrix is orthogonal if and only if each of its entries is real.

Example 27. We give some examples of unitary and orthogonal matrices.

(a) A $1 \times 1$ matrix [c] is orthogonal if and only if $c=\pm 1$, and unitary if and only if $\vec{c} c=1$. The latter condition means (of course) that $|c|=1$, or $c=e^{i \theta}$, where $\theta$ is real.

(b) Let

$$
A=\left[\begin{array}{ll}
a & b \\
c & d
\end{array}\right]
$$

Then $A$ is orthogonal if and only if

$$
A^{t}=A^{-1}=\frac{1}{a d-b c}\left[\begin{array}{rr}
d & -b \\
-c & a
\end{array}\right]
$$

The determinant of any orthogonal matrix is easily seen to be $\pm 1$. Thus $A$ is orthogonal if and only if

$$
A=\left[\begin{array}{rr}
a & b \\
-b & a
\end{array}\right]
$$

or

$$
A=\left[\begin{array}{rr}
a & b \\
b & -a
\end{array}\right]
$$

where $a^{2}+b^{2}=1$. The two cases are distinguished by the value of $\operatorname{det} A$.

(c) The well-known relations between the trigonometric functions show that the matrix

$$
A_{\theta}=\left[\begin{array}{rr}
\cos \theta & -\sin \theta \\
\sin \theta & \cos \theta
\end{array}\right]
$$

is orthogonal. If $\theta$ is a real number, then $A_{\theta}$ is the matrix in the standard ordered basis for $R^{2}$ of the linear operator $U_{\theta}$, rotation through the angle $\theta$. The statement that $A_{\theta}$ is a real orthogonal matrix (hence unitary) simply means that $U_{\theta}$ is a unitary operator, i.e., preserves dot products.

(d) Let

$$
A=\left[\begin{array}{ll}
a & b \\
c & d
\end{array}\right]
$$

Then $A$ is unitary if and only if

$$
\left[\begin{array}{ll}
\bar{a} & \bar{c} \\
\bar{b} & \bar{d}
\end{array}\right]=\frac{1}{a d-b c}\left[\begin{array}{rr}
d & -b \\
-c & a
\end{array}\right] .
$$

The determinant of a unitary matrix has absolute value 1 , and is thus a complex number of the form $e^{i \theta}, \theta$ real. Thus $A$ is unitary if and only if

$$
A=\left[\begin{array}{cc}
a & b \\
-e^{i \theta} \bar{b} & e^{i \theta} \bar{a}
\end{array}\right]=\left[\begin{array}{cc}
1 & 0 \\
0 & e^{i \theta}
\end{array}\right]\left[\begin{array}{cc}
a & b \\
-\bar{b} & \bar{a}
\end{array}\right]
$$

where $\theta$ is a real number, and $a, b$ are complex numbers such that $|a|^{2}+|b|^{2}=1$

As noted earlier, the unitary operators on an inner product space form a group. From this and Theorem 13 it follows that the set $U(n)$ of all $n \times n$ unitary matrices is also a group. Thus the inverse of a unitary matrix and the product of two unitary matrices are again unitary. Of course this is easy to see directly. An $n \times n$ matrix $A$ with complex entries is unitary if and only if $A^{-1}=A^{*}$. Thus, if $A$ is unitary, we have $\left(A^{-1}\right)^{-1}=$ $A=\left(A^{*}\right)^{-1}=\left(A^{-1}\right)^{*}$. If $A$ and $B$ are $n \times n$ unitary matrices, then $(A B)^{-1}=B^{-1} A^{-1}=B^{*} A^{*}=(A B)^{*}$.

The Gram-Schmidt process in $C^{n}$ has an interesting corollary for matrices that involves the group $U(n)$.

Theorem 14. For every invertible complex $\mathrm{n} \times \mathrm{n}$ matrix $\mathrm{B}$ there exists a unique lower-triangular matrix $\mathrm{M}$ with positive entries on the main diagonal such that MB is unitary.

Proof. The rows $\beta_{1}, \ldots, \beta_{n}$ of $B$ form a basis for $C^{n}$. Let $\alpha_{1}, \ldots, \alpha_{n}$ be the vectors obtained from $\beta_{1}, \ldots, \beta_{n}$ by the Gram-Schmidt process. Then, for $1 \leq k \leq n,\left\{\alpha_{1}, \ldots, \alpha_{k}\right\}$ is an orthogonal basis for the subspace spanned by $\left\{\beta_{1}, \ldots, \beta_{k}\right\}$, and 

$$
\alpha_{k}=\beta_{k}-\sum_{j<k} \frac{\left(\beta_{k} \mid \alpha_{j}\right)}{\left\|\alpha_{j}\right\|^{2}} \alpha_{j}
$$

Hence, for each $k$ there exist unique scalars $C_{k j}$ such that

$$
\alpha_{k}=\beta_{k}-\sum_{j<k} C_{k j} \beta_{j}
$$

Let $U$ be the unitary matrix with rows

$$
\frac{\alpha_{1}}{\left\|\alpha_{1}\right\|}, \ldots, \frac{\alpha_{n}}{\left\|\alpha_{n}\right\|}
$$

and $M$ the matrix defined by

$$
M_{k j}= \begin{cases}-\frac{1}{\left\|\alpha_{k}\right\|} \cdot C_{k j}, \quad \text { if } j<k \\ \frac{1}{\left\|\alpha_{k}\right\|}, & \text { if } j=k \\ 0, \quad \text { if } j>k .\end{cases}
$$

Then $M$ is lower-triangular, in the sense that its entries above the main diagonal are 0 . The entries $M_{k k}$ of $M$ on the main diagonal are all $>0$, and

$$
\frac{\alpha_{k}}{\left\|\alpha_{k}\right\|}=\sum_{j=1}^{n} M_{k j} \beta_{j}, \quad 1 \leq k \leq n .
$$

Now these equations simply say that

$$
U=M B
$$

To prove the uniqueness of $M$, let $T^{+}(n)$ denote the set of all complex $n \times n$ lower-triangular matrices with positive entries on the main diagonal. Suppose $M_{1}$ and $M_{2}$ are elements of $T^{+}(n)$ such that $M_{i} B$ is in $U(n)$ for $i=1,2$. Then because $U(n)$ is a group

$$
\left(M_{1} B\right)\left(M_{2} B\right)^{-1}=M_{1} M_{2}^{-1}
$$

lies in $U(n)$. On the other hand, although it is not entirely obvious, $T^{+}(n)$ is also a group under matrix multiplication. One way to see this is to consider the geometric properties of the linear transformations

$$
X \rightarrow M X, \quad\left(M \text { in } T^{+}(n)\right)
$$

on the space of column matrices. Thus $M_{2}^{-1}, M_{1} M_{2}^{-1}$, and $\left(M_{1} M_{2}^{-1}\right)^{-1}$ are all in $T^{+}(n)$. But, since $M_{1} M_{2}^{-1}$ is in $U(n),\left(M_{1} M_{2}^{-1}\right)^{-1}=\left(M_{1} M_{2}^{-1}\right)^{*}$. The transpose or conjugate transpose of any lower-triangular matrix is an upper-triangular matrix. Therefore, $M_{1} M_{2}^{-1}$ is simultaneously upperand lower-triangular, i.e., diagonal. A diagonal matrix is unitary if and only if each of its entries on the main diagonal has absolute value 1 ; if the diagonal entries are all positive, they must equal 1 . Hence $M_{1} M_{2}^{-1}=I$ and $M_{1}=M_{2}$. Let $G L(n)$ denote the set of all invertible complex $n \times n$ matrices. Then $G L(n)$ is also a group under matrix multiplication. This group is called the general linear group. Theorem 14 is equivalent to the following result.

Corollary. For each $\mathrm{B}$ in $\mathrm{GL}(\mathrm{n})$ there exist unique matrices $\mathrm{N}$ and $\mathrm{U}$ such that $\mathrm{N}$ is in $\mathrm{T}^{+}(\mathrm{n}), \mathrm{U}$ is in $\mathrm{U}(\mathrm{n})$, and

$$
\mathrm{B}=\mathbf{N} \cdot \mathrm{U} \text {. }
$$

Proof. By the theorem there is a unique matrix $M$ in $T^{+}(n)$ such that $M B$ is in $U(n)$. Let $M B=U$ and $N=M^{-1}$. Then $N$ is in $T^{+}(n)$ and $B=N \cdot U$. On the other hand, if we are given any elements $N$ and $U$ such that $N$ is in $T^{+}(n), U$ is in $U(n)$, and $B=N \cdot U$, then $N^{-1} B$ is in $U(n)$ and $N^{-1}$ is the unique matrix $M$ which is characterized by the theorem; furthermore $U$ is necessarily $N^{-1} B$.

EXAMPLE 28. Let $x_{1}$ and $x_{2}$ be real numbers such that $x_{1}^{2}+x_{2}^{2}=1$ and $x_{1} \neq 0$. Let

$$
B=\left[\begin{array}{lll}
x_{1} & x_{2} & 0 \\
0 & 1 & 0 \\
0 & 0 & 1
\end{array}\right] \text {. }
$$

Applying the Gram-Schmidt process to the rows of $B$, we obtain the vectors

$$
\begin{aligned}
\alpha_{1} & =\left(x_{1}, x_{2}, 0\right) \\
\alpha_{2} & =(0,1,0)-x_{2}\left(x_{1}, x_{2}, 0\right) \\
& =x_{1}\left(-x_{2}, x_{1}, 0\right) \\
\alpha_{3} & =(0,0,1) .
\end{aligned}
$$

Let $U$ be the matrix with rows $\alpha_{1},\left(\alpha_{2} / x_{1}\right), \alpha_{3}$. Then $U$ is unitary, and

$$
U=\left[\begin{array}{ccc}
x_{1} & x_{2} & 0 \\
-x_{2} & x_{1} & 0 \\
0 & 0 & 1
\end{array}\right]=\left[\begin{array}{ccc}
1 & 0 & 0 \\
-\frac{x_{2}}{x_{1}} & \frac{1}{x_{1}} & 0 \\
0 & 0 & 1
\end{array}\right]\left[\begin{array}{ccc}
x_{1} & x_{2} & 0 \\
0 & 1 & 0 \\
0 & 0 & 1
\end{array}\right]
$$

Now multiplying by the inverse of

$$
M=\left[\begin{array}{ccc}
1 & 0 & 0 \\
-\frac{x_{2}}{x_{1}} & \frac{1}{x_{1}} & 0 \\
0 & 0 & 1
\end{array}\right]
$$

we find that

$$
\left[\begin{array}{lll}
x_{1} & x_{2} & 0 \\
0 & 1 & 0 \\
0 & 0 & 1
\end{array}\right]=\left[\begin{array}{lll}
1 & 0 & 0 \\
x_{2} & x_{1} & 0 \\
0 & 0 & 1
\end{array}\right]\left[\begin{array}{rll}
x_{1} & x_{2} & 0 \\
-x_{2} & x_{1} & 0 \\
0 & 0 & 1
\end{array}\right]
$$

Let us now consider briefly change of coordinates in an inner product space. Suppose $V$ is a finite-dimensional inner product space and that $\mathbb{B}=\left\{\alpha_{1}, \ldots, \alpha_{n}\right\}$ and $\mathbb{B}^{\prime}=\left\{\alpha_{1}^{\prime}, \ldots, \alpha_{n}^{\prime}\right\}$ are two ordered orthonormal bases for $V$. There is a unique (necessarily invertible) $n \times n$ matrix $P$ such that

$$
[\alpha]_{\Theta^{\prime}}=P^{-1}[\alpha]_{\circlearrowleft}
$$

for every $\alpha$ in $V$. If $U$ is the unique linear operator on $V$ defined by $U \alpha_{j}=\alpha_{j}^{\prime}$, then $P$ is the matrix of $U$ in the ordered basis $\mathbb{B}$ :

$$
\alpha_{k}^{\prime}=\sum_{j=1}^{n} P_{j k} \alpha_{j} .
$$

Since $Q$ and $\mathbb{B}^{\prime}$ are orthonormal bases, $U$ is a unitary operator and $P$ is a unitary matrix. If $T$ is any linear operator on $V$, then

$$
[T]_{Q^{\prime}}=P^{-1}[T]_{\mathscr{Q}} P=P^{*}[T]_{\mathscr{Q}} P .
$$

Definition. Let $\mathrm{A}$ and $\mathrm{B}$ be complex $\mathrm{n} \times \mathrm{n}$ matrices. We say that $\mathrm{B}$ is unitarily equivalent to $\mathrm{A}$ if there is an $\mathrm{n} \times \mathrm{n}$ unitary matrix $\mathrm{P}$ such that $\mathrm{B}=\mathrm{P}^{-1} \mathrm{AP}$. We say that $\mathrm{B}$ is orthogonally equivalent to $\mathrm{A}$ if there is an $\mathrm{n} \times \mathrm{n}$ orthogonal matrix $\mathrm{P}$ such that $\mathrm{B}=\mathrm{P}^{-1} \mathrm{AP}$.

With this definition, what we observed above may be stated as follows: If $B$ and $B^{\prime}$ are two ordered orthonormal bases for $V$, then, for each linear operator $T$ on $V$, the matrix $[T]_{G^{\prime}}$ is unitarily equivalent to the matrix $[T]_{\mathscr{Q}}$. In case $V$ is a real inner product space, these matrices are orthogonally equivalent, via a real orthogonal matrix.

\section{Exercises}

1. Find a unitary matrix which is not orthogonal, and find an orthogonal matrix which is not unitary.

2. Let $V$ be the space of complex $n \times n$ matrices with inner product $(A \mid B)=$ $\operatorname{tr}\left(A B^{*}\right)$. For each $M$ in $V$, let $T_{M}$ be the linear operator defined by $T_{M}(A)=M A$. Show that $T_{M}$ is unitary if and only if $M$ is a unitary matrix.

3. Let $V$ be the set of complex numbers, regarded as a real vector space.

(a) Show that $(\alpha \mid \beta)=\operatorname{Re}(\alpha \bar{\beta})$ defines an inner product on $V$.

(b) Exhibit an (inner product space) isomorphism of $V$ onto $R^{2}$ with the standard inner product.

(c) For each $\gamma$ in $V$, let $M_{\gamma}$ be the linear operator on $V$ defined by $M_{\gamma}(\alpha)=\gamma \alpha$. Show that $\left(M_{\gamma}\right)^{*}=M_{\bar{\gamma}}$.

(d) For which complex numbers $\gamma$ is $M_{\gamma}$ self-adjoint?

(e) For which $\gamma$ is $M_{\gamma}$ unitary? (f) For which $\gamma$ is $M_{\gamma}$ positive?

(g) What is $\operatorname{det}\left(M_{\gamma}\right)$ ?

(h) Find the matrix of $M_{\gamma}$ in the basis $\{1, i\}$.

(i) If $T$ is a linear operator on $V$, find necessary and sufficient conditions for $T$ to be an $M_{\gamma}$.

(j) Find a unitary operator on $V$ which is not an $M_{\gamma}$.

4. Let $V$ be $R^{2}$, with the standard inner product. If $U$ is a unitary operator on $V$, show that the matrix of $U$ in the standard ordered basis is either

$$
\left[\begin{array}{rr}
\cos \theta & -\sin \theta \\
\sin \theta & \cos \theta
\end{array}\right] \text { or }\left[\begin{array}{rr}
\cos \theta & \sin \theta \\
\sin \theta & -\cos \theta
\end{array}\right]
$$

for some real $\theta, 0 \leq \theta<2 \pi$. Let $U_{\theta}$ be the linear operator corresponding to the first matrix, i.e., $U_{\theta}$ is rotation through the angle $\theta$. Now convince yourself that every unitary operator on $V$ is either a rotation, or reflection about the $\epsilon_{1}$-axis followed by a rotation.

(a) What is $U_{\theta} U_{\phi}$ ?

(b) Show that $U_{\theta}^{*}=U_{-\theta}$.

(c) Let $\phi$ be a fixed real number, and let $B=\left\{\alpha_{1}, \alpha_{2}\right\}$ be the orthonormal basis obtained by rotating $\left\{\epsilon_{1}, \epsilon_{2}\right\}$ through the angle $\phi$, i.e., $\alpha_{j}=U_{\phi} \epsilon_{j}$. If $\theta$ is another real number, what is the matrix of $U_{\theta}$ in the ordered basis $B$ ?

5. Let $V$ be $R^{3}$, with the standard inner product. Let $W$ be the plane spanned by $\alpha=(1,1,1)$ and $\beta=(1,1,-2)$. Let $U$ be the linear operator defined, geometrically, as follows: $U$ is rotation through the angle $\theta$, about the straight line through the origin which is orthogonal to $W$. There are actually two such rotations - choose one. Find the matrix of $U$ in the standard ordered basis. (Here is one way you might proceed. Find $\alpha_{1}$ and $\alpha_{2}$ which form an orthonormal basis for $W$. Let $\alpha_{3}$ be a vector of norm 1 which is orthogonal to $W$. Find the matrix of $U$ in the basis $\left\{\alpha_{1}, \alpha_{2}, \alpha_{3}\right\}$. Perform a change of basis.)

6. Let $V$ be a finite-dimensional inner product space, and let $W$ be a subspace of $V$. Then $V=W \oplus W^{\perp}$, that is, each $\alpha$ in $V$ is uniquely expressible in the form $\alpha=\beta+\gamma$, with $\beta$ in $W$ and $\gamma$ in $W^{\perp}$. Define a linear operator $U$ by $U \alpha=\beta-\gamma$.

(a) Prove that $U$ is both self-adjoint and unitary.

(b) If $V$ is $R^{3}$ with the standard inner product and $W$ is the subspace spanned by $(1,0,1)$, find the matrix of $U$ in the standard ordered basis.

7. Let $V$ be a complex inner product space and $T$ a self-adjoint linear operator on $V$. Show that

(a) $\|\alpha+i T \alpha\|=\|\alpha-i T \alpha\|$ for every $\alpha$ in $V$.

(b) $\alpha+i T \alpha=\beta+i T \beta$ if and only if $\alpha=\beta$.

(c) $I+i T$ is non-singular.

(d) $I-i T$ is non-singular.

(e) Now suppose $V$ is finite-dimensional, and prove that

$$
U=(I-i T)(I+i T)^{-1}
$$

is a unitary operator; $U$ is called the Cayley transform of $T$. In a certain sense, $U=f(T)$, where $f(x)=(1-i x) /(1+i x)$. 8. If $\theta$ is a real number, prove that the following matrices are unitarily equivalent

$$
\left[\begin{array}{rr}
\cos \theta & -\sin \theta \\
\sin \theta & \cos \theta
\end{array}\right], \quad\left[\begin{array}{cc}
e^{i \theta} & 0 \\
0 & e^{-i \theta}
\end{array}\right] .
$$

9. Let $V$ be a finite-dimensional inner product space and $T$ a positive linear operator on $V$. Let $p_{T}$ be the inner product on $V$ defined by $p_{T}(\alpha, \beta)=(T \alpha \mid \beta)$. Let $U$ be a linear operator on $V$ and $U^{*}$ its adjoint with respect to ( $\mid$ ). Prove that $U$ is unitary with respect to the inner product $p_{T}$ if and only if $T=U^{*} T U$.

10. Let $V$ be a finite-dimensional inner product space. For each $\alpha, \beta$ in $V$, let $T_{\alpha, \beta}$ be the linear operator on $V$ defined by $T_{\alpha, \beta}(\gamma)=(\gamma \mid \beta) \alpha$. Show that

(a) $T_{\alpha, \beta}^{*}=T_{\beta, \alpha}$.

(b) trace $\left(T_{\alpha, \beta}\right)=(\alpha \mid \beta)$.

(c) $T_{\alpha, \beta} T_{\gamma, \delta}=T_{\alpha,(\beta \mid \gamma) \delta}$.

(d) Under what conditions is $T_{\alpha, \beta}$ self-adjoint?

11. Let $V$ be an $n$-dimensional inner product space over the field $F$, and let $L(V, V)$ be the space of linear operators on $V$. Show that there is a unique inner product on $L(V, V)$ with the property that $\left\|T_{\alpha, \beta}\right\|^{2}=\|\alpha\|^{2}\|\beta\|^{2}$ for all $\alpha, \beta$ in $V$. $\left(T_{\alpha, \beta}\right.$ is the operator defined in Exercise 10.) Find an isomorphism between $L(V, V)$ with this inner product and the space of $n \times n$ matrices over $F$, with the inner product $(A \mid B)=\operatorname{tr}\left(A B^{*}\right)$.

12. Let $V$ be a finite-dimensional inner product space. In Exercise 6, we showed how to construct some linear operators on $V$ which are both self-adjoint and unitary. Now prove that there are no others, i.e., that every self-adjoint unitary operator arises from some subspace $W$ as we described in Exercise 6 .

13. Let $V$ and $W$ be finite-dimensional inner product spaces having the same dimension. Let $U$ be an isomorphism of $V$ onto $W$. Show that:

(a) The mapping $T \rightarrow U T U^{-1}$ is an isomorphism of the vector space $L(V, V)$ onto the vector space $L(W, W)$.

(b) trace $\left(U T U^{-1}\right)=$ trace $(T)$ for each $T$ in $L(V, V)$.

(c) $U T_{\alpha, \beta} U^{-1}=T_{U \alpha, U \beta}\left(T_{\alpha, \beta}\right.$ defined in Exercise 10$)$.

(d) $\left(U T U^{-1}\right)^{*}=U T^{*} U^{-1}$.

(e) If we equip $L(V, V)$ with inner product $\left(T_{1} \mid T_{2}\right)=$ trace $\left(T_{1} T_{2}^{*}\right)$, and similarly for $L(W, W)$, then $T \rightarrow U T U^{-1}$ is an inner product space isomorphism.

14. If $V$ is an inner product space, a rigid motion is any function $T$ from $V$ into $V$ (not necessarily linear) such that $\|T \alpha-T \beta\|=\|\alpha-\beta\|$ for all $\alpha, \beta$ in $V$. One example of a rigid motion is a linear unitary operator. Another example is translation by a fixed vector $\gamma$ :

$$
T_{\gamma}(\alpha)=\alpha+\gamma
$$

(a) Let $V$ be $R^{2}$ with the standard inner product. Suppose $T$ is a rigid motion of $V$ and that $T(0)=0$. Prove that $T$ is linear and a unitary operator.

(b) Use the result of part (a) to prove that every rigid motion of $R^{2}$ is composed of a translation, followed by a unitary operator.

(c) Now show that a rigid motion of $R^{2}$ is either a translation followed by a rotation, or a translation followed by a reflection followed by a rotation. 15. A unitary operator on $R^{4}$ (with the standard inner product) is simply a linear operator which preserves the quadratic form

$$
\|(x, y, z, t)\|^{2}=x^{2}+y^{2}+z^{2}+t^{2}
$$

that is, a linear operator $U$ such that $\|U \alpha\|^{2}=\|\alpha\|^{2}$ for all $\alpha$ in $R^{4}$. In a certain part of the theory of relativity, it is of interest to find the linear operators $T$ which preserve the form

$$
\|(x, y, z, t)\|_{L}^{2}=t^{2}-x^{2}-y^{2}-z^{2} .
$$

Now \|\|$_{L}^{2}$ does not come from an inner product, but from something called the 'Lorentz metric' (which we shall not go into). For that reason, a linear operator $T$ on $R^{4}$ such that $\|T \alpha\|_{L}^{2}=\|\alpha\|_{L}^{2}$, for every $\alpha$ in $R^{4}$, is called a Lorentz transformation.

(a) Show that the function $U$ defined by

$$
U(x, y, z, t)=\left[\begin{array}{cc}
t+x & y+i z \\
y-i z & t-x
\end{array}\right]
$$

is an isomorphism of $R^{4}$ onto the real vector space $H$ of all self-adjoint $2 \times 2$ complex matrices.

(b) Show that $\|\alpha\|_{L}^{2}=\operatorname{det}(U \alpha)$.

(c) Suppose $T$ is a (real) linear operator on the space $H$ of $2 \times 2$ self-adjoint matrices. Show that $L=U^{-1} T U$ is a linear operator on $R^{4}$.

(d) Let $M$ be any $2 \times 2$ complex matrix. Show that $T_{M}(A)=M^{*} A M$ defines a linear operator $T_{M}$ on $H$. (Be sure you check that $T_{M}$ maps $H$ into $H$.)

(e) If $M$ is a $2 \times 2$ matrix such that $|\operatorname{det} M|=1$, show that $L_{M}=U^{-1} T_{M} U$ is a Lorentz transformation on $R^{4}$.

(f) Find a Lorentz transformation which is not an $L_{M}$.

\subsection{Normal Operators}

The principal objective in this section is the solution of the following problem. If $T$ is a linear operator on a finite-dimensional inner product space $V$, under what conditions does $V$ have an orthonormal basis consisting of characteristic vectors for $T$ ? In other words, when is there an orthonormal basis $B$ for $V$, such that the matrix of $T$ in the basis $B$ is diagonal?

We shall begin by deriving some necessary conditions on $T$, which we shall subsequently show are sufficient. Suppose $B=\left\{\alpha_{1}, \ldots, \alpha_{n}\right\}$ is an orthonormal basis for $V$ with the property

$$
T \alpha_{j}=c_{j} \alpha_{j}, \quad j=1, \ldots, n .
$$

This simply says that the matrix of $T$ in the ordered basis $B$ is the diagonal matrix with diagonal entries $c_{1}, \ldots, c_{n}$. The adjoint operator $T^{*}$ is represented in this same ordered basis by the conjugate transpose matrix, i.e., the diagonal matrix with diagonal entries $\bar{c}_{1}, \ldots, \bar{c}_{n}$. If $V$ is a real inner product space, the scalars $c_{1}, \ldots, c_{n}$ are (of course) real, and so it must be that $T=T^{*}$.In other words, if $V$ is a finite-dimensional real inner probut space and $T$ is a linear operator for which there is an orthonormal basis of characteristic vectos, then $T$ must be self-adjoint. If $V$ is a complex inner product space, the scalars $c_{1}, \ldots, c_{n}$ need not be real, i.e., $T$ need not be self-adjoint. But notice that $T$ must satisfy

$$
T T^{*}=T^{*} T .
$$

For, any two diagonal matrices commute, and since $T$ and $T^{*}$ are both represented by diagonal matrices in the ordered basis $B$, we have (8-17). It is a rather remarkable fact that in the complex case this condition is also sufficient to imply the existence of an orthonormal basis of characteristic vectors.

Definition. Let $\mathrm{V}$ be a finite-dimensional inner product space and $\mathrm{T}$ a linear operator on $\mathrm{V}$. We say that $\mathrm{T}$ is normal if it commutes with its adjoint i.e., $\mathrm{TT}^{*}=\mathrm{T}^{*} \mathrm{~T}$.

Any self-adjoint operator is normal, as is any unitary operator. Any scalar multiple of a normal operator is normal; however, sums and products of normal operators are not generally normal. Although it is by no means necessary, we shall begin our study of normal operators by considering self-adjoint operators.

Theorem 15. Let $\mathrm{V}$ be an inner product space and $\mathrm{T}$ a self-adjoint linear operator on $\mathrm{V}$. Then each characteristic value of $\mathrm{T}$ is real, and characteristic vectors of $\mathrm{T}$ associated with distinct characteristic values are orthogonal.

Proof. Suppose $c$ is a characteristic value of $T$, i.e., that $T \alpha=c \alpha$ for some non-zero vector $\alpha$. Then

$$
\begin{aligned}
c(\alpha \mid \alpha) & =(c \alpha \mid \alpha) \\
& =(T \alpha \mid \alpha) \\
& =(\alpha \mid T \alpha) \\
& =(\alpha \mid c \alpha) \\
& =\overline{\boldsymbol{c}}(\alpha \mid \alpha) .
\end{aligned}
$$

Since $(\alpha \mid \alpha) \neq 0$, we must have $c=\bar{c}$. Suppose we also have $T \beta=d \beta$ with $\beta \neq 0$. Then

$$
\begin{aligned}
c(\alpha \mid \beta) & =(T \alpha \mid \beta) \\
& =(\alpha \mid T \beta) \\
& =(\alpha \mid d \beta) \\
& =\bar{d}(\alpha \mid \beta) \\
& =d(\alpha \mid \beta) .
\end{aligned}
$$

If $c \neq d$, then $(\alpha \mid \beta)=0$. It should be pointed out that Theorem 15 says nothing about the existence of characteristic values or characteristic vectors.

Theorem 16. On a finite-dimensional inner product space of positive dimension, every self-adjoint operator has a (non-zero) characteristic vector.

Proof. Let $V$ be an inner product space of dimension $n$, where $n>0$, and let $T$ be a self-adjoint operator on $V$. Choose an orthonormal basis $B$ for $V$ and let $A=[T]_{B}$. Since $T=T^{*}$, we have $A=A^{*}$. Now let $W$ be the space of $n \times 1$ matrices over $C$, with inner product $(X \mid Y)=$ $Y^{*} X$. Then $U(X)=A X$ defines a self-adjoint linear operator $U$ on $W$. The characteristic polynomial, $\operatorname{det}(x I-A)$, is a polynomial of degree $n$ over the complex numbers; every polynomial over $C$ of positive degree has a root. Thus, there is a complex number $c$ such that $\operatorname{det}(c I-A)=0$. This means that $A-c I$ is singular, or that there exists a non-zero $X$ such that $A X=c X$. Since the operator $U$ (multiplication by $A$ ) is selfadjoint, it follows from Theorem 15 that $c$ is real. If $V$ is a real vector space, we may choose $X$ to have real entries. For then $A$ and $A-c I$ have real entries, and since $A-c I$ is singular, the system $(A-c I) X=0$ has a non-zero real solution $X$. It follows that there is a non-zero vector $\alpha$ in $V$ such that $T \alpha=c \alpha$.

There are several comments we should make about the proof.

(1) The proof of the existence of a non-zero $X$ such that $A X=c X$ had nothing to do with the fact that $A$ was Hermitian (self-adjoint). It shows that any linear operator on a finite-dimensional complex vector space has a characteristic vector. In the case of a real inner product space, the self-adjointness of $A$ is used very heavily, to tell us that each characteristic value of $A$ is real and hence that we can find a suitable $X$ with real entries.

(2) The argument shows that the characteristic polynomial of a selfadjoint matrix has real coefficients, in spite of the fact that the matrix may not have real entries.

(3) The assumption that $V$ is finite-dimensional is necessary for the theorem; a self-adjoint operator on an infinite-dimensional inner product space need not have a characteristic value.

Example 29. Let $V$ be the vector space of continuous complexvalued (or real-valued) continuous functions on the unit interval, $0 \leq t \leq 1$, with the inner product

$$
(f \mid g)=\int_{0}^{1} f(t) \overline{g(t)} d t .
$$

The operator 'multiplication by $t$,' $(T f)(t)=t f(t)$, is self-adjoint. Let us suppose that $T f=c f$. Then 

$$
(t-c) f(t)=0, \quad 0 \leq t \leq 1
$$

and so $f(t)=0$ for $t \neq c$. Since $f$ is continuous, $f=0$. Hence $T$ has no characteristic values (vectors).

Theorem 17. Let $\mathrm{V}$ be a finite-dimensional inner product space, and let $\mathrm{T}$ be any linear operator on $\mathrm{V}$. Suppose $\mathrm{W}$ is a subspace of $\mathrm{V}$ which is invariant under $\mathrm{T}$. Then the orthogonal complement of $\mathrm{W}$ is invariant under $\mathrm{T}^{*}$.

Proof. We recall that the fact that $W$ is invariant under $T$ does not mean that each vector in $W$ is left fixed by $T$; it means that if $\alpha$ is in $W$ then $T \alpha$ is in $W$. Let $\beta$ be in $W^{\perp}$. We must show that $T^{*} \beta$ is in $W^{\perp}$, that is, that $\left(\alpha \mid T^{*} \beta\right)=0$ for every $\alpha$ in $W$. If $\alpha$ is in $W$, then $T \alpha$ is in $W$, so $(T \alpha \mid \beta)=0$. But $(T \alpha \mid \beta)=\left(\alpha \mid T^{*} \beta\right)$.

Theorem 18. Let $\mathrm{V}$ be a finite-dimensional inner product space, and let $\mathrm{T}$ be a self-adjoint linear operator on $\mathrm{V}$. Then there is an orthonormal basis for $\mathrm{V}$, each vector of which is a characteristic vector for $\mathrm{T}$.

Proof. We are assuming $\operatorname{dim} V>0$. By Theorem 16, $T$ has a characteristic vector $\alpha$. Let $\alpha_{1}=\alpha /\|\alpha\|$ so that $\alpha_{1}$ is also a characteristic vector for $T$ and $\left\|\alpha_{1}\right\|=1$. If $\operatorname{dim} V=1$, we are done. Now we proceed by induction on the dimension of $V$. Suppose the theorem is true for inner product spaces of dimension less than $\operatorname{dim} V$. Let $W$ be the one-dimensional subspace spanned by the vector $\alpha_{1}$. The statement that $\alpha_{1}$ is a characteristic vector for $T$ simply means that $W$ is invariant under $T$. By Theorem 17, the orthogonal complement $W^{\perp}$ is invariant under $T^{*}=T$. Now $W^{\perp}$, with the inner product from $V$, is an inner product space of dimension one less than the dimension of $V$. Let $U$ be the linear operator induced on $W^{\perp}$ by $T$, that is, the restriction of $T$ to $W^{\perp}$. Then $U$ is self-adjoint, and by the induction hypothesis, $W^{\perp \perp}$ has an orthonormal basis $\left\{\alpha_{2}, \ldots, \alpha_{n}\right\}$ consisting of characteristic vectors for $U$. Now each of these vectors is also a characteristic vector for $T$, and since $V=W \oplus W^{\perp}$, we conclude that $\left\{\alpha_{1}, \ldots, \alpha_{n}\right\}$ is the desired basis for $V$.

Corollary. Let $\mathrm{A}$ be an $\mathrm{n} \times \mathrm{n}$ Hermitian (self-adjoint) matrix. Then there is a unitary matrix $\mathrm{P}$ such that $\mathrm{P}^{-1} \mathrm{AP}$ is diagonal ( $\mathrm{A}$ is unitarily equivalent to a diagonal matrix). If $\mathrm{A}$ is a real symmetric matrix, there is a real orthogonal matrix $\mathrm{P}$ such that $\mathrm{P}^{-1} \mathrm{AP}$ is diagonal.

Proof. Let $V$ be $C^{n \times 1}$, with the standard inner product, and let $T$ be the linear operator on $V$ which is represented by $A$ in the standard ordered basis. Since $A=A^{*}$, we have $T=T^{*}$. Let $B=\left\{\alpha_{1}, \ldots, \alpha_{n}\right\}$ be an ordered orthonormal basis for $V$, such that $T \alpha_{j}=c_{j} \alpha_{j}, j=1, \ldots, n$. If $D=[T]_{B}$, then $D$ is the diagonal matrix with diagonal entries $c_{1}, \ldots, c_{n}$. Let $P$ be the matrix with column vectors $\alpha_{1}, \ldots, \alpha_{n}$. Then $D=P^{-1} A P$. In case each entry of $A$ is real, we can take $V$ to be $R^{n}$, with the standard inner product, and repeat the argument. In this case, $P$ will be a unitary matrix with real entries, i.e., a real orthogonal matrix.

Combining Theorem 18 with our comments at the beginning of this section, we have the following: If $V$ is a finite-dimensional real inner product space and $T$ is a linear operator on $V$, then $V$ has an orthonormal basis of characteristic vectors for $T$ if and only if $T$ is self-adjoint. Equivalently, if $A$ is an $n \times n$ matrix with real entries, there is a real orthogonal matrix $P$ such that $P^{t} A P$ is diagonal if and only if $A=A^{t}$. There is no such result for complex symmetric matrices. In other words, for complex matrices there is a significant difference between the conditions $A=A^{t}$ and $A=A^{*}$.

Having disposed of the self-adjoint case, we now return to the study of normal operators in general. We shall prove the analogue of Theorem 18 for normal operators, in the complex case. There is a reason for this restriction. A normal operator on a real inner product space may not have any non-zero characteristic vectors. This is true, for example, of all but two rotations in $R^{2}$.

Theorem 19. Let $\mathrm{V}$ be a finite-dimensional inner product space and T a normal operator on V. Suppose $\alpha$ is a vector in V. Then $\alpha$ is a characteristic vector for $\mathrm{T}$ with characteristic value $\mathrm{e}$ if and only if $\alpha$ is a characteristic vector for $\mathrm{T}^{*}$ with characteristic value $\overline{\text { c }}$.

Proof. Suppose $U$ is any normal operator on $V$. Then $\|U \alpha\|=$ $\left\|U^{*} \alpha\right\|$. For using the condition $U U^{*}=U^{*} U$ one sees that

$$
\begin{aligned}
\|U \alpha\|^{2}=(U \alpha \mid U \alpha) & =\left(\alpha \mid U^{*} U \alpha\right) \\
& =\left(\alpha \mid U U^{*} \alpha\right)=\left(U^{*} \alpha \mid U^{*} \alpha\right)=\left\|U^{*} \alpha\right\|^{2} .
\end{aligned}
$$

If $c$ is any scalar, the operator $U=T-c I$ is normal. For $(T-c I)^{*}=$ $T^{*}-\bar{c} I$, and it is easy to check that $U U^{*}=U^{*} U$. Thus

$$
\|(T-c I) \alpha\|=\left\|\left(T^{*}-c I\right) \alpha\right\|
$$

so that $(T-c I) \alpha=0$ if and only if $\left(T^{*}-\bar{c} I\right) \alpha=0$.

Definition. A complex $\mathrm{n} \times \mathrm{n}$ matrix $\mathrm{A}$ is called normal if $\mathrm{AA}^{*}=$ $\mathrm{A} * \mathrm{~A}$

It is not so easy to understand what normality of matrices or operators really means; however, in trying to develop some fecling for the concept, the reader might find it helpful to know that a triangular matrix is normal if and only if it is diagonal.

Theorem 20. Let $\mathrm{V}$ be a finite-dimensional inner preduct space, $\mathrm{T}$ a linear operator on $\mathrm{V}$, and $\mathbb{B}$ an orthonormal basis for $\mathrm{V}$. Suppose that the matrix $\mathrm{A}$ of $\mathrm{T}$ in the basis $B$ is upper triangular. Then $\mathrm{T}$ is normal if and only if $\mathrm{A}$ is a diagonal matrix.

Proof. Since $B$ is an orthonormal basis, $A^{*}$ is the matrix of $T^{*}$ in $B$. If $A$ is diagonal, then $A A^{*}=A^{*} A$, and this implies $T T^{*}=T^{*} T^{*}$. Conversely, suppose $T$ is normal, and let $\mathfrak{B}=\left\{\alpha_{1}, \ldots, \alpha_{n}\right\}$. Then, since $A$ is upper-triangular, $T \alpha_{1}=A_{11} \alpha_{1}$. By Theorem 19 this implies, $T^{*} \alpha_{1}=$ $\bar{A}_{11} \alpha_{1}$. On the other hand,

$$
\begin{aligned}
T^{*} \alpha_{1} & =\sum_{j}\left(A^{*}\right)_{j 1} \alpha_{j} \\
& =\sum_{j} \bar{A}_{1 j} \alpha_{j} .
\end{aligned}
$$

Therefore, $A_{1 j}=0$ for every $j>1$. In particular, $A_{12}=0$, and since $A$ is upper-triangular, it follows that

$$
T \alpha_{2}=A_{22} \alpha_{2} .
$$

Thus $T^{*} \alpha_{2}=\bar{A}_{22} \alpha_{2}$ and $A_{2 j}=0$ for all $j \neq 2$. Continuing in this fashion, we find that $A$ is diagonal.

Theorem 21. Let $\mathrm{V}$ be a finite-dimensional complex inner product space and let $\mathrm{T}$ be any linear operator on $\mathrm{V}$. Then there is an orthonormal basis for $\mathrm{V}$ in which the matrix of $\mathrm{T}$ is upper triangular.

Proof. Let $n$ be the dimension of $V$. The theorem is true when $n=1$, and we proceed by induction on $n$, assuming the result is true for linear operators on complex inner product spaces of dimension $n-1$. Since $V$ is a finite-dimensional complex inner product space, there is a unit vector $\alpha$ in $V$ and a scalar $c$ such that

$$
T^{*} \alpha=c \alpha \text {. }
$$

Let $W$ be the orthogonal complement of the subspace spanned by $\alpha$ and let $S$ be the restriction of $T$ to $W$. By Theorem 17, $W$ is invariant under $T$. Thus $S$ is a linear operator on $W$. Since $W$ has dimension $n-1$, our inductive assumption implies the existence of an orthonormal basis $\left\{\alpha_{1}, \ldots, \alpha_{n-1}\right\}$ for $W$ in which the matrix of $S$ is upper-triangular; let $\alpha_{n}=\alpha$. Then $\left\{\alpha_{1}, \ldots, \alpha_{n}\right\}$ is an orthonormal basis for $V$ in which the matrix of $T$ is upper-triangular.

This theorem implies the following result for matrices.

Corollary. For every complex $\mathrm{n} \times \mathrm{n}$ matrix $\mathrm{A}$ there is a unitary matrix $\mathrm{U}$ such that $\mathrm{U}^{-1} \mathrm{AU}$ is upper-triangular.

Now combining Theorem 21 and Theorem 20, we immediately obtain the following analogue of Theorem 18 for normal operators. Sec. $8.5$

Theorem 22. Let $\mathrm{V}$ be a finite-dimensional complex inner product space and $\mathrm{T}$ a normal operator on $\mathrm{V}$. Then $\mathrm{V}$ has an orthonormal basis consisting of characteristic vectors for $\mathrm{T}$.

Again there is a matrix interpretation.

Corollary. For every normal matrix A there is a unitary matrix $\mathrm{P}$ such that $\mathrm{P}^{-1} \mathrm{AP}$ is a diagonal matrix.

\section{Exercises}

1. For each of the following real symmetric matrices $A$, find a real orthogonal matrix $P$ such that $P^{t} A P$ is diagonal.

$$
\left[\begin{array}{ll}
1 & 1 \\
1 & 1
\end{array}\right], \quad\left[\begin{array}{ll}
1 & 2 \\
2 & 1
\end{array}\right], \quad\left[\begin{array}{rr}
\cos \theta & \sin \theta \\
\sin \theta & -\cos \theta
\end{array}\right]
$$

2. Is a complex symmetric matrix self-adjoint? Is it normal?

3. For

$$
A=\left[\begin{array}{lll}
1 & 2 & 3 \\
2 & 3 & 4 \\
3 & 4 & 5
\end{array}\right]
$$

there is a real orthogonal matrix $P$ such that $P^{t} A P=D$ is diagonal. Find such a diagonal matrix $D$.

4. Let $V$ be $C^{2}$, with the standard inner product. Let $T$ be the linear operator on $V$ which is represented in the standard ordered basis by the matrix

$$
A=\left[\begin{array}{ll}
1 & i \\
i & 1
\end{array}\right]
$$

Show that $T$ is normal, and find an orthonormal basis for $V$, consisting of characteristic vectors for $T$.

5. Give an example of a $2 \times 2$ matrix $A$ such that $A^{2}$ is normal, but $A$ is not normal.

6. Let $T$ be a normal operator on a finite-dimensional complex inner product space. Prove that $T$ is self-adjoint, positive, or unitary according as every characteristic value of $T$ is real, positive, or of absolute value 1 . (Use Theorem 22 to reduce to a similar question about diagonal matrices.)

7. Let $T$ be a linear operator on the finite-dimensional inner product space $V$, and suppose $T$ is both positive and unitary. Prove $T=I$.

8. Prove $T$ is normal if and only if $T=T_{1}+i T_{2}$, where $T_{1}$ and $T_{2}$ are selfadjoint operators which commute.

9. Prove that a real symmetric matrix has a real symmetric cube root; i.e., if $A$ is real symmetric, there is a real symmetric $B$ such that $B^{3}=A$.

10. Prove that every positive matrix is the square of a positive matrix. 11. Prove that a normal and nilpotent operator is the zero operator.

12. If $T$ is a normal operator, prove that characteristic vectors for $T$ which are associated with distinct characteristic values are orthogonal.

13. Let $T$ be a normal operator on a finite-dimensional complex inner product space. Prove that there is a polynomial $f$, with complex coefficients, such that $T^{*}=f(T)$. (Represent $T$ by a diagonal matrix, and see what $f$ must be.)

14. If two normal operators commute, prove that their product is normal. 

\section{Operators on Inner Product Spaces}

\subsection{Introduction}

We regard most of the topics treated in Chapter 8 as fundamental, the material that everyone should know. The present chapter is for the more advanced student or for the reader who is eager to expand his knowledge concerning operators on inner product spaces. With the exception of the Principal Axis theorem, which is essentially just another formulation of Theorem 18 on the orthogonal diagonalization of self adjoint operators, and the other results on forms in Section 9.2, the material presented here is more sophisticated and generally more involved technically. We also make more demands of the reader, just as we did in the later parts of Chapters 5 and 7 . The arguments and proofs are written in a more condensed style, and there are almost no examples to smooth the way; however, we have seen to it that the reader is well supplied with generous sets of exercises.

The first three sections are devoted to results concerning forms on inner product spaces and the relation between forms and linear operators. The next section deals with spectral theory, i.e., with the implications of Theorems 18 and 22 of Chapter 8 concerning the diagonalization of selfadjoint and normal operators. In the final section, we pursue the study of normal operators treating, in particular, the real case, and in so doing we examine what the primary decomposition theorem of Chapter 6 says about nornal operators. 

\subsection{Forms on Inner Product Spaces}

If $T$ is a linear operator on a finite-dimensional inner product space $V$ the function $f$ defined on $V \times V$ by

$$
f(\alpha, \beta)=(T \alpha \mid \beta)
$$

may be regarded as a kind of substitute for $T$. Many questions about $T$ are equivalent to questions concerning $f$. In fact, it is easy to see that $f$ determines $T$. For if $B=\left\{\alpha_{1}, \ldots, \alpha_{n}\right\}$ is an orthonormal basis for $V$, then the entries of the matrix of $T$ in $B$ are given by

$$
A_{j k}=f\left(\alpha_{k}, \alpha_{j}\right) .
$$

It is important to understand why $f$ determines $T$ from a more abstract point of view. The crucial properties of $f$ are described in the following definition.

Definition. A (sesqui-linear) form on a real or complex vector space $\mathrm{V}$ is a function $\mathrm{f}$ on $\mathrm{V} \times \mathrm{V}$ with values in the field of scalars such that

(a) $\quad \mathrm{f}(\mathrm{c} \alpha+\beta, \gamma)=\operatorname{cf}(\alpha, \gamma)+\mathrm{f}(\beta, \gamma)$

(b) $\quad \mathrm{f}(\alpha, \mathrm{c} \beta+\gamma)=\overline{\mathrm{c}} \mathrm{f}(\alpha, \beta)+\mathrm{f}(\alpha, \gamma)$

for all $\alpha, \beta, \gamma$ in $\mathrm{V}$ and all scalars $\mathrm{c}$.

Thus, a sesqui-linear form is a function on $V \times V$ such that $f(\alpha, \beta)$ is a linear function of $\alpha$ for fixed $\beta$ and a conjugate-linear function of $\beta$ for fixed $\alpha$. In the real case, $f(\alpha, \beta)$ is linear as a function of each argument; in other words, $f$ is a bilinear form. In the complex case, the sesquilinear form $f$ is not bilinear unless $f=0$. In the remainder of this chapter, we shall omit the adjective 'sesqui-linear' unless it seems important to include it.

If $f$ and $g$ are forms on $V$ and $c$ is a scalar, it is easy to check that $c f+g$ is also a form. From this it follows that any linear combination of forms on $V$ is again a form. Thus the set of all forms on $V$ is a subspace of the vector space of all scalar-valued functions on $V \times V$.

Theorem 1. Let $\mathrm{V}$ be a finite-dimensional inner product space and $\mathrm{f}$ a form on $\mathrm{V}$. Then there is a unique linear operator $\mathrm{T}$ on $\mathrm{V}$ such that

$$
\mathrm{f}(\alpha, \beta)=(\mathrm{T} \alpha \mid \beta)
$$

for all $\alpha, \beta$ in $\mathrm{V}$, and the map $\mathrm{f} \rightarrow \mathrm{T}$ is an isomorphism of the space of forms onto $\mathrm{L}(\mathrm{V}, \mathrm{V})$.

Proof. Fix a vector $\beta$ in $V$. Then $\alpha \rightarrow f(\alpha, \beta)$ is a linear function on $V$. By Theorem 6 there is a unique vector $\beta^{\prime}$ in $V$ such that $f(\alpha, \beta)=$ $\left(\alpha \mid \beta^{\prime}\right)$ for every $\alpha$. We define a function $U$ from $V$ into $V$ by setting $U \beta=$ $\beta^{\prime}$. Then Sec. $9.2$

$$
\begin{aligned}
f(\alpha \mid c \beta+\gamma) & =(\alpha \mid U(c \beta+\gamma)) \\
& =\bar{c} f(\alpha, \beta)+f(\alpha, \gamma) \\
& =\bar{c}(\alpha \mid U \beta)+(\alpha \mid U \gamma) \\
& =(\alpha \mid c U \beta+U \gamma)
\end{aligned}
$$

for all $\alpha, \beta, \gamma$ in $V$ and all scalars $c$. Thus $U$ is a linear operator on $V$ and $T=U^{*}$ is an operator such that $f(\alpha, \beta)=(T \alpha \mid \beta)$ for all $\alpha$ and $\beta$. If we also have $f(\alpha, \beta)=\left(T^{\prime} \alpha \mid \beta\right)$, then

$$
\left(T \alpha-T^{\prime} \alpha \mid \beta\right)=0
$$

for all $\alpha$ and $\beta$; so $T \alpha=T^{\prime} \alpha$ for all $\alpha$. Thus for each form $f$ there is a unique linear operator $T_{f}$ such that

$$
f(\alpha, \beta)=\left(T_{f} \alpha \mid \beta\right)
$$

for all $\alpha, \beta$ in $V$. If $f$ and $g$ are forms and $c$ a scalar, then

$$
\begin{aligned}
(c f+g)(\alpha, \beta) & =\left(T_{c f+\theta} \alpha \mid \beta\right) \\
& =c f(\alpha, \beta)+g(\alpha, \beta) \\
& =c\left(T_{f} \alpha \mid \beta\right)+\left(T_{g} \alpha \mid \beta\right) \\
& =\left(\left(c T_{f}+T_{g}\right) \alpha \mid \beta\right)
\end{aligned}
$$

for all $\alpha$ and $\beta$ in $V$. Therefore,

$$
T_{c f+o}=c T_{f}+T_{o}
$$

so $f \rightarrow T_{f}$ is a linear map. For each $T$ in $L(V, V)$ the equation

$$
f(\alpha, \beta)=(T \alpha \mid \beta)
$$

defines a form such that $T_{f}=T$, and $T_{f}=0$ if and only if $f=0$. Thus $f \rightarrow T_{f}$ is an isomorphism.

Corollary. The equation

$$
(\mathrm{f} \mid \mathrm{g})=\operatorname{tr}\left(\mathrm{T}_{\mathrm{f}} \mathrm{T}_{g}^{*}\right)
$$

defines an inner product on the space of forms with the property that

$$
(\mathrm{f} \mid \mathrm{g})=\sum_{j, k} \mathrm{f}\left(\alpha_{\mathrm{k}}, \alpha_{\mathrm{j}}\right) \overline{\mathrm{g}\left(\alpha_{\mathrm{k}}, \alpha_{\mathrm{j}}\right)}
$$

for every orthonormal basis $\left\{\alpha_{1}, \ldots, \alpha_{n}\right\}$ of $\mathrm{V}$.

Proof. It follows easily from Example 3 of Chapter 8 that $(T, U) \rightarrow \operatorname{tr}\left(T U^{*}\right)$ is an inner product on $L(V, V)$. Since $f \rightarrow T_{f}$ is an isomorphism, Example 6 of Chapter 8 shows that

$$
(f \mid g)=\operatorname{tr}\left(T_{f} T_{g}^{*}\right)
$$

is an inner product. Now suppose that $A$ and $B$ are the matrices of $T_{f}$ and $T_{0}$ in the orthonormal basis $B=\left\{\alpha_{1}, \ldots, \alpha_{n}\right\}$. Then

$$
A_{j k}=\left(T_{ر} \alpha_{k} \mid \alpha_{j}\right)=f\left(\alpha_{k}, \alpha_{j}\right)
$$

and $B_{j k}=\left(T_{,} \alpha_{k} \mid \alpha_{j}\right)=g\left(\alpha_{k}, \alpha_{j}\right)$. Since $A B^{*}$ is the matrix of $T_{f} T_{0}^{*}$ in the basis $Q$, it follows that

$$
(f \mid g)=\operatorname{tr}\left(A B^{*}\right)=\sum_{j, k} A_{j k} \bar{B}_{j k}
$$

Definition. If $\mathrm{f}$ is a form and $B=\left\{\alpha_{1}, \ldots, \alpha_{\mathrm{n}}\right\}$ an arbitrary ordered basis of $\mathrm{V}$, the matrix $\mathrm{A}$ with entries

$$
\mathrm{A}_{\mathrm{jk}}=\mathrm{f}\left(\alpha_{\mathrm{k}}, \alpha_{\mathrm{j}}\right)
$$

is called the matrix of $\mathrm{f}$ in the ordered basis $B$.

When $Q$ is a a orthonormal basis, the matrix of $f$ in $B$ is also the matrix of the linear transformation $T_{f}$, but in general this is not the case.

If $A$ is the matrix of $f$ in the ordered basis $B=\left\{\alpha_{1}, \ldots, \alpha_{n}\right\}$, it follows that

![](https://cdn.mathpix.com/cropped/2023_01_16_0f6afdf132d84aaf6bdeg-331.jpg?height=81&width=455&top_left_y=807&top_left_x=581)

for all scalars $x_{s}$ and $y_{r}(1 \leq r, s \leq n)$. In other words, the matrix $A$ has the property that

$$
f(\alpha, \beta)=Y^{*} A X
$$

where $X$ and $Y$ are the respective coordinate matrices of $\alpha$ and $\beta$ in the ordered basis $B$.

The matrix of $f$ in another basis

$$
\alpha_{j}^{\prime}=\sum_{i=1}^{n} P_{i j} \alpha_{i}, \quad(1 \leq j \leq n)
$$

is given by the equation

$$
A^{\prime}=P^{*} A P .
$$

For

$$
\begin{aligned}
& A_{j k}^{\prime}=f\left(\alpha_{k}^{\prime}, \alpha_{j}^{\prime}\right) \\
& =f\left(\sum_{\boldsymbol{s}} P_{s k} \alpha_{s}, \sum_{r} P_{r j} \alpha_{r}\right) \\
& =\sum_{r, s} \bar{P}_{r j} A_{r s} P_{s k} \\
& =\left(P^{*} A P\right)_{j k} \text {. }
\end{aligned}
$$

Since $P^{*}=P^{-1}$ for unitary matrices, it follows from (9-2) that results concerning unitary equivalence may be applied to the study of forms.

Theorem 2. Let $\mathrm{f}$ be a form on a finite-dimensional complex inner product space $\mathrm{V}$. Then there is an orthonormal basis for $\mathrm{V}$ in which the matrix of $\mathrm{f}$ is upper-triangular.

Proof. Let $T$ be the linear operator on $V$ such that $f(\alpha, \beta)=$ $(T \alpha \mid \beta)$ for all $\alpha$ and $\beta$. By Theorem 21, there is an orthonormal basis $\left\{\alpha_{1}, \ldots, \alpha_{n}\right\}$ in which the matrix of $T$ is upper-triangular. Hence,

when $j>k$.

$$
f\left(\alpha_{k}, \alpha_{j}\right)=\left(T \alpha_{k} \mid \alpha_{j}\right)=0
$$

Definition. A form $\mathrm{f}$ on a real or complex vector space $\mathrm{V}$ is called Hermitian if

$$
\mathrm{f}(\alpha, \beta)=\overline{\mathrm{f}(\beta, \alpha)}
$$

for all $\alpha$ and $\beta$ in $\mathrm{V}$.

If $T$ is a linear operator on a finite-dimensional inner product space $V$ and $f$ is the form

$$
f(\alpha, \beta)=(T \alpha \mid \beta)
$$

then $\overline{f(\beta, \alpha)}=(\alpha \mid T \beta)=\left(T^{*} \alpha \mid \beta\right)$; so $f$ is Hermitian if and only if $T$ is selfadjoint.

When $f$ is Hermitian $f(\alpha, \alpha)$ is real for every $\alpha$, and on complex spaces this property characterizes Hermitian forms.

Theorem 3. Let $\mathrm{V}$ be a complex vector space and $\mathrm{f}$ a form on $\mathrm{V}$ such that $\mathrm{f}(\alpha, \alpha)$ is real for every $\alpha$. Then $\mathrm{f}$ is Hermitian.

Proof. Let $\alpha$ and $\beta$ be vectors in $V$. We must show that $f(\alpha, \beta)=$ $\overline{f(\beta, \alpha)}$. Now

$$
f(\alpha+\beta, \alpha+\beta)=f(\alpha, \beta)+f(\alpha, \beta)+f(\beta, \alpha)+f(\beta, \beta) .
$$

Since $f(\alpha+\beta, \alpha+\beta), f(\alpha, \alpha)$, and $f(\beta, \beta)$ are real, the number $f(\alpha, \beta)+$ $f(\beta, \alpha)$ is real. Looking at the same argument with $\alpha+i \beta$ instead of $\alpha+\beta$, we see that $-i f(\alpha, \beta)+i f(\beta, \alpha)$ is real. Having concluded that two numbers are real, we set them equal to their complex conjugates and obtain

$$
\begin{aligned}
f(\alpha, \beta)+f(\beta, \alpha) & =\overline{f(\alpha, \beta)}+\overline{f(\beta, \alpha)} \\
-i f(\alpha, \beta)+i f(\beta, \alpha) & =i \overline{f(\alpha, \beta)}-i \overline{f(\beta, \alpha)}
\end{aligned}
$$

If we multiply the second equation by $i$ and add the result to the first equation, we obtain

$$
2 f(\alpha, \beta)=2 f(\beta, \alpha) .
$$

Corollary. Let $\mathrm{T}$ be a linear operator on a complex finite-dimensional inner product space $\mathrm{V}$. Then $\mathrm{T}$ is self-adjoint if and only if $(\mathrm{T} \alpha \mid \alpha)$ is real for every $\alpha$ in $\mathrm{V}$.

Theorem 4 (Principal Axis Theorem). For every Hermitian form $\mathrm{f}$ on a finite-dimensional inner product space $\mathrm{V}$, there is an orthonormal basis of $\mathrm{V}$ in which $\mathrm{f}$ is represented by a diagonal matrix with real entries. Proof. Let $T$ be the linear operator such that $f(\alpha, \beta)=(T \alpha \mid \beta)$ for all $\alpha$ and $\beta$ in $V$. Then, since $f(\alpha, \beta)=\overline{f(\beta, \alpha)}$ and $(\overline{T \beta \mid \alpha})=(\alpha \mid T \beta)$, it follows that

$$
(T \alpha \mid \beta)=\overline{f(\beta, \alpha)}=(\alpha \mid T \beta)
$$

for all $\alpha$ and $\beta$; hence $T=T^{*}$. By Theorem 18 of Chapter 8 , there is an orthonormal basis of $V$ which consists of characteristic vectors for $T$. Suppose $\left\{\alpha_{1}, \ldots, \alpha_{n}\right\}$ is an orthonormal basis and that

for $1 \leq j \leq n$. Then

$$
T \alpha_{j}=c_{j} \alpha_{j}
$$

$$
f\left(\alpha_{k}, \alpha_{j}\right)=\left(T \alpha_{k} \mid \alpha_{j}\right)=\delta_{k j} c_{k}
$$

and by Theorem 15 of Chapter 8 each $c_{k}$ is real.

Corollary. Under the above conditions

$$
\mathrm{f}\left(\sum_{j} \mathrm{x}_{\mathrm{j}} \alpha_{\mathrm{j}}, \sum_{k} \mathrm{y}_{\mathrm{k}} \alpha_{k}\right)=\sum_{j} c_{j} \mathrm{X}_{\mathrm{j}} \bar{y}_{\mathrm{j}} \text {. }
$$

\section{Exercises}

1. Which of the following functions $f$, defined on vectors $\alpha=\left(x_{1}, x_{2}\right)$ and $\beta=$ $\left(y_{1}, y_{2}\right)$ in $C^{2}$, are (sesqui-linear) forms on $C^{2}$ ?
(a) $f(\alpha, \beta)=1$.
(b) $f(\alpha, \beta)=\left(x_{1}-\bar{y}_{1}\right)^{2}+x_{2} \bar{y}_{2}$.
(c) $f(\alpha, \beta)=\left(x_{1}+\bar{y}_{1}\right)^{2}-\left(x_{1}-\bar{y}_{1}\right)^{2}$.
(d) $f(\alpha, \beta)=x_{1} \bar{y}_{2}-\check{x}_{2} y_{1}$.

2. Let $f$ be the form on $R^{2}$ defined by

$$
f\left(\left(x_{1}, y_{1}\right),\left(x_{2}, y_{2}\right)\right)=x_{1} y_{1}+x_{2} y_{2} .
$$

Find the matrix of $f$ in each of the following bases:

$$
\{(1,0),(0,1)\},\{(1,-1),(1,1)\},\{(1,2),(3,4)\} \text {. }
$$

3. Let

$$
A=\left[\begin{array}{cc}
1 & i \\
-i & 2
\end{array}\right]
$$

and let $g$ be the form (on the space of $2 \times 1$ complex matrices) defined by $g(X, Y)=$ $Y^{*} A X$. Is $g$ an inner product?

4. Let $V$ be a complex vector space and let $f$ be a (sesqui-linear) form on $V$ which is symmetric: $f(\alpha, \beta)=f(\beta, \alpha)$. What is $f$ ?

5. Let $f$ be the form on $R^{2}$ given by

$$
f\left(\left(x_{1}, x_{2}\right),\left(y_{1}, y_{2}\right)\right)=x_{1} y_{1}+4 x_{2} y_{2}+2 x_{1} y_{2}+2 x_{2} y_{1} .
$$

Find an ordered basis in which $f$ is represented by a diagonal matrix.

6. Call the form $f$ (left) non-degenerate if 0 is the only vector $\alpha$ such that $f(\alpha, \beta)=0$ for all $\beta$. Let $f$ be a form on an inner product space $V$. Prove that $f$ is non-degenerate if and only if the associated linear operator $T_{f}$ (Theorem 1 ) is non-singular.

7. Let $f$ be a form on a finite-dimensional vector space $V$. Look at the definition of left non-degeneracy given in Exercise 6 . Define right non-degeneracy and prove that the form $f$ is left non-degenerate if and only if $f$ is right non-degenerate.

8. Let $f$ be a non-degenerate form (Exercises 6 and 7 ) on a finite-dimensional space $V$. Let $L$ be a linear functional on $V$. Show that there exists one and only one vector $\beta$ in $V$ such that $L(\alpha)=f(\alpha, \beta)$ for all $\alpha$.

9. Let $f$ be a non-degenerate form on a finite-dimensional space $V$. Show that each linear operator $S$ has an 'adjoint relative to $f$,' i.e., an operator $S^{\prime}$ such that $f(S \alpha, \beta)=f\left(\alpha, S^{\prime} \beta\right)$ for all $\alpha, \beta$

\subsection{Positive Forms}

In this section, we shall discuss non-negative (sesqui-linear) forms and their relation to a given inner product on the underlying vector space.

Definitions. A form $\mathrm{f}$ on a real or complex vector space $\mathrm{V}$ is nonnegative if it is Hermitian and $\mathrm{f}(\alpha, \alpha) \geq 0$ for every $\alpha$ in $\mathrm{V}$. The form $\mathrm{f}$ is positive if $\mathrm{f}$ is Hermitian and $\mathrm{f}(\alpha, \alpha)>0$ for all $\alpha \neq 0$.

A positive form on $V$ is simply an inner product on $V$. A non-negative form satisfies all of the properties of an inner product except that some nonzero vectors may be 'orthogonal' to themselves.

Let $f$ be a form on the finite-dimensional space $V$. Let $B=\left\{\alpha_{1}, \ldots, \alpha_{n}\right\}$ be an ordered basis for $V$, and let $A$ be the matrix of $f$ in the basis $B$, that is, $A_{j k}=f\left(\alpha_{k}, \alpha_{j}\right)$. If $\alpha=x_{1} \alpha_{1}+\cdots+x_{n} \alpha_{n}$, then

$$
\begin{aligned}
f(\alpha, \alpha) & =f\left(\sum_{j} x_{j} \alpha_{j}, \sum_{k} x_{k} \alpha_{k}\right) \\
& =\sum_{j} \sum_{k} x_{j} \bar{x}_{k} f\left(\alpha_{j}, \alpha_{k}\right) \\
& =\sum_{j} \sum_{k} A_{k j} x_{j} \bar{x}_{k} .
\end{aligned}
$$

So, we see that $f$ is non-negative if and only if

$$
A=A^{*}
$$

and

$$
\sum_{j} \sum_{k} A_{k j} x_{j} \bar{x}_{k} \geq 0 \text { for all scalars } x_{1}, \ldots, x_{n} \text {. }
$$

In order that $f$ should be positive, the inequality in (9-3) must be strict for all $\left(x_{1}, \ldots, x_{n}\right) \neq 0$. The conditions we have derived state that $f$ is a positive form on $V$ if and only if the function

$$
g(X, Y)=Y^{*} A X
$$

is a positive form on the space of $n \times 1$ column matrices over the scalar field.

Theorem 5. Let $\mathrm{F}$ be the field of real numbers or the field of complex numbers. Let $\mathrm{A}$ be an $\mathrm{n} \times \mathrm{n}$ matrix over $\mathrm{F}$. The function $\mathrm{g}$ defined by

$$
\mathrm{g}(\mathrm{X}, \mathrm{Y})=\mathrm{Y}^{*} \mathrm{AX}
$$

is a positive form on the space $\mathrm{F}^{n \times 1}$ if and only if there exists an invertible $\mathrm{n} \times \mathrm{n}$ matrix $\mathrm{P}$ with entries in $\mathrm{F}$ such that $\mathrm{A}=\mathrm{P}^{*} \mathrm{P}$.

Proof. For any $n \times n$ matrix $A$, the function $g$ in (9-4) is a form on the space of column matrices. We are trying to prove that $g$ is positive if and only if $A=P^{*} P$. First, suppose $A=P^{*} P$. Then $g$ is Hermitian and

$$
\begin{aligned}
g(X, X) & =X^{*} P^{*} P X \\
& =(P X)^{*} P X \\
& \geq 0 .
\end{aligned}
$$

If $P$ is invertible and $X \neq 0$, then $(P X)^{*} P X>0$.

Now, suppose that $g$ is a positive form on the space of column matrices. Then it is an inner product and hence there exist column matrices $Q_{1}, \ldots$, $Q_{n}$ such that

$$
\begin{aligned}
\delta_{j k} & =g\left(Q_{i}, Q_{k}\right) \\
& =Q_{k}^{*} A Q_{j} .
\end{aligned}
$$

But this just says that, if $Q$ is the matrix with columns $Q_{1}, \ldots, Q_{n}$, then $Q^{*} A Q=I$. Since $\left\{Q_{1}, \ldots, Q_{n}\right\}$ is a basis, $Q$ is invertible. Let $P=Q^{-1}$ and we have $A=P^{*} P$.

In practice, it is not easy to verify that a given matrix $A$ satisfies the criteria for positivity which we have given thus far. One consequence of the last theorem is that if $g$ is positive then $\operatorname{det} A>0$, because $\operatorname{det} A=$ $\operatorname{det}\left(P^{*} P\right)=\operatorname{det} P^{*} \operatorname{det} P=|\operatorname{det} P|^{2}$. The fact that $\operatorname{det} A>0$ is by no means sufficient to guarantee that $g$ is positive; however, there are $n$ determinants associated with $A$ which have this property: If $A=A^{*}$ and if each of those determinants is positive, then $g$ is a positive form.

Definition. Let $\mathrm{A}$ be an $\mathrm{n} \times \mathrm{n}$ matrix over the field $\mathrm{F}$. The principal minors of $\mathrm{A}$ are the scalars $\Delta_{\mathbf{k}}(\mathrm{A})$ defined by

$$
\Delta_{\mathrm{k}}(\mathrm{A})=\operatorname{det}\left[\begin{array}{ccc}
\mathrm{A}_{11} & \cdots & \mathrm{A}_{1 \mathrm{k}} \\
\vdots & & \vdots \\
A_{\mathrm{k} 1} & \cdots & \mathrm{A}_{\mathrm{kk}}
\end{array}\right], \quad 1 \leq \mathrm{k} \leq \mathrm{n} .
$$

Lemma. Let $\mathrm{A}$ be an invertible $\mathrm{n} \times \mathrm{n}$ matrix with entries in a field $\mathrm{F}$. The following two statements are equivalent. (a) There is an upper-triangular matrix $\mathrm{P}$ with $\mathrm{P}_{\mathrm{kk}}=1(1 \leq \mathrm{k} \leq \mathrm{n})$ such that the matrix $\mathrm{B}=\mathrm{AP}$ is lower-triangular.

(b) The principal minors of $\mathrm{A}$ are all different from 0.

Proof. Let $P$ be any $n \times n$ matrix and set $B=A P$. Then

![](https://cdn.mathpix.com/cropped/2023_01_16_0f6afdf132d84aaf6bdeg-336.jpg?height=79&width=253&top_left_y=409&top_left_x=505)

If $P$ is upper-triangular and $P_{k k}=1$ for every $k$, then

$$
\sum_{r=1}^{k-1} A_{j r} P_{r k}=B_{j k}-A_{k k}, \quad k>1 .
$$

Now $B$ is lower-triangular provided $B_{j k}=0$ for $j<k$. Thus $B$ will be lower-triangular if and only if

$$
\begin{array}{ll}
\sum_{r=1}^{k-1} A_{j r} P_{r k}=-A_{k k}, & 1 \leq j \leq k-1 \\
& 2 \leq k \leq n .
\end{array}
$$

So, we see that statement (a) in the lemma is equivalent to the statement that there exist scalars $P_{r k}, 1 \leq r \leq k, 1 \leq k \leq n$, which satisfy (9-5) and $P_{k k}=1,1 \leq k \leq n$.

In (9-5), for each $k>1$ we have a system of $k-1$ linear equations for the unknowns $P_{1 k}, P_{2 k}, \ldots, P_{k-1, k}$. The coefficient matrix of that system is

$$
\left[\begin{array}{ccc}
A_{11} & \cdots & A_{1, k-1} \\
\vdots & & \vdots \\
A_{k-1} & \cdots & A_{k-1, k-1}
\end{array}\right]
$$

and its determinant is the principal minor $\Delta_{k-1}(A)$. If each $\Delta_{k-1}(A) \neq 0$, the systems (9-5) have unique solutions. We have shown that statement (b) implies statement (a) and that the matrix $P$ is unique.

Now suppose that (a) holds. Then, as we shall see,

$$
\begin{aligned}
\Delta_{k}(A) & =\Delta_{k}(B) \\
& =B_{11} B_{22} \cdots B_{k k}, \quad k=1, \ldots, n .
\end{aligned}
$$

To verify (9-6), let $A_{1}, \ldots, A_{n}$ and $B_{1}, \ldots, B_{n}$ be the columns of $A$ and $B$, respectively. Then

$$
\begin{aligned}
& B_{1}=A_{1} \\
& B_{r}=\sum_{j=1}^{r-1} P_{j r} A_{j}+A_{r}, \quad r>1 .
\end{aligned}
$$

Fix $k, 1 \leq k \leq n$. From (9-7) we see that the $r$ th column of the matrix

$$
\left[\begin{array}{ccc}
B_{11} & \cdots & B_{k k} \\
\vdots & & \vdots \\
B_{k 1} & \cdots & B_{k k}
\end{array}\right]
$$

is obtained by adding to the $r$ th column of 

$$
\left[\begin{array}{ccc}
A_{11} & \cdots & A_{1 k} \\
\vdots & & \vdots \\
A_{k 1} & \cdots & A_{k k}
\end{array}\right]
$$

a linear combination of its other columns. Such operations do not change determinants. That proves (9-6), except for the trivial observation that because $B$ is triangular $\Delta_{k}(B)=B_{11} \cdots B_{k k}$. Since $A$ and $P$ are invertible, $B$ is invertible. Therefore,

$$
\Delta(B)=B_{11} \cdots B_{n n} \neq 0
$$

and so $\Delta_{k}(A) \neq 0, k=1, \ldots, n$.

Theorem 6. Let $\mathrm{f}$ be a form on a finite-dimensional vector space $\mathrm{V}$ and let $\mathrm{A}$ be the matrix of $\mathrm{f}$ in an ordered basis $\circledR$. Then $\mathrm{f}$ is a positive form if and only if $\mathrm{A}=\mathrm{A}^{*}$ and the principal minors of $\mathrm{A}$ are all positive.

Proof. Let's do the interesting half of the theorem first. Suppose that $A=A^{*}$ and $\Delta_{k}(A)>0,1 \leq k \leq n$. By the lemma, there exists an (unique) upper-triangular matrix $P$ with $P_{k k}=1$ such that $B=A P$ is lower-triangular. The matrix $P^{*}$ is lower-triangular, so that $P^{*} B=P^{*} A P$ is also lower-triangular. Since $A$ is self-adjoint, the matrix $D=P^{*} A P$ is self-adjoint. A self-adjoint triangular matrix is necessarily a diagonal matrix. By the same reasoning which led to $(9-6)$,

$$
\begin{aligned}
\Delta_{k}(D) & =\Delta_{k}\left(P^{*} B\right) \\
& =\Delta_{k}(B) \\
& =\Delta_{k}(A) .
\end{aligned}
$$

Since $D$ is diagonal, its principal minors are

$$
\Delta_{k}(D)=D_{11} \cdots D_{k k} .
$$

From $\Delta_{k}(D)>0,1 \leq k \leq n$, we obtain $D_{k k}>0$ for each $k$.

If $A$ is the matrix of the form $f$ in the ordered basis $B=\left\{\alpha_{1}, \ldots, \alpha_{n}\right\}$, then $D=P^{*} A P$ is the matrix of $f$ in the basis $\left\{\alpha_{1}^{\prime}, \ldots, \alpha_{n}^{\prime}\right\}$ defined by

$$
\alpha_{j}^{\prime}=\sum_{i=1}^{n} P_{i j} \alpha_{i} .
$$

See (9-2). Since $D$ is diagonal with positive entries on its diagonal, it is obvious that

$$
X^{*} D X>0, \quad X \neq 0
$$

from which it follows that $f$ is a positive form.

Now, suppose we start with a positive form $f$. We know that $A=A^{*}$. How do we show that $\Delta_{k}(A)>0,1 \leq k \leq n$ ? Let $V_{k}$ be the subspace spanned by $\alpha_{1}, \ldots, \alpha_{k}$ and let $f_{k}$ be the restriction of $f$ to $V_{k} \times V_{k}$. Evi- dently $f_{k}$ is a positive form on $V_{k}$ and, in the basis $\left\{\alpha_{1}, \ldots, \alpha_{k}\right\}$ it is represented by the matrix

$$
\left[\begin{array}{ccc}
A_{11} & \cdots & A_{1 k} \\
\vdots & & \vdots \\
A_{k 1} & \cdots & A_{k k}
\end{array}\right] \text {. }
$$

As a consequence of Theorem 5, we noted that the positivity of a form implies that the determinant of any representing matrix is positive.

There are some comments we should make, in order to complete our discussion of the relation between positive forms and matrices. What is it that characterizes the matrices which represent positive forms? If $f$ is a form on a complex vector space and $A$ is the matrix of $f$ in some ordered basis, then $f$ will be positive if and only if $A=A^{*}$ and

$$
X^{*} A X>0, \quad \text { for all complex } X \neq 0 \text {. }
$$

It follows from Theorem 3 that the condition $A=A^{*}$ is redundant, i.e., that (9-8) implies $A=A^{*}$. On the other hand, if we are dealing with a real vector space the form $f$ will be positive if and only if $A=A^{t}$ and

$$
X^{t} A X>0, \quad \text { for all real } X \neq 0 \text {. }
$$

We want to emphasize that if a real matrix $A$ satisfies (9-9), it does not follow that $A=A^{t}$. One thing which is true is that, if $A=A^{t}$ and (9-9) holds, then (9-8) holds as well. That is because

$$
\begin{aligned}
(X+i Y)^{*} A(X+i Y) & =\left(X^{t}-i Y^{t}\right) A(X+i Y) \\
& =X^{t} A X+Y^{t} A Y+i\left[X^{t} A Y-Y^{t} A X\right]
\end{aligned}
$$

and if $\Lambda=\Lambda^{t}$ then $Y^{t} \Lambda X=X^{t} \Lambda Y$.

If $A$ is an $n \times n$ matrix with complex entries and if $A$ satisfies (9-9), we shall call $A$ a positive matrix. The comments which we have just made may be summarized by saying this: In either the real or complex case, a form $f$ is positive if and only if its matrix in some (in fact, every) ordered basis is a positive matrix.

Now suppose that $V$ is a finite-dimensional inner product space. Let $f$ be a non-negative form on $V$. There is a unique self-adjoint linear operator $T$ on $V$ such that

$$
f(\alpha, \beta)=(T \alpha \mid \beta) .
$$

and $T$ has the additional property that $(\Gamma \alpha \mid \alpha) \geq 0$.

Definition. A linear operator $\mathrm{T}$ on a finite-dimensional inner product space $\mathrm{V}$ is non-negative if $\mathrm{T}=\mathrm{T}^{*}$ and $(\mathrm{T} \alpha \mid \alpha) \geq 0$ for all $\alpha$ in $\mathrm{V}$. A positive linear operator is one such that $\mathrm{T}=\mathrm{T}^{*}$ and $(\mathrm{T} \alpha \mid \alpha)>0$ for all $\alpha \neq 0$. If $V$ is a finite-dimensional (real or complex) vector space and if $(\cdot \mid \cdot)$ is an inner product on $V$, there is an associated class of positive linear operators on $V$. Via (9-10) there is a one-one correspondence between that class of positive operators and the collection of all positive forms on $V$. We shall use the exercises for this section to emphasize the relationships between positive operators, positive forms, and positive matrices. The following summary may be helpful.

If $A$ is an $n \times n$ matrix over the field of complex numbers, the following are equivalent.

(1) $A$ is positive, i.e., $\sum_{j} \sum_{k} A_{k j} x_{j} \bar{x}_{k}>0$ whenever $x_{1}, \ldots, x_{n}$ are complex numbers, not all 0.

(2) $(X \mid Y)=Y^{*} A X$ is an inner product on the space of $n \times 1$ complex matrices.

(3) Relative to the standard inner product $(X \mid Y)=Y^{*} X$ on $n \times 1$ matrices, the linear operator $X \rightarrow A X$ is positive.

(4) $A=P^{*} P$ for some invertible $n \times n$ matrix $P$ over $C$.

(5) $A=A^{*}$, and the principal minors of $A$ are positive.

If each entry of $A$ is real, these are equivalent to:

(6) $A=A^{t}$, and $\sum_{j} \sum_{k} A_{k j} x_{j} x_{k}>0$ whenever $x_{1}, \ldots, x_{n}$ are real numbers not all 0.

(7) $(X \mid Y)=Y^{t} A X$ is an inner product on the space of $n \times 1$ real matrices.

(8) Relative to the standard inner product $(X \mid Y)=Y^{t} X$ on $n \times 1$ real matrices, the linear operator $X \rightarrow A X$ is positive.

(9) There is an invertible $n \times n$ matrix $P$, with real entries, such that $A=P^{t} P$.

\section{Exercises}

1. Let $V$ be $C^{2}$, with the standard inner product. For which vectors $\alpha$ in $V$ is there a positive linear operator $T$ such that $\alpha=T \epsilon_{1}$ ?

2. Let $V$ be $R^{2}$, with the standard inner product. If $\theta$ is a real number, let $T$ be the linear operator 'rotation through $\theta$,'

$$
T_{\theta}\left(x_{1}, x_{2}\right)=\left(x_{1} \cos \theta-x_{2} \sin \theta, x_{1} \sin \theta+x_{2} \cos \theta\right) \text {. }
$$

For which values of $\theta$ is $T_{\theta}$ a positive operator?

3. Let $V$ be the space of $n \times 1$ matrices over $C$, with the inner product $(X \mid Y)=$ $Y^{*} G X$ (where $G$ is an $n \times n$ matrix such that this is an inner product). Let $A$ be an $n \times n$ matrix and $T$ the linear operator $T(X)=A X$. Find $T^{*}$. If $Y$ is a fixed element of $V$, find the element $Z$ of $V$ which determines the linear functional $X \rightarrow Y^{*} X$. In other words, find $Z$ such that $Y^{*} X=(X \mid Z)$ for all $X$ in $V$. 4. Let $V$ be a finite-dimensional inner product space. If $T$ and $U$ are positive linear operators on $V$, prove that $(T+U)$ is positive. Give an example which shows that $T U$ need not be positive.

5. Let

$$
A=\left[\begin{array}{ll}
1 & \frac{1}{2} \\
\frac{1}{2} & \frac{1}{3}
\end{array}\right]
$$

(a) Show that $A$ is positive.

(b) Let $V$ be the space of $2 \times 1$ real matrices, with the inner product $(X \mid Y)=Y^{t} A X$. Find an orthonormal basis for $V$, by applying the Gram-Schmidt process to the basis $\left\{X_{1}, X_{2\}}\right\}$ defined by

$$
X_{1}=\left[\begin{array}{l}
1 \\
0
\end{array}\right], \quad X_{2}=\left[\begin{array}{l}
0 \\
1
\end{array}\right] .
$$

(c) Find an invertible $2 \times 2$ real matrix $P$ such that $A=P^{t} P$.

6. Which of the following matrices are positive?

$$
\left[\begin{array}{ll}
1 & 2 \\
3 & 4
\end{array}\right], \quad\left[\begin{array}{cc}
1 & 1+i \\
1-i & 3
\end{array}\right], \quad\left[\begin{array}{ccc}
1 & -1 & 1 \\
2 & -1 & 1 \\
3 & -1 & 1
\end{array}\right], \quad\left[\begin{array}{ccc}
1 & \frac{1}{2} & \frac{1}{3} \\
\frac{1}{2} & \frac{1}{3} & \frac{1}{4} \\
\frac{1}{3} & \frac{1}{4} & \frac{1}{5}
\end{array}\right] .
$$

7. Give an example of an $n \times n$ matrix which has all its principal minors positive, but which is not a positive matrix.

8. Does $\left(\left(x_{1}, x_{2}\right) !\left(y_{1}, y_{2}\right)\right)=x_{1} \bar{y}_{1}+2 x_{2} \bar{y}_{1}+2 x_{1} \bar{y}_{2}+x_{2} \bar{y}_{2}$ define an inner product on $C^{2}$ ?

9. Prove that every entry on the main diagonal of a positive matrix is positive.

10. Let $V$ be a finite-dimensional inner product space. If $T$ and $U$ are linear operators on $V$, we write $T<U$ if $U-T$ is a positive operator. Prove the following:

(a) $T<U$ and $U<T$ is impossible.

(b) If $T<U$ and $U<S$, then $T<S$.

(c) If $T<U$ and $0<S$, it need not be that $S T<S U$.

11. Let $V$ be a finite-dimensional inner product space and $E$ the orthogonal projection of $V$ onto some subspace.

(a) Prove that, for any positive number $c$, the operator $c I+E$ is positive.

(b) Express in terms of $E$ a self-adjoint linear operator $T$ such that $T^{2}=I+E$.

12. Let $n$ be a positive integer and $A$ the $n \times n$ matrix

$$
A=\left[\begin{array}{ccccc}
1 & \frac{1}{2} & \frac{1}{3} & \cdots & \frac{1}{n} \\
\frac{1}{2} & \frac{1}{3} & \frac{1}{4} & \cdots & \frac{1}{n+1} \\
\vdots & \vdots & \vdots & & \vdots \\
\frac{1}{n} & \frac{1}{n+1} & \frac{1}{n+2} & \cdots & \frac{1}{2 n-1}
\end{array}\right]
$$

Prove that $A$ is positive. 13. Let $A$ be a self-adjoint $n \times n$ matrix. Prove that there is a real number $c$ such that the matrix $c I+A$ is positive.

14. Prove that the product of two positive linear operators is positive if and only if they commute.

15. Let $S$ and $T$ be positive operators. Prove that every characteristic value of $S T$ is positive.

\subsection{More on Forms}

This section contains two results which give more detailed information about (sesqui-linear) forms.

Theorem 7. Let $\mathrm{f}$ be a form on a real or complex vector space $\mathrm{V}$ and $\left\{\alpha_{1}, \ldots, \alpha_{\mathrm{r}}\right\}$ a basis for the finite-dimensional subspace $\mathrm{W}$ of $\mathrm{V}$. Let $\mathrm{M}$ be the $\mathrm{r} \times \mathrm{r}$ matrix with entries

$$
\mathrm{M}_{\mathrm{jk}}=\mathrm{f}\left(\alpha_{\mathrm{k}}, \alpha_{\mathrm{j}}\right)
$$

and $\mathrm{W}^{\prime}$ the set of all vectors $\beta$ in $\mathrm{V}$ such that $\mathrm{f}(\alpha, \beta)=0$ for all $\alpha$ in $\mathrm{W}$. Then $\mathrm{W}^{\prime}$ is a subspace of $\mathrm{V}$, and $\mathrm{W} \cap \mathrm{W}^{\prime}=\{0\}$ if and only if $\mathrm{M}$ is invertible. When this is the case, $\mathrm{V}=\mathrm{W}+\mathrm{W}^{\prime}$.

Proof. If $\beta$ and $\gamma$ are vectors in $W^{\prime}$ and $c$ is a scalar, then for every $\alpha$ in $W$

$$
\begin{aligned}
f(\alpha, c \beta+\gamma) & =\bar{c} f(\alpha, \beta)+f(\alpha, \gamma) \\
& =0 .
\end{aligned}
$$

Hence, $W^{\prime}$ is a subspace of $V$.

Now suppose $\alpha=\sum_{k=1}^{r} x_{k} \alpha_{k}$ and that $\beta=\sum_{j=1}^{r} y_{j} \alpha_{j}$. Then

$$
\begin{aligned}
f(\alpha, \beta) & =\sum_{j, k} \bar{y}_{j} M_{j k} x_{k} \\
& =\sum_{k}\left(\sum_{j} \bar{y}_{j} M_{j k}\right) x_{k} .
\end{aligned}
$$

It follows from this that $W \cap W^{\prime} \neq\{0\}$ if and only if the homogeneous system

$$
\sum_{j=1}^{r} \bar{y}_{j} M_{j k}=0, \quad 1 \leq k \leq r
$$

has a non-trivial solution $\left(y_{1}, \ldots, y_{r}\right)$. Hence $W \cap W^{\prime}=\{0\}$ if and only if $M^{*}$ is invertible. But the invertibility of $M^{*}$ is equivalent to the invertibility of $M$.

Suppose that $M$ is invertible and let

$$
A=\left(M^{*}\right)^{-1}=\left(M^{-1}\right)^{*} .
$$

Define $g_{j}$ on $V$ by the equation

Then

$$
g_{j}(\beta)=\sum_{k=1}^{r} A_{j k} \overline{f\left(\alpha_{k}, \beta\right)} .
$$

$$
\begin{aligned}
g_{j}(c \beta+\gamma) & =\sum_{k} A_{j k} \overline{f\left(\alpha_{k}, c \beta+\gamma\right)} \\
& =c \sum_{k} A_{j k} f\left(\alpha_{k}, \beta\right)+\sum_{k} A_{j k} f\left(\alpha_{k}, \gamma\right) \\
& =c g_{j}(\beta)+g_{j}(\gamma) .
\end{aligned}
$$

Hence, each $g_{j}$ is a linear function on $V$. Thus we may define a linear operator $E$ on $V$ by setting

Since

$$
E \beta=\sum_{j=1}^{r} g_{j}(\beta) \alpha_{j} .
$$

$$
\begin{aligned}
g_{j}\left(\alpha_{n}\right) & =\sum_{k} A_{j k} \overline{f\left(\alpha_{k}, \alpha_{n}\right)} \\
& =\sum_{k} A_{j k}\left(M^{*}\right)_{k n} \\
& =\delta_{j n}
\end{aligned}
$$

it follows that $E\left(\alpha_{n}\right)=\alpha_{n}$ for $1 \leq n \leq r$. This implies $E \alpha=\alpha$ for every $\alpha$ in $W$. Therefore, $E$ maps $V$ onto $W$ and $E^{2}=E$. If $\beta$ is an arbitrary vector in $V$, then

$$
\begin{aligned}
f\left(\alpha_{n}, E \beta\right) & =f\left(\alpha_{n}, \sum_{j} g_{j}(\beta) \alpha_{j}\right) \\
& =\sum_{j} \overline{g_{j}(\beta)} f\left(\alpha_{n}, \alpha_{j}\right) \\
& =\sum_{j}\left(\sum_{k} \bar{A}_{j k} f\left(\alpha_{k}, \beta\right)\right) f\left(\alpha_{n}, \alpha_{j}\right) .
\end{aligned}
$$

Since $A^{*}=M^{-1}$, it follows that

$$
\begin{aligned}
f\left(\alpha_{n}, E \beta\right) & =\sum_{k}\left(\sum_{j}\left(M^{-1}\right)_{k j} M_{j n}\right) f\left(\alpha_{k}, \beta\right) \\
& =\sum_{k} \delta_{k n} f\left(\alpha_{k}, \beta\right) \\
& =f\left(\alpha_{n}, \beta\right) .
\end{aligned}
$$

This implies $f(\alpha, E \beta)=f(\alpha, \beta)$ for every $\alpha$ in $W$. Hence

$$
f(\alpha, \beta-E \beta)=0
$$

for all $\alpha$ in $W$ and $\beta$ in $V$. Thus $I-E$ maps $V$ into $W^{\prime}$. The equation

$$
\beta=E \beta+(I-E) \beta
$$

shows that $V=W+W^{\prime}$. One final point should be mentioned. Since $W \cap W^{\prime}=\{0\}$, every vector in $V$ is uniquely the sum of a vector in $W$ and a vector in $W^{\prime}$. If $\beta$ is in $W^{\prime}$, it follows that $E \beta=0$. Hence $I-E$ maps $V$ onto $W^{\prime}$.

The projection $E$ constructed in the proof may be characterized as follows: $E \beta=\alpha$ if and only if $\alpha$ is in $W$ and $\beta-\alpha$ belongs to $W^{\prime}$. Thus $E$ is independent of the basis of $W$ that was used in its construction. Hence we may refer to $E$ as the projection of $\mathrm{V}$ on $\mathrm{W}$ that is determined by the direct sum decomposition

$$
V=W \oplus W^{\prime} .
$$

Note that $E$ is an orthogonal projection if and only if $W^{\prime}=W^{\perp}$.

Theorem 8. Let $\mathrm{f}$ be a form on a real or complex vector space $\mathrm{V}$ and $\mathrm{A}$ the matrix of $\mathrm{f}$ in the ordered basis $\left\{\alpha_{1}, \ldots, \alpha_{n}\right\}$ of $\mathrm{V}$. Suppose the principal minors of $\mathrm{A}$ are all different from 0. Then there is a unique upper-triangular matrix $\mathrm{P}$ with $\mathrm{P}_{\mathrm{kk}}=1(1 \leq \mathrm{k} \leq \mathrm{n})$ such that

$$
\mathrm{P}^{*} \mathrm{AP}
$$

is upper-triangular.

Proof. Since $\Delta_{k}\left(A^{*}\right)=\overline{\Delta_{k}(A)}(1 \leq k \leq n)$, the principal minors of $A^{*}$ are all different from 0 . Hence, by the lemma used in the proof of Theorem 6, there exists an upper-triangular matrix $P$ with $P_{k k}=1$ such that $A^{*} P$ is lower-triangular. Therefore, $P^{*} A=\left(A^{*} P\right)^{*}$ is upper-triangular. Since the product of two upper-triangular matrices is again uppertriangular, it follows that $P^{*} A P$ is upper-triangular. This shows the existence but not the uniqueness of $P$. However, there is another more geometric argument which may be used to prove both the existence and uniqueness of $P$.

Let $W_{k}$ be the subspace spanned by $\alpha_{1}, \ldots, \alpha_{k}$ and $W_{k}^{\prime}$ the set of all $\beta$ in $V$ such that $f(\alpha, \beta)=0$ for every $\alpha$ in $W_{k}$. Since $\Delta_{k}(A) \neq 0$, the $k \times k$ matrix $M$ with entries

$$
M_{i j}=f\left(\alpha_{j}, \alpha_{i}\right)=A_{i j}
$$

$(1 \leq i, j \leq k)$ is invertible. By Theorem 7

$$
V=W_{k} \oplus W_{k}^{\prime} \text {. }
$$

Let $E_{k}$ be the projection of $V$ on $W_{k}$ which is determined by this decomposition, and $\operatorname{set} E_{0}=0$. Let

$$
\beta_{k}=\alpha_{k}-E_{k-1} \alpha_{k}, \quad(1 \leq k \leq n) .
$$

Then $\beta_{1}=\alpha_{1}$, and $E_{k-1} \alpha_{k}$ belongs to $W_{k-1}$ for $k>1$. Thus when $k>1$, there exist unique scalars $P_{j k}$ such that

$$
E_{k-1} \alpha_{k}=-\sum_{j=1}^{k-1} P_{j k} \alpha_{j} .
$$

Setting $P_{k k}=1$ and $P_{j k}=0$ for $j>k$, we then have an $n \times n$ uppertriangular matrix $P$ with $P_{k \kappa}=1$ and

$$
\beta_{k}=\sum_{j=1}^{k} P_{j k} \alpha_{j}
$$

for $k=1, \ldots, n$. Suppose $1 \leq i<k$. Then $\beta_{i}$ is in $W_{i}$ and $W_{i} \subset W_{k-1}$. Since $\beta_{k}$ belongs to $W_{k-1}^{\prime}$, it follows that $f\left(\beta_{i}, \beta_{k}\right)=0$. Let $B$ denote the matrix of $f$ in the ordered basis $\left\{\beta_{1}, \ldots, \beta_{n}\right\}$. Then

$$
B_{k i}=f\left(\beta_{i}, \beta_{k}\right)
$$

so $B_{k i}=0$ when $k>i$. Thus $B$ is upper-triangular. On the other hand,

$$
B=P^{*} A P \text {. }
$$

Conversely, suppose $P$ is an upper-triangular matrix with $P_{k k}=1$ such that $P^{*} A P$ is upper-triangular. Set

$$
\beta_{k}=\sum_{j} P_{j k} \alpha_{j}, \quad(1 \leq k \leq n) .
$$

Then $\left\{\beta_{1}, \ldots, \beta_{k}\right\}$ is evidently a basis for $W_{k}$. Suppose $k>1$. Then $\left\{\beta_{1}, \ldots, \beta_{k-1}\right\}$ is a basis for $W_{k-1}$, and since $f\left(\beta_{i}, \beta_{k}\right)=0$ when $i<k$, we see that $\beta_{k}$ is a vector in $W_{k-1}^{\prime}$. The equation defining $\beta_{k}$ implies

$$
\alpha_{k}=-\left(\sum_{j=1}^{k-1} P_{j k} \alpha_{j}\right)+\beta_{k} .
$$

Now $\sum_{j=1}^{k-1} P_{j k} \alpha_{j}$ belongs to $W_{k-1}$ and $\beta_{k}$ is in $W_{k-1}^{\prime}$. Therefore, $P_{1 k}, \ldots, P_{k-1 k}$ are the unique scalars such that

$$
E_{k-1} \alpha_{k}=-\sum_{j=1}^{k-1} P_{j k} \alpha_{j}
$$

so that $P$ is the matrix constructed earlier.

\subsection{Spectral Theory}

In this section, we pursue the implications of Theorems 18 and 22 of Chapter 8 concerning the diagonalization of self-adjoint and normal operators.

Theorem 9 (Spectral Theorem). Let $\mathrm{T}$ be a normal operator on a finite-dimensional complex inner product space $\mathrm{V}$ or a self-adjoint operator on a finite-dimensional real inner product space $\mathrm{V}$. Let $\mathbf{c}_{1}, \ldots, \mathbf{c}_{\mathbf{k}}$ be the distinct characteristic values of $\mathrm{T}$. Let $\mathrm{W}_{\mathrm{j}}$ be the characteristic space associated with $\mathbf{c}_{\mathrm{j}}$ and $\mathrm{E}_{\mathrm{j}}$ the orthogonal projection of $\mathrm{V}$ on $\mathrm{W}_{\mathrm{j}}$. Then $\mathrm{W}_{\mathrm{j}}$ is orthogonal to $\mathrm{W}_{\mathrm{i}}$ when $\mathrm{i} \neq \mathrm{j}, \mathrm{V}$ is the direct sum of $\mathrm{W}_{1}, \ldots, \mathrm{W}_{\mathrm{k}}$, and

$$
\mathrm{T}=\mathrm{c}_{1} \mathrm{E}_{1}+\cdots+c_{\mathrm{k}} \mathrm{E}_{\mathrm{k}} .
$$

Proof. Let $\alpha$ be a vector in $W_{j}, \beta$ a vector in $W_{i}$, and suppose $i \neq j$. Then $c_{j}(\alpha \mid \boldsymbol{\beta})=(T \alpha \mid \beta)=\left(\alpha \mid T^{*} \beta\right)=\left(\alpha \mid \bar{c}_{i} \beta\right)$. Hence $\left(c_{j}-c_{i}\right)(\alpha \mid \beta)=$ 0 , and since $c_{j}-c_{i} \neq 0$, it follows that $(\alpha \mid \beta)=0$. Thus $W_{j}$ is orthogonal to $W_{i}$ when $i \neq j$. From the fact that $V$ has an orthonormal basis consisting of characteristic vectors (cf. Theorems 18 and 22 of Chapter 8), it follows that $V=W_{1}+\cdots+W_{k}$. If $\alpha_{j}$ belongs to $V_{j}(1 \leq j \leq k)$ and $\alpha_{1}+\cdots+\alpha_{k}=0$, then

$$
\begin{aligned}
0 & =\left(\alpha_{i} \mid \sum_{j} \alpha_{j}\right)=\sum_{j}\left(\alpha_{i} \mid \alpha_{j}\right) \\
& =\left\|\alpha_{i}\right\|^{2}
\end{aligned}
$$

for every $i$, so that $V$ is the direct sum of $W_{1}, \ldots, W_{k}$. Therefore $E_{1}+$ $\cdots+E_{k}=I$ and

$$
\begin{aligned}
T & =T E_{1}+\cdots+T E_{k} \\
& =c_{1} E_{1}+\cdots+c_{k} E_{k} .
\end{aligned}
$$

The decomposition (9-11) is called the spectral resolution of $T$. This terminology arose in part from physical applications which caused the spectrum of a linear operator on a finite-dimensional vector space to be defined as the set of characteristic values for the operator. It is important to note that the orthogonal projections $E_{1}, \ldots, E_{k}$ are canonically associated with $T$; in fact, they are polynomials in $T$.

Corollary. If $\mathrm{e}_{\mathrm{j}}=\mathrm{II}_{i \neq j}\left(\frac{\mathrm{x}-\mathrm{c}_{\mathrm{i}}}{\mathrm{c}_{\mathrm{j}}-\mathrm{c}_{\mathrm{i}}}\right)$, then $\mathrm{E}_{\mathrm{j}}=\mathrm{e}_{\mathrm{j}}(\mathrm{T})$ for $1 \leq \mathrm{j} \leq \mathrm{k}$.

$$
\text { Proof. Since } \begin{aligned}
E_{i} E_{j} & =0 \text { when } i \neq j, \text { it follows that } \\
T^{2} & =c_{1}^{2} E_{1}+\cdots+c_{k}^{2} E_{k}
\end{aligned}
$$

and by an easy induction argument that

$$
T^{n}=c_{1}^{n} E_{1}+\cdots+c_{k}^{n} E_{k}
$$

for every integer $n \geq 0$. For an arbitrary polynomial

we have

$$
f=\sum_{n=0}^{r} a_{n} x^{n}
$$

$$
\begin{aligned}
f(T) & =\sum_{n=0}^{r} a_{n} T^{n} \\
& =\sum_{n=0}^{r} a_{n} \sum_{j=1}^{k} c_{j}^{n} E_{j} \\
& =\sum_{j=1}^{k}\left(\sum_{n=0}^{r} a_{n} c_{j}^{n}\right) E_{j} \\
& =\sum_{j=1}^{k} f\left(c_{j}\right) E_{j} .
\end{aligned}
$$

Since $e_{j}\left(c_{m}\right)=\delta_{j m}$, it follows that $e_{j}(T)=E_{j}$. Because $E_{1}, \ldots, E_{k}$ are canonically associated with $T$ and

$$
I=E_{1}+\cdots+E_{k}
$$

the family of projections $\left\{E_{1}, \ldots, E_{k}\right\}$ is called the resolution of the identity defined by $T$.

There is a comment that should be made about the proof of the spectral theorem. We derived the theorem using Theorems 18 and 22 of Chapter 8 on the diagonalization of self-adjoint and normal operators. There is another, more algebraic, proof in which it must first be shown that the minimal polynomial of a normal operator is a product of distinct prime factors. Then one proceeds as in the proof of the primary decomposition theorem (Theorem 12, Chapter 6). We shall give such a proof in the next section.

In various applications it is necessary to know whether one may compute certain functions of operators or matrices, e.g., square roots. This may be done rather simply for diagonalizable normal operators.

Definition. Let T be a diagonalizable normal operator on a finitedimensional inner product space and

$$
\mathrm{T}=\sum_{j=1}^{k} \mathrm{c}_{j} \mathrm{E}_{j}
$$

its spectral resolution. Suppose $\mathrm{f}$ is a function whose domain includes the spectrum of $\mathrm{T}$ that has values in the field of scalars. Then the linear operator $\mathrm{f}(\mathrm{T})$ is defined by the equation

$$
f(T)=\sum_{j=1}^{k} f\left(c_{j}\right) E_{j}
$$

Theorem 10. Let $\mathrm{T}$ be a diagonalizable normal operator with spectrum $\mathrm{S}$ on a finite-dimensional inner product space V. Suppose $\mathrm{f}$ is a function whose domain contains $\mathrm{S}$ that has values in the field of scalars. Then $\mathrm{f}(\mathrm{T})$ is a diagonalizable normal operator with spectrum $\mathrm{f}(\mathrm{S})$. If $\mathrm{U}$ is a unitary map of $\mathrm{V}$ onto $\mathrm{V}^{\prime}$ and $\mathrm{T}^{\prime}=\mathrm{UTU}^{-1}$, then $\mathrm{S}$ is the spectrum of $\mathrm{T}^{\prime}$ and

$$
\mathrm{f}\left(\mathrm{T}^{\prime}\right)=\mathrm{Uf}(\mathrm{T}) \mathrm{U}^{-1} .
$$

Proof. The normality of $f(T)$ follows by a simple computation from (9-12) and the fact that

$$
f(T)^{*}=\sum_{j} \overline{f\left(c_{j}\right)} E_{j}
$$

Moreover, it is clear that for every $\alpha$ in $E_{j}(V)$

$$
f(T) \alpha=f\left(c_{j}\right) \alpha .
$$

Thus, the $\operatorname{set} f(S)$ of all $f(c)$ with $c$ in $S$ is contained in the spectrum of $f(T)$. Conversely, suppose $\alpha \neq 0$ and that

$$
f(\boldsymbol{T}) \alpha=b \alpha .
$$

Then $\alpha=\sum_{j} E_{j} \alpha$ and

Hence,

$$
\begin{aligned}
f(T) \alpha & =\sum_{j} f\left(T^{\prime}\right) E_{j} \alpha \\
& =\sum_{j} f\left(c_{j}\right) E_{j} \alpha \\
& =\sum_{j} b E_{j} \alpha .
\end{aligned}
$$

$$
\begin{aligned}
\left\|\sum_{j}\left(f\left(c_{j}\right)-b\right) E_{j} \alpha\right\|^{2} & =\sum_{j}\left|f\left(c_{j}\right)-b\right|^{2}|| E_{j} \alpha \|^{2} \\
& =0 .
\end{aligned}
$$

Therefore, $f\left(c_{j}\right)=b$ or $E_{j} \alpha=0$. By assumption, $\alpha \neq 0$, so there exists an index $i$ such that $E_{i} \alpha \neq 0$. It follows that $f\left(c_{i}\right)=b$ and hence that $f(S)$ is the spectrum of $f(T)$. Suppose, in fact, that

$$
f(S)=\left\{b_{1}, \ldots, b_{r}\right\}
$$

where $b_{m} \neq b_{n}$ when $m \neq n$. Let $X_{m}$ be the set of indices $i$ such that $1 \leq i \leq k$ and $f\left(c_{i}\right)=b_{m}$. Let $P_{m}=\sum_{i} E_{i}$, the sum being extended over the indices $i$ in $X_{m}$. Then $P_{m}$ is the orthogonal projection of $V$ on the subspace of characteristic vectors belonging to the characteristic value $b_{m}$ of $f(\boldsymbol{T})$, and

$$
f(T)=\sum_{m=1}^{r} b_{m} P_{m}
$$

is the spectral resolution of $f(T)$.

Now suppose $U$ is a unitary transformation of $V$ onto $V^{\prime}$ and that $T^{\prime}=U T U^{-1}$. Then the equation

holds if and only if

$$
T \alpha=c \alpha
$$

$$
T^{\prime} U \alpha=c U \alpha .
$$

Thus $S$ is the spectrum of $T^{\prime}$, and $U$ maps each characteristic subspace for $T$ onto the corresponding subspace for $T^{\prime}$. In fact, using (9-12), we see that

$$
T^{\prime}=\sum_{j} c_{j} E_{j}^{\prime}, \quad E_{j}^{\prime}=U E_{j} U^{-1}
$$

is the spectral resolution of $7^{\prime}$. Hence

$$
\begin{aligned}
f\left(T^{\prime}\right) & =\sum_{j} f\left(c_{j}\right) E_{j}^{\prime} \\
& =\sum_{j} f\left(c_{j}\right) U E_{j} U^{-1} \\
& =U\left(\sum_{j} f\left(c_{j}\right) E_{j}\right) U^{-1} \\
& =U f(T) U^{-1} .
\end{aligned}
$$

In thinking about the preceding discussion, it is important for one to keep in mind that the spectrum of the normal operator $T$ is the set

$$
S=\left\{c_{1}, \ldots, c_{k}\right\}
$$

of distinct characteristic values. When $T$ is represented by a diagonal matrix in a basis of characteristic vectors, it is necessary to repeat each value $c_{j}$ as many times as the dimension of the corresponding space of characteristic vectors. This is the reason for the change of notation in the following result.

Corollary. With the assumptions of Theorem 10, suppose that $\mathrm{T}$ is represented in the ordered basis $B=\left\{\alpha_{1}, \ldots, \alpha_{n}\right\}$ by the diagonal matrix $\mathrm{D}$ with entries $\mathrm{d}_{1}, \ldots, \mathrm{d}_{\mathrm{n}}$. Then, in the basis $\Theta, \mathrm{f}(\mathrm{T})$ is represented by the diagonal matrix $\mathrm{f}(\mathrm{D})$ with entries $\mathrm{f}\left(\mathrm{d}_{1}\right), \ldots, \mathrm{f}\left(\mathrm{d}_{\mathrm{n}}\right)$. If $\Omega^{\prime}=\left\{\alpha_{1}^{\prime}, \ldots, \alpha_{\mathrm{n}}^{\prime}\right\}$ is any other ordered basis and $\mathrm{P}$ the matrix such that

$$
\alpha_{\mathrm{j}}^{\prime}=\sum_{i} \mathrm{P}_{\mathrm{ij}} \alpha_{\mathrm{i}}
$$

then $\mathrm{P}^{-1} \mathrm{f}(\mathrm{D}) \mathrm{P}$ is the matrix of $\mathrm{f}(\mathrm{T})$ in the basis $\mathbb{B}^{\prime}$.

Proof. For each index $i$, there is a unique $j$ such that $1 \leq j \leq k$, $\alpha_{i}$ belongs to $E_{j}(V)$, and $d_{i}=c_{j}$. Hence $f(T) \alpha_{i}=f\left(d_{i}\right) \alpha_{i}$ for every $i$, and

$$
\begin{aligned}
f(T) \alpha_{j}^{\prime} & =\sum_{i} P_{i j} f(T) \alpha_{i} \\
& =\sum_{i} d_{i} P_{i j} \alpha_{i} \\
& =\sum_{i}(D P)_{i j} \alpha_{i} \\
& =\sum_{i}(D P)_{i j} \sum_{k} P_{k i}^{-1} \alpha_{k}^{\prime} \\
& =\sum_{k}\left(P^{-1} D P\right)_{k j} \alpha_{k}^{\prime} .
\end{aligned}
$$

It follows from this result that one may form certain functions of a normal matrix. For suppose $A$ is a normal matrix. Then there is an invertible matrix $P$, in fact a unitary $P$, such that $P A P^{-1}$ is a diagonal matrix, say $D$ with entries $d_{1}, \ldots, d_{n}$. Let $f$ be a complex-valued function which can be applied to $d_{1}, \ldots, d_{n}$, and let $f(D)$ be the diagonal matrix with entries $f\left(d_{1}\right), \ldots, f\left(d_{n}\right)$. Then $P^{-1} f(D) P$ is independent of $D$ and just a function of $A$ in the following sense. If $Q$ is another invertible matrix such that $Q A Q^{-1}$ is a diagonal matrix $D^{\prime}$, then $f$ may be applied to the diagonal entries of $D^{\prime}$ and

$$
P^{-1} f(D) P=Q^{-1} f\left(D^{\prime}\right) Q .
$$

Definition. Under the above conditions, $\mathrm{f}(\mathrm{A})$ is defined as $\mathrm{P}^{-1} \mathrm{f}(\mathrm{D}) \mathrm{P}$.

The matrix $f(\Lambda)$ may also be characterized in a different way. In doing this, we state without proof some of the results on normal matrices that one obtains by formulating the matrix analogues of the preceding theorems.

Theorem 11. Let $\mathrm{A}$ be a normal matrix and $\mathrm{c}_{1}, \ldots, \mathrm{c}_{\mathrm{k}}$ the distinct complex roots of det (xI - A). Let

$$
\mathrm{e}_{i}=\prod_{j \neq i}\left(\frac{\mathrm{x}-\mathrm{c}_{j}}{\mathrm{c}_{\mathrm{i}}-\mathrm{c}_{\mathrm{j}}}\right)
$$

and $\mathrm{E}_{\mathrm{i}}=\mathrm{e}_{\mathrm{i}}(\mathrm{A})(1 \leq \mathrm{i} \leq \mathrm{k})$. Then $\mathrm{E}_{\mathrm{i}} \mathrm{E}_{\mathrm{j}}=0$ when $\mathrm{i} \neq \mathrm{j}, \mathrm{E}_{1}^{2}=\mathrm{E}_{\mathrm{i}}, \mathrm{E}_{\mathrm{i}}^{*}=\mathrm{E}_{\mathrm{i}}$, and

$$
\mathrm{I}=\mathrm{E}_{1}+\cdots+\mathrm{E}_{\mathrm{k}} .
$$

If $\mathrm{f}$ is a complex-valued function whose domain includes $\mathrm{c}_{1}, \ldots, \mathrm{c}_{\mathrm{k}}$, then

$$
\mathrm{f}(\mathrm{A})=\mathrm{f}\left(\mathrm{c}_{1}\right) \mathrm{E}_{1}+\cdots+\mathrm{f}\left(\mathrm{c}_{\mathrm{k}}\right) \mathrm{E}_{\mathrm{k}} ;
$$

in particular, $\mathrm{A}=\mathrm{c}_{1} \mathrm{E}_{1}+\cdots+\mathrm{c}_{\mathrm{k}} \mathrm{E}_{\mathbf{k}}$.

We recall that an operator on an inner product space $V$ is non-negative if $T$ is self-adjoint and $(T \alpha \mid \alpha) \geq 0$ for every $\alpha$ in $V$.

Theorem 12. Let $\mathrm{T}$ be a diagonalizable normal operator on a finitedimensional inner product space $\mathrm{V}$. Then $\mathrm{T}$ is self-adjoint, non-negative, or unitary according as each characteristic value of $\mathrm{T}$ is real, non-negative, or of absolute value 1.

Proof. Suppose $T$ has the spectral resolution $T=c_{1} E_{1}+\cdots+$ $c_{k} E_{k}$, then $T^{*}=\bar{c}_{1} E_{1}+\cdots+\bar{c}_{k} E_{k}$. To say $T$ is self-adjoint is to say $T=T^{*}$, or

$$
\left(c_{1}-\bar{c}_{1}\right) E_{1}+\cdots+\left(c_{k}-\bar{c}_{k}\right) E_{k}=0 .
$$

Using the fact that $E_{i} E_{j}=0$ for $i \neq j$, and the fact that no $E_{j}$ is the zero operator, we see that $T$ is self-adjoint if and only if $c_{j}=\vec{c}_{j}, j=1, \ldots, k$. To distinguish the normal operators which are non-negative, let us look at

$$
\begin{aligned}
(T \alpha \mid \alpha) & =\left(\sum_{j=1}^{k} c_{j} E_{j} \alpha \mid \sum_{i=1}^{k} E_{i} \alpha\right) \\
& =\sum_{i} \sum_{j} c_{j}\left(E_{j} \alpha \mid E_{i} \alpha\right) \\
& =\sum_{j} c_{j}\left\|E_{j} \alpha\right\|^{2} .
\end{aligned}
$$

We have used the fact that $\left(E_{j} \alpha \mid E_{i} \alpha\right)=0$ for $i \neq j$. From this it is clear that the condition $(T \alpha \mid \alpha) \geq 0$ is satisfied if and only if $c_{j} \geq 0$ for each $j$. To distinguish the unitary operators, observe that

$$
\begin{aligned}
T^{*} & =c_{1} c_{1} E_{1}+\cdots+c_{k} c_{k} E_{k} \\
& =\left|c_{1}\right|^{2} E_{1}+\cdots+\left|c_{k}\right|^{2} E_{k} .
\end{aligned}
$$

If $T T^{*}=I$, then $I=\left|c_{1}\right|^{2} E_{1}+\cdots+\left|c_{k}\right|^{2} E_{k}$, and operating with $E_{j}$

$$
E_{j}=\left|c_{j}\right|^{2} E_{j} .
$$

Since $E_{j} \neq 0$, we have $\left|c_{j}\right|^{2}=1$ or $\left|c_{j}\right|=1$. Conversely, if $\left|c_{j}\right|^{2}=1$ for each $j$, it is clear that $T T^{*}=I$.

It is important to note that this is a theorem about normal operators. If $T$ is a general linear operator on $V$ which has real characteristic values, it does not follow that $T$ is self-adjoint. The theorem states that if $T$ has real characteristic values, and if $T$ is diagonalizable and normal, then $T$ is self-adjoint. A theorem of this type serves to strengthen the analogy between the adjoint operation and the process of forming the conjugate of a complex number. A complex number $z$ is real or of absolute value 1 according as $z=\bar{z}$, or $\bar{z} z=1$. An operator $T$ is self-adjoint or unitary according as $T=T^{*}$ or $T^{*} T=I$.

We are going to prove two theorems now, which are the analogues of these two statements:

(1) Every non-negative number has a unique non-negative square root.

(2) Every complex number is expressible in the form $r u$, where $r$ is non-negative and $|u|=1$. This is the polar decomposition $z=r e^{i \theta}$ for complex numbers.

Theorem 13. Let $\mathrm{V}$ be a finite-dimensional inner product space and $\mathrm{T}$ a non-negative operator on $\mathrm{V}$. Then $\mathrm{T}$ has a unique non-negative square root, that is, there is one and only one non-negative operator $\mathrm{N}$ on $\mathrm{V}$ such that $\mathrm{N}^{2}=\mathrm{T}$.

Proof. Let $T=c_{1} E_{1}+\cdots+c_{k} E_{k}$ be the spectral resolution of $T$. By Theorem 12 , each $c_{j} \geq 0$. If $c$ is any non-negative real number, let $\sqrt{c}$ denote the non-negative square root of $c$. Then according to Theorem 11 and (9-12) $N=\sqrt{T}$ is a well-defined diagonalizable normal operator on $V$. It is non-negative by Theorem 12 , and, by an obvious computation, $N^{2}=T$.

Now let $P$ be a non-negative operator on $V$ such that $P^{2}=T$. We shall prove that $P=N$. Let

$$
P=d_{1} F_{1}+\cdots+d_{r} F_{r}
$$

be the spectral resolution of $P$. Then $d_{j} \geq 0$ for each $j$, since $P$ is nonnegative. From $P^{2}=T$ we have

$$
T=d_{1}^{2} F_{1}+\cdots+d_{r}^{2} F_{r} .
$$

Now $F_{1}, \ldots, F_{r}$ satisfy the conditions $I=F_{1}+\cdots+F_{r}, F_{i} F_{j}=0$ for $i \neq j$, and no $F_{j}$ is 0 . The numbers $d_{1}^{2}, \ldots, d_{r}^{2}$ are distinct, because distinct non-negative numbers have distinct squares. By the uniqueness of the spectral resolution of $T$, we must have $r=k$, and (perhaps reordering) $F_{j}=E_{j}, d_{j}^{2}=c_{j}$. Thus $P=N$. Theorem 14. Let $\mathrm{V}$ be a finite-dimensional inner product space and let $\mathrm{T}$ be any linear operator on $\mathrm{V}$. Then there exist a unitary operator $\mathrm{U}$ on $\mathrm{V}$ and a non-negative operator $\mathrm{N}$ on $\mathrm{V}$ such that $\mathrm{T}=\mathrm{UN}$. The non-negative operator $\mathrm{N}$ is unique. If $\mathrm{T}$ is invertible, the operator $\mathrm{U}$ is also unique.

Proof. Suppose we have $T=U N$, where $U$ is unitary and $N$ is non-negative. Then $T^{*}=(U N)^{*}=N^{*} U^{*}=N U^{*}$. Thus $T^{*} T=$ $N U^{*} U N=N^{2}$. This shows that $N$ is uniquely determined as the nonnegative square root of the non-negative operator $T^{*} T$.

So, to begin the proof of the existence of $U$ and $N$, we use Theorem 13 to define $N$ as the unique non-negative square root of $T^{*} T$. If $T$ is invertible, then so is $N$ because

$$
(N \alpha \mid N \alpha)=\left(N^{2} \alpha \mid \alpha\right)=\left(T^{*} T \alpha \mid \alpha\right)=(T \alpha \mid T \alpha) .
$$

In this case, we define $U=T N^{-1}$ and prove that $U$ is unitary. Now $U^{*}=\left(T N^{-1}\right)^{*}=\left(N^{-1}\right)^{*} T^{*}=\left(N^{*}\right)^{-1} T^{*}=N^{-1} T^{*}$. Thus

$$
\begin{aligned}
U U^{*} & =T N^{-1} N^{-1} T^{*} \\
& =T\left(N^{-1}\right)^{2} T^{*} \\
& =T\left(N^{2}\right)^{-1} T^{*} \\
& =T\left(T^{*} T\right)^{-1} T^{*} \\
& =T T^{-1}\left(T^{*}\right)^{-1} T^{*} \\
& =I
\end{aligned}
$$

and $U$ is unitary.

If $T$ is not invertible, we shall have to do a bit more work to define $U$. We first define $U$ on the range of $N$. Let $\alpha$ be a vector in the range of $N$, say $\alpha=N \beta$. We define $U \alpha=T \beta$, motivated by the fact that we want $U N \beta=T \beta$. We must verify that $U$ is well-defined on the range of $N$; in other words, if $N \beta^{\prime}=N \beta$ then $T \beta^{\prime}=T \beta$. We verified above that $\|N \gamma\|^{2}=\|T \gamma\|^{2}$ for every $\gamma$ in $V$. Thus, with $\gamma=\beta-\beta^{\prime}$, we see that $N\left(\beta-\beta^{\prime}\right)=0$ if and only if $T\left(\beta-\beta^{\prime}\right)=0$. So $U$ is well-defined on the range of $N$ and is clearly linear where defined. Now if $W$ is the range of $N$, we are going to define $U$ on $W^{\perp \perp}$. To do this, we need the following observation. Since $T$ and $N$ have the same null space, their ranges have the same dimension. Thus $W^{\llcorner\perp}$ has the same dimension as the orthogonal complement of the range of $T$. Therefore, there exists an (inner product space) isomorphism $U_{0}$ of $W^{\perp}$ onto $T(V)^{\perp}$. Now we have defined $U$ on $W$, and we define $U$ on $W^{\cdot L}$ to be $U_{0}$.

Let us repeat the definition of $U$. Since $V=W \oplus W \cdot$, each $\alpha$ in $V$ is uniquely expressible in the form $\alpha=N \beta+\gamma$, where $N \beta$ is in the range $W$ of $N$, and $\gamma$ is in $W^{\perp}$. We define

$$
U \alpha=T \beta+U_{0} \gamma .
$$

This $U$ is clearly linear, and we verified above that it is well-defined. Also Sec. $9.5$

$$
\begin{aligned}
(U \alpha \mid U \alpha) & =\left(T \beta+U_{0} \gamma \mid T \beta+U_{0} \gamma\right) \\
& =(T \beta \mid T \beta)+\left(U_{0} \gamma \mid U_{0} \gamma\right) \\
& =(N \beta \mid N \beta)+(\gamma \mid \gamma) \\
& =(\alpha \mid \alpha)
\end{aligned}
$$

and so $U$ is unitary. We also have $U N \beta=T \beta$ for each $\beta$.

We call $T=U N$ a polar decomposition for $T$. We certainly cannot call it the polar decomposition, since $U$ is not unique. Even when $T$ is invertible, so that $U$ is unique, we have the difficulty that $U$ and $N$ may not commute. Indeed, they commute if and only if $T$ is normal. For example, if $T=U N=N U$, with $N$ non-negative and $U$ unitary, then

$$
T^{*}=(N U)(N U)^{*}=N U U^{*} N=N^{2}=T^{*} T .
$$

The general operator $T$ will also have a decomposition $T=N_{1} U_{1}$, with $N_{1}$ non-negative and $U_{1}$ unitary. Here, $N_{1}$ will be the non-negative square root of $T T^{*}$. We can obtain this result by applying the theorem just proved to the operator $T^{*}$, and then taking adjoints.

We turn now to the problem of what can be said about the simultaneous diagonalization of commuting families of normal operators. For this purpose the following terminology is appropriate.

Definitions. Let $\mathcal{F}$ be a family of operators on an inner product space $\mathrm{V}$. A function $\mathrm{r}$ on $\mathfrak{F}$ with values in the field $\mathrm{F}$ of scalars will be called a root of $\mathcal{F}$ if there is a non-zero $\alpha$ in $\mathrm{V}$ such that

$$
\mathrm{T} \alpha=\mathrm{r}(\mathrm{T}) \alpha
$$

for all $\mathrm{T}$ in $\mathscr{F}$. For any function $\mathrm{r}$ from $\mathfrak{F}$ to $\mathrm{F}$, let $\mathrm{V}(\mathrm{r})$ be the set of all $\alpha$ in $\mathrm{V}$ such that $\mathrm{T} \alpha=\mathrm{r}(\mathrm{T}) \alpha$ for every $\mathrm{T}$ in $\mathfrak{F}$.

Then $V(r)$ is a subspace of $V$, and $r$ is a root of $\mathcal{F}$ if and only if $V(r) \neq$ $\{0\}$. Each non-zero $\alpha$ in $V(r)$ is simultaneously a characteristic vector for every $T$ in $\mathscr{F}$.

Theorem 15. Let $\mathcal{F}$ be a commuting family of diagonalizable normal operators on a finite-dimensional inner product space V. Then $\mathfrak{F}$ has only a finite number of roots. If $\mathrm{r}_{1}, \ldots, \mathrm{r}_{\mathrm{k}}$ are the distinct roots of $\mathcal{F}$, then

(i) $\mathrm{V}\left(\mathrm{r}_{\mathrm{i}}\right)$ is orthogonal to $\mathrm{V}\left(\mathrm{r}_{\mathrm{j}}\right)$ when $\mathrm{i} \neq \mathrm{j}$, and

(ii) $\mathrm{V}=\mathrm{V}\left(\mathrm{r}_{1}\right) \oplus \cdots \oplus \mathrm{V}\left(\mathrm{r}_{\mathrm{k}}\right)$.

Proof. Suppose $r$ and $s$ are distinct roots of $F$. Then there is an operator $T$ in $\mathfrak{F}$ such that $r(T) \neq s(T)$. Since characteristic vectors belonging to distinct characteristic values of $T$ are necessarily orthogonal, it follows that $V(r)$ is orthogonal to $V(s)$. Because $V$ is finite-dimensional, this implies $\mathcal{F}$ has at most a finite number of roots. Let $r_{1}, \ldots, r_{k}$ be the roots of $F$. Suppose $\left\{T_{1}, \ldots, T_{m}\right\}$ is a maximal linearly independent subset of $\mathcal{F}$, and let

$$
\left\{E_{i 1}, E_{i 2}, \ldots\right\}
$$

be the resolution of the identity defined by $T_{i}(1 \leq i \leq m)$. Then the projections $E_{i j}$ form a commutative family. For each $E_{i j}$ is a polynomial in $T_{i}$ and $T_{1}, \ldots, T_{m}$ commute with one another. Since

$$
I=\left(\sum_{j_{1}} E_{1 j_{1}}\right)\left(\sum_{j_{2}} E_{2 j_{2}}\right) \cdots\left(\sum_{j_{m}} E_{m j_{m}}\right)
$$

each vector $\alpha$ in $V$ may be written in the form

$$
\alpha=\underset{j_{1}, \ldots, j_{\mu}}{\Sigma} E_{1 j_{1}}^{\prime} E_{2 j_{2}} \cdots E_{m j_{m}} \alpha .
$$

Suppose $j_{1}, \ldots, j_{m}$ are indices for which $\beta=E_{1 j_{1}} E_{2 j_{2}} \cdots E_{m j_{m}} \alpha \neq 0$. Let

$$
\beta_{i}=\left(\prod_{n \neq i} E_{n j_{n}}\right) \alpha .
$$

Then $\beta=E_{i j} \beta_{i}$; hence there is a scalar $c_{i}$ such that

$$
T_{i} \beta=c_{i} \beta, \quad 1 \leq i \leq m .
$$

For each $T$ in $\mathscr{F}$, there exist unique scalars $b_{i}$ such that

$$
T=\sum_{i=1}^{m} b_{i} T_{i} .
$$

Thus

$$
\begin{aligned}
T \beta & =\sum_{i} b_{i} T_{i} \beta \\
& =\left(\sum_{i} b_{i} c_{i}\right) \beta .
\end{aligned}
$$

The function $T \rightarrow \sum_{i} b_{i} c_{i}$ is evidently one of the roots, say $r_{t}$ of $\mathcal{F}$, and $\beta$ lies in $V\left(r_{t}\right)$. Therefore, each non-zero term in (9-13) belongs to one of the spaces $V\left(r_{1}\right), \ldots, V\left(r_{k}\right)$. It follows that $V$ is the orthogonal direct sum of $V\left(r_{1}\right), \ldots, V\left(r_{k}\right)$

Corollary. Under the assumptions of the theorem, let $\mathrm{P}_{\mathrm{j}}$ be the orthogonal projection of $\mathrm{V}$ on $\mathrm{V}\left(\mathrm{r}_{\mathrm{j}}\right),(1 \leq \mathrm{j} \leq \mathrm{k})$. Then $\mathrm{P}_{\mathrm{i}} \mathrm{P}_{\mathrm{j}}=0$ when $\mathrm{i} \neq \mathrm{j}$,

$$
\mathrm{I}=\mathrm{P}_{1}+\cdots+\mathrm{P}_{\mathrm{k}} \text {, }
$$

and every $\mathrm{T}$ in $\mathfrak{F}$ may be written in the form

$$
\mathrm{T}=\sum_{j} \mathrm{r}_{j}(\mathrm{~T}) \mathrm{P}_{\mathrm{j}} \text {. }
$$

Definitions. The family of orthogonal projections $\left\{\mathrm{P}_{1}, \ldots, \mathrm{P}_{\mathrm{k}}\right\}$ is called the resolution of the identity determined by $\mathcal{F}$, and (9-14) is the spectral resolution of $\mathrm{T}$ in terms of this family.

Although the projections $P_{1}, \ldots, P_{k}$ in the preceding corollary are canonically associated with the family $\mathcal{F}$, they are generally not in $\mathfrak{F}$ nor even linear combinations of operators in $\mathcal{F}$; however, we shall show that they may be obtained by forming certain products of polynomials in elements of $\mathcal{F}$.

In the study of any family of linear operators on an inner product space, it is usually profitable to consider the self-adjoint algebra generated by the family.

Definition. A self-adjoint algebra of operators on an inner product space $\mathrm{V}$ is a linear subalgebra of $\mathrm{L}(\mathrm{V}, \mathrm{V})$ which contains the adjoint of each of its members.

An example of a self-adjoint algebra is $L(V, V)$ itself. Since the intersection of any collection of self-adjoint algebras is again a self-adjoint algebra, the following terminology is meaningful.

Definition. If $\mathcal{F}$ is a family of linear operators on a finite-dimensional inner product space, the self-adjoint algebra generated by $\mathcal{F}$ is the smallest self-adjoint algebra which contains $\mathfrak{F}$.

Theorem 16. Let 9: be a commuting family of diagonalizable normal operators on a finite-dimensional inner product space $\mathrm{V}$, and let $Q$ be the selfadjoint algebra generated by $\mathcal{F}$ and the identity operator. Let $\left\{\mathrm{P}_{1}, \ldots, \mathrm{P}_{\mathrm{k}}\right\}$ be the resolution of the identity defined by $\mathfrak{F}$. Then $Q$ is the set of all operators on $\mathrm{V}$ of the form

$$
\mathrm{T}=\sum_{j=1}^{k} \mathbf{c}_{j} \mathbf{P}_{\mathrm{j}}
$$

where $\mathrm{c}_{1}, \ldots, \mathrm{c}_{\mathrm{k}}$ are arbitrary scalars.

Proof. Let $\mathfrak{e}$ denote the set of all operators on $V$ of the form (9-15). Then $\mathcal{C}$ contains the identity operator and the adjoint

$$
T^{*}=\sum_{j} \bar{c}_{j} P_{j}
$$

of each of its members. If $T=\sum_{j} c_{j} P_{j}$ and $U=\sum_{j} d_{j} P_{j}$, then for every scalar $a$

and

$$
a T+U=\sum_{j}\left(a c+d_{j}\right) P_{j}
$$

$$
\begin{aligned}
T U & =\sum_{i, j} c_{i} d_{j} P_{i} P_{j} \\
& =\sum_{j} c_{j} d_{j} P_{j} \\
& =U T .
\end{aligned}
$$

Thus $\mathcal{C}$ is a self-adjoint commutative algebra containing $\mathfrak{F}$ and the identity operator. Therefore $\mathcal{C}$ contains $\mathbb{Q}$. Now let $r_{1}, \ldots, r_{k}$ be all the roots of $\mathscr{F}$. Then for each pair of indices $(i, n)$ with $i \neq n$, there is an operator $T_{i n}$ in $\mathscr{F}$ such that $r_{i}\left(T_{i n}\right) \neq r_{n}\left(T_{i n}\right)$. Let $a_{i n}=r_{i}\left(T_{i n}\right)-r_{n}\left(T_{i n}\right)$ and $b_{i n}=r_{n}\left(T_{i n}\right)$. Then the linear operator

$$
Q_{i}=\prod_{n \neq i} a_{i n}^{-1}\left(T_{i n}-b_{i n} I\right)
$$

is an element of the algebra $Q$. We will show that $Q_{i}=P_{i}(1 \leq i \leq k)$. For this, suppose $j \neq i$ and that $\alpha$ is an arbitrary vector in $V\left(r_{j}\right)$. Then

$$
\begin{aligned}
T_{i j} \alpha & =r_{j}\left(T_{i j}\right) \alpha \\
& =b_{i j} \alpha
\end{aligned}
$$

so that $\left(T_{i j}-b_{i j} I\right) \alpha=0$. Since the factors in $Q_{i}$ all commute, it follows that $Q_{i} \alpha=0$. Hence $Q_{i}$ agrees with $P_{i}$ on $V\left(r_{j}\right)$ whenever $j \neq i$. Now suppose $\alpha$ is a vector in $V\left(r_{i}\right)$. Then $T_{i n} \alpha=r_{i}\left(T_{i n}\right) \alpha$, and

$$
a_{i n}^{-1}\left(T_{i n}-b_{i n} I\right) \alpha=a_{i n}^{-1}\left[r_{i}\left(T_{i n}\right)-r_{n}\left(T_{i n}\right)\right] \alpha=\alpha .
$$

Thus $Q_{i} \alpha=\alpha$ and $Q_{i}$ agrees with $P_{i}$ on $V\left(r_{i}\right)$; therefore, $Q_{i}=P_{i}$ for $i=1, \ldots, k$. From this it follows that $Q=\mathfrak{C}$.

The theorem shows that the algebra $Q$ is commutative and that each element of $Q$ is a diagonalizable normal operator. We show next that $Q$ has a single generator.

Corollary. Under the assumptions of the theorem, there is an operator $\mathrm{T}$ in $\mathbb{Q}$ such that every member of $\mathbb{Q}$ is a polynomial in $\mathrm{T}$.

Proof. Let $T=\sum_{j=1}^{k} t_{j} P_{j}$ where $t_{1}, \ldots, t_{k}$ are distinct scalars. Then for $n=1,2, \ldots$ If

$$
T^{n}=\sum_{j=1}^{k} t_{j}^{n} P_{j}
$$

it follows that

$$
f=\sum_{n=1}^{8} a_{n} x^{n}
$$

$$
\begin{aligned}
f(T) & =\sum_{n=1}^{s} a_{n} T^{n}=\sum_{n=1}^{s} \sum_{j=1}^{k} a_{n} t_{j}^{n} P_{j} \\
& =\sum_{j=1}^{k}\left(\sum_{n=1}^{8} a_{n} t_{j}^{n}\right) P_{j} \\
& =\sum_{j=1}^{k} f\left(t_{j}\right) P_{j} .
\end{aligned}
$$

Given an arbitrary

$$
U=\sum_{j=1}^{k} c_{j} P_{j}
$$

in $Q$, there is a polynomial $f$ such that $f\left(t_{j}\right)=c_{j}(1 \leq j \leq k)$, and for any such $f, U=f(T)$. 

\section{Exercises}

1. Give a reasonable definition of a non-negative $n \times n$ matrix, and then prove that such a matrix has a unique non-negative square root.

2. Let $A$ be an $n \times n$ matrix with complex entries such that $A^{*}=-A$, and let $B=e^{A}$. Show that

(a) $\operatorname{det} B=e^{\operatorname{tr} A}$;

(b) $B^{*}=e^{-A}$;

(c) $B$ is unitary.

3. If $U$ and $T$ are normal operators which commute, prove that $U+T$ and $U T$ are normal.

4. Let $T$ be a linear operator on the finite-dimensional complex inner product space $V$. Prove that the following ten statements about $T$ are equivalent.

(a) $T$ is normal.

(b) $\|T \alpha\|=\left\|T^{*} \alpha\right\|$ for every $\alpha$ in $V$.

(c) $T=T_{1}+i T_{2}$, where $T_{1}$ and $T_{2}$ are self-adjoint and $T_{1} T_{2}=T_{2} T_{1}$.

(d) If $\alpha$ is a vector and $c$ a scalar such that $T \alpha=c \alpha$, then $T^{*} \alpha=\bar{c} \alpha$.

(e) There is an orthonormal basis for $V$ consisting of characteristic vectors for $T$.

(f) There is an orthonormal basis $B$ such that $[T]_{\mathrm{B}}$ is diagonal.

(g) There is a polynomial $g$ with complex coefficients such that $T^{*}=g(T)$.

(h) Every subspace which is invariant under $T$ is also invariant under $T^{*}$.

(i) $T=N U$, where $N$ is non-negative, $U$ is unitary, and $N$ commutes with $U$.

(j) $T=c_{1} E_{1}+\cdots+c_{k} E_{k}$, where $I=E_{1}+\cdots+E_{k}, E_{i} E_{j}=0$ for $i \neq j$, and $E_{i}^{2}=E_{j}=E_{i}^{*}$.

5. Use Exercise 3 to show that any commuting family of normal operators (not necessarily diagonalizable ones) on a finite-dimensional inner product space generates a commutative self-adjoint algebra of normal operators.

6. Let $V$ be a finite-dimensional complex inner product space and $U$ a unitary operator on $V$ such that $U \alpha=\alpha$ implies $\alpha=0$. Let

$$
f(z)=i \frac{(1+z)}{(1-z)}, \quad z \neq 1
$$

and show that

(a) $f(U)=i(I+U)(I-U)^{-1}$

(b) $f(U)$ is self-adjoint;

(c) for every self-adjoint operator $T$ on $V$, the operator

$$
U=(T-i I)(T+i I)^{-1}
$$

is unitary and such that $T=f(U)$.

7. Let $V$ be the space of complex $n \times n$ matrices equipped with the inner product

$$
(A \mid B)=\operatorname{tr}\left(A B^{*}\right) .
$$

If $B$ is an element of $V$, let $L_{B}, R_{B}$, and $T_{B}$ denote the linear operators on $V$ defined by

(a) $L_{B}(A)=B A$.

(b) $R_{B}(A)=A B$.

(c) $T_{B}(A)=B A-A B$.

Consider the three families of operators obtained by letting $B$ vary over all diagonal matrices. Show that each of these families is a commutative self-adjoint algebra and find their spectral resolutions.

8. If $B$ is an arbitrary member of the inner product space in Exercise 7, show that $L_{B}$ is unitarily equivalent to $R_{B}$ '.

9. Let $V$ be the inner product space in Exercise 7 and $G$ the group of unitary matrices in $V$. If $B$ is in $G$, let $C_{B}$ denote the linear operator on $V$ defined by

Show that

$$
C_{B}(A)=B A B^{-1} \text {. }
$$

(a) $C_{B}$ is a unitary operator on $V$;

(b) $C_{B_{1} B_{2}}=C_{B_{1} C_{B_{2}}}$

(c) there is no unitary transformation $U$ on $V$ such that

$$
U L_{B} U^{-1}=C_{B}
$$

for all $B$ in $G$.

10. Let $\mathcal{F}$ be any family of linear operators on a finite-dimensional inner product space $V$ and $Q$ the self-adjoint algebra generated by $\mathscr{F}$. Show that

(a) each root of $Q$ defines a root of $\mathscr{F}$;

(b) each root $r$ of $Q$ is a multiplicative linear function on $A$, i.e.,

$$
\begin{aligned}
r(T U) & =r(T) r(U) \\
r(c T+U) & =c r(T)+r(U)
\end{aligned}
$$

for all $T$ and $U$ in $Q$ and all scalars $c$.

11. Let $\mathcal{F}$ be a commuting family of diagonalizable normal operators on a finitedimensional inner product space $V$; and let $Q$ be the self-adjoint algebra generated by $\mathfrak{F}$ and the identity operator $I$. Show that each root of $Q$ is d.fferent from 0 , and that for each root $r$ of $\mathcal{F}$ there is a unique root $s$ of $Q$ such that $s(T)=r(T)$ for all $T$ in $\mathscr{F}$.

12. Let $\mathfrak{F}$ be a commuting family of diagonalizable normal operators on a finitedimensional inner product space $V$ and $A_{0}$ the self-adjoint algebra generated by $\mathscr{F}$. Let $Q$ be the self-adjoint algebra generated by $\mathcal{F}$ and the identity operator $I$. Show that

(a) $Q$ is the set of all operators on $V$ of the form $c I+T$ where $c$ is a scalar and $T$ an operator in $Q_{0}$

(b) There is at most onc root $r$ of $Q$ such that $r(T)=0$ for all $T$ in $Q_{0}$.

(c) If one of the roots of $Q$ is 0 on $Q_{0}$, the projections $P_{1}, \ldots, P_{k}$ in the resolution of the identity defined by $\mathfrak{F}$ may be indexed in such a way that $Q_{0}$ consists of all operators on $V$ of the form Sec. $9.6$

$$
T=\sum_{j=2}^{k} c_{j} P_{j}
$$

where $c_{2}, \ldots, c_{k}$ are arbitrary scalars.

(d) $Q=Q_{0}$ if and only if for each root $r$ of $Q$ there exists an operator $T$ in $Q_{0}$ such that $r(T) \neq 0$.

\subsection{Further Properties of Normal Operators}

In Section $8.5$ we developed the basic properties of self-adjoint and normal operators, using the simplest and most direct methods possible. In Section $9.5$ we considered various aspects of spectral theory. Here we prove some results of a more technical nature which are mainly about normal operators on real spaces.

We shall begin by proving a sharper version of the primary decomposition theorem of Chapter 6 for normal operators. It applies to both the real and complex cases.

Theorem 17. Let T be a normal operator on a finite-dimensional inner product space $\mathrm{V}$. Let $\mathrm{p}$ be the minimal polynomial for $\mathrm{T}$ and $\mathrm{p}_{1}, \cdots, \mathrm{p}_{\mathrm{k}}$ its distinct monic prime factors. Then each $\mathrm{p}_{\mathrm{j}}$ occurs with multiplicity 1 in the factorization of $\mathrm{p}$ and has degree 1 or 2 . Suppose $\mathrm{W}_{\mathrm{j}}$ is the null space of $\mathrm{p}_{\mathrm{j}}(\mathrm{T})$. Then

(i) $\mathrm{W}_{\mathrm{j}}$ is orthogonal to $\mathrm{W}_{\mathrm{i}}$ when $\mathrm{i} \neq \mathrm{j}$;

(ii) $\mathrm{V}=\mathrm{W}_{1} \bigoplus \ldots \bigoplus \mathrm{W}_{\mathrm{k}}$;

(iii) $\mathrm{W}_{\mathrm{j}}$ is invariant under $\mathrm{T}$, and $\mathrm{p}_{\mathrm{j}}$ is the minimal polynomial for the restriction of $\mathrm{T}$ to $\mathrm{W}_{\mathrm{j}}$;

(iv) for every $\mathrm{j}$, there is a polynomial $\mathrm{e}_{\mathrm{j}}$ with coefficients in the scalar field such that $\mathrm{e}_{\mathrm{j}}(\mathrm{T})$ is the orthogonal projection of $\mathrm{V}$ on $\mathrm{W}_{\mathrm{j}}$.

In the proof we use certain basic facts which we state as lemmas.

Lemma 1. Let $\mathrm{N}$ be a normal operator on an inner product space $\mathrm{W}$. Then the null space of $\mathrm{N}$ is the orthogonal complement of its range.

Proof. Suppose $(\alpha \mid N \beta)=0$ for all $\beta$ in $W$. Then $\left(N^{*} \alpha \mid \beta\right)=0$ for all $\beta$; hence $N^{*} \alpha=0$. By Theorem 19 of Chapter 8 , this implies $N \alpha=0$. Conversely, if $N \alpha=0$, then $N^{*} \alpha=0$, and

for all $\beta$ in $W$.

$$
\left(N^{*} \alpha \mid \beta\right)=(\alpha \mid N \beta)=0
$$

Lemma 2. If $\mathrm{N}$ is a normal operator and $\alpha$ is a vector such that $\mathrm{N}^{2} \alpha=0$, then $\mathrm{N} \alpha=0$. Proof. Suppose $N$ is normal and that $N^{2} \alpha=0$. Then $N \alpha$ lies in the range of $N$ and also lies in the null space of $N$. By Lemma 1, this implies $N \alpha=0$.

Lemma 3. Let $\mathrm{T}$ be a normal operator and $\mathrm{f}$ any polynomial with coeficients in the scalar field. Then $\mathrm{f}(\mathrm{T})$ is also normal.

Proof. Suppose $f=a_{0}+a_{1} x+\cdots+a_{n} x^{n}$. Then

$$
f(T)=a_{0} l+a_{1} T+\cdots+a_{n} T^{n}
$$

and

$$
f(T)^{*}=\bar{a}_{0} I+\bar{a}_{1} T^{*}+\cdots+\bar{a}_{n}\left(T^{*}\right)^{n} .
$$

Since $T^{*} T=T T^{*}$, it follows that $f(T)$ commutes with $f(T)^{*}$.

Lemma 4. Let $\mathrm{T}$ be a normal operator and $\mathrm{f}, \mathrm{g}$ relatively prime polynomials with coefficients in the scalar field. Suppose $\alpha$ and $\beta$ are vectors such that $\mathrm{f}(\mathrm{T}) \alpha=0$ and $\operatorname{g}(\mathrm{T}) \beta=0$. Then $(\boldsymbol{\alpha} \mid \beta)=0$.

Proof. There are polynomials $a$ and $b$ with coefficients in the scalar field such that $a f+b g=1$. Thus

$$
a(T) f(T)+b(T) g(T)=I
$$

and $\alpha=g(T) b(T) \alpha$. It follows that

$$
(\alpha \mid \beta)=(g(T) b(T) \alpha \mid \beta)=\left(b(T) \alpha \mid g(T)^{*} \beta\right) .
$$

By assumption $g(T) \beta=0$. By Lemma $3, g(T)$ is normal. Therefore, by Theorem 19 of Chapter $8, g(T)^{*} \beta=0$; hence $(\alpha \mid \beta)=0$.

Proof of Theorem 17. Recall that the minimal polynomial for $T$ is the monic polynomial of least degree among all polynomials $f$ such that $f(T)=0$. The existence of such polynomials follows from the assumption that $V$ is finite-dimensional. Suppose some prime factor $p_{j}$ of $p$ is repeated. Then $p=p_{j}^{2} g$ for some polynomial $g$. Since $p(T)=0$, it follows that

$$
\left(p_{j}(T)\right)^{2} g(T) \alpha=0
$$

for every $\alpha$ in $V$. By Lemma 3, $p_{j}(T)$ is normal. Thus Lemma 2 implies

$$
p_{j}(T) g(T) \alpha=0
$$

for every $\alpha$ in $V$. But this contradicts the assumption that $p$ has least degree among all $f$ such that $f(T)=0$. Therefore, $p=p_{1} \cdots p_{k}$. If $V$ is a complex inner product space each $p_{j}$ is necessarily of the form

$$
p_{j}=x-c_{j}
$$

with $c_{j}$ real or complex. On the other hand, if $V$ is a real inner product space, then $p_{j}=x_{j}-c_{j}$ with $c_{j}$ in $R$ or

$$
p_{j}=(x-c)(x-\bar{c})
$$

where $c$ is a non-real complex number. Now let $f_{j}=p / p_{j}$. Then, since $f_{1}, \ldots, f_{k}$ are relatively prime, there exist polynomials $g_{j}$ with coefficients in the scalar field such that

$$
\mathbf{1}=\sum_{j} f_{j} g_{j} \text {. }
$$

We briefly indicate how such $g_{j}$ may be constructed. If $p_{j}=x-c_{j}$, then $f_{j}\left(c_{j}\right) \neq 0$, and for $g_{j}$ we take the scalar polynomial $1 / f_{j}\left(c_{j}\right)$. When every $p_{j}$ is of this form, the $f_{j} g_{j}$ are the familiar Lagrange polynomials associated with $c_{1}, \ldots, c_{k}$, and (9-16) is clearly valid. Suppose some $p_{j}=(x-c)(x-\bar{c})$ with $c$ a non-real complex number. Then $V$ is a real inner product space, and we take

$$
g_{j}=\frac{x-\bar{c}}{s}+\frac{x-c}{\bar{s}}
$$

where $s=(c-\bar{c}) f_{j}(c)$. Then

$$
g_{j}=\frac{(s+\bar{s}) x-(c s+\bar{c} s)}{s \vec{s}}
$$

so that $g_{j}$ is a polynomial with real coefficients. If $p$ has degree $n$, then

$$
1-\sum_{j} f_{j} g_{j}
$$

is a polynomial with real coefficients of degree at most $n-1$; moreover, it vanishes at each of the $n$ (complex) roots of $p$, and hence is identically 0 .

Now let $\alpha$ be an arbitrary vector in $V$. Then by (9-16)

$$
\alpha=\sum_{j} f_{j}(T) g_{j}(T) \alpha
$$

and since $p_{j}(T) f_{j}(T)=0$, it follows that $f_{j}(T) g_{j}(T) \alpha$ is in $W_{j}$ for every $j$. By Lemma $4, W_{j}$ is orthogonal to $W_{i}$ whenever $i \neq j$. Therefore, $V$ is the orthogonal direct sum of $W_{1}, \ldots, W_{k}$. If $\beta$ is any vector in $W_{j}$, then

$$
p_{j}(T) T \beta=T p_{j}(T) \beta=0 ;
$$

thus $W_{j}$ is invariant under $T$. Let $T_{j}$ be the restriction of $T$ to $W_{j}$. Then $p_{j}\left(T_{j}\right)=0$, so that $p_{j}$ is divisible by the minimal polynomial for $T_{j}$. Since $p_{j}$ is irreducible over the scalar field, it follows that $p_{j}$ is the minimal polynomial for $T_{j}$.

Next, let $e_{j}=f_{j} g_{j}$ and $E_{j}=e_{j}(T)$. Then for every vector $\alpha$ in $V$, $E_{j}^{\prime} \alpha$ is in $W_{j}$, and

$$
\alpha=\sum_{j} E_{j} \alpha .
$$

Thus $\alpha-E_{i} \alpha=\sum_{j \neq i} E_{j} \alpha$; since $W_{j}$ is orthogonal to $W_{i}$ when $j \neq i$, this implies that $\alpha-E_{i} \alpha$ is in $W_{i}{ }^{\perp}$. It now follows from Theorem 4 of Chapter 8 that $E_{i}$ is the orthogonal projection of $V$ on $W_{i}$.

Definition. We call the subspaces $\mathrm{W}_{\mathrm{j}}(1 \leq \mathrm{j} \leq \mathrm{k})$ the primary components of $\mathrm{V}$ under $\mathrm{T}$. Corollary. Let $\mathrm{T}$ be a normal operator on a finite-dimensional inner product space $\mathrm{V}$ and $\mathrm{W}_{\mathbf{1}}, \ldots, \mathrm{W}_{\mathrm{k}}$ the primary components of $\mathrm{V}$ under $\mathrm{T}$. Suppose $\mathrm{W}$ is a subspace of $\mathrm{V}$ which is invariant under T. Then

$$
\mathrm{W}=\boldsymbol{\sum}_{\mathrm{j}} \mathrm{W} \cap \mathrm{W}_{\mathrm{j}} \text {. }
$$

Proof. Clearly $W$ contains $\Sigma_{j} W \cap W_{j}$. On the other hand, $W$, being invariant under $T$, is invariant under every polynomial in $T$. In particular, $W$ is invariant under the orthogonal projection $E_{j}$ of $V$ on $W_{j}$. If $\alpha$ is in $W$, it follows that $E_{j} \boldsymbol{\alpha}$ is in $W \cap W_{j}$, and, at the same time, $\alpha={\underset{j}{j}}^{\boldsymbol{E}} E_{j} \alpha$. Therefore $W$ is contained in $\sum_{\mathrm{j}} W \cap W_{j}$.

Theorem 17 shows that every normal operator $T$ on a finitedimensional inner product space is canonically specified by a finite number of normal operators $T_{j}$, defined on the primary components $W_{j}$ of $V$ under $T$, each of whose minimal polynomials is irreducible over the field of scalars. To complete our understanding of normal operators it is necessary to study normal operators of this special type.

A normal operator whose minimal polynomial is of degree 1 is clearly just a scalar multiple of the identity. On the other hand, when the minimal polynomial is irreducible and of degree 2 the situation is more complicated.

Example 1. Suppose $r>0$ and that $\theta$ is a real number which is not an integral multiple of $\pi$. Let $T$ be the linear operator on $R^{2}$ whose matrix in the standard orthonormal basis is

$$
A=r\left[\begin{array}{rr}
\cos \theta & -\sin \theta \\
\sin \theta & \cos \theta
\end{array}\right] \text {. }
$$

Then $T$ is a scalar multiple of an orthogonal transformation and hence normal. Let $\boldsymbol{p}$ be the characteristic polynomial of $T$. Then

$$
\begin{aligned}
p & =\operatorname{det}(x I-A) \\
& =(x-r \cos \theta)^{2}+r^{2} \sin ^{2} \theta \\
& =x-2 r \cos \theta x+r^{2} .
\end{aligned}
$$

Let $a=r \cos \theta, b=r \sin \theta$, and $c=a+i b$. Then $b \neq 0, c=r e^{i \theta}$

$$
A=\left[\begin{array}{rr}
a & -b \\
b & a
\end{array}\right]
$$

and $p=(x-c)(x-\bar{c})$. Hence $p$ is irreducible over $R$. Since $p$ is divisible by the minimal polynomial for $T$, it follows that $p$ is the minimal polynomial.

This example suggests the following converse. Theorem 18. Let T be a normal operator on a finite-dimensional real inner product space $\mathrm{V}$ and $\mathrm{p}$ its minimal polynomial. Suppose

$$
\mathrm{p}=(\mathrm{x}-\mathrm{a})^{2}+\mathrm{b}^{2}
$$

where $\mathrm{a}$ and $\mathrm{b}$ are real and $\mathrm{b} \neq 0$. Then there is an integer $\mathrm{s}>0$ such that $\mathrm{p}^{\mathrm{s}}$ is the characteristic polynomial for $\mathrm{T}$, and there exist subspaces $\mathrm{V}_{1}, \ldots, \mathrm{V}_{\mathrm{s}}$ of $\mathrm{V}$ such that

(i) $\mathrm{V}_{\mathrm{j}}$ is orthogonal to $\mathrm{V}_{\mathrm{i}}$ when $\mathrm{i} \neq \mathrm{j}$;

(ii) $\mathrm{V}=\mathrm{V}_{1} \oplus \cdots \oplus \mathrm{V}_{\mathrm{s}}$;

(iii) each $\mathrm{V}_{\mathrm{j}}$ has an orthonormal basis $\left\{\alpha_{\mathrm{j}}, \beta_{\mathrm{j}}\right\}$ with the property that

$$
\begin{aligned}
& \mathrm{T} \alpha_{\mathrm{j}}=\mathrm{a} \alpha_{\mathrm{j}}+\mathrm{b} \beta_{\mathrm{j}} \\
& \mathrm{T} \beta_{\mathrm{j}}=-\mathrm{b} \alpha_{\mathrm{j}}+\mathrm{a} \beta_{\mathrm{j}} .
\end{aligned}
$$

In other words, if $r=\sqrt{a^{2}+b^{2}}$ and $\theta$ is chosen so that $a=r \cos \theta$ and $b=r \sin \theta$, then $V$ is an orthogonal direct sum of two-dimensional subspaces $V_{j}$ on each of which $T$ acts as ' $r$ times rotation through the angle $\theta^{\prime}$.

The proof of Theorem 18 will be based on the following result.

Lemma. Let $\mathrm{V}$ be a real inner product space and $\mathrm{S}$ a normal operator on $\mathrm{V}$ such that $\mathrm{S}^{2}+\mathrm{I}=0$. Let $\alpha$ be any vector in $\mathrm{V}$ and $\beta=\mathrm{S} \alpha$. Then

$$
\begin{aligned}
& \mathrm{S}^{*} \alpha=-\beta \\
& \mathrm{S}^{*} \beta=a
\end{aligned}
$$

$(\alpha \mid \beta)=0$, and $\|\alpha\|=\|\beta\|$.

Proof. We have $S \alpha=\beta$ and $S \beta=S^{2} \alpha=-\alpha$. Therefore

$0=\|S \alpha-\beta\|^{2}+\|S \beta+\alpha\|^{2}=\|S \alpha\|^{2}-2(S \alpha \mid \beta)+\|\beta\|^{2}$

Since $S$ is normal, it follows that

$$
+\|S \beta\|^{2}+2(S \beta \mid \alpha)+\|\alpha\|^{2} \text {. }
$$

$0=\left\|S^{*} \alpha\right\|^{2}-2\left(S^{*} \beta \mid \alpha\right)+\|\beta\|^{2}+\left\|S^{*} \beta\right\|^{2}+2\left(S^{*} \alpha \mid \beta\right)+\|\alpha\|^{2}$

$$
=\left\|S^{*} \alpha+\beta\right\|^{2}+\left\|S^{*} \beta-\alpha\right\|^{2} \text {. }
$$

This implies $(9-17)$; hence

and $(\alpha \mid \beta)=0$. Similarly

$$
\begin{aligned}
(\alpha \mid \beta) & =\left(S^{*} \boldsymbol{\beta} \mid \beta\right)=(\beta \mid S \beta) \\
& =(\beta \mid-\alpha) \\
& =-(\alpha \mid \beta)
\end{aligned}
$$

$$
\|\alpha\|^{2}=\left(S^{*} \beta \mid \alpha\right)=(\beta \mid S \alpha)=\|\beta\|^{2} .
$$

Proof of Theorem 18. Let $V_{1}, \ldots, V_{s}$ be a maximal collection of two-dimensional subspaces satisfying (i) and (ii), and the additional conditions 

$$
\begin{aligned}
& T^{*} \alpha_{j}=\boldsymbol{\alpha} \alpha_{j}-b \beta_{j}, \\
& T^{*} \beta_{j}=b \alpha_{j}+a \beta_{j} \quad 1 \leq j \leq s .
\end{aligned}
$$

Let $W=V_{1}+\cdots+V_{8}$. Then $W$ is the orthogonal direct sum of $V_{1}, \ldots, V_{8}$. We shall show that $W=V$. Suppose that this is not the case. Then $W^{\perp} \neq\{0\}$. Moreover, since (iii) and (9-18) imply that $W$ is invariant under $T$ and $T^{*}$, it follows that $W^{\perp}$ is invariant under $T^{*}$ and $T=T^{* *}$. Let $S=b^{-1}(T-a I)$. Then $S^{*}=b^{-1}\left(T^{*}-a I\right), S^{*} S=S S^{*}$, and $W^{\perp}$ is invariant under $S$ and $S^{*}$. Since $(T-a I)^{2}+b^{2} I=0$, it follows that $S^{2}+I=0$. Let $\alpha$ be any vector of norm 1 in $W^{\perp}$ and set $\beta=S \alpha$. Then $\beta$ is in $W^{\perp}$ and $S \beta=-\alpha$. Since $T=a I+b S$, this implies

$$
\begin{aligned}
& T \alpha=a \alpha+b \beta \\
& T \beta=-b \alpha+a \beta .
\end{aligned}
$$

By the lemma, $S^{*} \alpha=-\beta, S^{*} \beta=\alpha,(\alpha \mid \beta)=0$, and $\|\beta\|=1$. Because $T^{*}=a I+b S^{*}$, it follows that

$$
\begin{aligned}
& T^{*} \alpha=a \alpha-b \beta \\
& T^{*} \beta=b \alpha+\alpha \beta .
\end{aligned}
$$

But this contradicts the fact that $V_{1}, \ldots, V_{8}$ is a maximal collection of subspaces satisfying (i), (iii), and (9-18). Therefore, $W=V$, and since

$$
\operatorname{det}\left[\begin{array}{cc}
x-a & b \\
-b & x-a
\end{array}\right]=(x-a)^{2}+b^{2}
$$

it follows from (i), (ii) and (iii) that

$$
\operatorname{det}(x I-T)=\left[(x-a)^{2}+b^{2}\right]^{s} .
$$

Corollary. Under the conditions of the theorem, $\mathrm{T}$ is invertible, and

$$
\mathrm{T}^{*}=\left(\mathrm{a}^{2}+\mathrm{b}^{2}\right) \mathrm{T}^{-1} \text {. }
$$

Proof. Since

$$
\left[\begin{array}{rr}
a & -b \\
b & a
\end{array}\right]\left[\begin{array}{rr}
a & b \\
-b & a
\end{array}\right]=\left[\begin{array}{cc}
a^{2}+b^{2} & 0 \\
0 & a^{2}+b^{2}
\end{array}\right]
$$

it follows from (iii) and (9-18) that $T T^{*}=\left(a^{2}+b^{2}\right) I$. Hence $T$ is invertible and $T^{*}=\left(a^{2}+b^{2}\right) T^{-1}$.

Theorem 19. Let $\mathrm{T}$ be a normal operator on a finite-dimensional inner product space $\mathrm{V}$. Then any linear operator that commutes with $\mathrm{T}$ also commutes with $\mathrm{T}^{*}$. Moreover, every subspace invariant under $\mathrm{T}$ is also invariant under $\mathrm{T}^{*}$.

Proof. Suppose $U$ is a linear operator on $V$ that commutes with $T$. Let $E_{j}$ be the orthogonal projection of $V$ on the primary component $W_{j}(1 \leq j \leq k)$ of $V$ under $T$. Then $E_{j}$ is a polynomial in $T$ and hence commutes with $U$. Thus

$$
E_{j} U E_{j}=U E_{j}^{2}=U E_{j} .
$$

Thus $U\left(W_{j}\right)$ is a subset of $W_{j}$. Let $T_{j}$ and $U_{j}$ denote the restrictions of $T$ and $U$ to $W_{j}$. Suppose $I_{j}$ is the identity operator on $W_{j}$. Then $U_{j}$ commutes with $T_{j}$, and if $T_{j}=c_{j} I_{j}$, it is clear that $U_{j}$ also commutes with $T_{j}^{*}=\bar{c}_{j} I_{j}$. On the other hand, if $T_{j}$ is not a scalar multiple of $I_{j}$, then $T_{j}$ is invertible and there exist real numbers $a_{j}$ and $b_{j}$ such that

$$
T_{j}^{*}=\left(a_{j}^{2}+b_{j}^{2}\right) T_{j}^{-1} .
$$

Since $U_{j} T_{j}=T_{j} U_{j}$, it follows that $T_{j}^{-1} U_{j}=U_{j} T_{j}^{-1}$. Therefore $U_{j}$ commutes with $T_{j}^{*}$ in both cases. Now $T^{*}$ also commutes with $E_{j}$, and hence $W_{j}$ is invariant under $T^{*}$. Moreover for every $\alpha$ and $\beta$ in $W_{j}$

$$
\left(T_{j} \alpha \mid \beta\right)=(T \alpha \mid \beta)=\left(\alpha \mid T^{*} \beta\right)=\left(\alpha \mid T_{j}^{*} \beta\right) .
$$

Since $T^{*}\left(W_{j}\right)$ is contained in $W_{j}$, this implies $T_{j}^{*}$ is the restriction of $T^{*}$ to $W_{j}$. Thus

$$
U^{*} \alpha_{j}=T^{*} U \alpha_{j}
$$

for every $\alpha_{j}$ in $W_{j}$. Since $V$ is the sum of $W_{1}, \ldots, W_{k}$, it follows that

$$
U T^{*} \alpha=T^{*} U \alpha
$$

for every $\alpha$ in $V$ and hence that $U$ commutes with $T^{*}$.

Now suppose $W$ is a subspace of $V$ that is invariant under $T$, and let $Z_{j}=W \cap W_{j}$. By the corollary to Theorem $17, W=\sum_{j} Z_{j}$. Thus it suffices to show that each $Z_{j}$ is invariant under $T_{j}^{*}$. This is clear if $T_{j}=c_{j} I$. When this is not the case, $T_{j}$ is invertible and maps $Z_{j}$ into and hence onto $Z_{j}$. Thus $T_{j}^{-1}\left(Z_{j}\right)=Z_{j}$, and since

$$
T_{j}^{*}=\left(a_{j}^{2}+b_{j}^{2}\right) T_{j}^{-1}
$$

it follows that $T^{*}\left(Z_{j}\right)$ is contained in $Z_{j}$, for every $j$.

Suppose $T$ is a normal operator on a finite-dimensional inner product space $V$. Let $W$ be a subspace invariant under $T$. Then the preceding corollary shows that $W$ is invariant under $T^{*}$. From this it follows that $W^{\perp}$ is invariant under $T^{* *}=T$ (and hence under $T^{*}$ as well). Using this fact one can easily prove the following strengthened version of the cyclic decomposition theorem given in Chapter 7.

Theorem 20. Let $\mathrm{T}$ be a normal linear operator on a finite-dimensional inner product space $\mathrm{V}(\operatorname{dim} \mathrm{V} \geq 1)$. Then there exist $\mathrm{r}$ non-zero vectors $\alpha_{1}, \ldots, \alpha_{\mathrm{r}}$ in $\mathrm{V}$ with respective T-annihilators $\mathrm{e}_{1}, \ldots, \mathrm{e}_{\mathrm{r}}$ such that

(i) $\mathrm{V}=\mathrm{Z}\left(\alpha_{1} ; \mathrm{T}\right) \oplus \cdots \oplus \mathrm{Z}\left(\alpha_{\mathrm{r}} ; \mathrm{T}\right)$;

(ii) if $1 \leq \mathrm{k} \leq \mathrm{r}-1$, then $\mathrm{e}_{\mathrm{k}+1}$ divides $\mathrm{e}_{\mathrm{k}}$; (iii) $\mathrm{Z}\left(\alpha_{\mathrm{j}} ; \mathrm{T}\right)$ is orthogonal to $\mathrm{Z}\left(\alpha_{\mathrm{k}} ; \mathrm{T}\right)$ when $\mathrm{j} \neq \mathrm{k}$. Furthermore, the integer $\mathrm{r}$ and the annihilators $\mathrm{e}_{1}, \ldots, \mathrm{e}_{\mathrm{r}}$ are uniquely determined by conditions (i) and (ii) and the fact that no $\alpha_{\mathrm{k}}$ is 0 .

Corollary. If A is a normal matrix with real (complex) entries, then there is a real orthogonal (unitary) matrix $\mathrm{P}$ such that $\mathrm{P}^{-1} \mathrm{AP}$ is in rational canonical form.

It follows that two normal matrices $A$ and $B$ are unitarily equivalent if and only if they have the same rational form; $A$ and $B$ are orthogonally equivalent if they have real entries and the same rational form.

On the other hand, there is a simpler criterion for the unitary equivalence of normal matrices and normal operators.

Definitions. Let $\mathrm{V}$ and $\mathrm{V}^{\prime}$ be inner product spaces over the same field. A linear transformation

$$
\mathrm{U}: \mathrm{V} \rightarrow \mathrm{V}^{\prime}
$$

is called a unitary transformation if it maps $\mathbf{V}$ onto $\mathbf{V}^{\prime}$ and preserves inner products. If $\mathrm{T}$ is a linear operator on $\mathrm{V}$ and $\mathrm{T}^{\prime}$ a linear operator on $\mathrm{V}^{\prime}$, then $\mathrm{T}$ is unitarily equivalent to $\mathrm{T}^{\prime}$ if there exists a unitary transformation $\mathrm{U}$ of $\mathrm{V}$ onto $\mathrm{V}^{\prime}$ such that

$$
\mathrm{UTU}^{-1}=\mathrm{T}^{\prime} .
$$

Lemma. Let $\mathrm{V}$ and $\mathrm{V}^{\prime}$ be finite-dimensional inner product spaces over the same field. Suppose $\mathrm{T}$ is a linear operator on $\mathrm{V}$ and that $\mathrm{T}^{\prime}$ is a linear operator on $\mathrm{V}^{\prime}$. Then $\mathrm{T}$ is unitarily equivalent to $\mathrm{T}^{\prime}$ if and only if there is an orthonormal basis $B$ of $\mathrm{V}$ and an orthonormal basis $B^{\prime}$ of $\mathrm{V}^{\prime}$ such that

$$
[\mathrm{T}]_{\omega}=\left[\mathrm{T}^{\prime}\right]_{\sigma^{\prime}} .
$$

Proof. Suppose there is a unitary transformation $U$ of $V$ onto $V^{\prime}$ such that $U T U^{-1}=T^{\prime}$. Let $B=\left\{\alpha_{1}, \ldots, \alpha_{n}\right\}$ be any (ordered) orthonormal basis for $V$. Let $\alpha_{j}^{\prime}=U \alpha_{j}(1 \leq j \leq n)$. Then $\mathbb{B}^{\prime}=\left\{\alpha_{1}^{\prime}, \ldots\right.$, $\left.\alpha_{n}^{\prime}\right\}$ is an orthonormal basis for $V^{\prime}$ and setting

we see that

$$
T \alpha_{j}=\sum_{k=1}^{n} A_{k j} \alpha_{k}
$$

$$
\begin{aligned}
T^{\prime} \alpha_{j}^{\prime} & =U T \alpha_{j} \\
& =\sum_{k} A_{k j} U \alpha_{k} \\
& =\sum_{k} A_{k j} \alpha_{k}^{\prime}
\end{aligned}
$$

Hence $[T]_{\mathscr{Q}}=A=\left[T^{\prime}\right]_{\Omega^{\prime}}$. Conversely, suppose there is an orthonormal basis $B$ of $V$ and an orthonormal basis $\mathbb{B}^{\prime}$ of $V^{\prime}$ such that

$$
[T]_{B}=\left[T^{\prime}\right]_{B^{\prime}}
$$

and let $A=[T]_{\mathscr{B}}$. Suppose $B=\left\{\alpha_{1}, \ldots, \alpha_{n}\right\}$ and that $B^{\prime}=\left\{\alpha_{1}^{\prime}, \ldots, \alpha_{n}^{\prime}\right\}$. Let $U$ be the linear transformation of $V$ into $V^{\prime}$ such that $U \alpha_{j}=\alpha_{j}^{\prime}$ $(1 \leq j \leq n)$. Then $U$ is a unitary transformation of $V$ onto $V^{\prime}$, and

$$
\begin{aligned}
U T U^{-1} \alpha_{j}^{\prime} & =U T \alpha_{j} \\
& =U \sum_{k} A_{k j} \alpha_{k} \\
& =\sum_{k} A_{k j} \alpha_{k}^{\prime} .
\end{aligned}
$$

Therefore, $U T U^{-1} \alpha_{j}^{\prime}=T^{\prime} \alpha_{j}^{\prime}(1 \leq j \leq n)$, and this implies $U T U^{-1}=$ $T^{\prime}$.

It follows immediately from the lemma that unitarily equivalent operators on finite-dimensional spaces have the same characteristic polynomial. For normal operators the converse is valid.

Theorem 21. Let $\mathrm{V}$ and $\mathrm{V}^{\prime}$ be finite-dimensional inner product spaces over the same field. Suppose $\mathrm{T}$ is a normal operator on $\mathrm{V}$ and that $\mathrm{T}^{\prime}$ is a normal operator on $\mathrm{V}^{\prime}$. Then $\mathrm{T}$ is unitarily equivalent to $\mathrm{T}^{\prime}$ if and only if $\mathrm{T}$ and $\mathrm{T}^{\prime}$ have the same characteristic polynomial.

Proof. Suppose $T$ and $T^{\prime}$ have the same characteristic polynomial $f$. Let $W_{j}(1 \leq j \leq k)$ be the primary components of $V$ under $T$ and $T_{j}$ the restriction of $T$ to $W_{j}$. Suppose $I_{j}$ is the identity operator on $W_{j}$. Then

$$
f=\prod_{j=1}^{k} \operatorname{det}\left(x I_{j}-T_{j}\right) .
$$

Let $p_{j}$ be the minimal polynomial for $T_{j}$. If $p_{j}=x-c_{j}$ it is clear that

$$
\operatorname{det}\left(x I_{j}-T_{j}\right)=\left(x-c_{j}\right)^{s_{j}}
$$

where $s_{j}$ is the dimension of $W_{j}$. On the other hand, if $p_{j}=\left(x-a_{j}\right)^{2}+b_{j}^{2}$ with $a_{j}, b_{j}$ real and $b_{j} \neq 0$, then it follows from Theorem 18 that

$$
\operatorname{det}\left(x I_{j}-T_{j}\right)=p_{j}^{s_{j}}
$$

where in this case $2 s_{j}$ is the dimension of $W_{j}$. Therefore $f=\prod_{j} p_{j}^{s_{i}}$. Now we can also compute $f$ by the same method using the primary components of $V^{\prime}$ under $T^{\prime}$. Since $p_{1}, \ldots, p_{k}$ are distinct primes, it follows from the uniqueness of the prime factorization of $f$ that there are exactly $k$ primary components $W_{j}^{\prime}(1 \leq j \leq k)$ of $V^{\prime}$ under $T^{\prime}$ and that these may be indexed in such a way that $p_{j}$ is the minimal polynomial for the restriction $T_{j}^{\prime}$ of $T^{\prime}$ to $W_{j}^{\prime}$. If $p_{j}=x-c_{j}$, then $T_{j}=c_{j} I_{j}$ and $T_{j}^{\prime}=c_{j} I_{j}^{\prime}$ where $I_{j}^{\prime}$ is the identity operator on $W_{j}^{\prime}$. In this case it is evident that $T_{j}$ is unitarily equivalent to $T_{j}^{\prime}$. If $p_{j}=\left(x-a_{j}\right)^{2}+b_{j}^{2}$, as above, then using the lemma and Theorem 20, we again see that $T_{j}$ is unitarily equivalent to $T_{j}^{\prime}$. Thus for each $j$ there are orthonormal bases $B_{j}$ and $B_{j}^{\prime}$ of $W_{j}$ and $W_{j}^{\prime}$, respectively, such that

$$
\left[T_{j}\right]_{\mathbb{Q}_{i}}=\left[T_{j}^{\prime}\right]_{\mathbb{Q}_{i}{ }^{\prime}} .
$$

Now let $U$ be the linear transformation of $V$ into $V^{\prime}$ that maps each $B_{j}$ onto $\mathbb{B}_{j}^{\prime}$. Then $U$ is a unitary transformation of $V$ onto $V^{\prime}$ such that $U T U^{-1}=T^{\prime}$ 

\section{Bilinear Forms}

\subsection{Bilinear Forms}

In this chapter, we treat bilinear forms on finite-dimensional vector spaces. The reader will probably observe a similarity between some of the material and the discussion of determinants in Chapter 5 and of inner products and forms in Chapter 8 and in Chapter 9 . The relation between bilinear forms and inner products is particularly strong; however, this chapter does not presuppose any of the material in Chapter 8 or Chapter 9 . The reader who is not familiar with inner products would probably profit by reading the first part of Chapter 8 as he reads the discussion of bilinear forms.

This first section treats the space of bilinear forms on a vector space of dimension $n$. The matrix of a bilinear form in an ordered basis is introduced, and the isomorphism between the space of forms and the space of $n \times n$ matrices is established. The rank of a bilinear form is defined, and non-degenerate bilinear forms are introduced. The second section discusses symmetric bilinear forms and their diagonalization. The third section treats skew-symmetric bilinear forms. The fourth section discusses the group preserving a non-degenerate bilinear form, with special attention given to the orthogonal groups, the pseudo-orthogonal groups, and a particular pseudo-orthogonal group-the Lorentz group.

Definition. Let $\mathrm{V}$ be a vector space over the field $\mathrm{F}$. A bilinear form on $\mathrm{V}$ is a function $\mathrm{f}$, which assigns to each ordered pair of vectors $\alpha, \beta$ in $\mathrm{V}$ a scalar $\mathrm{f}(\alpha, \boldsymbol{\beta})$ in $\mathrm{F}$, and which satisfies 

$$
\begin{aligned}
& \mathrm{f}\left(\mathrm{c} \alpha_{1}+\dot{\alpha}_{2}, \beta\right)=\operatorname{cf}\left(\alpha_{1}, \beta\right)+\mathrm{f}\left(\alpha_{2}, \beta\right) \\
& \mathrm{f}\left(\alpha, \mathrm{c} \beta_{1}+\beta_{2}\right)=\operatorname{cf}\left(\alpha, \beta_{1}\right)+\mathrm{f}\left(\alpha, \beta_{2}\right) .
\end{aligned}
$$

If we let $V \times V$ denote the set of all ordered pairs of vectors in $V$, this definition can be rephrased as follows: A bilinear form on $V$ is a function $f$ from $V \times V$ into $F$ which is linear as a function of either of its arguments when the other is fixed. The zero function from $V \times V$ into $F$ is clearly a bilinear form. It is also true that any linear combination of bilinear forms on $V$ is again a bilinear form. To prove this, it is sufficient to consider linear combinations of the type $c f+g$, where $f$ and $g$ are bilinear forms on $V$. The proof that $c f+g$ satisfies (10-1) is similar to many others we have given, and we shall thus omit it. All this may be summarized by saying that the set of all bilinear forms on $V$ is a subspace of the space of all functions from $V \times V$ into $F$ (Example 3, Chapter 2). We shall denote the space of bilinear forms on $V$ by $L(V, V, F)$.

Example 1. Let $V$ be a vector space over the field $F$ and let $L_{1}$ and $L_{2}$ be linear functions on $V$. Define $f$ by

$$
f(\alpha, \beta)=L_{1}(\alpha) L_{2}(\beta) .
$$

If we fix $\beta$ and regard $f$ as a function of $\alpha$, then we simply have a scalar multiple of the linear functional $L_{1}$. With $\alpha$ fixed, $f$ is a scalar multiple of $L_{2}$. Thus it is clear that $f$ is a bilinear form on $V$.

Example 2. Let $m$ and $n$ be positive integers and $F$ a field. Let $V$ be the vector space of all $m \times n$ matrices over $F$. Let $A$ be a fixed $m \times m$ matrix over $F$. Define

$$
f_{A}(X, Y)=\operatorname{tr}\left(X^{t} A Y\right) .
$$

Then $f_{A}$ is a bilinear form on $V$. For, if $X, Y$, and $Z$ are $m \times n$ matrices over $F$,

$$
\begin{aligned}
f_{A}(c X+Z, Y) & =\operatorname{tr}\left[(c X+Z)^{t} A Y\right] \\
& =\operatorname{tr}(c X t A Y)+\operatorname{tr}\left(Z^{t} A Y\right) \\
& =c f_{A}(X, Y)+f_{A}(Z, Y)
\end{aligned}
$$

Of course, we have used the fact that the transpose operation and the trace function are linear. It is even easier to show that $f_{A}$ is linear as a function of its second argument. In the special case $n=1$, the matrix $X^{t} A Y$ is $1 \times 1$, i.e., a scalar, and the bilinear form is simply

$$
\begin{aligned}
f_{A}(X, Y) & =X^{t} A Y \\
& =\sum_{i} \sum_{j} A_{i j} x_{i} y_{j} .
\end{aligned}
$$

We shall presently show that every bilinear form on the space of $m \times 1$ matrices is of this type, i.e., is $f_{A}$ for some $m \times m$ matrix $A$. Example 3. Let $F$ be a field. Let us find all bilinear forms on the space $F^{2}$. Suppose $f$ is such a bilinear form. If $\alpha=\left(x_{1}, x_{2}\right)$ and $\beta=\left(y_{1}, y_{2}\right)$ are vectors in $F^{2}$, then

$$
\begin{aligned}
f(\alpha, \beta) & =f\left(x_{1} \epsilon_{1}+x_{2} \epsilon_{2}, \beta\right) \\
& =x_{1} f\left(\epsilon_{1}, \beta\right)+x_{2} f\left(\epsilon_{2}, \beta\right) \\
& =x_{1} f\left(\epsilon_{1}, y_{1} \epsilon_{1}+y_{2} \epsilon_{2}\right)+x_{2} f\left(\epsilon_{2}, y_{1} \epsilon_{1}+y_{2} \epsilon_{2}\right) \\
& =x_{1} y_{1} f\left(\epsilon_{1}, \epsilon_{1}\right)+x_{1} y_{2} f\left(\epsilon_{1}, \epsilon_{2}\right)+x_{2} y_{1} f\left(\epsilon_{2}, \epsilon_{1}\right)+x_{2} y_{2} f\left(\epsilon_{2}, \epsilon_{2}\right) .
\end{aligned}
$$

Thus $f$ is completely determined by the four scalars $A_{i j}=f\left(\epsilon_{i}, \epsilon_{j}\right)$ by

$$
\begin{aligned}
f(\alpha, \beta) & =A_{11} x_{1} y_{1}+A_{12} x_{1} y_{2}+A_{21} x_{2} y_{1}+A_{22} x_{2} y_{2} \\
& =\sum_{i, j} A_{i j} x_{i} y_{j} .
\end{aligned}
$$

If $X$ and $Y$ are the coordinate matrices of $\alpha$ and $\beta$, and if $A$ is the $2 \times 2$ matrix with entries $A(i, j)=A_{i j}=f\left(\epsilon_{i}, \epsilon_{j}\right)$, then

$$
f(\alpha, \beta)=X^{t} A Y .
$$

We observed in Example 2 that if $A$ is any $2 \times 2$ matrix over $F$, then (10-2) defines a bilinear form on $F^{2}$. We see that the bilinear forms on $F^{2}$ are precisely those obtained from a $2 \times 2$ matrix as in (10-2).

The discussion in Example 3 can be generalized so as to describe all bilinear forms on a finite-dimensional vector space. Let $V$ be a finitedimensional vector space over the field $F$ and let $B=\left\{\alpha_{1}, \ldots, \alpha_{n}\right\}$ be an ordered basis for $V$. Suppose $f$ is a bilinear form on $V$. If

$$
\alpha=x_{1} \alpha_{1}+\cdots+x_{n} \alpha_{n} \text { and } \beta=y_{1} \alpha_{1}+\cdots+y_{n} \alpha_{n}
$$

are vectors in $V$, then

$$
\begin{aligned}
f(\alpha, \beta) & =f\left(\sum_{i} x_{i} \alpha_{i}, \beta\right) \\
& =\sum_{i} x_{i} f\left(\alpha_{i}, \beta\right) \\
& =\sum_{i} x_{i} f\left(\alpha_{i}, \sum_{j} y_{j} \alpha_{j}\right) \\
& =\sum_{i} \sum_{j} x_{i} y_{j} f\left(\alpha_{i}, \alpha_{j}\right) .
\end{aligned}
$$

If we let $A_{i j}=f\left(\alpha_{i}, \alpha_{j}\right)$, then

$$
\begin{aligned}
f(\alpha, \beta) & =\sum_{i} \sum_{j} A_{i j} x_{i} y_{j} \\
& =X^{t} A Y
\end{aligned}
$$

where $X$ and $Y$ are the coordinate matrices of $\alpha$ and $\beta$ in the ordered basis $B$. Thus every bilinear form on $V$ is of the type

$$
f(\alpha, \beta)=[\alpha]_{\mathscr{Q}}^{t} A[\beta]_{\leftrightarrow}
$$

for some $n \times n$ matrix $A$ over $F$. Conversely, if we are given any $n \times n$ matrix $A$, it is easy to see that $(10-3)$ defines a bilinear form $f$ on $V$, such that $A_{i j}=f\left(\alpha_{i}, \alpha_{j}\right)$. Definition. Let $\mathrm{V}$ be a finite-dimensional vector space, and let $\mathbb{B}=\left\{\alpha_{1}, \ldots, \alpha_{\mathrm{n}}\right\}$ be an ordered basis for $\mathrm{V}$. If $\mathrm{f}$ is a bilinear form on $\mathrm{V}$, the matrix of $\mathrm{f}$ in the ordered basis $B$ is the $\mathrm{n} X \mathrm{n}$ matrix $\mathrm{A}$ with entries $\mathrm{A}_{\mathrm{ij}}=\mathrm{f}\left(\alpha_{\mathrm{i}}, \alpha_{\mathrm{j}}\right)$. At times, we shall denote this matrix by $[\mathrm{f}]_{\Omega}$.

Theorem 1. Let $\mathrm{V}$ be a finite-dimensional vector space over the field $\mathrm{F}$. For each ordered basis $B$ of $\mathrm{V}$, the function which associates with each bilinear form on $\mathrm{V}$ its matrix in the ordered basis $B$ is an isomorphism of the space $\mathrm{L}(\mathrm{V}, \mathrm{V}, \mathrm{F})$ onto the space of $\mathrm{n} \times \mathrm{n}$ matrices over the field $\mathrm{F}$.

Proof. We observed above that $f \rightarrow[f]_{B}$ is a one-one correspondence between the set of bilinear forms on $V$ and the set of all $n \times n$ matrices over $F$. That this is a linear transformation is easy to see, because

$$
(c f+g)\left(\alpha_{i}, \alpha_{j}\right)=c f\left(\alpha_{i}, \alpha_{j}\right)+g\left(\alpha_{i}, \alpha_{j}\right)
$$

for each $i$ and $j$. This simply says that

$$
[c f+g]_{\odot}=c[f]_{\Theta}+[g]_{\omega} .
$$

Corollary. If $\mathbb{B}=\left\{\alpha_{1}, \ldots, \alpha_{\mathrm{n}}\right\}$ is an ordered basis for $\mathrm{V}$, and ه̊ $^{*}=\left\{\mathrm{L}_{1}, \ldots, \mathrm{L}_{\mathrm{n}}\right\}$ is the dual basis for $\mathrm{V}^{*}$, then the $\mathrm{n}^{2}$ bilinear forms

$$
\mathrm{f}_{\mathrm{i} j}(\alpha, \beta)=\mathrm{L}_{\mathrm{i}}(\alpha) \mathrm{L}_{\mathrm{j}}(\beta), \quad 1 \leq \mathrm{i} \leq \mathrm{n}, 1 \leq \mathrm{j} \leq \mathrm{n}
$$

form $a$ basis for the space $\mathrm{L}(\mathrm{V}, \mathrm{V}, \mathrm{F})$. In particular, the dimension of $\mathrm{L}(\mathrm{V}, \mathrm{V}, \mathrm{F})$ is $\mathrm{n}^{2}$.

Proof. The dual basis $\left\{L_{1}, \ldots, L_{n}\right\}$ is essentially defined by the fact that $L_{i}(\alpha)$ is the $i$ th coordinate of $\alpha$ in the ordered basis $B$ (for any $\alpha$ in $V$ ). Now the functions $f_{i j}$ defined by

$$
f_{i j}(\alpha, \beta)=L_{i}(\alpha) L_{j}(\beta)
$$

are bilinear forms of the type considered in Example 1. If

$$
\alpha=x_{1} \alpha_{1}+\cdots+x_{n} \alpha_{n} \text { and } \beta=y_{1} \alpha_{1}+\cdots+y_{n} \alpha_{n},
$$

then

$$
f_{i j}(\alpha, \beta)=x_{i} y_{j} .
$$

Let $f$ be any bilinear form on $V$ and let $A$ be the matrix of $f$ in the ordered basis \&. Then

which simply says that

![](https://cdn.mathpix.com/cropped/2023_01_16_0f6afdf132d84aaf6bdeg-371.jpg?height=72&width=302&top_left_y=1719&top_left_x=664)

$$
f=\sum_{i, j} A_{i j} f_{i j} .
$$

It is now clear that the $n^{2}$ forms $f_{i j}$ comprise a basis for $L(V, V, F)$.

One can rephrase the proof of the corollary as follows. The bilinear form $f_{i j}$ has as its matrix in the ordered basis $\&$ the matrix 'unit' $E^{i, j}$, whose only non-zero entry is a 1 in row $i$ and column $j$. Since these matrix units comprise a basis for the space of $n \times n$ matrices, the forms $f_{i j}$ comprise a basis for the space of bilinear forms.

The concept of the matrix of a bilinear form in an ordered basis is similar to that of the matrix of a linear operator in an ordered basis. Just as for linear operators, we shall be interested in what happens to the matrix representing a bilinear form, as we change from one ordered basis to another. So, suppose $B=\left\{\alpha_{1}, \ldots, \alpha_{n}\right\}$ and $\mathscr{B}^{\prime}=\left\{\alpha_{1}^{\prime}, \ldots, \alpha_{n}^{\prime}\right\}$ are two ordered bases for $V$ and that $f$ is a bilinear form on $V$. How are the matrices $[f]_{\mathscr{Q}}$ and $[f]_{Q_{Q^{\prime}}}$ related? Well, let $P$ be the (invertible) $n \times n$ matrix such that

$$
[\alpha]_{\mathscr{Q}}=P[\alpha]_{Q^{\prime}}
$$

for all $\alpha$ in $V$. In other words, define $P$ by

$$
\alpha_{j}^{\prime}=\sum_{i=1}^{n} P_{i j} \alpha_{i} .
$$

For any vectors $\alpha, \beta$ in $V$

$$
\begin{aligned}
& f(\alpha, \beta)=[\alpha]_{\leftrightarrow}^{t}[f]_{B}[\beta]_{\Theta} \\
& =\left(P[\alpha]_{Q^{\prime}}\right)^{t}[f]_{\mathscr{\Omega}} P[\beta]_{Q^{\prime}} \\
& =[\alpha]_{\beta^{\prime}}^{l^{\prime}}\left(P^{t}[f]_{\beta} P\right)[\beta]_{\beta^{\prime}} \text {. }
\end{aligned}
$$

By the definition and uniqueness of the matrix representing $f$ in the ordered basis $\mathbb{B}^{\prime}$, we must have

$$
[f]_{\bigotimes^{\prime}}=P^{t}[f]_{\mathscr{B}} P .
$$

Example 4. Let $V$ be the vector space $R^{2}$. Let $f$ be the bilinear form defined on $\alpha=\left(x_{1}, x_{2}\right)$ and $\beta=\left(y_{1}, y_{2}\right)$ by

Now

$$
f(\alpha, \beta)=x_{1} y_{1}+x_{1} y_{2}+x_{2} y_{1}+x_{2} y_{2} .
$$

$$
f(\alpha, \beta)=\left[x_{1}, x_{2}\right]\left[\begin{array}{ll}
1 & 1 \\
1 & 1
\end{array}\right]\left[\begin{array}{l}
y_{1} \\
y_{2}
\end{array}\right]
$$

and so the matrix of $f$ in the standard ordered basis $B=\left\{\epsilon_{1}, \epsilon_{2}\right\}$ is

$$
[f]_{Q}=\left[\begin{array}{ll}
1 & 1 \\
1 & 1
\end{array}\right] \text {. }
$$

Let $\mathbb{B}^{\prime}=\left\{\epsilon_{1}^{\prime}, \epsilon_{2}^{\prime}\right\}$ be the ordered basis defined by $\epsilon_{1}^{\prime}=(1,-1), \epsilon_{2}^{\prime}=(1,1)$. In this case, the matrix $P$ which changes coordinates from $B^{\prime}$ to $B$ is

$$
P=\left[\begin{array}{rr}
1 & 1 \\
-1 & 1
\end{array}\right]
$$

Thus

$$
\begin{aligned}
{[f]_{\mathbb{Q}^{\prime}} } & =P^{t}[f]_{\mathscr{B}} P \\
& =\left[\begin{array}{rr}
1 & -1 \\
1 & 1
\end{array}\right]\left[\begin{array}{ll}
1 & 1 \\
1 & 1
\end{array}\right]\left[\begin{array}{rr}
1 & 1 \\
-1 & 1
\end{array}\right]
\end{aligned}
$$



$$
\begin{aligned}
& =\left[\begin{array}{rr}
1 & -1 \\
1 & 1
\end{array}\right]\left[\begin{array}{ll}
0 & 2 \\
0 & 2
\end{array}\right] \\
& =\left[\begin{array}{ll}
0 & 0 \\
0 & 4
\end{array}\right] .
\end{aligned}
$$

What this means is that if we express the vectors $\alpha$ and $\beta$ by means of their coordinates in the basis $\mathbb{B}^{\prime}$, say

then

$$
\alpha=x_{1}^{\prime} \epsilon_{1}^{\prime}+x_{2}^{\prime} \epsilon_{2}^{\prime}, \quad \beta=y_{1}^{\prime} \epsilon_{1}^{\prime}+y_{2}^{\prime} \epsilon_{2}^{\prime}
$$

$$
f(\alpha, \beta)=4 x_{2}^{\prime} y_{2}^{\prime} \text {. }
$$

One consequence of the change of basis formula $(10-4)$ is the following: If $A$ and $B$ are $n \times n$ matrices which represent the same bilinear form on $V$ in (possibly) different ordered bases, then $A$ and $B$ have the same rank. For, if $P$ is an invertible $n \times n$ matrix and $B=P^{t} A P$, it is evident that $A$ and $B$ have the same rank. This makes it possible to define the rank of a bilinear form on $V$ as the rank of any matrix which represents the form in an ordered basis for $V$.

It is desirable to give a more intrinsic definition of the rank of a bilinear form. This can be done as follows: Suppose $f$ is a bilinear form on the vector space $V$. If we fix a vector $\alpha$ in $V$, then $f(\alpha, \beta)$ is linear as a function of $\beta$. In this way, each fixed $\alpha$ determines a linear functional on $V$; let us denote this linear functional by $L_{f}(\alpha)$. To repeat, if $\alpha$ is a vector in $V$, then $L_{f}(\alpha)$ is the linear functional on $V$ whose value on any vector $\beta$ is $f(\alpha, \beta)$. This gives us a transformation $\alpha \rightarrow L_{j}(\alpha)$ from $V$ into the dual space $V^{*}$. Since

we see that

$$
f\left(c \alpha_{1}+\alpha_{2}, \beta\right)=c f\left(\alpha_{1}, \beta\right)+f\left(\alpha_{2}, \beta\right)
$$

$$
L_{j}\left(c \alpha_{1}+\alpha_{2}\right)=c L_{f}\left(\alpha_{1}\right)+L_{f}\left(\alpha_{2}\right)
$$

that is, $L_{f}$ is a linear transformation from $V$ into $V^{*}$.

In a similar manner, $f$ determines a linear transformation $R_{f}$ from $V$ into $V^{*}$. For each fixed $\beta$ in $V, f(\alpha, \beta)$ is linear as a function of $\alpha$. We define $R_{f}(\beta)$ to be the linear functional on $V$ whose value on the vector $\alpha$ is $f(\alpha, \beta)$.

Theorem 2. Let $\mathrm{f}$ be a bilinear form on the finite-dimensional vector space $\mathrm{V}$. Let $\mathrm{L}_{\mathrm{f}}$ and $\mathrm{R}_{\mathrm{f}}$ be the linear transformations from $\mathrm{V}$ into $\mathrm{V}^{*}$ defined by $\left(\mathrm{L}_{\mathrm{f}} \alpha\right)(\beta)=\mathrm{f}(\alpha, \beta)=\left(\mathrm{R}_{\mathrm{f}} \beta\right)(\alpha)$. Then rank $\left(\mathrm{L}_{\mathrm{f}}\right)=\operatorname{rank}\left(\mathrm{R}_{\mathrm{f}}\right)$.

Proof. One can give a 'coordinate free' proof of this theorem. Such a proof is similar to the proof (in Section 3.7) that the row-rank of a matrix is equal to its column-rank. So, here we shall give a proof which proceeds by choosing a coordinate system (basis) and then using the 'row-rank equals column-rank' theorem.

To prove rank $\left(L_{f}\right)=\operatorname{rank}\left(R_{f}\right)$, it will suffice to prove that $L_{f}$ and $R_{f}$ have the same nullity. Let $\leftrightarrow$ be an ordered basis for $V$, and let $A=[f]_{\leftrightarrow}$. If $\alpha$ and $\beta$ are vectors in $V$, with coordinate matrices $X$ and $Y$ in the ordered basis $\beta$, then $f(\alpha, \beta)=X^{t} A Y$. Now $R_{f}(\beta)=0$ means that $f(\alpha, \beta)=0$ for every $\alpha$ in $V$, i.e., that $X^{t} A Y=0$ for every $n \times 1$ matrix $X$. The latter condition simply says that $A Y=0$. The nullity of $R_{f}$ is therefore equal to the dimension of the space of solutions of $A Y=0$.

Similarly, $L_{f}(\alpha)=0$ if and only if $X^{t} A Y=0$ for every $n \times 1$ matrix $Y$. Thus $\alpha$ is in the null space of $L_{f}$ if and only if $X^{t} A=0$, i.e., $A^{t} X=0$. The nullity of $L_{f}$ is therefore equal to the dimension of the space of solutions of $A^{t} X=0$. Since the matrices $A$ and $A^{t}$ have the same columnrank, we see that

$$
\text { nullity }\left(L_{f}\right)=\operatorname{nullity}\left(R_{f}\right) \text {. }
$$

Definition. If $\mathrm{f}$ is a bilinear form on the finite-dimensional space $\mathrm{V}$, the rank of $\mathrm{f}$ is the integer $\mathrm{r}=\operatorname{rank}\left(\mathrm{L}_{\mathrm{f}}\right)=\operatorname{rank}\left(\mathrm{R}_{\mathrm{f}}\right)$.

Corollary 1. The rank of a bilinear form is equal to the rank of the matrix of the form in any ordered basis.

Corollary 2. If $\mathrm{f}$ is a bilinear form on the $\mathrm{n}$-dimensional vector space $\mathrm{V}$, the following are equivalent:

(a) $\operatorname{rank}(\mathrm{f})=\mathrm{n}$.

(b) For each non-zero $\alpha$ in $\mathrm{V}$, there is a $\beta$ in $\mathrm{V}$ such that $\mathrm{f}(\alpha, \beta) \neq 0$.

(c) For each non-zero $\beta$ in $\mathrm{V}$, there is an $\alpha$ in $\mathrm{V}$ such that $\mathrm{f}(\alpha, \beta) \neq 0$.

Proof. Statement (b) simply says that the null space of $L_{f}$ is the zero subspace. Statement (c) says that the null space of $R_{f}$ is the zero subspace. The linear transformations $L_{f}$ and $R_{f}$ have nullity 0 if and only if they have rank $n$, i.e., if and only if $\operatorname{rank}(f)=n$.

Definition. A bilinear form $\mathrm{f}$ on a vector space $\mathrm{V}$ is called nondegenerate (or non-singular) if it satisfies conditions (b) and (c) of Corollary 2.

If $V$ is finite-dimensional, then $f$ is non-degenerate provided $f$ satisfies any one of the three conditions of Corollary 2. In particular, $f$ is nondegenerate (non-singular) if and only if its matrix in some (every) ordered basis for $V$ is a non-singular matrix.

Example 5. Let $V=R^{n}$, and let $f$ be the bilinear form defined on $\alpha=\left(x_{1}, \ldots, x_{n}\right)$ and $\beta=\left(y_{1}, \ldots, y_{n}\right)$ by

$$
f(\alpha, \beta)=x_{1} y_{1}+\cdots+x_{n} y_{n} .
$$

Then $f$ is a non-degenerate bilinear form on $R^{n}$. The matrix of $f$ in the standard ordered basis is the $n \times n$ identity matrix:

$$
f(X, Y)=X^{t} Y .
$$

This $f$ is usually called the dot (or scalar) product. The reader is probably familiar with this bilinear form, at least in the case $n=3$. Geometrically, the number $f(\alpha, \beta)$ is the product of the length of $\alpha$, the length of $\beta$, and the cosine of the angle between $\alpha$ and $\beta$. In particular, $f(\alpha, \beta)=0$ if and only if the vectors $\alpha$ and $\beta$ are orthogonal (perpendicular).

\section{Exercises}

1. Which of the following functions $f$, defined on vectors $\alpha=\left(x_{1}, x_{2}\right)$ and $\beta=$ $\left(y_{1}, y_{2}\right)$ in $R^{2}$, are bilinear forms?
(a) $f(\alpha, \beta)=1$.
(b) $f(\alpha, \beta)=\left(x_{1}-y_{1}\right)^{2}+x_{2} y_{2}$.
(c) $f(\alpha, \beta)=\left(x_{1}+y_{1}\right)^{2}-\left(x_{1}-y_{1}\right)^{2}$.
(d) $f(\alpha, \beta)=x_{1} y_{2}-x_{2} y_{1}$.

2. Let $f$ be the bilinear form on $R^{2}$ defined by

$$
f\left(\left(x_{1}, y_{1}\right),\left(x_{2}, y_{2}\right)\right)=x_{1} y_{1}+x_{2} y_{2} \text {. }
$$

Find the matrix of $f$ in each of the following bases:

$$
\{(1,0),(0,1)\}, \quad\{(1,-1),(1,1)\}, \quad\{(1,2),(3,4)\} \text {. }
$$

3. Let $V$ be the space of all $2 \times 3$ matrices over $R$, and let $f$ be the bilinear form on $V$ defined by $f(X, Y)=\operatorname{trace}\left(X^{t} A Y\right)$, where

$$
A=\left[\begin{array}{ll}
1 & 2 \\
3 & 4
\end{array}\right] .
$$

Find the matrix of $f$ in the ordered basis

$$
\left\{E^{11}, E^{12}, E^{13}, E^{21}, E^{22}, E^{23}\right\}
$$

where $E^{i i}$ is the matrix whose only non-zero entry is a 1 in row $i$ and column $j$.

4. Describe explicitly all bilinear forms $f$ on $R^{3}$ with the property that $f(\alpha, \beta)=$ $f(\beta, \alpha)$ for all $\alpha, \beta$.

5. Describe the bilinear forms on $R^{3}$ which satisfy $f(\alpha, \beta)=-f(\beta, \alpha)$ for all $\alpha, \beta$.

6. Let $n$ be a positive integer, and let $V$ be the space of all $n \times n$ matrices over the field of complex numbers. Show that the equation

$$
f(A, B)=n \operatorname{tr}(A B)-\operatorname{tr}(A) \operatorname{tr}(B)
$$

defines a bilinear form $f$ on $V$. Is it true that $f(A, B)=f(B, A)$ for all $A, B$ ?

7. Let $f$ be the bilinear form defined in Exercise 6. Show that $f$ is degenerate (not non-degenerate). Let $V_{1}$ be the subspace of $V$ consisting of the matrices of trace 0 , and let $f_{1}$ be the restriction of $f$ to $V_{1}$. Show that $f_{1}$ is non-degenerate. 8. Let $f$ be the bilinear form defined in Exercise 6, and let $V_{2}$ be the subspace of $V$ consisting of all matrices $A$ such that trace $(A)=0$ and $A^{*}=-A\left(\Lambda^{*}\right.$ is the conjugate transpose of $A$ ). Denote by $f_{2}$ the restrietion of $f$ to $V_{2}$. Show that $f_{2}$ is negative definite, i.e., that $f_{2}(A, A)<0$ for each non-zero $A$ in $V_{2}$.

9. Let $f$ be the bilinear form defined in Exercise 6 . Let $W$ be the set of all matrices $A$ in $V$ such that $f(A, B)=0$ for all $B$. Show that $W$ is a subspace of $V$. Describe $W$ explicitly and find its dimension.

10. Let $f$ be any bilinear form on a finite-dimensional vector space $V$. Let $W$ be the subspace of all $\beta$ such that $f(\alpha, \beta)=0$ for every $\alpha$. Show that

$$
\operatorname{rank} f=\operatorname{dim} V-\operatorname{dim} W \text {. }
$$

Use this result and the result of Exercise 9 to compute the rank of the bilinear form defined in Exercise 6.

11. Let $f$ be a bilinear form on a finite-dimensional vector space $V$. Suppose $V_{1}$ is a subspace of $V$ with the property that the restriction of $f$ to $V_{1}$ is non-degenerate. Show that rank $f \geq \operatorname{dim} V_{1}$.

12. Let $f, g$ be bilinear forms on a finite-dimensional vector space $V$. Suppose $g$ is non-singular. Show that there exist unique linear operators $T_{1}, T_{2}$ on $V$ such that

for all $\alpha, \beta$.

$$
f(\alpha, \beta)=g\left(T_{1} \alpha, \beta\right)=g\left(\alpha, T_{2} \beta\right)
$$

13. Show that the result given in Exercise 12 need not be true if $g$ is singular.

14. Let $f$ be a bilinear form on a finite-dimensional vector space $V$. Show that $f$ can be expressed as a product of two linear functionals (i.e., $f(\alpha, \beta)=L_{1}(\alpha) L_{2}(\beta)$ for $L_{1}, L_{2}$ in $V^{*}$ ) if and only if $f$ has rank 1.

\subsection{Symmetric Bilinear Forms}

The main purpose of this section is to answer the following question: If $f$ is a bilinear form on the finite-dimensional vector space $V$, when is there an ordered basis $B$ for $V$ in which $f$ is represented by a diagonal matrix? We prove that this is possible if and only if $f$ is a symmetric bilinear form, i.e., $f(\alpha, \beta)=f(\beta, \alpha)$. The theorem is proved only when the scalar field has characteristic zero, that is, that if $n$ is a positive integer the sum $1+\cdots+1$ ( $n$ times) in $F$ is not 0 .

Definition. Let $\mathrm{f}$ be a bilinear form on the vector space $\mathrm{V}$. We say that $\mathrm{f}$ is symmetric if $\mathrm{f}(\alpha, \beta)=\mathrm{f}(\beta, \alpha)$ for all vectors $\alpha, \beta$ in $\mathrm{V}$.

If $V$ is a finite-dimensional, the bilinear form $f$ is symmetric if and only if its matrix $A$ in some (or every) ordered basis is symmetric, $A^{t}=A$. To see this, one inquires when the bilinear form

$$
f(X, Y)=X^{t} A Y
$$

is symmetric. This happens if and only if $X^{t} A Y=Y^{t} A X$ for all column matrices $X$ and $Y$. Since $X^{t} A Y$ is a $1 \times 1$ matrix, we have $X^{t} A Y=Y^{t} A^{t} X$. Thus $f$ is symmetric if and only if $Y^{t} A^{t} X=Y^{t} A X$ for all $X, Y$. Clearly this just means that $A=A^{t}$. In particular, one should note that if there is an ordered basis for $V$ in which $f$ is represented by a diagonal matrix, then $f$ is symmetric, for any diagonal matrix is a symmetric matrix.

If $f$ is a symmetric bilinear form, the quadratic form associated with $f$ is the function $q$ from $V$ into $F$ defined by

$$
q(\alpha)=f(\alpha, \alpha) .
$$

If $F$ is a subfield of the complex numbers, the symmetric bilinear form $f$ is completely determined by its associated quadratic form, according to the polarization identity

$$
f(\alpha, \beta)=\frac{1}{4} q(\alpha+\beta)-\frac{1}{4} q(\alpha-\beta) .
$$

The establishment of (10-5) is a routine computation, which we omit. If $f$ is the bilinear form of Example 5, the dot product, the associated quadratic form is

$$
q\left(x_{1}, \ldots, x_{n}\right)=x_{1}^{2}+\cdots+x_{n}^{2} .
$$

In other words, $q(\alpha)$ is the square of the length of $\alpha$. For the bilinear form $f_{A}(X, Y)=X^{t} A Y$, the associated quadratic form is

$$
q_{A}(X)=X^{t} A X=\sum_{i, j} A_{i j} x_{i} x_{j} .
$$

One important class of symmetric bilinear forms consists of the inner products on real vector spaces, discussed in Chapter 8. If $V$ is a real vector space, an inner product on $V$ is a symmetric bilinear form $f$ on $V$ which satisfies

$$
f(\alpha, \alpha)>0 \text { if } \alpha \neq 0 .
$$

A bilinear form satisfying (10-6) is called positive definite. Thus, an inner product on a real vector space is a positive definite, symmetric bilinear form on that space. Note that an inner product is non-degenerate. Two vectors $\alpha, \beta$ are called orthogonal with respect to the inner product $f$ if $f(\alpha, \beta)=0$. The quadratic form $q(\alpha)=f(\alpha, \alpha)$ takes only non-negative values, and $q(\alpha)$ is usually thought of as the square of the length of $\alpha$. Of course, these concepts of length and orthogonality stem from the most important example of an inner product-the dot product of Example 5.

If $f$ is any symmetric bilinear form on a vector space $V$, it is convenient to apply some of the terminology of inner products to $f$. It is especially convenient to say that $\alpha$ and $\beta$ are orthogonal with respect to $f$ if $f(\alpha, \beta)=0$. It is not advisable to think of $f(\alpha, \alpha)$ as the square of the length of $\alpha$; for example, if $V$ is a complex vector space, we may have $f(\alpha, \alpha)=\sqrt{-1}$, or on a real vector space, $f(\alpha, \alpha)=-2$.

We turn now to the basic theorem of this section. In reading the proof, the reader should find it helpful to think of the special case in which $V$ is a real vector space and $f$ is an inner product on $V$.

Theorem 3. Let $\mathrm{V}$ be a finite-dimensional vector space over a field of characteristic zero, and let $\mathrm{f}$ be a symmetric bilinear form on $\mathrm{V}$. Then there is an ordered basis for $\mathrm{V}$ in which $\mathrm{f}$ is represented by a diagonal matrix.

Proof. What we must find is an ordered basis

$$
B=\left\{\alpha_{1}, \ldots, \alpha_{n}\right\}
$$

such that $f\left(\alpha_{i}, \alpha_{j}\right)=0$ for $i \neq j$. If $f=0$ or $n=1$, the theorem is obviously true. Thus we may suppose $f \neq 0$ and $n>1$. If $f(\alpha, \alpha)=0$ for every $\alpha$ in $V$, the associated quadratic form $q$ is identically 0 , and the polarization identity (10-5) shows that $f=0$. Thus there is a vector $\alpha$ in $V$ such that $f(\alpha, \alpha)=q(\alpha) \neq 0$. Let $W$ be the one-dimensional subspace of $V$ which is spanned by $\alpha$, and let $W^{\lrcorner}$be the set of all vectors $\beta$ in $V$ such that $f(\alpha, \beta)=0$. Now we claim that $V=W \oplus W^{\lrcorner}$. Certainly the subspaces $W$ and $W^{\perp}$ are independent. A typical vector in $W$ is $c \alpha$, where $c$ is a scalar. If $c \alpha$ is also in $W^{\cdot\llcorner}$, then $f(c \alpha, c \alpha)=c^{2} f(\boldsymbol{\alpha}, \alpha)=0$. Hut $f(\alpha, \alpha) \neq 0$, thus $c=0$. Also, each vector in $V$ is the sum of a vector in $W$ and a vector in $W^{\perp \perp}$. For, let $\gamma$ be any vector in $V$, and put

$$
\beta=\gamma-\frac{f(\gamma, \alpha)}{f(\alpha, \alpha)} \alpha .
$$

Then

$$
f(\alpha, \beta)=f(\alpha, \gamma)-\frac{f(\gamma, \alpha)}{f(\alpha, \alpha)} f(\alpha, \alpha)
$$

and since $f$ is symmetric, $f(\alpha, \beta)=0$. Thus $\beta$ is in the subspace $W^{\lrcorner}$. The expression

$$
\gamma=\frac{f(\gamma, \alpha)}{f(\alpha, \alpha)} \alpha+\beta
$$

shows us that $V=W+W \cdot$.

The restriction of $f$ to $W^{\llcorner}$is a symmetric bilinear form on $W^{\lrcorner}$. Since $W^{\llcorner}$has dimension $(n-1)$, we may assume by induction that $W^{\circ\llcorner}$ has a basis $\left\{\alpha_{2}, \ldots, \alpha_{n}\right\}$ such that

$$
f\left(\alpha_{i}, \alpha_{j}\right)=0, \quad i \neq j(i \geq 2, j \geq 2) .
$$

Putting $\alpha_{1}=\alpha$, we obtain a basis $\left\{\alpha_{1}, \ldots, \alpha_{n}\right\}$ for $V$ such that $f\left(\alpha_{i}, \alpha_{j}\right)=0$ for $i \neq j$.

Corollary. Let $\mathrm{F}$ be a subfield of the complex numbers, and let $\mathrm{A}$ be a symmetric $\mathrm{n} \times \mathrm{n}$ matrix over $\mathrm{F}$. Then there is an invertible $\mathrm{n} \times \mathrm{n}$ matrix $\mathrm{P}$ over $\mathrm{F}$ such that $\mathrm{P}^{\mathrm{t}} \mathrm{AP}$ is diagonal.

In case $F$ is the field of real numbers, the invertible matrix $P$ in this corollary can be chosen to be an orthogonal matrix, i.e., $P^{t}=P^{-1}$. In other words, if $A$ is a real symmetric $n \times n$ matrix, there is a real orthogonal matrix $P$ such that $P^{t} A P$ is diagonal; however, this is not at all apparent from what we did above (see Chapter 8).

Theorem 4. Let $\mathrm{V}$ be a finite-dimensional vector space over the field of complex numbers. Let $\mathrm{f}$ be a symmetric bilinear form on $\mathrm{V}$ which has rank $\mathrm{r}$. Then there is an ordered basis $\mathbb{B}=\left\{\beta_{1}, \ldots, \boldsymbol{\beta}_{\mathrm{n}}\right\}$ for $\mathrm{V}$ such that

(i) the matrix of $\mathrm{f}$ in the ordered basis $B$ is diagonal;

(ii) $\mathrm{f}\left(\beta_{\mathrm{j}}, \beta_{\mathrm{j}}\right)= \begin{cases}1, & \mathrm{j}=1, \ldots, \mathrm{r} \\ 0, & \mathrm{j}>\mathrm{r}\end{cases}$

Proof. By Theorem 3 , there is an ordered basis $\left\{\alpha_{1}, \ldots, \alpha_{n}\right\}$ for $V$ such that

$$
f\left(\alpha_{i}, \alpha_{j}\right)=0 \text { for } i \neq j .
$$

Since $f$ has rank $r$, so does its matrix in the ordered basis $\left\{\alpha_{1}, \ldots, \alpha_{n}\right\}$. Thus we must have $f\left(\alpha_{j}, \alpha_{j}\right) \neq 0$ for precisely $r$ values of $j$. By reordering the vectors $\alpha_{j}$, we may assume that

$$
f\left(\alpha_{j}, \alpha_{j}\right) \neq 0, \quad j=1, \ldots, r .
$$

Now we use the fact that the scalar field is the field of complex numbers. If $\sqrt{f\left(\alpha_{j}, \alpha_{j}\right)}$ denotes any complex square root of $f\left(\alpha_{j}, \alpha_{j}\right)$, and if we put

$$
\beta_{j}=\left\{\begin{array}{l}
\frac{1}{\sqrt{f\left(\alpha_{j}, \alpha_{j}\right)}} \alpha_{j}, \quad j=1, \ldots, r \\
\alpha_{j}, \quad j>r
\end{array}\right.
$$

the basis $\left\{\beta_{1}, \ldots, \beta_{n}\right\}$ satisfies conditions (i) and (ii).

Of course, Theorem 4 is valid if the scalar field is any subfield of the complex numbers in which each element has a square root. It is not valid, for example, when the scalar field is the field of real numbers. Over the field of real numbers, we have the following substitute for Theorem 4.

Theorem 5. Let $\mathrm{V}$ be an $\mathrm{n}$-dimensional vector space over the field of real numbers, and let $\mathrm{f}$ be a symmetric bilinear form on $\mathrm{V}$ which has rank $\mathrm{r}$. Then there is an ordered basis $\left\{\boldsymbol{\beta}_{1}, \beta_{2}, \ldots, \beta_{n}\right\}$ for $\mathrm{V}$ in which the matrix of $\mathrm{f}$ is diagonal and such that

$$
\mathrm{f}\left(\beta_{\mathrm{j}}, \beta_{\mathrm{j}}\right)=\pm 1, \quad \mathrm{j}=1, \ldots, \mathrm{r} .
$$

Furthermore, the number of basis vectors $\beta_{\mathrm{j}}$ for which $\mathrm{f}\left(\beta_{\mathrm{j}}, \beta_{\mathrm{j}}\right)=1$ is independent of the choice of basis.

Proof. There is a basis $\left\{\alpha_{1}, \ldots, \alpha_{n}\right\}$ for $V$ such that

$$
\begin{array}{ll}
f\left(\alpha_{i}, \alpha_{j}\right)=0, & i \neq j \\
f\left(\alpha_{j}, \alpha_{j}\right) \neq 0, & 1 \leq j \leq r \\
f\left(\alpha_{j}, \alpha_{j}\right)=0, & j>r .
\end{array}
$$

Let

$$
\begin{aligned}
& \beta_{j}=\left|f\left(\alpha_{j}, \alpha_{j}\right)\right|^{-1 / 2} \alpha_{j}, \quad 1 \leq j \leq r \\
& \beta_{j}=\alpha_{j}, \quad j>r .
\end{aligned}
$$

Then $\left\{\beta_{1}, \ldots, \beta_{n}\right\}$ is a basis with the stated properties.

Let $p$ be the number of basis vectors $\beta_{j}$ for which $f\left(\beta_{j}, \beta_{j}\right)=1$; we must show that the number $p$ is independent of the particular basis we have, satisfying the stated conditions. Let $V^{+}$be the subspace of $V$ spanned by the basis vectors $\beta_{j}$ for which $f\left(\beta_{j}, \beta_{j}\right)=1$, and let $V^{-}$be the subspace spanned by the basis vectors $\beta_{j}$ for which $f\left(\beta_{j}, \beta_{j}\right)=-1$. Now $p=\operatorname{dim} V^{+}$, so it is the uniqueness of the dimension of $V^{+}$which we must demonstrate. It is easy to see that if $\alpha$ is a non-zero vector in $V^{+}$, then $f(\alpha, \alpha)>0$; in other words, $f$ is positive definite on the subspace $V^{+}$. Similarly, if $\alpha$ is a non-zero vector in $V^{-}$, then $f(\alpha, \alpha)<0$, i.e., $f$ is negative definite on the subspace $V^{-}$. Now let $V^{\perp}$ be the subspace spanned by the basis vectors $\beta_{j}$ for which $f\left(\beta_{j}, \beta_{j}\right)=0$. If $\alpha$ is in $V^{\perp}$, then $f(\alpha, \beta)=0$ for all $\beta$ in $V$.

Since $\left\{\beta_{1}, \ldots, \beta_{n}\right\}$ is a basis for $V$, we have

$$
V=V^{+} \oplus V^{-} \oplus V^{\perp} \text {. }
$$

Furthermore, we claim that if $W$ is any subspace of $V$ on which $f$ is positive definite, then the subspaces $W, V^{-}$, and $V^{\perp}$ are independent. For, suppose $\alpha$ is in $W, \beta$ is in $V^{-}, \gamma$ is in $V^{\perp}$, and $\alpha+\beta+\gamma=0$. Then

$$
\begin{aligned}
& 0=f(\alpha, \alpha+\beta+\gamma)=f(\alpha, \alpha)+f(\alpha, \beta)+f(\alpha, \gamma) \\
& 0=f(\beta, \alpha+\beta+\gamma)=f(\beta, \alpha)+f(\beta, \beta)+f(\beta, \gamma) .
\end{aligned}
$$

Since $\gamma$ is in $V^{\perp}, f(\alpha, \gamma)=f(\beta, \gamma)=0$; and since $f$ is symmetric, w e obtain

$$
\begin{aligned}
& 0=f(\alpha, \alpha)+f(\alpha, \beta) \\
& 0=f(\beta, \beta)+f(\alpha, \beta)
\end{aligned}
$$

hence $f(\alpha, \alpha)=f(\beta, \beta)$. Since $f(\alpha, \alpha) \geq 0$ and $f(\beta, \beta) \leq 0$, it follows that

$$
f(\alpha, \alpha)=f(\beta, \beta)=0 .
$$

But $f$ is positive definite on $W$ and negative definite on $V^{-}$. We conclude that $\alpha=\beta=0$, and hence that $\gamma=0$ as well.

Since

$$
V=V^{+} \oplus V^{-} \oplus V^{\perp}
$$

and $W, V^{-}, V^{\perp}$ are independent, we see that $\operatorname{dim} W \leq \operatorname{dim} V^{+}$. That is, if $W$ is any subspace of $V$ on which $f$ is positive definite, the dimension of $W$ cannot exceed the dimension of $V^{+}$. If $B_{1}$ is another ordered basis for $V$ which satisfies the conditions of the theorem, we shall have corresponding subspaces $V_{1}^{+}, V_{1}^{-}$, and $V_{1}^{\perp}$; and, the argument above shows that $\operatorname{dim} V_{1}^{+} \leq \operatorname{dim} V^{+}$. Reversing the argument, we obtain $\operatorname{dim} V^{+} \leq$ $\operatorname{dim} V_{1}^{+}$, and consequently

$$
\operatorname{dim} V^{+}=\operatorname{dim} V_{1}^{+} .
$$

There are several comments we should make about the basis $\left\{\beta_{1}, \ldots, \beta_{n}\right\}$ of Theorem 5 and the associated subspaces $V^{+}, V^{-}$, and $V^{\perp}$ First, note that $V^{\perp}$ is exactly the subspace of vectors which are 'orthogonal' to all of $V$. We noted above that $V^{\circ \perp}$ is contained in this subspace; but,

$$
\operatorname{dim} V^{\perp}=\operatorname{dim} V-\left(\operatorname{dim} V^{+}+\operatorname{dim} V^{-}\right)=\operatorname{dim} V-\operatorname{rank} f
$$

so every vector $\alpha$ such that $f(\alpha, \beta)=0$ for all $\beta$ must be in $V^{. \perp}$. Thus, the subspace $V^{\perp}$ is unique. The subspaces $V^{+}$and $V^{-}$are not unique; however, their dimensions are unique. The proof of Theorem 5 shows us that dim $\mathrm{V}^{+}$is the largest possible dimension of any subspace on which $f$ is positive definite. Similarly, dim $V^{-}$is the largest dimension of any subspace on which $f$ is negative definite. Of course

$$
\operatorname{dim} V^{+}+\operatorname{dim} V^{-}=\operatorname{rank} f .
$$

The number

$$
\operatorname{dim} V^{+}-\operatorname{dim} V^{-}
$$

is of ten called the signature of $f$. It is introduced because the dimensions of $V^{+}$and $V^{-}$are easily determined from the rank of $f$ and the signature of $f$.

Perhaps we should make one final comment about the relation of symmetric bilinear forms on real vector spaces to inner products. Suppose $V$ is a finite-dimensional real vector space and that $V_{1}, V_{2}, V_{3}$ are subspaces of $V$ such that

$$
V=V_{1} \oplus V_{2} \oplus V_{3} .
$$

Suppose that $f_{1}$ is an inner product on $V_{1}$, and $f_{2}$ is an inner product on $V_{2}$. We can then define a symmetric bilinear form $f$ on $V$ as follows: If $\alpha, \beta$ are vectors in $V$, then we can write

$$
\alpha=\alpha_{1}+\alpha_{2}+\alpha_{3} \text { and } \beta=\beta_{1}+\beta_{2}+\beta_{3}
$$

with $\alpha_{j}$ and $\beta_{j}$ in $V_{j}$. Let

$$
f(\alpha, \beta)=f_{1}\left(\alpha_{1}, \beta_{1}\right)-f_{2}\left(\alpha_{2}, \beta_{2}\right) .
$$

The subspace $V^{\perp}$ for $f$ will be $V_{3}, V_{1}$ is a suitable $V^{+}$for $f$, and $V_{2}$ is a suitable $V^{-}$. One part of the statement of Theorem 5 is that every symmetric bilinear form on $V$ arises in this way. The additional content of the theorem is that an inner product is represented in some ordered basis by the identity matrix.

\section{Exercises}

1. The following expressions define quadratic forms $q$ on $R^{2}$. Find the symmetric bilinear form $f$ corresponding to each $q$. 
(a) $a x_{1}^{2}$
(e) $x_{1}^{2}+9 x_{2}^{2}$
(b) $b x_{1} x_{2}$
(f) $3 x_{1} x_{2}-x_{2}^{2}$.
(c) $c x_{2}^{2}$.
(g) $4 x_{1}^{2}+6 x_{1} x_{2}-3 x_{2}^{2}$
(d) $2 x_{1}^{2}-\frac{1}{3} x_{1} x_{2}$

2. Find the matrix, in the standard ordered basis, and the rank of each of the bilinear forms determined in Exercise 1. Indicate which forms are non-degenerate.

3. Let $q\left(x_{1}, x_{2}\right)=a x_{1}^{2}+b x_{1} x_{2}+c x_{2}^{2}$ be the quadratic form associated with a symmetric bilinear form $f$ on $R^{2}$. Show that $f$ is non-degenerate if and only if $b^{2}-4 a c \neq 0$.

4. Let $V$ be a finite-dimensional vector space over a subfield $F$ of the complex numbers, and let $S$ be the set of all symmetric bilinear forms on $V$.

(a) Show that $S$ is a subspace of $L(V, V, F)$.

(b) Find $\operatorname{dim} S$.

Let $Q$ be the set of all quadratic forms on $V$.

(c) Show that $Q$ is a subspace of the space of all functions from $V$ into $F$.

(d) Describe explicitly an isomorphism $T$ of $Q$ onto $S$, without reference to a basis.

(e) Let $U$ be a linear operator on $V$ and $q$ an element of $Q$. Show that the equation $\left(U^{\dagger} g\right)(\alpha)=q(U \alpha)$ defines a quadratic form $U^{\dagger} q$ on $V$.

(f) If $U$ is a linear operator on $V$, show that the function $U^{\dagger}$ defined in part (e) is a linear operator on $Q$. Show that $U^{\dagger}$ is invertible if and only if $U$ is invertible.

5. Let $q$ be the quadratic form on $R^{2}$ given by

$$
q\left(x_{1}, x_{2}\right)=a x_{1}^{2}+2 b x_{1} x_{2}+c x_{2}^{2}, \quad a \neq 0 .
$$

Find an invertible linear operator $U$ on $R^{2}$ such that

$$
\left(U^{\dagger} q\right)\left(x_{1}, x_{2}\right)=a x_{1}^{2}+\left(c-\frac{b^{2}}{a}\right) x_{2}^{2} .
$$

(Hint: To find $U^{-1}$ (and hence $U$ ), complete the square. For the definition of $U^{\dagger}$, see part (e) of Exercise 4.)

6. Let $q$ be the quadratic form on $R^{2}$ given by

$$
q\left(x_{1}, x_{2}\right)=2 b x_{1} x_{2} .
$$

Find an invertible linear operator $U$ on $R^{2}$ such that

$$
\left(U^{\dagger} q\right)\left(x_{1}, x_{2}\right)=2 b x_{1}^{2}-2 b x_{2}^{2} .
$$

7. Let $q$ be the quadratic form on $R^{3}$ given by

$$
q\left(x_{1}, x_{2}, x_{3}\right)=x_{1} x_{2}+2 x_{1} x_{3}+x_{3}^{2} .
$$

Find an invertible linear operator $U$ on $R^{3}$ such that

$$
\left(U^{\dagger} q\right)\left(x_{1}, x_{2}, x_{3}\right)=x_{1}^{2}-x_{2}^{2}+x_{3}^{2} .
$$

(Hint: Express $U$ as a product of operators similar to those used in Exercises 5 and 6.) 8. Let $A$ be a symmetric $n \times n$ matrix over $R$, and let $q$ be the quadratic form on $R^{n}$ given by

$$
q\left(x_{1}, \ldots, x_{n}\right)=\sum_{i, j} A_{i j} x_{i} x_{j} .
$$

Generalize the method used in Exercise 7 to show that there is an invertible linear operator $U$ on $\boldsymbol{R}^{n}$ such that

where $c_{i}$ is $1,-1$, or $0, i=1, \ldots, n$.

$$
\left(U^{\dagger} \varphi\right)\left(x_{1}, \ldots, x_{n}\right)=\sum_{i=1}^{n} c_{i} x_{i}^{2}
$$

9. Let $f$ be a symmetric bilinear form on $R^{n}$. Use the result of Exercise 8 to prove the existence of af ordered basis $\mathbb{B}$ such that $[f]_{\mathbb{Q}}$ is diagonal.

10. Let $V$ be the real vector space of all $2 \times 2$ (complex) Hermitian matrices, that is, $2 \times 2$ complex matrices $A$ which satisfy $A_{i j}=\bar{A}_{i \text { i }}$.

(a) Show that the equation $q(A)=\operatorname{det} A$ defines a quadratic form $q$ on $V$.

(b) Let $W$ be the subspace of $V$ of matrices of trace 0 . Show that the bilinear form $f$ determined by $q$ is negative definite on the subspace $W$.

11. Let $V$ be a finite-dimensional vector space and $f$ a non-degenerate symmetric bilinear form on $V$. Show that for each linear operator $T$ on $V$ there is a unique linear operator $T^{\prime}$ on $V$ such that $f(T \alpha, \beta)=f\left(\alpha, T^{\prime} \beta\right)$ for all $\alpha, \beta$ in $V$. Also show that

$$
\begin{aligned}
\left(T_{1} T_{2}\right)^{\prime} & =T_{2}^{\prime} T_{1}^{\prime} \\
\left(c_{1} T_{1}+c_{2} T_{2}\right)^{\prime} & =c_{1} T_{1}^{\prime}+c_{2} T_{2}^{\prime} \\
\left(T^{\prime}\right)^{\prime} & =T .
\end{aligned}
$$

How much of the above is valid without the assumption that $T$ is non-degenerate?

12. Let $F$ be a field and $V$ the space of $n \times 1$ matrices over $F$. Suppose $A$ is a fixed $n \times n$ matrix over $F$ and $f$ is the bilinear form on $V$ defined by $f(X, Y)=$ $X^{t} A Y$. Suppose $f$ is symmetric and non-degenerate. Let $B$ be an $n \times n$ matrix over $F$ and $T$ the linear operator on $V$ sending $X$ into $B X$. Find the operator $T^{\prime}$ of Exercise 11.

13. Let $V$ be a finite-dimensional vector space and $f$ a non-degenerate symmetric bilinear form on $V$. Associated with $f$ is a 'natural' isomorphism of $V$ onto the dual space $V^{*}$, this isomorphism being the transformation $L_{f}$ of Section 10.1. Using $L_{f}$, show that for each basis $B=\left\{\alpha_{1}, \ldots, \alpha_{n}\right\}$ of $V$ there exists a unique basis $\mathbb{B}^{\prime}=\left\{\alpha_{1}^{\prime}, \ldots, \alpha_{n}^{\prime}\right\}$ of $V$ such that $f\left(\alpha_{i}, \alpha_{i}^{\prime}\right)=\delta_{i j}$. Then show that for every vector $\alpha$ in $V$ we have

$$
\alpha=\sum_{i} f\left(\alpha, \alpha_{i}^{\prime}\right) \alpha_{i}=\sum_{i} f\left(\alpha_{i}, \alpha\right) \alpha_{i}^{\prime} .
$$

14. Let $V, f, \mathbb{Q}$, and $\mathbb{B}^{\prime}$ be as in Exercise 13. Suppose $T$ is a linear operator on $V$ and that $T^{\prime}$ is the operator which $f$ associates with $T$ as in Exercise 11. Show that

(a) $\left[T^{\prime}\right]_{Q^{\prime}}=[T]_{Q^{\prime}}^{t}$.

(b) $\operatorname{tr}(T)=\operatorname{tr}\left(T^{\prime \prime}\right)=\sum_{i}^{\Sigma} f\left(T \alpha_{i}, \alpha_{i}^{\prime}\right)$.

15. Let $V, f, \mathbb{B}$, and $B^{\prime}$ be as in Exercise 13. Suppose $[f]_{\mathbb{B}}=A$. Show that

$$
\alpha_{i}^{\prime}=\sum_{j}\left(A^{-1}\right)_{i j} \alpha_{j}=\sum_{j}\left(A^{-1}\right)_{i i} \alpha_{j} .
$$

16. Let $F$ be a field and $V$ the space of $n \times 1$ matrices over $F$. Suppose $A$ is an invertible, symmetric $n \times n$ matrix over $F$ and that $f$ is the bilinear form on $V$ defined by $f(X, Y)=X^{t} A Y$. Let $P$ be an invertible $n \times n$ matrix over $F$ and $B$ the basis for $V$ consisting of the columns of $P$. Show that the basis $B^{\prime}$ of Fxercise 13 consists of the columns of the matrix $A^{-1}\left(P^{t}\right)^{-1}$.

17. Let $V$ be a finite-dimensional vector space over a field $F$ and $f$ a symmetric bilinear form on $V$. For each subspace $W$ of $V$, let $W^{\perp}$ be the set of all vectors $\alpha$ in $V$ such that $f(\alpha, \beta)=0$ for every $\beta$ in $W$. Show that

(a) $W^{\perp}$ is a subspace.

(b) $V=\{0\}^{\cdot L}$.

(c) $V^{\perp}=\{0\}$ if and only if $f$ is non-degenerate.

(d) $\operatorname{rank} f=\operatorname{dim} V-\operatorname{dim} V^{\perp}$.

(e) If $\operatorname{dim} V=n$ and $\operatorname{dim} W=m$, then $\operatorname{dim} W^{\perp} \geq n-m$. (Hint: Let $\left\{\beta_{1}, \ldots, \beta_{m}\right\}$ be a basis of $W$ and consider the mapping

of $V$ into $F^{m}$.)

$$
\alpha \rightarrow\left(f\left(\alpha, \beta_{1}\right), \ldots, f\left(\alpha, \beta_{m}\right)\right)
$$

(f) The restriction of $f$ to $W$ is non-degenerate if and only if

$$
W \cap W^{\perp}=\{0\} \text {. }
$$

(g) $V=W \oplus W^{\perp}$ if and only if the restriction of $f$ to $W$ is non-degenerate.

18. Let $V$ be a finite-dimensional vector space over $C$ and $f$ a non-degenerate symmetric bilinear form on $V$. Prove that there is a basis $B$ of $V$ such that $\Omega^{\prime}=B$. (Se. Exercise 13 for a definition of $\mathrm{B}^{\prime}$.)

\subsection{Skew-Symmetric Bilinear Forms}

Throughout this section $V$ will be a vector space over a subfield $F$ of the field of complex numbers. A bilinear form $f$ on $V$ is called skewsymmetric if $f(\alpha, \beta)=-f(\beta, \alpha)$ for all vectors $\alpha, \beta$ in $V$. We shall prove one theorem concerning the simplification of the matrix of a skewsymmetric bilinear form on a finite-dimensional space $V$. First, let us make some general observations.

Suppose $f$ is any bilinear form on $V$. If we let

$$
\begin{aligned}
& g(\alpha, \beta)=\frac{1}{2}[f(\alpha, \beta)+f(\beta, \alpha)] \\
& h(\alpha, \beta)=\frac{1}{2}[f(\alpha, \beta)-f(\beta, \alpha)]
\end{aligned}
$$

then it is easy to verify that $g$ is a symmetric bilinear form on $V$ and $h$ is a skcw-symmetric bilinear form on $V$. Also $f=g+h$. Furthermore, this expression for $V$ as the sum of a symmetric and a skew-symmetric form is unique. Thus, the space $L(V, V, F)$ is the direct sum of the subspace of symmetric forms and the subspace of skew-symmetric forms.

If $V$ is finite-dimensional, the bilinear form $f$ is skew-symmetric if and only if its matrix $A$ in some (or every) ordered basis is skew-symmetric, $A^{t}=-A$. This is proved just as one proves the corresponding fact about symmetric bilinear forms. When $f$ is skew-symmetric, the matrix of $f$ in any ordered basis will have all its diagonal entries 0 . This just corresponds to the observation that $f(\alpha, \alpha)=0$ for every $\alpha$ in $V$, since $f(\alpha, \alpha)=$ $-f(\alpha, \alpha)$.

Let us suppose $f$ is a non-zero skew-symmetric bilinear form on $V$. Since $f \neq 0$, there are vectors $\alpha, \beta$ in $V$ such that $f(\alpha, \beta) \neq 0$. Multiplying $\alpha$ by a suitable scalar, we may assume that $f(\alpha, \beta)=1$. Let $\gamma$ be any vector in the subspace spanned by $\alpha$ and $\beta$, say $\gamma=c \alpha+d \beta$. Then

$$
\begin{gathered}
f(\gamma, \alpha)=f(c \alpha+d \beta, \alpha)=d f(\beta, \alpha)=-d \\
f(\gamma, \beta)=f(c \alpha+d \beta, \beta)=c f(\alpha, \beta)=c
\end{gathered}
$$

and so

$$
\gamma=f(\gamma, \beta) \alpha-f(\gamma, \alpha) \beta .
$$

In particular, note that $\alpha$ and $\beta$ are necessarily linearly independent; for, if $\gamma=0$, then $f(\gamma, \alpha)=f(\gamma, \beta)=0$.

Let $W$ be the two-dimensional subspace spanned by $\alpha$ and $\beta$. Let $W^{\perp}$ be the set of all vectors $\delta$ in $V$ such that $f(\delta, \alpha)=f(\delta, \beta)=0$, that is, the set of all $\delta$ such that $f(\delta, \gamma)=0$ for every $\gamma$ in the subspace $W$. We claim that $V=W \oplus W^{\perp}$. For, let $\epsilon$ be any vector in $V$, and

$$
\begin{aligned}
& \gamma=f(\epsilon, \beta) \alpha-f(\epsilon, \alpha) \beta \\
& \delta=\epsilon-\gamma .
\end{aligned}
$$

Then $\gamma$ is in $W$, and $\delta$ is in $W^{\perp}$, for

$$
\begin{aligned}
f(\delta, \alpha) & =f(\epsilon-f(\epsilon, \beta) \alpha+f(\epsilon, \alpha) \beta, \alpha) \\
& =f(\epsilon, \alpha)+f(\epsilon, \alpha) f(\beta, \alpha) \\
& =0
\end{aligned}
$$

and similarly $f(\delta, \beta)=0$. Thus every $\epsilon$ in $V$ is of the form $\epsilon=\gamma+\delta$, with $\gamma$ in $W$ and $\delta$ in $W^{\perp}$. From $(9-7)$ it is clear that $W \cap W^{\lrcorner}=\{0\}$, and so $V=W \oplus W^{\perp}$.

Now the restriction of $f$ to $W^{\llcorner\mathrm{L}}$ is a skew-symmetric bilinear form on $W^{\perp}$. This restriction may be the zero form. If it is not, there are vectors $\alpha^{\prime}$ and $\beta^{\prime}$ in $W^{\perp}$ such that $f\left(\alpha^{\prime}, \beta^{\prime}\right)=1$. If we let $W^{\prime}$ be the two-dimensional subspace spanned by $\alpha^{\prime}$ and $\beta^{\prime}$, then we shall have

$$
V=W \oplus W^{\prime} \oplus W_{0}
$$

where $W_{0}$ is the set of all vectors $\delta$ in $W^{\perp}$ such that $f\left(\alpha^{\prime}, \delta\right)=f\left(\beta^{\prime}, \delta\right)=0$. If the restriction of $f$ to $W_{0}$ is not the zero form, we may select vectors $\alpha^{\prime \prime}, \beta^{\prime \prime}$ in $W_{0}$ such that $f\left(\alpha^{\prime \prime}, \beta^{\prime \prime}\right)=1$, and continue.

In the finite-dimensional case it should be clear that we obtain a finite sequence of pairs of vectors,

$$
\left(\alpha_{1}, \beta_{1}\right),\left(\alpha_{2}, \beta_{2}\right), \ldots,\left(\alpha_{k}, \beta_{k}\right)
$$

with the following properties: (a) $f\left(\alpha_{j}, \beta_{j}\right)=1, j=1, \ldots, k$.

(b) $f\left(\alpha_{i}, \alpha_{j}\right)=f\left(\beta_{i}, \beta_{j}\right)=f\left(\alpha_{i}, \beta_{j}\right)=0, i \neq j$.

(c) If $W_{j}$ is the two-dimensional subspace spanned by $\alpha_{j}$ and $\beta_{j}$, then

$$
V=W_{1} \oplus \cdots \oplus W_{k} \oplus W_{0}
$$

where every vector in $W_{0}$ is 'orthogonal' to all $\alpha_{j}$ and $\beta_{j}$, and the restriction of $f$ to $W_{0}$ is the zero form.

Theorem 6. Let $\mathrm{V}$ be an $\mathrm{n}$-dimensional vector space over a subfield of the complex numbers, and let $\mathrm{f}$ be a skew-symmetric bilinear form on $\mathrm{V}$. Then the rank $\mathrm{r}$ of $\mathrm{f}$ is even, and if $\mathrm{r}=2 \mathrm{k}$ there is an ordered basis for $\mathrm{V}$ in which the matrix of $\mathrm{f}$ is the direct sum of the $(\mathrm{n}-\mathrm{r}) \times(\mathrm{n}-\mathrm{r})$ zero matrix and $\mathrm{k}$ copies of the $2 \times 2$ matrix

$$
\left[\begin{array}{rr}
0 & 1 \\
-1 & 0
\end{array}\right]
$$

Proof. Let $\alpha_{1}, \beta_{1}, \ldots, \alpha_{k}, \beta_{k}$ be vectors satisf ying conditions (a), (b), and (c) above. Let $\left\{\gamma_{1}, \ldots, \gamma_{s}\right\}$ be any ordered basis for the subspace $W_{0}$. Then

$$
a=\left\{\alpha_{1}, \beta_{1}, \alpha_{2}, \beta_{2}, \ldots, \alpha_{k}, \beta_{k}, \gamma_{1}, \ldots, \gamma_{s}\right\}
$$

is an ordered basis for $V$. From (a), (b), and (c) it is clear that the matrix of $f$ in the ordered basis $B$ is the direct sum of the $(n-2 k) \times(n-2 k)$ zero matrix and $k$ copies of the $2 \times 2$ matrix

$$
\left[\begin{array}{rr}
0 & 1 \\
-1 & 0
\end{array}\right]
$$

Furthermore, it is clear that the rank of this matrix, and hence the rank of $f$, is $2 k$.

One consequence of the above is that if $f$ is a non-degenerate, skewsymmetric bilinear form on $V$, then the dimension of $V$ must be even. If $\operatorname{dim} V=2 k$, there will be an ordered basis $\left\{\alpha_{1}, \beta_{1}, \ldots, \alpha_{k}, \beta_{k}\right\}$ for $V$ such that

$$
\begin{aligned}
f\left(\alpha_{i}, \beta_{j}\right) & = \begin{cases}0, & i \neq j \\
1, & i=j\end{cases} \\
f\left(\alpha_{i}, \alpha_{j}\right) & =f\left(\beta_{i}, \beta_{j}\right)=0 .
\end{aligned}
$$

The matrix of $f$ in this ordcred basis is the direct sum of $k$ copies of the $2 \times 2$ skew-symmetric matrix (10-8). We obtain another standard form for the matrix of a non-degenerate skew-symmetric form if, instead of the ordered basis above, we consider the ordered basis

$$
\left\{\alpha_{1}, \ldots, \alpha_{k}, \beta_{k}, \ldots, \beta_{1}\right\} \text {. }
$$

The reader should find it easy to verify that the matrix of $f$ in the latter ordered basis has the block form

$$
\left[\begin{array}{rr}
0 & J \\
-J & 0
\end{array}\right]
$$

where $J$ is the $k \times k$ matrix

$$
\left[\begin{array}{cccc}
0 & \cdots & 0 & 1 \\
0 & \cdots & 1 & 0 \\
\vdots & \cdots & \vdots & \vdots \\
1 & \cdots & 0 & 0
\end{array}\right] .
$$

\section{Exercises}

1. Let $V$ be a vector space over a field $F$. Show that the set of all skew-symmetric bilinear forms on $V$ is a subspace of $L(V, V, F)$.

2. Find all skew-symmetric bilinear forms on $R^{3}$.

3. Find a basis for the space of all skew-symmetric bilinear forms on $R^{n}$.

4. Let $f$ be a symmetric bilinear form on $C^{n}$ and $g$ a skew-symmetric bilinear form on $C^{n}$. Suppose $f+g=0$. Show that $f=g=0$.

5. Let $V$ be an $n$-dimensional vector space over a subfield $F$ of $C$. Prove the following.

(a) The equation $(P f)(\alpha, \beta)=\frac{1}{2} f(\alpha, \beta)-\frac{1}{2} f(\beta, \alpha)$ defines a linear operator $P$ on $L(V, V, F)$.

(b) $P^{2}=P$, i.e., $P$ is a projection.

(c) rank $P=\frac{n(n-1)}{2}$; nullity $P=\frac{n(n+1)}{2}$.

(d) If $U$ is a linear operato on $V$, the equation $\left(U_{t} f\right)(\alpha, \beta)=f(U \alpha, U \beta)$ defines a linear operator $U^{\dagger}$ on $L(V, V, F)$.

(e) For every linear operator $U$, the projection $P$ commutes with $U^{\dagger}$.

6. Prove an analogue of Exercise 11 in Section $10.2$ for non-dcgenerate, skewsymmetric bilinear forms.

7. Let $f$ be a bilinear form on a vector space $V$. Let $L_{f}$ and $\boldsymbol{R}_{f}$ be the mappings of $V$ into $V^{*}$ associated with $f$ in Section 10.1. Prove that $f$ is skew-symmetric if and only if $L_{f}=-R_{f}$.

8. Prove an analogue of Exercise 17 in Section $10.2$ for skew-symmetric forms.

9. Let $V$ be a finite-dimensional vector space and $L_{1}, L_{2}$ linear functionals on $V$. Show that the equation

$$
f(\alpha, \beta)=L_{1}(\alpha) L_{2}(\beta)-L_{1}(\beta) L_{2}(\alpha)
$$

defines a skew-symmetric bilinear form on $V$. Show that $f=0$ if and only if $L_{1}, L_{2}$ are linearly dependent.

10. Let $V$ be a finite-dimensional vector space over a subfield of the complex numbers and $f$ a skew-symmetric bilinear form on $V$. Show hat $f$ has rank 2 if and only if there exist linearly independent linear functionals $L_{1}, L_{2}$ on $V$ such that

$$
f(\alpha, \beta)=L_{1}(\alpha) L_{2}(\beta)-L_{1}(\beta) L_{2}(\alpha) .
$$

11. Let $f$ be any skew-symmetric bilinear form on $R^{3}$. Prove that there are linear functionals $L_{1}, L_{2}$ such that

$$
f(\alpha, \beta)=L_{1}(\alpha) L_{2}(\beta)-L_{1}(\beta) L_{2}(\alpha) .
$$

12. Let $V$ be a finite-dimensional vector space over a subfield of the complex numbers, and let $f, g$ be skew-symmetric bilinear forms on $V$. Show that there is an invertible linear operator $T$ on $V$ such that $f(T \alpha, T \beta)=g(\alpha, \beta)$ for all $\alpha, \beta$ if and only if $f$ and $g$ have the same rank.

13. Show that the result of Exercise 12 is valid for symmetric bilinear forms on a complex vector space, but is not valid for symmetric bilinear forms on a real vector space.

\subsection{Groups Preserving Bilinear Forms}

Let $f$ be a bilinear form on the vector space $V$, and let $T$ be a linear operator on $V$. We say that $T$ preserves $f$ if $f(T \alpha, T \beta)=f(\alpha, \beta)$ for all $\alpha, \beta$ in $V$. For any $T$ and $f$ the function $g$, defined by $g(\alpha, \beta)=f(T \alpha, T \beta)$, is easily seen to be a bilinear form on $V$. To say that $T$ preserves $f$ is simply to say $g=f$. The identity operator preserves every bilinear form. If $S$ and $T$ are linear operators which preserve $f$, the product $S T$ also preserves $f$; for $f(S T \alpha, S T \beta)=f(T \alpha, T \beta)=f(\alpha, \beta)$. In other words, the collection of linear operators which preserve a given bilinear form is closed under the formation of (operator) products. In general, one cannot say much more about this collection of operators; however, if $f$ is non-degenerate, we have the following.

Theorem 7. Let $\mathrm{f}$ be a non-degenerate bilinear form on a finitedimensional vector space $\mathrm{V}$. The set of all linear operators on $\mathrm{V}$ which preserve $\mathrm{f}$ is a group under the operation of composition.

Proof. Let $G$ be the set of linear operators preserving $f$. We observed that the identity operator is in $G$ and that whenever $S$ and $T$ are in $G$ the composition $S T$ is also in $G$. From the fact that $f$ is nondegenerate, we shall prove that any operator $T$ in $G$ is invertible, and $T^{-1}$ is also in $G$. Suppose $T$ preserves $f$. Let $\alpha$ be a vector in the null space of $T$. Then for any $\beta$ in $V$ we have

$$
f(\alpha, \beta)=f(T \alpha, T \beta)=f(0, T \beta)=0 .
$$

Since $f$ is non-degenerate, $\alpha=0$. Thus $T$ is invertible. Clearly $T^{-1}$ also preserves $f$; for

$$
f\left(T^{-1} \alpha, T^{-1} \beta\right)=f\left(T T^{-1} \alpha, T T^{-1} \beta\right)=f(\alpha, \beta) .
$$

If $f$ is a non-degenerate bilinear form on the finite-dimensional space $V$, then each ordered basis $B$ for $V$ determines a group of matrices 'preserving' $f$. The set of all matrices $[T]_{\mathbb{B}}$, where $T$ is a linear operator preserving $f$, will be a group under matrix multiplication. There is an alternative description of this group of matrices, as follows. Let $A=[f]_{\mathbb{B}}$, so that if $\alpha$ and $\beta$ are vectors in $V$ with respective coordinate matrices $X$ and $Y$ relative to $\Theta$, we shall have

$$
f(\alpha, \beta)=X^{t} A Y .
$$

Let $T$ be any linear operator on $V$ and $M=[T]_{\mathscr{\Omega}}$. Then

$$
\begin{aligned}
f(T \alpha, T \beta) & =(M X)^{t} A(M Y) \\
& =X^{t}\left(M^{t} A M\right) Y .
\end{aligned}
$$

Accordingly, $T$ preserves $f$ if and only if $M^{t} A M=A$. In matrix language then, Theorem 7 says the following: If $A$ is an invertible $n \times n$ matrix, the set of all $n \times n$ matrices $M$ such that $M^{t} A M=A$ is a group under matrix multiplication. If $A=[f]_{\omega}$, then $M$ is in this group of matrices if and only if $M=[T]_{B}$, where $T$ is a linear operator which preserves $f$.

Before turning to some examples, let us make one further remark. Suppose $f$ is a bilinear form which is symmetric. A linear operator $T$ preserves $f$ if and only if $T$ preserves the quadratic form

$$
q(\alpha)=f(\alpha, \alpha)
$$

associated with $f$. If $T$ preserves $f$, we certainly have

$$
q(T \alpha)=f(\boldsymbol{T} \alpha, T \alpha)=f(\alpha, \alpha)=q(\alpha)
$$

for every $\alpha$ in $V$. Conversely, since $f$ is symmetric, the polarization identity

$$
f(\alpha, \beta)=\frac{1}{4} q(\alpha+\beta)-\frac{1}{4} q(\alpha-\beta)
$$

shows us that $T$ preserves $f$ provided that $q(T \gamma)=q(\gamma)$ for each $\gamma$ in $V$. (We are assuming here that the scalar field is a subfield of the complex numbers.)

Example 6 . Let $V$ be either the space $R^{n}$ or the space $C^{n}$. Let $f$ be the bilinear form

$$
f(\alpha, \beta)=\sum_{j=1}^{n} x_{j} y_{j}
$$

where $\alpha=\left(x_{1}, \ldots, x_{n}\right)$ and $\beta=\left(y_{1}, \ldots, y_{n}\right)$. The group preserving $f$ is called the $n$-dimensional (real or complex) orthogonal group. The name 'orthogonal group' is more commonly applied to the associated group of matrices in the standard ordered basis. Since the matrix of $f$ in the standard basis is $I$, this group consists of the matrices $M$ which satisfy $M^{t} M=I$. Such a matrix $M$ is called an $n \times n$ (real or complex) orthogonal matrix. The two $n \times n$ orthogonal groups are usually de- noted $O(n, R)$ and $O\left(n, C^{\prime}\right)$. Of course, the orthogonal group is also the group which preserves the quadratic form

$$
q\left(x_{1}, \ldots, x_{n}\right)=x_{1}^{2}+\cdots+x_{n}^{2} .
$$

Example 7. Let $f$ be the symmetric bilinear form on $R^{n}$ with quadratic form

$$
q\left(x_{1}, \ldots, x_{n}\right)=\sum_{j=1}^{p} x_{j}^{2}-\sum_{j=p+1}^{n} x_{j}^{2} .
$$

Then $f$ is non-degenerate and has signature $2 p-n$. The group of matrices preserving a form of this type is called a pseudo-orthogonal group. When $p=n$, we obtain the orthogonal group $O(n, R)$ as a particular type of pseudo-orthogonal group. For each of the $n+1$ values $p=0,1,2, \ldots$, $n$, we obtain different bilinear forms $f$; however, for $p=k$ and $p=n-k$ the forms are negatives of one another and hence have the same associated group. Thus, when $n$ is odd, we have $(n+1) / 2$ pseudo-orthogonal groups of $n \times n$ matrices, and when $n$ is even, we have $(n+2) / 2$ such groups.

Theorem 8. Let $\mathrm{V}$ be an $\mathrm{n}$-dimensional vector space over the field of complex numbers, and let $\mathrm{f}$ be a non-degenerate symmetric bilinear form on $\mathrm{V}$. Then the group preserving $\mathrm{f}$ is isomorphic to the complex orthogonal group $\mathrm{O}(\mathrm{n}, \mathrm{C})$.

Proof. Of course, by an isomorphism between two groups, we mean a one-one correspondence between their elements which 'preserves' the group operation. Let $G$ be the group of linear operators on $V$ which preserve the bilinear form $f$. Since $f$ is both symmetric and non-degenerate, Theorem 4 tells us that there is an ordered basis $B$ for $V$ in which $f$ is represented by the $n \times n$ identity matrix. Therefore, a linear operator $T$ preserves $f$ if and only if its matrix in the ordered basis $B$ is a complex orthogonal matrix. Hence

$$
T \rightarrow[T]_{\boldsymbol{Q}}
$$

is an isomorphism of $G$ onto $O(n, C)$.

Theorem 9. Let $\mathrm{V}$ be an $\mathrm{n}$-dimensional vector space over the field of real numbers, and let $\mathrm{f}$ be a non-degenerate symmetric bilinear form on $\mathrm{V}$. Then the group preserving $\mathrm{f}$ is isomorphic to an $\mathrm{n} \times \mathrm{n}$ pseudo-orthogonal group.

Proof. Repeat the proof of Theorem 8, using Theorem 5 instead of Theorem 4.

Example 8. Let $f$ be the symmetric bilinear form on $R^{4}$ with quadratic form

$$
q(x, y, z, t)=t^{2}-x^{2}-y^{2}-z^{2} .
$$

A linear operator $T$ on $R^{4}$ which preserves this particular bilinear (or quadratic) form is called a Lorentz transformation, and the group preserving $f$ is called the Lorentz group. We should like to give one method of describing some Lorentz transformations.

Let $H$ be the real vector space of all $2 \times 2$ complex matrices $A$ which are Hermitian, $A=A^{*}$. It is easy to verify that

$$
\Phi(x, y, z, t)=\left[\begin{array}{cc}
t+x & y+i z \\
y-i z & t-x
\end{array}\right]
$$

defines an isomorphism $\Phi$ of $R^{4}$ onto the space $H$. Under this isomorphism, the quadratic form $q$ is carried onto the determinant function, that is

$$
q(x, y, z, t)=\operatorname{det}\left[\begin{array}{cc}
t+x & y+i z \\
y-i z & t-x
\end{array}\right]
$$

or

$$
q(\alpha)=\operatorname{det} \Phi(\alpha) .
$$

This suggests that we might study Lorentz transformations on $R^{4}$ by studying linear operators on $H$ which preserve determinants.

Let $M$ be any complex $2 \times 2$ matrix and for a Hermitian matrix $A$ define

$$
U_{M}(A)=M A M^{*} .
$$

Now $M A M^{*}$ is also Hermitian. From this it is easy to see that $U_{M}$ is a (real) linear operator on $H$. Let us ask when it is true that $U_{M}$ 'preserves' determinants, i.e., $\operatorname{det}\left[U_{M}(A)\right]=\operatorname{det} A$ for each $A$ in $H$. Since the determinant of $M^{*}$ is the complex conjugate of the determinant of $M$, we see that

$$
\operatorname{det}\left[U_{M}(A)\right]=|\operatorname{det} M|^{2} \operatorname{det} A .
$$

Thus $U_{M}$ preserves determinants exactly when $\operatorname{det} M$ has absolute value 1 .

So now let us select any $2 \times 2$ complex matrix $M$ for which $|\operatorname{det} M|=1$. Then $U_{M}$ is a linear operator on $H$ which preserves determinants. Define

$$
T_{M}=\Phi^{-1} U_{M} \Phi .
$$

Since $\Phi$ is an isomorphism, $T_{M}$ is a linear operator on $R^{4}$. Also, $T_{M}$ is a Lorentz transformation; for

$$
\begin{aligned}
q\left(T_{M} \alpha\right) & =q\left(\Phi^{-1} U_{M} \Phi \alpha\right) \\
& =\operatorname{det}\left(\Phi \Phi^{-1} U_{M} \Phi \alpha\right) \\
& =\operatorname{det}\left(U_{M} \Phi \alpha\right) \\
& =\operatorname{det}(\Phi \alpha) \\
& =q(\alpha)
\end{aligned}
$$

and so $T_{M}$ preserves the quadratic form $q$.

By using specific $2 \times 2$ matrices $M$, one can use the method above to compute specific Lorentz transformations. There are two comments which we might make here; they are not difficult to verify. (1) If $M_{1}$ and $M_{2}$ are invertible $2 \times 2$ matrices with complex entries, then $U_{M_{1}}=U_{M_{2}}$ if and only if $M_{2}$ is a scalar multiple of $M_{1}$. Thus, all of the Lorentz transformations exhibited above are obtainable from unimodular matrices $M$, that is, from matrices $M$ satisfying $\operatorname{det} M=1$. If $M_{1}$ and $M_{2}$ are unimodular matrices such that $M_{1} \neq M_{2}$ and $M_{1} \neq-M_{2}$, then $T_{M_{1}} \neq T_{M_{2}}$

(2) Not every Lorentz transformation is obtainable by the above method.

\section{Exercises}

1. Let $M$ be a member of the complex orthogonal group, $O(n, C)$. Show that $M^{t}$, $\bar{M}$, and $M^{*}=\bar{M}^{t}$ also belong to $O(n, C)$.

2. Suppose $M$ belongs to $O(n, C)$ and that $M^{\prime}$ is similar to $M$. Does $M^{\prime}$ also belong to $O(n, C)$ ?

3. Let

$$
y_{j}=\sum_{k=1}^{n} M_{j k} x_{k}
$$

where $M$ is a member of $O(n, C)$. Show that

$$
\sum_{j} y_{i}^{2}=\sum_{j} x_{i}^{2} .
$$

4. Let $M$ be an $n \times n$ matrix over $C$ with columns $M_{1}, M_{2}, \ldots, M_{n}$. Show that $M$ belongs to $O(n, C)$ if and only if

$$
M_{j}^{t} M_{k}=\delta_{j k} .
$$

5. Let $X$ be an $n \times 1$ matrix over $C$. Under what conditions does $O(n, C)$ contain a matrix $M$ whose first column is $X$ ?

6. Find a matrix in $O(3, C)$ whose first row is $(2 i, 2 i, 3)$.

7. Let $V$ be the space of all $n \times 1$ matrices over $C$ and $f$ the bilinear form on $V$ given by $f(X, Y)=X^{t} Y$. Let $M$ belong to $O(n, C)$. What is the matrix of $f$ in the basis of $V$ consisting of the columns $M_{1}, M_{2}, \ldots, M_{n}$ of $M$ ?

8. Let $X$ be an $n \times 1$ matrix over $C$ such that $X^{t} X=1$, and $I_{j}$ be the $j$ th column of the identity matrix. Show there is a matrix $M$ in $O(n, C)$ such that $M X=I_{j}$. If $X$ has real entries, show there is an $M$ in $O(n, R)$ with the property that $M X=I_{j}$.

9. Let $V$ be the space of all $n \times 1$ matrices over $C, A$ an $n \times n$ matrix over $C$, and $f$ the bilinear form on $V$ given by $f(X, Y)=X^{t} A Y$. Show that $f$ is invariant under $O(n, C)$, i.e., $f(M X, M Y)=f(X, Y)$ for all $X, Y$ in $V$ and $M$ in $O(n, C)$, if and only if $A$ commutes with each member of $O(n, C)$.

10. Let $S$ be any set of $n \times n$ matrices over $C$ and $S^{\prime}$ the set of all $n \times n$ matrices over $C$ which commute with each element of $S$. Show that $S^{\prime}$ is an algebra over $C$. 11. Let $F$ be a subfield of $C, V$ a finite-dimensional vector space over $F$, and $f$ a non-singular bilinear form on $V$. If $T$ is a linear operator on $V$ preserving $f$, prove that $\operatorname{det} T=\pm 1$.

12. Let $F$ be a subfield of $C, V$ the space of $n \times 1$ matrices over $F, A$ an invertible $n \times n$ matrix over $F$, and $f$ the bilinear form on $V$ given by $f(X, Y)=X^{t} A Y$. If $M$ is an $n \times n$ matrix over $F$, show that $M$ preserves $f$ if and only if $A^{-1} M^{t} A=$ $M^{-1}$.

13. Let $g$ be a non-singular bilinear form on a finite-dimensional vector space $V$. Suppose $T$ is an invertible linear operator on $V$ and that $f$ is the bilinear form on $V$ given by $f(\boldsymbol{\alpha}, \boldsymbol{\beta})=g(\boldsymbol{\alpha}, \boldsymbol{T} \boldsymbol{\beta})$. If $U$ is a linear operator on $V$, find necessary and sufficient conditions for $U$ to preserve $f$.

14. Let $T$ be a linear operator on $C^{2}$ which preserves the quadratic form $x_{1}^{2}-x_{2}^{2}$. Show that

(a) $\operatorname{det}(T)=\pm 1$.

(b) If $M$ is the matrix of $T$ in the standard basis, then $M_{22}=\pm M_{11}, M_{21}=$ $\pm M_{12}, M_{11}^{2}-M_{12}^{2}=1$.

(c) If $\operatorname{det} M=1$, then there is a non-zero complex number $c$ such that

$$
M=\frac{1}{2}\left[\begin{array}{ll}
c+\frac{1}{c} & c-\frac{1}{c} \\
c-\frac{1}{c} & c+\frac{1}{c}
\end{array}\right] .
$$

(d) If $\operatorname{det} M=-1$ then there is a complex number $c$ such that

$$
M=\frac{1}{2}\left[\begin{array}{rr}
c+\frac{1}{c} & c-\frac{1}{c} \\
-c+\frac{1}{c} & -c-\frac{1}{c}
\end{array}\right] .
$$

15. Let $f$ be the bilinear form on $C^{2}$ defined by

Show that

$$
f\left(\left(x_{1}, x_{2}\right),\left(y_{1}, y_{2}\right)\right)=x_{1} y_{2}-x_{2} y_{1} .
$$

(a) if $T$ is a linear operator on $C^{2}$, then $f(T \alpha, T \beta)=(\operatorname{det} T) f(\alpha, \beta)$ for all $\alpha, \beta$ in $C^{2}$.

(b) $T$ preserves $f$ if and only if $\operatorname{det} T=+1$.

(c) What does (b) say about the group of $2 \times 2$ matrices $M$ such that $M^{t} A M=A$ where

$$
A=\left[\begin{array}{rr}
0 & 1 \\
-1 & 0
\end{array}\right] ?
$$

16. Let $n$ be a positive integer, $I$ the $n \times n$ identity matrix over $C$, and $J$ the $2 n \times 2 n$ matrix given by

$$
J=\left[\begin{array}{rr}
0 & I \\
-I & 0
\end{array}\right] .
$$

Let $M$ be a $2 n \times 2 n$ matrix over $C$ of the form

$$
M=\left[\begin{array}{ll}
A & B \\
C & D
\end{array}\right]
$$

where $A, B, C, D$ are $n \times n$ matrices over $C$. Find necessary and sufficient conditions on $A, B, C, D$ in order that $M^{t} J M=J$.

17. Find all bilinear forms on the space of $n \times 1$ matrices over $R$ which are invariant under $O(n, R)$.

18. Find all bilinear forms on the space of $n \times 1$ matrices over $C$ which are invariant under $O(n, C)$. 

\section{Appendix}

This Appendix separates logically into two parts. The first part, comprising the first three sections, contains certain fundamental concepts which occur throughout the book (indeed, throughout mathematics). It is more in the nature of an introduction for the book than an appendix. The second part is more genuinely an appendix to the text.

Section 1 contains a discussion of sets, their unions and intersections. Section 2 discusses the concept of function, and the related ideas of range, domain, inverse function, and the restriction of a function to a subset of its domain. Section 3 treats equivalence relations. The material in these three sections, especially that in Sections 1 and 2, is presented in a rather concise manner. It is treated more as an agreement upon terminology than as a detailed exposition. In a strict logical sense, this material constitutes a portion of the prerequisites for reading the book; however, the reader should not be discouraged if he does not completely grasp the significance of the ideas on his first reading. These ideas are important, but the reader who is not too familiar with them should find it easier to absorb them if he reviews the discussion from time to time while reading the text proper.

Sections 4 and 5 deal with equivalence relations in the context of linear algebra. Section 4 contains a brief discussion of quotient spaces. It can be read at any time after the first two or three chapters of the book. Section 5 takes a look at some of the equivalence relations which arise in the book, attempting to indicate how some of the results in the book might be interpreted from the point of view of equivalence relations. Section 6 describes the Axiom of choice and its implications for linear algebra. 

\section{A.1. Sets}

We shall use the words 'set,' 'class,' 'collection,' and 'family' interchangeably, although we give preference to 'set.' If $S$ is a set and $x$ is an object in the set $S$, we shall say that $x$ is a member of $S$, that $x$ is an element of $S$, that $x$ belongs to $S$, or simply that $x$ is in $S$. If $S$ has only a finite number of members, $x_{1}, \ldots, x_{n}$, we shall of den describe $S$ by displaying its members inside braces:

$$
S=\left\{x_{1}, \ldots, x_{n}\right\} .
$$

Thus, the set $S$ of positive integers from 1 through 5 would be

$$
S=\{1,2,3,4,5\} .
$$

If $S$ and $T$ are sets, we say that $S$ is a subset of $T$, or that $S$ is contained in $T$, if each member of $S$ is a member of $T$. Each set $S$ is a subset of itself. If $S$ is a subset of $T$ but $S$ and $T$ are not identical, we call $S$ a proper subset of $T$. In other words, $S$ is a proper subset of $T$ provided that $S$ is contained in $T$ but $T$ is not contained in $S$.

If $S$ and $T$ are sets, the union of $S$ and $T$ is the set $S \cup T$, consisting of all objects $x$ which are members of either $S$ or $T$. The intersection of $S$ and $T$ is the set $S \cap T$, consisting of all $x$ which are members of both $S$ and $T$. For any two sets, $S$ and $T$, the intersection $S \cap T$ is a subset of the union $S \cup T$. This should help to clarif y the use of the word 'or' which will prevail in this book. When we say that $x$ is either in $S$ or in $T$, we do not preclude the possibility that $x$ is in both $S$ and $T$.

In order that the intersection of $S$ and $T$ should always be a set, it is necessary that one introduce the empty set, i.e., the set with no members. Then $S \cap T$ is the empty set if and only if $S$ and $T$ have no members in common.

We shall frequently need to discuss the union or intersection of several sets. If $S_{1}, \ldots, S_{n}$ are sets, their union is the set $\bigcup_{j=1}^{n} S_{j}$ consisting of all $x$ which are members of at least one of the sets $S_{1}, \ldots, S_{n}$. Their intersection is the set $\bigcap_{j=1}^{n} S_{j}$, consisting of all $x$ which are members of each of the sets $S_{1}, \ldots, S_{n}$. On a few occasions, we shall discuss the union or intersection of an infinite collection of sets. It should be clear how such unions and intersections are defined. The following example should clarify these definitions and a notation for them.

Example 1. Let $R$ denote the set of all real numbers (the real line). If $t$ is in $R$, we associate with $t$ a subset $S_{t}$ of $R$, defined as follows: $S_{t}$ consists of all real numbers $x$ which are not less than $t$. (a) $S_{t_{1}} \cup S_{t_{2}}=S_{t}$, where $t$ is the smaller of $t_{1}$ and $t_{2}$.

(b) $S_{t_{1}} \cap S_{t_{2}}=S_{t}$, where $t$ is the larger of $t_{1}$ and $t_{2}$.

(c) Let $I$ be the unit interval, that is, the set of all $t$ in $R$ satisfying $0 \leq t \leq 1$. Then

$$
\begin{aligned}
& \bigcup_{t \text { in } I} S_{t}=S_{0} \\
& \bigcap_{t \text { in } I} S_{t}=S_{1} .
\end{aligned}
$$

\section{A.2. Functions}

A function consists of the following:

(1) a set $X$, called the domain of the function;

(2) a set $Y$, called the co-domain of the function;

(3) a rule (or correspondence) $f$, which associates with each element $x$ of $X$ a single element $f(x)$ of $Y$.

If $(X, Y, f)$ is a function, we shall also say $f$ is a function from $X$ into $Y$. This is a bit sloppy, since it is not $f$ which is the function; $f$ is the rule of the function. However, this use of the same symbol for the function and its rule provides one with a much more tractable way of speaking about functions. Thus we shall say that $f$ is a function from $X$ into $Y$, that $X$ is the domain of $f$, and that $Y$ is the co-domain of $f$-all this meaning that $(X, Y, f)$ is a function as defined above. There are several other words which are commonly used in place of the word 'function.' Some of these are 'transformation,' 'operator,' and 'mapping.' These are used in contexts where they seem more suggestive in conveying the role played by a particular function.

If $f$ is a function from $X$ into $Y$, the range (or image) of $f$ is the set of all $f(x), x$ in $X$. In other words, the rarige of $f$ consists of all elements $y$ in $Y$ such that $y=f(x)$ for some $x$ in $X$. If the range of $f$ is all of $Y$, we say that $f$ is a function from $X$ onto $Y$, or simply that $f$ is onto. The range of $f$ is of ten denoted $f(X)$.

Example 2. (a) Let $X$ be the set of real numbers, and let $Y=X$. Let $f$ be the function from $X$ into $Y$ defined by $f(x)=x^{2}$. The range of $f$ is the set of all non-negative real numbers. Thus $f$ is not onto.

(b) Let $X$ be the Euclidean plane, and $Y=X$. Let $f$ be defined as follows: If $P$ is a point in the plane, then $f(P)$ is the point obtained by rotating $P$ through $90^{\circ}$ (about the origin, in the counterclockwise direction). The range of $f$ is all of $Y$, i.e., the entire plane, and so $f$ is onto.

(c) Again let $X$ be the Euclidean plane. Coordinatize $X$ as in analytic geometry, using two perpendicular lines to identify the points of $X$ with ordered pairs of real numbers $\left(x_{1}, x_{2}\right)$. Let $Y$ be the $x_{1}$-axis, that is, all points $\left(x_{1}, x_{2}\right)$ with $x_{2}=0$. If $P$ is a point of $X$, let $f(P)$ be the point obtained by projecting $P$ onto the $x_{1}$-axis, parallel to the $x_{2}$-axis. In other words, $f\left(\left(x_{1}, x_{2}\right)\right)=\left(x_{1}, 0\right)$. The range of $f$ is all of $Y$, and so $f$ is onto.

(d) Let $X$ be the set of real numbers, and let $Y$ be the set of positive real numbers. Define a function $f$ from $X$ into $Y$ by $f(x)=e^{x}$. Then $f$ is a function from $X$ onto $Y$.

(e) Let $X$ be the set of positive real numbers and $Y$ the set of all real numbers. Let $f$ be the natural logarithm function, that is, the function defined by $f(x)=\log x=\ln x$. Again $f$ is onto, i.e., every real number is the natural logarithm of some positive number.

Suppose that $X, Y$, and $Z$ are sets, that $f$ is a function from $X$ into $Y$, and that $g$ is a function from $Y$ into $Z$. There is associated with $f$ and $g$ a function $g \circ f$ from $X$ into $Z$, known as the composition of $g$ and $f$. It is defined by

$$
(g \circ f)(x)=g(f(x)) .
$$

For one simple example, let $X=Y=Z$, the set of real numbers; let $f, g, h$ be the functions from $X$ into $X$ defined by

$$
f(x)=x^{2}, \quad g(x)=e^{x}, \quad h(x)=e^{x^{2}}
$$

and then $h=g \circ f$. The composition $g \circ f$ is often denoted simply $g f$; however, as the above simple example shows, there are times when this may lead to confusion.

One question of interest is the following. Suppose $f$ is a function from $X$ into $Y$. When is there a function $g$ from $Y$ into $X$ such that $g(f(x))=x$ for each $x$ in $X$ ? If we denote by $I$ the identity function on $X$, that is, the function from $X$ into $X$ defined by $I(x)=x$, we are asking the following: When is there a function $g$ from $Y$ into $X$ such that $g \circ f=I$ ? Roughly speaking, we want a function $\boldsymbol{g}$ which 'sends each element of $Y$ back where it came from.' In order for such a $g$ to exist, $f$ clearly must be $1: 1$, that is, $f$ must have the property that if $x_{1} \neq x_{2}$ then $f\left(x_{1}\right) \neq f\left(x_{2}\right)$. If $f$ is $1: 1$, such a $g$ does exist. It is defined as follows: Let $y$ be an element of $Y$. If $y$ is in the range of $f$, then there is an element $x$ in $X$ such that $y=f(x)$; and since $f$ is $1: 1$, there is exactly one such $x$. Define $g(y)=x$. If $y$ is not in the range of $f$, define $g(y)$ to be any element of $X$. Clearly we then have $g \circ f=I$.

Let $f$ be a function from $X$ into $Y$. We say that $f$ is invertible if there is a function $g$ from $Y$ into $X$ such that

(1) $g \circ f$ is the identity function on $X$,

(2) $f \circ g$ is the identity function on $Y$.

We have just seen that if there is a $g$ satisf ying (1), then $f$ is $1: 1$. Similarly, one can see that if there is a $g$ satisfying (2), the range of $f$ is all of $Y$, i.e., $f$ is onto. Thus, if $f$ is invertible, $f$ is $1: 1$ and onto. Conversely, if $f$ is $1: 1$ and onto, there is a function $g$ from $Y$ into $X$ which satisfies (1) and (2). Furthermore, this $g$ is unique. It is the function from $Y$ into $X$ defined by this rule: if $y$ is in $Y$, then $g(y)$ is the one and only element $x$ in $X$ for which $f(x)=y$.

If $f$ is invertible ( $1: 1$ and onto), the inverse of $f$ is the unique function $f^{-1}$ from $Y$ into $X$ satisfying

$\left(1^{\prime}\right) f^{-1}(f(x))=x$, for each $x$ in $X$,

(2) $f\left(f^{-1}(y)\right)=y$, for each $y$ in $Y$.

EXample 3. Let us look at the functions in Example 2.

(a) If $X=Y$, the set of real numbers, and $f(x)=x^{2}$, then $f$ is not invertible. For $f$ is neither $1: 1$ nor onto.

(b) If $X=Y$, the Euclidean plane, and $f$ is 'rotation through $90^{\circ}$, then $f$ is both $1: 1$ and onto. The inverse function $f^{-1}$ is 'rotation through $-90^{\circ}$,' or 'rotation through $270^{\circ}$.'

(c) If $X$ is the plane, $Y$ the $x_{1}$-axis, and $f\left(\left(x_{1}, x_{2}\right)\right)=\left(x_{1}, 0\right)$, then $f$ is not invertible. For, although $f$ is onto, $f$ is riot $1: 1$.

(d) If $X$ is the set of real numbers, $Y$ the set of positive real numbers, and $f(x)=e^{x}$, then $f$ is invertible. The function $f^{-1}$ is the natural logarithm function of part (e): $\log e^{x}=x, e^{\log y}=y$.

(e) The inverse of this natural logarithm function is the exponential function of part (d).

Let $f$ be a function from $X$ into $Y$, and let $f_{0}$ be a function from $X_{0}$ into $Y_{0}$. We call $f_{0}$ a restriction of $f$ (or a restriction of $f$ to $X_{0}$ ) if

(1) $X_{0}$ is a subset of $X$,

(2) $f_{0}(x)=f(x)$ for each $x$ in $X_{0}$.

Of course, when $f_{0}$ is a restriction of $f$, it follows that $Y_{0}$ is a subset of $Y$. The name 'restriction' comes from the fact that $f$ and $f_{0}$ have the same rule, and differ chiefly because we have restricted the domain of definition of the rule to the subset $X_{0}$ of $X$.

If we are given the function $f$ and any subset $X_{0}$ of $X$, there is an obvious way to construct a restriction of $f$ to $X_{0}$. We define a function $f_{0}$ from $X_{0}$ into $Y$ by $f_{0}(x)=f(x)$ for each $x$ in $X_{0}$. One might wonder why we do not call this the restriction of $f$ to $X_{0}$. The reason is that in discussing restrictions of $f$ we want the freedom to change the co-domain $Y$, as well as the domain $X$.

Example 4. (a) Let $X$ be the set of real numbers and $f$ the function from $X$ into $X$ defined by $f(x)=x^{2}$. Then $f$ is not an invertible function, but it is if we restrict its domain to the non-negative real numbers. Let $X_{0}$ be the set of non-negative real numbers, and let $f_{0}$ be the function from $X_{0}$ into $X_{0}$ defined by $f_{0}(x)=x^{2}$. Then $f_{0}$ is a restriction of $f$ to $X_{0}$. Now $f$ is neither $1: 1$ nor onto, whereas $f_{0}$ is both $1: 1$ and onto. The latter statement simply says that each non-negative number is the square of exactly one non-negative number. The inverse function $f_{0}^{-1}$ is the function from $X_{0}$ into $X_{0}$ defined by $f_{0}^{-1}(x)=\sqrt{x}$.

(b) Let $X$ be the set of real numbers, and let $f$ be the function from $X$ into $X$ defined by $f(x)=x^{3}+x^{2}+1$. The range of $f$ is all of $X$, and so $f$ is onto. The function $f$ is certainly not $1: 1$, e.g., $f(-1)=f(0)$. But $f$ is $1: 1$ on $X_{0}$, the set of non-negative real numbers, because the derivative of $f$ is positive for $x>0$. As $x$ ranges over all non-negative numbers, $f(x)$ ranges over all real numbers $y$ such that $y \geq 1$. If we let $Y_{0}$ be the set of all $y \geq 1$, and let $f_{0}$ be the function from $X_{0}$ into $Y_{0}$ defined by $f_{0}(x)=f(x)$, then $f_{0}$ is a $1: 1$ function from $X_{0}$ onto $Y_{0}$. Accordingly, $f_{0}$ has an inverse function $f_{0}^{-1}$ from $Y_{0}$ onto $X_{0}$. Any formula for $f_{0}^{-1}(y)$ is rather complicated.

(c) Again let $X$ be the set of real numbers, and let $f$ be the sine function, that is, the function from $X$ into $X$ defined by $f(x)=\sin x$. The range of $f$ is the set of all $y$ such that $-1 \leq y \leq 1$; hence, $f$ is not onto. Since $f(x+2 \pi)=f(x)$, we see that $f$ is not $1: 1$. If we let $X_{0}$ be the interval $-\pi / 2 \leq x \leq \pi / 2$, then $f$ is $1: 1$ on $X_{0}$. Let $Y_{0}$ be the interval $-1 \leq y \leq 1$, and let $f_{0}$ be the function from $X_{0}$ into $Y_{0}$ defined by $f_{0}(x)=\sin x$. Then $f_{0}$ is a restriction of $f$ to the interval $X_{0}$, and $f_{0}$ is both $1: 1$ and onto. This is just another way of saying that, on the interval from $-\pi / 2$ to $\pi / 2$, the sine function takes each value between $-1$ and 1 exactly once. The function $f_{0}^{-1}$ is the inverse sine function:

$$
f_{0}^{-1}(y)=\sin ^{-1} y=\arcsin y .
$$

(d) This is a general example of a restriction of a function. It is much more typical of the type of restriction we shall use in this book than are the examples in (b) and (c) above. The example in (a) is a special case of this one. Let $X$ be a set and $f$ a function from $X$ into itself. Let $X_{0}$ be a subset of $X$. We say that $X_{0}$ is invariant under $f$ if for each $x$ in $X_{0}$ the element $f(x)$ is in $X_{0}$. If $X_{0}$ is invariant under $f$, then $f$ induces a function $f_{0}$ from $X_{0}$ into itself, by restricting the domain of its definition to $X_{0}$. The importance of invariance is that by restricting $f$ to $X_{0}$ we can obtain a function from $X_{0}$ into itself, rather than simply a function from $X_{0}$ into $X$.

\section{A.3. Equivalence Relations}

An equivalence relation is a specific type of relation between pairs of elements in a set. To define an equivalence relation, we must first decide what a 'relation' is.

Certainly a formal definition of 'relation' ought to encompass such familiar relations as ' $x=y$,' ' $^{\prime} x y$,' ' $x$ is the mother of $y$,' and ' $x$ is older than $y$.' If $X$ is a set, what does it take to determine a relation between pairs of elements of $X$ ? What it takes, evidently, is a rule for determining whether, for any two given elements $x$ and $y$ in $X, x$ stands in the given relationship to $y$ or not. Such a rule $R$, we shall call a (binary) relation on $X$. If we wish to be slightly more precise, we may proceed as follows. Let $X \times X$ denote the set of all ordered pairs $(x, y)$ of elements of $X$. A binary relation on $X$ is a function $R$ from $X \times X$ into the set $\{0,1\}$. In other words, $R$ assigns to each ordered pair $(x, y)$ either a 1 or a 0 . The idea is that if $R(x, y)=1$, then $x$ stands in the given relationship to $y$, and if $R(x, y)=0$, it does not.

If $R$ is a binary relation on the set $X$, it is convenient to write $x R y$ when $R(x, y)=1$. A binary relation $R$ is called

(1) reflexive, if $x R x$ for each $x$ in $X$;

(2) symmetric, if $y R x$ whenever $x R y$ :

(3) transitive, if $x R z$ whenever $x R y$ and $y R z$.

An equivalence relation on $X$ is a reflexive, symmetric, and transitive binary relation on $X$.

Example 5. (a) On any set, equality is an equivalence relation. In other words, if $x R y$ means $x=y$, then $R$ is an equivalence relation. For, $x=x$, if $x=y$ then $y=x$, if $x=y$ and $y=z$ then $x=z$. The relation ' $x \neq y$ ' is symmetric, but neither reflexive nor transitive.

(b) Let $X$ be the set of real numbers, and suppose $x R y$ means $x<y$. Then $R$ is not an equivalence relation. It is transitive, but it is neither reflexive nor symmetric. The relation ' $x \leq y$ ' is reflexive and transitive, but not symmetric.

(c) Let $E$ be the Euclidean plane, and let $X$ be the set of all triangles in the plane $E$. Then congruence is an equivalence relation on $X$, that is, $' T_{1} \cong T_{2}$ ' $\left(T_{1}\right.$ is congruent to $\left.T_{2}\right)$ is an equivalence relation on the set of all triangles in a plane.

(d) Let $X$ be the set of all integers:

$$
\ldots,-2,-1,0,1,2, \ldots
$$

Let $n$ be a fixed positive integer. Define a relation $R_{n}$ on $X$ by: $x R_{n} y$ if and only if $(x-y)$ is divisible by $n$. The relation $R_{n}$ is called congruence modulo $n$. Instead of $x R_{n} y$, one usually writes

$$
x \equiv y, \bmod n \quad(x \text { is congruent to } y \text { modulo } n)
$$

when $(x-y)$ is divisible by $n$. For each positive integer $n$, congruence modulo $n$ is an equivalence relation on the set of integers.

(e) Let $X$ and $Y$ be sets and $f$ a function from $X$ into $Y$. We define a relation $R$ on $X$ by: $x_{1} R x_{2}$ if and only if $f\left(x_{1}\right)=f\left(x_{2}\right)$. It is easy to verify that $R$ is an equivalence relation on the set $X$. As we shall see, this one example actually encompasses all equivalence relations. Suppose $R$ is an equivalence relation on the set $X$. If $x$ is an element of $X$, we let $E(x ; R)$ denote the set of all elements $y$ in $X$ such that $x R y$. This set $E(x ; R)$ is called the equivalence class of $x$ (for the equivalence relation $R$ ). Since $R$ is an equivalence relation, the equivalence classes have the following properties:

(1) Each $E(x ; R)$ is non-empty; for, since $x R x$, the element $x$ belongs to $E(x ; R)$.

(2) Let $x$ and $y$ be elements of $X$. Since $R$ is symmetric, $y$ belongs to $E(x ; R)$ if and only if $x$ belongs to $E(y ; R)$.

(3) If $x$ and $y$ are elements of $X$, the equivalence classes $E(x ; R)$ and $E(y ; R)$ are either identical or they have no members in common. First, suppose $x R y$. Let $z$ be any element of $E(x ; R)$ i.e., an element of $X$ such that $x R z$. Since $R$ is symmetric, we also have $z R x$. By assumption $x R y$, and because $R$ is transitive, we obtain $z R y$ or $y R z$. This shows that any member of $E(x ; R)$ is a member of $E(y ; E)$. By the symmetry of $R$, we likewise see that any member of $E(y ; R)$ is a member of $E(x ; R)$; hence $E(x ; R)=E(y ; R)$. Now we argue that if the relation $x R y$ does not hold, then $E(x ; R) \cap E(y ; R)$ is empty. For, if $z$ is in both these equivalence classes, we have $x R z$ and $y R z$, thus $x R z$ and $z R y$, thus $x R y$.

If we let $\mathfrak{F}$ be the family of equivalence classes for the equivalence relation $R$, we see that (1) each set in the family $\mathfrak{F}$ is non-empty, (2) each element $x$ of $X$ belongs to one and only one of the sets in the family $\mathcal{F}$, (3) $x R y$ if and only if $x$ and $y$ belong to the same set in the family $\mathfrak{F}$. Briefly, the equivalence relation $R$ subdivides $X$ into the union of a family of non-overlapping (non-empty) subsets. The argument also goes in the other direction. Suppose $F$ is any family of subsets of $X$ which satisfies conditions (1) and (2) immediately above. If we define a relation $R$ by (3), then $R$ is an equivalence relation on $X$ and $\mathfrak{F}$ is the family of equivalence classes for $\boldsymbol{R}$.

Example 6 . Let us see what the equivalence classes are for the equivalence relations in Example 5.

(a) If $R$ is equality on the set $X$, then the equivalence class of the element $x$ is simply the set $\{x\}$, whose only member is $x$.

(b) If $X$ is the set of all triangles in a plane, and $R$ is the congruence relation, about all one can say at the outset is that the equivalence class of the triangle $T$ consists of all triangles which are congruent to $T$. One of the tasks of plane geometry is to give other descriptions of these equivalence classes.

(c) If $X$ is the set of integers and $R_{n}$ is the relation 'congruence modulo $n$,' then there are precisely $n$ equivalence classes. Each integer $x$ is uniquely expressible in the form $x=q n+r$, where $q$ and $r$ are integers and $0 \leq r \leq n-1$. This shows that each $x$ is congruent modulo $n$ to exactly one of the $n$ integers $0,1,2, \ldots, n-1$. The equivalence classes are

$$
\begin{aligned}
& E_{0}=\{\ldots,-2 n,-n, 0, n, 2 n, \ldots\} \\
& E_{1}=\{\ldots, 1-2 n, 1-n, 1+n, 1+2 n, \ldots\} \\
& \vdots \vdots \\
& E_{n-1}=\{\ldots, n-1-2 n, n-1-n, n-1, n-1+n, \\
&n-1+2 n, \ldots\}
\end{aligned}
$$

(d) Suppose $X$ and $Y$ are sets, $f$ is a function from $X$ into $Y$, and $R$ is the equivalence relation defined by: $x_{1} R x_{2}$ if and only if $f\left(x_{1}\right)=f\left(x_{2}\right)$. The equivalence classes for $R$ are just the largest subsets of $X$ on which $f$ is 'constant.' Another description of the equivalence classes is this. They are in $1: 1$ correspondence with the members of the range of $f$. If $y$ is in the range of $f$, the set of all $x$ in $X$ such that $f(x)=y$ is an equivalence class for $R$; and this defines a 1:1 correspondence between the members of the range of $f$ and the equivalence classes of $R$.

Let us make one more comment about equivalence relations. Given an equivalence relation $R$ on $X$, let $\mathcal{F}$ be the family of equivalence classes for $R$. The association of the equivalence class $E(x ; R)$ with the element $x$, defines a function $f$ from $X$ into $\mathfrak{F}$ (indeed, onto $\mathfrak{F}$ ):

$$
f(x)=E(x ; R) .
$$

This shows that $R$ is the equivalence relation associated with a function whose domain is $X$, as in Example $5(\mathrm{e})$. What this tells us is that every equivalence relation on the set $X$ is determined as follows. We have a rule (function) $f$ which associates with each element $x$ of $X$ an object $f(x)$, and $x R y$ if and only if $f(x)=f(y)$. Now one should think of $f(x)$ as some property of $x$, so that what the equivalence relation does (roughly) is to lump together all those elements of $X$ which have this property in common. If the object $f(x)$ is the equivalence class of $x$, then all one has said is that the common property of the members of an equivalence class is that they belong to the same equivalence class. Obviously this doesn't say much. Generally, there are many different functions $f$ which determine the given equivalence relation as above, and one objective in the study of equivalence relations is to find such an $f$ which gives a meaningful and elementary description of the equivalence relation. In Section A.5 we shall see how this is accomplished for a few special equivalence relations which arise in linear algebra.

\section{A.4. Quotient Spaces}

Let $V$ be a vector space over the field $F$, and let $W$ be a subspace of $V$. There are, in general, many subspaces $W^{\prime}$ which are complementary to $W$, i.e., subspaces with the property that $V=W \oplus W^{\prime}$. If we have an inner product on $V$, and $W$ is finite-dimensional, there is a particular subspace which one would probably call the 'natural' complementary subspace for $W$. This is the orthogonal complement of $W$. But, if $V$ has no structure in addition to its vector space structure, there is no way of selecting a subspace $W^{\prime}$ which one could call the natural complementary subspace for $W$. However, one can construct from $V$ and $W$ a vector space $V / W$, known as the 'quotient' of $V$ and $W$, which will play the role of the natural complement to $W$. This quotient space is not a subspace of $V$, and so it cannot actually be a subspace complementary to $W$; but, it is a vector space defined only in terms of $V$ and $W$, and has the property that it is isomorphic to any subspace $W^{\prime}$ which is complementary to $W$.

Let $W$ be a subspace of the vector space $V$. If $\alpha$ and $\beta$ are vectors in $V$, we say that $\alpha$ is congruent to $\beta \bmod u l o W$, if the vector $(\alpha-\beta)$ is in the subspace $W$. If $\alpha$ is congruent to $\beta$ modulo $W$, we write

$$
\alpha \equiv \beta, \quad \bmod W .
$$

Now congruence modulo $W$ is an equivalence relation on $V$.

(1) $\alpha \equiv \alpha, \bmod W$, because $\alpha-\alpha=0$ is in $W$.

(2) If $\alpha \equiv \beta, \bmod W, \operatorname{then} \beta \equiv \alpha, \bmod W$. For, since $W$ is a subspace of $V$, the vector $(\alpha-\beta)$ is in $W$ if and only if $(\beta-\alpha)$ is in $W$.

(3) If $\alpha \equiv \beta, \bmod W$, and $\beta \equiv \gamma, \bmod W$, then $\alpha \equiv \gamma, \bmod W$. For, if $(\alpha-\beta)$ and $(\beta-\gamma)$ are in $W$, then $\alpha-\gamma=(\alpha-\beta)+\beta-\gamma)$ is in $W$.

The equivalence classes for this equivalence relation are known as the cosets of $W$. What is the equivalence class (coset) of a vector $\alpha$ ? It consists of all vectors $\beta$ in $V$ such that $(\beta-\alpha)$ is in $W$, that is, all vectors $\beta$ of the form $\beta=\alpha+\gamma$, with $\gamma$ in $W$. For this reason, the coset of the vector $\alpha$ is denoted by

$$
\alpha+W \text {. }
$$

It is appropriate to think of the coset of $\alpha$ relative to $W$ as the set of vectors obtained by translating the subspace $W$ by the vector $\alpha$. To picture these cosets, the reader might think of the following special case. Let $V$ be the space $R^{2}$, and let $W$ be a one-dimensional subspace of $V$. If we picture $V$ as the Euclidean plane, $W$ is a straight line through the origin. If $\alpha=\left(x_{1}, x_{2}\right)$ is a vector in $V$, the $\operatorname{coset} \alpha+W$ is the straight line which passes through the point $\left(x_{1}, x_{2}\right)$ and is parallel to $W$.

The collection of all cosets of $W$ will be denoted by $V / W$. We now define a vector addition and scalar multiplication on $V / W$ as follows:

$$
\begin{aligned}
(\alpha+W)+(\beta+W) & =(\alpha+\beta)+W \\
c(\alpha+W) & =(c \alpha)+W .
\end{aligned}
$$

In other words, the sum of the $\operatorname{coset}$ of $\alpha$ and the coset of $\beta$ is the coset of $(\alpha+\beta)$, and the product of the scalar $c$ and the coset of $\alpha$ is the coset of the vector $c \alpha$. Now many different vectors in $V$ will have the same coset relative to $W$, and so we must verify that the sum and product above depend only upon the cosets involved. What this means is that we must show the following:

(a) If $\alpha \equiv \alpha^{\prime}, \bmod W$, and $\beta \equiv \beta^{\prime}$, $\bmod W$, then

$$
\alpha+\beta \rightarrow \alpha^{\prime}+\beta^{\prime}, \bmod W^{\top} .
$$

(2) If $\alpha \equiv \alpha^{\prime}$, $\bmod W$, then $c \alpha \equiv c \alpha^{\prime}, \bmod W$.

These facts are easy to verify. (1) If $\alpha-\alpha^{\prime}$ is in $W$ and $\beta-\beta^{\prime}$ is in $W$, then since $(\alpha+\beta)-\left(\alpha^{\prime}-\beta^{\prime}\right)=\left(\alpha-\alpha^{\prime}\right)+\left(\beta-\beta^{\prime}\right)$, we see that $\alpha+\beta$ is congruent to $\alpha^{\prime}-\beta^{\prime}$ modulo $W$. (2) If $\alpha-\alpha^{\prime}$ is in $W$ and $c$ is any scalar, then $c \alpha-c \alpha^{\prime}=c\left(\alpha-\alpha^{\prime}\right)$ is in $W$.

It is now easy to verify that $V / W$, with the vector addition and scalar multiplication defined above, is a vector space over the field $F$. One must directly check each of the axioms for a vector space. Each of the properties of vector addition and scalar multiplication follows from the corresponding property of the operations in $V$. One comment should be made. The zero vector in $V / W$ will be the coset of the zero vector in $V$. In other words, $W$ is the zero vector in $V / W$.

The vector space $V / W$ is called the quotient (or difference) of $V$ and $W$. There is a natural linear transformation $Q$ from $V$ onto $V / W$. It is defined by $Q(\alpha)=\alpha+W$. One should see that we have defined the operations in $V / W$ just so that this transformation $Q$ would be linear. Note that the null space of $Q$ is exactly the subspace $W$. We call $Q$ the quotient transformation (or quotient mapping) of $V$ onto $V / W$.

The relation between the quotient spare $V / W$ and subspaces of $V$ which are complementary to $W$ can now be stated as follows.

Theorem. Let $\mathrm{W}$ be a subspace of the vector space $\mathrm{V}$, and let $\mathrm{Q}$ be the quotient mapping of $\mathrm{V}$ onto $\mathrm{V} / \mathrm{W}$. Suppose $\mathrm{W}^{\prime}$ is a subspace of $\mathrm{V}$. Then $\mathrm{V}=\mathrm{W} \oplus \mathrm{W}^{\prime}$ if and only if the restriction of $\mathrm{Q}$ to $\mathrm{W}^{\prime}$ is an isomorphism of $\mathrm{W}^{\prime}$ onto $\mathrm{V} / \mathrm{W}$.

Proof. Suppose $V=W \oplus W^{\prime}$. This means that each vector $\alpha$ in $V$ is uniquely expressible in the form $\alpha=\gamma+\gamma^{\prime}$, with $\gamma$ in $W$ and $\gamma^{\prime}$ in $W^{\prime}$. Then $Q \alpha=Q \gamma+Q \gamma^{\prime}=Q \gamma^{\prime}$, that is $\alpha+W=\gamma^{\prime}+W$. This shows that $Q$ maps $W^{\prime}$ onto $V / W$, i.e., that $Q\left(W^{\prime}\right)=V / W$. Also $Q$ is $1: 1$ on $W^{\prime}$; for suppose $\gamma_{1}^{\prime}$ and $\gamma_{2}^{\prime}$ are vectors in $W^{\prime}$ and that $Q \gamma_{1}^{\prime}=Q \gamma_{2}^{\prime}$. Then $Q\left(\gamma_{1}^{\prime}-\gamma_{2}^{\prime}\right)=0$ so that $\gamma_{1}^{\prime}-\gamma_{2}^{\prime}$ is in $W$. This vector is also in $W^{\prime}$, which is disjoint from $W$; hence $\gamma_{1}^{\prime}-\gamma_{2}^{\prime}=0$. The restriction of $Q$ to $W^{\prime}$ is therefore a one-one linear transformation of $W^{\prime}$ onto $V / W$.

Suppose $W^{\prime}$ is a subspace of $V$ such that $Q$ is one-one on $W^{\prime}$ and $Q\left(W^{\prime}\right)=V / W$. Let $\alpha$ be a vector in $V$. Then there is a vector $\gamma^{\prime}$ in $W^{\prime}$ such that $Q \gamma^{\prime}=Q \alpha$, i.e., $\gamma^{\prime}+W=\alpha+W$. This means that $\alpha=\gamma+\gamma^{\prime}$ for some vector $\gamma$ in $W$. Therefore $V=W+W^{\prime}$. To see that $W$ and $W^{\prime}$ are disjoint, suppose $\gamma$ is in both $W$ and $W^{\prime}$. Since $\gamma$ is in $W$, we have $Q \gamma=0$. But $Q$ is $1: 1$ on $W^{\prime}$, and so it must be that $\gamma=0$. Thus we have $V=W \oplus W^{\prime}$.

What this theorem really says is that $W^{\prime}$ is complementary to $W$ if and only if $W^{\prime}$ is a subspace which contains exactly one element from each coset of $W$. It shows that when $V=W \oplus W^{\prime}$, the quotient mapping $Q$ 'identifies' $W^{\prime}$ with $V / W$. Briefly $\left(W \oplus W^{\prime}\right) / W$ is isomorphic to $W^{\prime}$ in a 'natural' way.

One rather obvious fact should be noted. If $W$ is a subspace of the finite-dimensional vector space $V$, then

$$
\operatorname{dim} W+\operatorname{dim}(V / W)=\operatorname{dim} V .
$$

One can see this from the above theorem. Perhaps it is easier to observe that what this dimension formula says is

$$
\text { nullity }(Q)+\operatorname{rank}(Q)=\operatorname{dim} V \text {. }
$$

It is not our object here to give a detailed treatment of quotient spaces. But there is one fundamental result which we should prove.

Theorem. Let $\mathrm{V}$ and $\mathrm{Z}$ be vector spaces over the field $\mathrm{F}$. Suppose $\mathrm{T}$ is a linear transformation of $\mathrm{V}$ onto $\mathrm{Z}$. If $\mathrm{W}$ is the null space of $\mathrm{T}$, then $\mathrm{Z}$ is isomorphic to $\mathrm{V} / \mathrm{W}$.

Proof. We define a transformation $U$ from $V / W$ into $Z$ by $U(\alpha+W)=T \alpha$. We must verify that $U$ is well defined, i.e., that if $\alpha+W=\beta+W$ then $T \alpha=T \beta$. This follows from the fact that $W$ is the null space of $T$; for, $\alpha+W=\beta+W$ means $\alpha-\beta$ is in $W$, and this happens if and only if $T(\alpha-\beta)=0$. This shows not only that $U$ is well defined, but also that $U$ is one-one.

It is now easy to verify that $U$ is linear and sends $V / W$ onto $Z$, because $T$ is a linear transformation of $V$ onto $Z$.

\section{A.5. Equivalence Relations in Linear Algebra}

We shall consider some of the equivalence relations which arise in the text of this book. This is just a sampling of such relations.

(1) Let $m$ and $n$ be positive integers and $F$ a field. Let $X$ be the set of all $m \times n$ matrices over $F$. Then row-equivalence is an equivalence relation on the set $X$. The statement ' $A$ is row-equivalent to $B$ ' means that $A$ can be obtained from $B$ by a finite succession of elementary row operations. If we write $A \sim B$ for $A$ is row-equivalent to $B$, then it is not difficult to check the properties (i) $A \sim A$; (ii) if $A \sim B$, then $B \sim A$; (iii) if $A \sim B$ and $B \sim C$, then $A \sim C$. What do we know about this equivalence relation? Actually, we know a great deal. For example, we know that $A \sim B$ if and only if $A=P B$ for some invertible $m \times m$ matrix $P$; or, $A \sim B$ if and only if the homogeneous systems of linear equations $A X=0$ and $B X=0$ have the same solutions. We also have very explicit information about the equivalence classes for this relation. Each $m \times n$ matrix $A$ is row-equivalent to one and only one row-reduced echelon matrix. What this says is that each equivalence class for this relation contains precisely one row-reduced echelon matrix $R$; the equivalence class determined by $R$ consists of all matrices $A=P R$, where $P$ is an invertible $m \times m$ matrix. One can also think of this description of the equivalence classes in the following way. Given an $m \times n$ matrix $A$, we have a rule (function) $f$ which associates with $A$ the row-reduced echelon matrix $f(A)$ which is row-equivalent to $A$. Row-equivalence is completely determined by $f$. For, $A \sim B$ if and only if $f(A)=f(B)$, i.e., if and only if $A$ and $B$ have the same row-reduced echelon form.

(2) Let $n$ be a positive integer and $F$ a field. Let $X$ be the set of all $n \times n$ matrices over $F$. Then similarity is an equivalence relation on $X$; each $n \times n$ matrix $A$ is similar to itself; if $A$ is similar to $B$, then $B$ is similar to $A$; if $A$ is similar to $B$ and $B$ is similar to $C$, then $A$ is similar to $C$. We know quite a bit about this equivalence relation too. For example, $A$ is similar to $B$ if and only if $A$ and $B$ represent the same linear operator on $F^{n}$ in (possibly) different ordered bases. But, we know something much deeper than this. Each $n \times n$ matrix $A$ over $F$ is similar (over $F$ ) to one and only one matrix which is in rational form (Chapter 7). In other words, each equivalence class for the relation of similarity contains precisely one matrix which is in rational form. A matrix in rational form is determined by a $k$-tuple $\left(p_{1}, \ldots, p_{k}\right)$ of monic polynomials having the property that $p_{j+1}$ divides $p_{j}, j=1, \ldots, k-1$. Thus, we have a function $f$ which associates with each $n \times n$ matrix $A$ a $k$-tuple $f(A)=\left(p_{\mathbf{1}}, \ldots, p_{k}\right)$ satisfying the divisibility condition $p_{j+1}$ divides $p_{j}$. And, $A$ and $B$ are similar if and only if $f(A)=f(B)$.

(3) Here is a special case of Example 2 above. Let $X$ be the set of $3 \times 3$ matrices over a field $F$. We consider the relation of similarity on $X$. If $A$ and $B$ are $3 \times 3$ matrices over $F$, then $A$ and $B$ are similar if and only if they have the same characteristic polynomial and the same minimal polynomial. Attached to each $3 \times 3$ matrix $A$, we have a pair $(f, p)$ of monic polynomials satisfying

(a) $\operatorname{deg} f=3$,

(b) $p$ divides $f$

$f$ being the characteristic polynomial for $A$, and $p$ the minimal polynomial for $A$. Given monic polynomials $f$ and $p$ over $F$ which satisfy (a) and (b), it is easy to exhibit a $3 \times 3$ matrix over $F$, having $f$ and $p$ as its charac- teristic and minimal polynomials, respectively. What all this tells us is the following. If we consider the relation of similarity on the set of $3 \times 3$ matrices over $F$, the equivalence classes are in one-one correspondence with ordered pairs $(f, p)$ of monic polynomials over $F$ which satisfy (a) and (b).

\section{A.6. The Axiom of Choice}

Loosely speaking, the Axiom of Choice is a rule (or principle) of thinking which says that, given a family of non-empty sets, we can choose one element out of each set. To be more precise, suppose that we have an index set $A$ and for each $\alpha$ in $A$ we have an associated set $S_{\alpha}$, which is non-empty. To 'choose' one member of each $S_{\alpha}$ means to give a rule $f$ which associates with each $\alpha$ some element $f(\alpha)$ in the set $S_{\alpha}$. The axiom of choice says that this is possible, i.e., given the family of sets $\left\{S_{\alpha}\right\}$, there exists a function $f$ from $A$ into

$$
\bigcup_{\alpha} S_{\alpha}
$$

such that $f(\alpha)$ is in $S_{\alpha}$ for each $\alpha$. This principle is accepted by most mathematicians, although many situations arise in which it is far from clear how any explicit function $f$ can be found.

The Axiom of Choice has some startling consequences. Most of them have little or no bearing on the subject matter of this book; however, one consequence is worth mentioning: Every vector space has a basis. For example, the field of real numbers has a basis, as a vector space over the field of rational numbers. In other words, there is a subset $S$ of $R$ which is linearly independent over the field of rationals and has the property that each real number is a rational linear combination of some finite number of elements of $S$. We shall not stop to derive this vector space result from the Axiom of Choice. For a proof, we refer the reader to the book by Kelley in the bibliography. 

\section{Bibliography}

Halmos, P., Finite-Dimensional Vector Spaces, D. Van Nostrand Co., Princeton, 1958.

Jacobson, N., Lectures in Abstract Algebra, II, D. Van Nostrand Co., Princeton, 1953.

Kelley, John L., General Topology, D. Van Nostrand Co., Princeton, 1955.

MacLane, S. and Birkhoff, G., Algebra, The Marmillan Co., New York, 1967.

Schreier, O. and Sperner, E., Introduction to Modern Algebra and Matrix Theory, 2nd Ed., Chelsea Publishing Co., New York., 1955.

* van der Waerden, B. L., Modern Algebra (two volumes), Rev. Ed., Frederick Ungar Publishing Co., New York, 1969. 

\section{$\mathbf{A}$}

Adjoint:

classical, 148, 159

of transformation, 295

Admissible subspace, 232

Algebra, 117

of formal power series, 119

self-adjoint, 345

Algebraically closed field, 138

Alternating $n$-linear function, 144, 169

Annihilator:

of subset, 101

of sum and intersection, 106(Ex. 11)

of vector ( $T$-annihilator), 201, 202, 228

Approximation, 283

Associativity, 1 of matrix multiplication, 19,90

of vector addition, 28

Augmented matrix, 14

Axiom of choice, 400

\section{$\mathbf{B}$}

Basis, 41

change of, 92

dual, 99, 165

for module, 164

ordered, 50

orthonormal, 281

standard basis of $F^{\prime n}, 41$ Bessel's inequality, 287

Bilinear form, 166, 320, 359

diagonalization of, 370

group preserving, 379

matrix of, 362

non-degenerate (non-singular), 365

positive definite, 368

rank of, 365

signature of, 372

skew-symmetric, 375

symmetric, 367

\section{C.}

Cauchy-Schwarz inequality, 278

Cayley-Hamilton theorem, 194, 237

Cayley transform, 309(Ex. 7)

Characteristic:

of a field, 3

polynomial, 183

space, 182

value, 182,183

vector, 182

Classical adjoint, 148, 159

Coefficients of polynomial, 120

Cofactor, 158

Column:

equivalence, 256

operations, 26, 256

rank, 72,114 Commutative:

algebra, 117

group, 83

ring, 140

Companion matrix, 230

Complementary subspace, 231 orthogonal, 286

Composition, 390

Conductor, 201, 202, 232

Congruence, 139, 393, 396

Conjugate, 271

transpose, 272

Conjugation, 276(Ex. 13)

Coordinates, 50 coordinate matrix, 51

Coset, 177, 396

Cramer's rule, 161

Cyclic:

decomposition theorem, 233

subspace, 227

vector, 227

\section{D}

Degree:

of multilinear form, 166

of polynomial, 119

Dependence, linear, 40, 47

Derivative of polynomial, 129, 266

Determinant function, 144 existence of, 147

for linear transformations, 172 uniqueness of 152

Determinant rank, 163(Ex. 9)

Diagonalizable: operator, 185

part of linear operator, 222

simultaneously, 207

Diagonalization, 204, 207, 216

of Hermitian form, 323

of normal matrix (operator), 317

of self-adjoint matrix (operator), 314

of symmetric bilinear form, 370 unitary, 317

Differential equations, 223(Ex. 14), 249(Ex. 8)

Dimension, 44

formula, 46

Direct sum, 210

invariant, 214

of matrices, 214

of operators, 214 Disjoint subspaces (see Independent: subspaces)

Distance, 289(Ex. 4)

Division with remainder, 128

Dual:

basis, 99,165

module, 165

space, 98

\section{$\mathbf{E}$}

Eigenvalue (see Characteristic: value)

Elementary:

column operation, 26, 256

Jordan matrix, 245

matrix, 20, 253

row operation, 6,252

Empty set, 388

Entries; of a matrix, 6

Equiv 8.lence relation, 393

Equivquent systems of equations, 4

Euclidean space, 277

Exterior (wedge) product, 175, 177

$\mathbf{F}$

$F^{m \times n}, 29$

$F^{n}, 29$

Factorization of polynomial, 136

Factors, invariant, 239, 261

Field, 2

algebraically closed, 138

subfield, 2

Finite-dimensional, 41

Finitely generated module, 165

Form:

alter nating, 169

bilinear, 166, 320, 359

Hermitian, 323

matrix of, 322

multilinear, 166

non-degenerate, 324(Ex. 6)

non-negative, 325

normal, 257, 261

positive, 325,328

quadratic, 273, 368

r-linear, 166

raticnal, 238

sesq'ui-linear, 320

Formal power series, 119

Free module, 164 Function, 389

determinant, 144

identity, 390

inverse of, 391

invertible, 390

linear, 67, 97, 291

multilinear, 166

$n$-linear, 142

polynomial function, 30

range of, 389

restriction of, 391

Fiundamental theorem of algebra, 138

\section{$\mathbf{G}$}

Gram-Schmidt process, 280, 287

Grassman ring, 180

Greatest common divisor, 133

Group, 82

commutative, 83

general linear, 307

Lorentz, 382

orthogonal, 380

preserving a form, 379

pseudo-orthogonal, 381

symmetric, 153

\section{$\mathbf{H}$}

Hermitian (see Self-adjoint)

Hermitian form, 323

Homogeneous system of linear equations, 4

Hyperspace, 101, 109

\section{I}

Ideal, 131

principal ideal, 131

I dempotent transformation (see Projection)

Identity:

element, 117,140

function, 390

matrix, 9

resolution of, 337, 344

Independence, linear, 40, 47

Independent:

linearly, 40, 47

subspaces, 209

Inner product, 271

matrix of, 274 Inner product (cont.):

quadratic form of, 273

space, 277

standard, 271, 272

Integers, 2

positive, 2

Interpolation, 124

Intersection, 388

of subspaces, 36

Invariant:

direct sum, 214

factors of a matrix, 239, 261

subset, 392

subspace, 199, 206, 314

Inverse:

of function, 391

left, 22

of matrix, 22, 160

right, 22

two-sided, 22

Invertible:

function, 390

linear transformation, 79

matrix, 22, 160

Irreducible polynomial, 135

Isomorphism:

of inner product spaces, 299

of vector spaces, 84

\section{$\mathbf{J}$}

Jordan form of matrix, 247

\section{$\mathbf{K}$}

Kronecker delta, 9

\section{$\mathbf{L}$}

Lagrange interpolation formula, 124

Laplace expansions, 179

Left inverse, 22

Linear algebra, 117

Linear combination:

of equations, 4

of vectors, 31

Linear equations (see System of linear equations)

Linear functional, 97

Linearly dependent (independent), 40, 47 404 Index

Linear transformation (operator), 67, 76 adjoint of, 295 cyclic decomposition of, 233 determinant of, 172 diagonalizable, 185 diagonalizable part of, 222 invertible, 79 matrix in orthonormal basis, 293 matrix of, 87,88 minimal polynomial of, 191 nilpotent, 222 non-negative, 329,341 non-singular, 79 normal, 312 nullity of, 71 orthogonal, 303 polar decomposition of, 343 positive, 329

product of, 76 quotient, 397 range of, 71 rank of, 71 self-adjoint, 298, 314 semi-simple, 263 trace of, 106(Ex. 15) transpose of, 112 triangulable, 202 unitary, 302

Lorentz: group, 382 transformation, 311(Ex. 15), 382

$\mathbf{M}$

Matrix, 6 augmented, 14 of bilinear form, 362 classical adjoint of, 148, 159 coefficient, 6 cofactors, 158 companion, 230 conjugate transpose, 272 coordinate, 51 elementary, 20, 253 elementary, Jordan, 245 of form, 322 identity, 9 of inner product, 274 invariant factors of, 239, 261 inverse of, 22, 160 invertible, 22, 160 Jordan form of, 247. Matrix (cont.) : of linear transformation, 87,88 minimal polynomial of, 191 nilpotent, 244 normal, 315 orthogonal, 162(Ex. 4), 380 positive, 329 principal minors of, 326 product, 17,90 rank of, 114 rational form of, 238 row rank of, $56,72,114$ row-reduced, 9 row-reduced echelon, 11,56 self-adjoint (Hermitian), 35, 314 similarity of, 94 skew-symmetric, 162(Ex. 3), 210 symmetric, 35, 210 trace of, 98 transpose of, 114 triangular, 155(Ex. 7) unitary, 163(Ex. 5), 303 upper-triangular, 27 Vandermonde, 125 zero, 12

Minimal polynomial, 191

Module, 164 basis for, 164 dual, 165 finitely generated, 165 free, 164 rank of, 165

Monic polynomial, 120

Multilinear function (form), 166 degree of, 166

Multiplicity, 130

\section{$\mathbf{N}$}

$n$-linear function, 142 alternating, 144, 169 $n$-tuple, 29

Nilpotent: matrix, 244 operator, 222

Non-degenerate: bilinear form, 365 form, 324(Ex. 6)

Non-negative: form, 325 operator, 329,341 Non-singular:

form (see Non-degenerate)

linear transformation, 79

Norm, 273

Normal:

form, 257, 261

matrix, 315

operator, 312

Nullity of linear transformation, 71

Null space, 71

Numbers:

complex, 2

rational, 3

real, 2

\section{0}

Onto, 389

Operator, linear, 76

Ordered basis, 50

Orthogonal:

complement, 285

equivalence of matrices, 308

group, 380

linear transformation, 304

matrix, 162(Ex. 4), 380

projection, 285

set, 278

vectors, 278,368

Orthogonalization, 280

Orthonormal:

basis, 281

set, 278

\section{$\mathbf{P}$}

Parallelogram law, 276(Ex. 9)

Permutation, 151

even, odd, 152

product of, 153

sign of, 152

Polar decomposition, 343

Polarization identities, 274, 368

Polynomial, 119

characteristic, 183

coefficients of, 120

degree of, 119

derivative of, 129,266

function, 30

irreducible (prime), 135

minimal, 191 Polynomial (cont.):

monic, 120

primary decomposition of, 137

prime (irreducible), 135

prime factorization of, 136

reducible, 135

root of, 129

scalar, 120

zero of, 129

Positive:

form, 325, 328

integers, 2

matrix, 329

operator, 329

Positive definite, 368

Power series, 119

Primary components, 351

Primary decomposition:

of polynomial, 137

theorem, 220

Prime:

factorization of polynomial, 136

polynomial, 135

Principal:

access theorem, 323

ideal, 131

minors, 326

Product:

exterior (wedge), 175, 177

of linear transformations, 76

of matrices, 14,90

of permutations, 153

tensor, 168

Projection, 211

Proper subset, 388

Pseudo-orthogonal group, 381

\section{$Q$}

Quadratic form, 273, 368

Quotient:

space, 397

transformation, 397

$\mathbf{R}$

Range, 71

Rank:

of bilinear form, 365

column, 72, 114

determinant, 163(Ex. 9) 

\section{Index}

Rank (cont.):

of linear transformation, 71

of matrix, 114

of module, 165

row, 56, 72, 114

Rational form of matrix, 238

Reducible polynomial, 135

Relation, 393

equivalence, 393

Relatively prime, 133

Resolution:

of the identity, 337, 344

spectral, 336, 344

Restriction:

of function, 391

operator, 199

Right inverse, 22

Rigid motion, 310(Ex. 14)

Ring, 140

Grassman, 180

Root:

of family of operators, 343

of polynomial, 129

Rotation, 54, 309(Ex. 4)

Row:

operations, 6,252

rank, 56, 72, 114

space, 39

vectors, 38

Row-equivalence, 7, 58, 253 summary of, 55

Row-reduced matrix, 9

row-reduced echelon matrix, 11, 56

$\mathbf{S}$

Scalar, 2

polynomial, 120

Self-adjoint:

algebra, 345

matrix, :35, 314

operator, 298, 314

Semi-simple operator, 263

Separating vector, 243(Ex. 14)

Sequence of vectors, 47

Sesqui-linear form, 320

Set, 388

element of (member of), 388

empty, 388

Shuffle, 171

Signature, 372

Sign of permutation, 152 Similar matrices, 94

Simultaneous:

diagonalization, 207

triangulation, 207

Skew-symmetric:

bilinear form, 375

matrix, 162(Ex. 3), 210

Solution space, 36

Spectral:

resolution, 336, 344

theorem, $33.5$

Spectrum, 336

Square root, 341

Standard basis of $F^{n}, 41$

Stuffer (das einstopfende Ideal), 201

Subfield, 2

Submatrix, 163(Ex.9)

Subset, 388

invariant, 392

proper, 388

Subspace, 34

annihilator of, 101

complementary, 231

cyclic, 227

independent subspaces, 209

invariant, 199, 206, 314

orthogonal complement of, 285

quotient by, 397

spanned by, 36

sum of subspaces, 37

T-admissible, 232

zero, 35

Sum:

direct, 210

of subspaces, 37

Symmetric:

bilinear form, 367

Igroup, 153

matrix, 3.5, 210

System of linear equations, 3

homogeneous, 4

\section{$\mathbf{T}$}

$T$-admissible subspace, 232

T-annihilator, 201, 202, 228

T-conductor, 201, 202, 232

Taylor's formula, 129, 266

Terisor, 166

product, 168

Trace:

of linear transformation, 106(Ex. 15)

of matrix, 98 Transformation:

differentiation, 67

linear, 67,76

zero, 67

Transpose:

conjugate, 272

of linear transformation, 112

of matrix, 114

Triangulable linear transformation, 202, 316

Triangular matrix, 155(Ex. 7)

Triangulation, 203, 207, 334

\section{$\mathbf{U}$}

Union, 388

Unitary:

diagonalization, 317

equivalence of linear transformations, 356

equivalence of matrices, 308

matrix, 163(Ex. 5), 303

operator, 302

space, 277

transformation, 356 Upper-triangular matrix, 27

\section{$\mathbf{V}$}

Vandermonde matrix, 125

Vector space, 28

basis of, 41

dimension of, 44

finite dimensional, 41

isomorphism of, 84

of $n$-tuples, 29

of polynomial functions, 30

quotient of, 397

of solutions to linear equations, 36

subspace of, 34

\section{$\mathbf{W}$}

Wedge (exterior) product, 175,177

$\mathbf{Z}$

Zero:

matrix, 12

of polynomial, 129
\end{document}